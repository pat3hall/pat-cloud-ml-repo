------------------------------------------------------

A Cloud Guru:
AWS Certified Machine Learning - Specialty (MLS-C01): Exploratory Data Analysis

------------------------------------------------------
Chapter 1 Introduction
------------------------------------------------------
1.1 Course Overview

  Course Outline
    - Santize and Prepare Data for Modeling Purposes
    - Perform Feature Engineering
    - Analyze and visualize data for ML
 
   By end of this course:
     - able to apply data processing and feature engineering techniques, and feed data into a ML algorithm

   Prerequisties
     - Basic Python Skills
     - Basic Knowledge of Using Jupyter Notebooks
     - Basic Knowledge of Machine Learning Algorithms

       
------------------------------------------------------

Chapter 2 Santize and Prepare Data for Modeling

------------------------------------------------------
2.1 Exploratory Data Analysis Framework 

  Machine Learning Life Cycle

     Data Collection --> Data Analysis --> Data Processing --> 

          --> Build Model --> Train Model --> Test model --> Deploy Model --> Monitor Model
                                          <--

    - In this course, we will focus primarily on data analysis and data processing phases

    Data analysis:
      - In the field of data science, we call this descriptive analysis
      Descriptive analysis:
        - understand the patterns and structure, and find relationship between the attributes.
        - a subset of statistical analysis.

    Data Processing
      - The data is then processed to detect the outliers, fill in the missing values, and address any 
        data format inconsistencies so that we can derive meaningful information from it

     Build Model
       - Once the data is processed, we need to build the model by selecting the right machine learning algorithm 
         based on the data characteristics.  

     Train Model
       - Then the algorithm is trained with the processed data, and a model is developed.
       - This model captures the underlying relationships that exist in the data.  
       - Usually the data will be split into training data and test data in an 80/20 ratio.
       - Training data is then used to train the model.

     Test Model
       - This model can then be used against the test data to check how well it scores against new data.
       - This is an iterative process to fine tune the accuracy of the model.

     Deploy
       - Once the desired accuracy is achieved, this model can then be deployed.
       - After deployment, your model is now ready to make real life predictions and it is monitored continuously.

  Data Analysis Phase

    Descriptive Statistics
      - use descriptive statistics to gain valuable insights into our data.
      - Too many features present in the data can lead to a high dimensionality and a complex model.
      - Too few instances may lead to inadequate data and produce this model with low accuracy.
      univariate analysis
      - When an analysis is performed on a single feature or an attribute, it's called an univariate analysis,
      - there are three different types:
        Measures of Frequency
          - The first one is a measures of frequency that is used to summarize the values in a dataset.
          Frequency:
            - number of occurrences of a specific value in a dataset.
            - For example, considering the sample set of data showing the student scores in a class, we can group 
              all the students with scores 95 and above as grade A, 85 to 95 as grade B, and less than 85 as grade C.
          Mode
            - the value that occurs the most in a dataset.
            multimodel dataset
              - If a dataset has more than single value that occurs frequently, it is called a multimodal dataset
        Measures of Central Tendency
          - the concept revolves around finding a central data that can best summarize the entire data set.
          Mean
            - average of all the values in the dataset
            - a mean value may get skewed in the presence of an outlier, 
            Outlier 
              - data science term for one-off data
          Median
            - the value in the  middle of a sorted dataset
        Measures of variability
          - shows the spread or dispersion of values in a dataset.  
          - Some of the commonly used metrics include the range
          Range
            - the difference between the maximum and the minimum value in the dataset.
          Standard Deviation
            - Average distance of each value from the mean of a dataset

            std deviation =  ( (1/n) SUM (x_i - x^)**2 ) ** 1/2    Where SUM is from i=1 to i=n  

                X_i  = individual observations
                x^ = mean value

          Variance
            - average squared deviation of each value from the mean

            variance  =  (1/n) SUM (x_i - x^)**2     Where SUM is from i=1 to i=n  

                X_i  = individual observations
                x^ = mean value

          IQR (InterQuartile Range)
            - difference between the third quartile and the first quartile
            - It is used to represent the spread of the middle 50%, which is a great measure to use if you 
              suspect your data may contain outliers on both the lower and the higher sides.

            IQR = Q_3 - Q_1

             
    Multivariate Analysis
      - multivariate analysis is performed when you want to measure the relationship between two attributes or features.
      - example: the price of a real estate property based on its size or 
      - example: the purchase history in an E-commerce website based on the demographics.
      Scatterplot
        - A scatterplot is an excellent visualization tool to study the relationship between two attributes,
        - a linear relationship between two attributes will give us a clue of the strong relationship between them.
       Correlation matrix 
         - often used to show the relationship between two variables, 
         - the values typically range from minus one to positive one,
            - a negative one indicates a perfect negative linear relationship
            - positive one indicates a perfect positive linear relationship
        Confusion matrix,
          - used in evaluating the performance of a classification model.

   Data Processing Phase
     - It is also referred to as a data preparation phase.
     - the data scientists spend most of their time in cleaning on processing the data so that they can 
       effectively answer the business question and develop a model that predicts reliably and accurately.


  Framework
                        --> Preparing Instances 
     Data Preparation
                        --> Preparing Features 


      Preparing Instances includes:
        - removing duplicates
        - Handling outliers
        - Imputing missing values
        - resampling imbalanced data

      Preparing Instances includes:
        - Feature selection
        - encoding techniques
        - normnalizing and scaling techniques
        - binning and transforming techniques

------------------------------------------------------
2.2 Imputing Missing Data

  Resources (I found):
    Medium: Synthetic Minority Over-sampling TEchnique (SMOTE)
      https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c

    video resources for Machine Learning Algorithms, Sagemaker, etc, by Prof Ryan Ahmed 
       K Nearest Neighbors Algorithm (KNN)
          https://www.youtube.com/watch?v=v5CcxPiYSlA

       Overview of AWS SageMaker Built-in Algorithms
         https://www.youtube.com/watch?v=79y4WtA-zqA


  Resources:

    Github Link-  impute missing data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/impute%20missing%20data

    Impute Missing data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\impute-missing-data\Missing Values.ipynb

  Missing Data - Root Causes
    - User unwilling to share requested data during data collection phase
    - corrupted data
    - data format discrepancies
    - data transformation or filtering errors

  Categories of Missing Data
    - Understanding the missingness mechanism is crucial for choosing the right methods to handle missing data during 
      data analysis.
    - For example, we may use imputation techniques, like mean, median, or KNN imputation for MCAR data.
    - Techniques like MICE are commonly used to address MAR data.
    - Addressing MNAR is challenging and techniques like selection models and shared parameter models can be used.

    MCAR (Missing Completely At Random)
      - missing data is unrelated to both observed and unobserved data
      - analogy: imagine a set of students who randomly missed school due to illness or transportation delays 
        or any other random one-off reasons.
    MAR (Missing At Random)
      - missing data is dependent on the observed data
      - analogy: if there is a student absence data sheet where there is a direct correlation between a student 
        missing the school and him being sick or participating in any extracurricular activities.
    MNAR (Missing Not At Random)
      - missing data is related to both observed and unobserved data
      - analogy: this is like a student intentionally missing school due to academic challenges or other 
        personal issues that is not recorded.


  Impute Missing Data Demo
    Employee_missing.csv
      - This dataset has a few missing age values, which is a numerical feature, and department, which is a categorical feature.

    Missingno python library
      - provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you 
         to get a quick visual summary of the completeness (or lack thereof) of your dataset. 
      - to install:  !pip install missingno
      - to import:   import missingno as mn    # or 'as msno'

       matrix
         - msno.matrix nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion.
         - example
              import missingno as msno
              %matplotlib inline
              msno.matrix(collisions.sample(250))

    Missing Values:
      - missing values: department: 4 missing values, age: 3 missing values, and salary: 2 mssing values 
      - If you have a large data set and you're missing one or two values, it's not a bad idea to just drop 
        the row containing the missing values and it is a valid business strategy.
      - But in demo case, we have a total of 100 records, so we don't have the luxury of dropping the records.
      - You can also use more sophisticated methods of imputation by constructing an ML model to predict the 
        value of your missing data (e.g. sklearn.impute.KNNImputer)
      Mean
        - used mean to impute missing age values
        - using mean is not recommended if the dataset has outliers.  For those cases, we can use median

       
    pandas.DataFrame.mode(axis=0, numeric_only=False, dropna=True) 
      - Get the mode(s) of each element along the selected axis.
      - The mode of a set of values is the value that appears most often. It can be multiple values, if each 
        value appear the same number of time.
    
     sklearn.impute.KNNImputer(missing_values=nan, n_neighbors=5, ...)
       - Imputation for completing missing values using k-Nearest Neighbors.
       - Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found 
         in the training set. Two samples are close if the features that neither is missing are close.
       Parameters
         missing_values:
           - The placeholder for the missing values. All occurrences of missing_values will be imputed.
         n_neighborsint:
           - Number of neighboring samples to use for imputation.
         
     code: Using mean value to impute missing age values and change age from float to int:

        >>> df = pd.read_csv('Employee_missing.csv')
        >>> df['age'] = df['age'].fillna(df['age'].mean())
        >>> df['age'] = df['age'].astype(int)


     code: Using missingno.matrix(datafram) to visualizally show missing values (in white)

        >>> df = pd.read_csv('Employee_missing.csv')

        >>> import missingno as mn
        >>> mn.matrix(df)

        >>> df.isnull().sum()
            employee_id    0
            first_name     0
            last_name      0
            age            3
            gender         0
            department     4
            salary         2
            dtype: int64

     code: Using pandas.DataFrame.mode() to impute department missing values 

        >>> df['department'].mode()   # Returns value(s) with most occurance(s), and HR, Marketing, & Sales all occurred 17 times 
            0           HR
            1    Marketing
            2        Sales
            Name: department, dtype: object

            # replace missing department values '.mode()[0]'  -> HR
        >>> df['department'] = df['department'].fillna(df['department'].mode()[0])

        >>> df.isnull().sum()
            employee_id    0
            first_name     0
            last_name      0
            age            3
            gender         0
            department     0
            salary         2
            dtype: int64

     code: KNNImputer to impute missing age values

        >>> from sklearn.impute import KNNImputer
        >>> impute = KNNImputer(n_neighbors=2)

        >>> df = pd.read_csv('Employee_missing.csv')
        >>> df.head()

        >>> column = df['age']
        >>> df_imputed = impute.fit_transform(column.values.reshape(-1,1))
        >>> df_imputed = df_imputed.astype('int')

        >>> df_imp = df.copy()
        >>> df_imp['age'] = df_imputed.flatten()
        >>> df_imp.head()
    

  Multivariate Imputation By Chained Equation (MICE)
    - a statistical method of imputing missing data
    - runs multiple imputation threads parallelly
    - combines the results to produce the final dataset
    - The reasoning behind running multiple threads is to maintains the relationship between the variables 
      in the original data and reduces the bias introduced by the imputed values.
    - effective for datasets with large amounts of missing data
    - computationally intensive because of the mutliple parallel runs
    - Overall MICE has been proven as a valuable tool for addressing missing data that is more reliable and accurate

------------------------------------------------------
2.3 Preparing Text Data

  Resources:

    Github Link -  prepare text data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/prepare%20text%20data

    Prepare Text Data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\Prepare-text-data\Prepare text data.ipynb


  Source of Text Data include:
    - social media posts
    - blogs
    - new articles

  Natural language processing (NLP)
    - is a field of AI that is used to pre-process text data.
    - text data processing challenges
      Understanding context-specific words
       - The first and the most important one is understanding the context-specific words.
       - includes:
          - presence of irony and sarcasm
          - slang and colloquialisim
          - misspelled and misprounced errors

      Stop Words
        - Stop words are the most commonly used words; includes
            - Articles like a, the, and an.
            - Prepositions like for, by, at, in, and on.
            - Pronouns like they, this, their, and many more.
        - These are frequently occurring words in our language, but carry little meaning.
        - removing these words will have very little negative consequences in the model we want to build

        Advantages of Removing Stop Words
          - reduce the training time
            - significantly reduces the data set size which will have a direct impact on the training time.
          - increases performance
            - significantly increases the performance as the algorithm needs to consider fewer tokens,
              which helps improving the accuracy.
        Caution on Removing Stop Words
         - If we don't pay attention, we might potentially remove a stop word that forms the core meaning.
         - example: removing 'not' in 'I am not happy with the product' review

      NLP Libraries for removing Stop Words
        NLTK (Natural Language ToolKit) Library
          - leading platform for building Python programs to work with human language data.
          - It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along 
            with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, 
            & semantic reasoning, and wrappers for industrial-strength NLP libraries
        SpaCy 
          - open source library used for advanced NLP
          - spaCy is a library for advanced Natural Language Processing in Python and Cython
          - It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, 
            text classification and more, multi-task learning with pretrained transformers like BERT
          

  nltk.tokenize.puntk module:
    - The NLTK data package includes a pre-trained Punkt tokenizer for English.
    Punkt Sentence Tokenizer:
      - This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a 
        model for abbreviation words, collocations, and words that start sentences.
        Punkt Sentence Tokenizer:
      - It must be trained on a large collection of plaintext in the target language before it can be used.
      Collocations:
        - phrases or expressions containing multiple words, that are highly likely to co-occur. 
        - For example — 'social media', 'school holiday', 'machine learning


  Prepare Text Data Demo

  Code: Use NLTK pre-trained Punkt tokenizer with NLTL default stop words list to tokenize 
        a sentence and remove stop words

            #  import NLTK the stop words package
        >>> import nltk
        >>> from nltk.corpus import stopwords

            # download the stop words package from the NLTK library.
            # This package contains the default list of stop words.
        >>> nltk.download('stopwords')

            # download punkt, which is a tokenizer that splits the text into a list of sentences.
        >>> nltk.download('punkt')

            # import word_tokenize that accepts a sentence as input and breaks it down into a list of words 
            #  using white space or punctuation as a delimiter.
        >>> from nltk.tokenize import word_tokenize

        >>> text = "I am excited to take machine learning certification"
        >>> tkns = word_tokenize(text)
        >>> tkns
            ['I', 'am', 'excited', 'to', 'take', 'machine', 'learning', 'certification']

        >>> tkns_sw = [word for word in tkns if not word in stopwords.words('english')]
        >>> tkns_sw
            ['excited', 'take', 'machine', 'learning', 'certification']

            # customize tokenizer to include 'I' as stopword
        >>> default = stopwords.words('english')
        >>> default.append('I')
        >>> tkns_sw_custom = [word for word in tkns if not word in default]
        >>> tkns_sw_custom
            ['excited', 'take', 'machine', 'learning', 'certification']



  Customizing Stop words
    - It is recommended to customize the stop words list to meet your business needs

------------------------------------------------------
2.4 Resampling Imbalanced Data

  Resources:

    Github Link -  resampling imbalanced data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/resampling%20imbalanced%20data

    Resampling Imbalanced Data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\resampling-imbalanced-data\imbalanced.ipynb


  Imbalanced Dataset
    - uneven distribution of the data where one of the categorical feature values is significantly more compared 
      to the other is an imbalanced dataset.
    Examples :
      - if credit card transactions are legal (non-fraud) or fraud.
          - Your value or the category, legal, represents the majority class.  
          - And the category fraud represents a minority class.
       - Diagnosing a patient's blood smaple for infectious disease
       - identifying if an incoming email is spam or not spam

  Imbalanced Dataset Challenges
    Biased Model
      - machine learning model will be highly biased towards the majority class, leading to poor prediction 
        of minority classes
    Misleading Metrics
      - Your model may have high accuracy metrics for predicting the majority class, but fail miserably while 
        predicting the minority class.
    Increased model complexity,
      - need to increase the dataset size artificially, which may lead to the model consuming more computation 
        resources during the training phase.

  Handling Imbalanced Dataset Options
    Increase the weight of minority class
      - some algorithms provide this feature, but this is a hyperparameter setting and must be tuned properly, 
      - not many algorithms provide us with this hyperparameter option.
    Increase the size of the dataset
      - to increase the dataset size by under sampling or over sampling.
      undersampling the majority class
         - excluding rows of the majority class so that there are similar number of rows with the minority classes
         - The challenge with this approach is that we'll be losing a lot of data.
      oversampling the minority class
        - increase the number of instances containing the minority class, by randomly duplicating them so 
          that it matches with the majority class.
        - The challenge with this approach is that it may lead to overfitting, because the model is learning 
          from the same set of examples.
      SOMTE (Synthetic Minority Oversampling TEchnique)
        - a common approach used in the industry to address imbalanced dataset
        - generating synthetic samples for the minority classes, to balance the class distribution.

 SOMTE (Synthetic Minority Oversampling TEchnique)
    Algorithm
      - algorithm can be described as follows: 
        - Take difference between a sample and its nearest neighbour
        - Multiply the difference by a random number between 0 and 1
        - Add this difference to the sample to generate a new synthetic example in feature space
        - Continue on with next nearest neighbour up to user-defined number
        oversampling calculation
          - This process is repeated with other nearest neighbors based on the over sampling needed.
          - For example, if 200% more sampling is needed, this process is repeated with 200 divided by 100,
             - that is two neighbors. So, each row will give two synthetic data points for us.
             - This process is then repeated with the next row, and its two nearest neighbors.


      - the feature containing the minority class depends on other features in your dataset.
      - example: the time of transaction and the transaction amount, may give us a clue if the 
        credit card transaction is fraudulent or not

  Employee Dataset
    - has new column 'expired pto' - whether employee has used all their PTO days or Not (y/n)
      - most employees still have PTO


  sklearn.utils.resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)
    - Resample arrays or sparse matrices in a consistent way.
    - The default strategy implements one step of the bootstrapping procedure.
    parameters:
      replace: bool, default=True
        - Implements resampling with replacement. If False, this will implement (sliced) random permutations.
      n_samples: int, default=None
        - Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. 
        - If replace is False it should not be larger than the length of arrays.
      random_state: int, RandomState instance or None, default=None
        - Determines random number generation for shuffling the data. 
        - Pass an int for reproducible results across multiple function calls. 

     Returns:
        Sequence of resampled copies of the collections. The original arrays are not impacted.
       

   sklean SMOTE:
     imblearn.over_sampling.SMOTE(*, sampling_strategy='auto', random_state=None, k_neighbors=5, n_jobs=None)
       - Class to perform over-sampling using SMOTE.
       - This object is an implementation of SMOTE - Synthetic Minority Over-sampling Technique 
       Parameters:
         sampling_strategy: float, str, dict or callable, default=’auto’
           - Sampling information to resample the data set.
           - When str, specify the class targeted by the resampling. The number of samples in the different 
             classes will be equalized. Possible choices are:
                'minority': resample only the minority class;
                'not minority': resample all classes but the minority class;
                'not majority': resample all classes but the majority class;
                'all': resample all classes;
                'auto': equivalent to 'not majority'.

           random_state: int, RandomState instance, default=None
             - Control the randomization of the algorithm.
           k_neighbors: int or object, default=5
             - The nearest neighbors used to define the neighborhood of samples to use to generate the synthetic samples. 
               You can pass:
                 - an int corresponding to the number of neighbors to use. A ~sklearn.neighbors.NearestNeighbors instance 
                   will be fitted in this case.
                 - an instance of a compatible nearest neighbors algorithm that should implement both methods 
                   'kneighbors' and 'kneighbors_graph'

  Code: Handling imbalance dataset using a) resampling (oversampling minority) class and b) SMOTE approaches 

        >>> import pandas as pd
        >>> import seaborn as sb

        >>> df = pd.read_csv('Employee_imbalanced.csv')
        >>> df.head()

            # display <Axes: xlabel='count', ylabel='expired_pto'>
        >>> sb.countplot(df['expired_pto'])

        >>> pd.Series(df['expired_pto']).value_counts()
            expired_pto
            n    95
            y     5
            Name: count, dtype: int64

            # Use sklearn resample to implement a oversampling of the minority class strategy
            #  randomly resamples original 'expired_pto' 5 instances and replaces with 99 instances
        >>> from sklearn.utils import resample
        >>> unexpired_pto = df[(df['expired_pto']=='n')]
        >>> expired_pto = df[(df['expired_pto']=='y')]
        >>> oversample = resample(expired_pto, replace=True, n_samples=99, random_state=40)
        >>> df_oversample = pd.concat([oversample, unexpired_pto])

        >>> df_oversample.info()

            # display <Axes: xlabel='count', ylabel='expired_pto'> for df_oversampled
        >>> sb.countplot(df_oversample['expired_pto'])

          
        >>> pd.Series(df_oversample['expired_pto']).value_counts()
            expired_pto
            n    99
            y    95
            Name: count, dtype: int64

            # install Sklean imblearn library
        >>> pip install imblearn

            # Use Sklearn Imblearn SMOTE to generate synthetic samples of the minoirty class
            #  based on only 'age' and 'salary'
        >>> from imblearn.over_sampling import SMOTE
        >>> df2 = pd.read_csv("Employee_imbalanced.csv")
        >>> smote = SMOTE(sampling_strategy='minority', random_state=42, k_neighbors=3)
        >>> X = df2[['age', 'salary']]
        >>> y = df2['expired_pto']
        >>> X

            # generate synthetic minority samples  (added 90 synthetic samples)
        >>> X_sm, y_sm = smote.fit_resample(X, y)
        >>> y_sm.count()
            190

        >>> oversampled = pd.concat([pd.DataFrame(y_sm), pd.DataFrame(X_sm)], axis=1)
        >>> oversampled.head()

        >>> pd.Series(oversampled['expired_pto']).value_counts()
            expired_pto
            n    95
            y    95
            Name: count, dtype: int64

  SMOTE:
    - avoids overfitting by oversampling the minority class without duplicating the data


------------------------------------------------------
2.5 Handling Outliers in Data

  Resources:

    Github Link -  handling outliers
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/handling%20outliers

    Handling Outliers Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\handling-outliers\Handle outliers.ipynb


  Outlier:
    - An outlier is an observation or a data point that differs significantly in value from other data points.  
    - In short, it is a one off data point or an extreme value in the dataset.

  Detecting Outliers: Z-score
    - the distance of an observation from the median in terms of standard deviation

      z-score = x  - mean / std_deviation   where x is a datapoint

      z-score outliers:
            -3  < datapoint > +3

  Detecting Outliers: Boxplot (also called wisker plot)

    - The box extends from the first quartile (Q1) to the third quartile (Q3) of the data, with a 
      line at the median. 
    - The whiskers extend from the box to the farthest data point lying within 1.5x the inter-quartile range (IQR) 
      from the box. 
        Interquartile range (IQR) : the distance between the upper and lower quartiles
            IQR = Q3 − Q1 = q_n( 0.75 ) − q_n ( 0.25 ) 
    - Flier points are those past the end of the whiskers. See https://en.wikipedia.org/wiki/Box_plot for reference.

               minimum         median        maximum
               Q1-1.5IQR   Q1     :    Q3    Q3+1.5IQR
                            |-----:-----|
            o      |--------|     :     |--------|    o  o
                            |-----:-----|
        outlier             <----------->            outliers
                                 IQR

  Fixing Outliers: options:

    - delete the outliers
       - not every data point that falls outside the range can be blindly treated as an outlier.  Sometimes it 
         could be a valid data element
       - discuss with domain expert before deleting them to determine if outliers are genine 

    - Log transformation
      - applying log transformations to compress the range and minimize the impact of extreme values

    - Replace the outlier with the median value
      - imputing or replacing the outlier with a median value or a predetermined higher or a lower threshold value.



  Outlier demo:

    numpy.nanmean
      numpy.nanmean(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)
        - Compute the arithmetic mean along the specified axis, ignoring NaNs.

    Numpy tolist() 
      - Return a copy of the array data as a (nested) Python list.
  
  Code:

        >>> import pandas as pd
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt
        >>> import seaborn as sns

        >>> employee_df = pd.read_csv('Employee_outliers.csv')
        >>> employee_df.head()

        >>> employee_df.info()
            <class 'pandas.core.frame.DataFrame'>
            RangeIndex: 100 entries, 0 to 99
            Data columns (total 7 columns):
             #   Column       Non-Null Count  Dtype 
            ---  ------       --------------  ----- 
             0   employee_id  100 non-null    int64 
             1   first_name   100 non-null    object
             2   last_name    100 non-null    object
             3   age          100 non-null    int64 
             4   gender       100 non-null    object
             5   department   100 non-null    object
             6   salary       100 non-null    int64 
            dtypes: int64(3), object(4)
            memory usage: 5.6+ KB

            # pandas boxplot of numeric features (employee_id, age, salary)
            # salary has 2 outliers (500k, 0) 
        >>> employee_df.boxplot()
        >>> plt.show()

            # Seaborn (sns) boxplot of 'salary' 
        >>> sns.boxplot(employee_df, x='salary')
        >>> plt.show()

            # using pandas quantile() and median() to calculate Q1, Q3 Quantile and IQR values
        >>> q1 = employee_df['salary'].quantile(0.25)
        >>> q2 = employee_df['salary'].median()
        >>> q3 = employee_df['salary'].quantile(0.75)
        >>> iqr = q3 - q1
        >>> iqr

        >>> print(f'Q1: {q1}, Median: {q2}, Q3: {q3}, IQR: {iqr}')
            Q1: 68000.0, Median: 78500.0, Q3: 95000.0, IQR: 27000.0

        >>> minimum = (q1 - (1.5*iqr))
        >>> maximum = (q3 + (1.5*iqr))
        >>> print (f'minium: {minimum},  maximum: {maximum}')
            minium: 27500.0,  maximum: 135500.0

            # determine the IQR outliers - found 2
        >>> cond1 = employee_df['salary'] < minimum
        >>> cond2 = employee_df['salary'] > maximum
        >>> outliers = employee_df[cond1 | cond2]
        >>> outliers
             	employee_id 	first_name 	last_name 	age 	gender 	department 	salary
             0          1 	John 	        Doe 	        35 	Male 	Marketing 	500000
            17          18 	Sofia 	        Moore 	        28 	Female 	IT       	90

            # drop outliers
        >>> employee_df.drop(index=outliers.index, inplace=True)

            # Seaborn (sns) boxplot of 'salary' after dropping outliers
        >>> sns.boxplot(employee_df,x='salary')
        >>> plt.show()

            # re-import in the Employee dataset to a dataframe
        >>> employee_df = pd.read_csv('Employee_outliers.csv')
        >>> employee_df.head()

        >>> data = employee_df['salary']
        >>> data.head()

            # Using numpy nanmean to return the salary mean and nanstd to return the std deviation (both ignore NaN values)
        >>> mean = np.nanmean(data.tolist())
        >>> std = np.nanstd(data.tolist())
        >>> print (f'std: {std}, mean: {mean}')
            std: 46267.20925439527, mean: 85460.9

            # caculate the z-score for each salary
        >>> zscore = (data - mean)/std
        >>> zscore.head()
            0    8.959674
            1   -0.550301
            2   -0.334165
            3   -0.658369
            4   -0.442233
            Name: salary, dtype: float64

            # determine z-score outliers - found  1 outlier
        >>> threshold = 3
        >>> data[(abs(zscore) > threshold)]
            0    500000
            Name: salary, dtype: int64


  IQR vs z-score
    - interquartile range (IQR) may detect more outliers than z-score especially for skewed data distributions
      with long tails


------------------------------------------------------
2.6 Formatting Data


  Resources:

    Github Link -  formatting data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/formatting%20data

    Formatting Data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\formatting-data\Formatting Errors.ipynb

  Common Formatting Issues

    Feature not having the proper [data] type
      - common for the string on float data types to be categorized as objects

    Heterogenous data
      - may be inconsistent spacing, incorrect capitalization and numerical data may have inconsistent number of 
        decimal points and many more

    Using different values for the same concept
      - a gender feature may have a value like male, female, empty, letter M to represent male and letter F to 
        represent female and so on


  Formatting Data Demo:

    Requirements for demo dataset
      - the first and last names must be lowercase and must not have any spaces
      - all departments must be a predefined value in the title case
      - gender must be identified as 'M' for male, 'F' for female, and 'U' for unknown or not provided
      - salary must be rounded to the nearest integer number


    Code: Fix formatting errors using pandas str.lower(), str.replace(), str.strip(), apply(), map(), round()

        >>> import pandas as pd
        >>> import numpy as np

            # head of the dataset shows all formatting errors described in the requirements
        >>> df = pd.read_csv('Employee_errors.csv')
        >>> df.head()
              employee_id 	first_name 	last_name 	age 	gender 	department 	salary
            0 	1       	JOHN    	DOE 	        35 	Male 	marketing 	50000.6700
            1 	2       	ja ne   	smith 	        28 	Female 	Saless 	        60000.0000
            2 	3       	Mic hael 	Johnson 	42 	Male 	Finance 	70000.6768
            3 	4       	Emily   	williams 	31 	Female 	HR 	        55000.0000
            4 	5       	JaMes   	BROwn 	        37 	M 	Operations 	65000.2200

        >>> df.dtypes
            employee_id      int64
            first_name      object
            last_name       object
            age              int64
            gender          object
            department      object
            salary         float64
            dtype: object

            # use pandas str.lower() to lowercase first and last names
        >>> df['first_name'] = df['first_name'].str.lower()
        >>> df['first_name'].head()

        >>> df['last_name'] = df['last_name'].str.lower()
        >>> df['last_name'].head()

            # use pandas str.replace() to replace spaces in first and last names
        >>> df['first_name'] = df['first_name'].str.replace(' ','')
        >>> df['last_name'] = df['last_name'].str.replace(' ','')
        >>> df['first_name'].head()

            # use pandas str.strip() to remove leading and trailing spaces in first/last names 
        >>> df['first_name'] = df['first_name'].str.strip()
        >>> df['last_name'] = df['last_name'].str.strip()
        >>> df['first_name'].head()

            # using pandas unique() to display unique department names
        >>> df['department'].unique()
            array(['marketing', 'Saless', 'Finance', 'HR', 'Operations', 'IT',
                   'Marketting', 'Sales', 'Marketing'], dtype=object)

            # function to fix department names inconsistencies
        >>> def fix_errors(e):
        >>>     if e == 'marketing':
        >>>         return 'Marketing'
        >>>     elif e == 'Saless':
        >>>         return 'Sales'
        >>>     elif e == 'Marketting':
        >>>         return 'Marketing'
        >>>     else :
        >>>         return e

            using pandas apply function to apply fix_errors function to the 'department' column
        >>> df['department'] = df['department'].apply(fix_errors)
        >>> df['department'].head()


            # use pandas unique() to display unique department names and verify department name fixes
        >>> df['department'].unique()
            array(['Marketing', 'Sales', 'Finance', 'HR', 'Operations', 'IT'],
            dtype=object)

            # create dict (gender) with gender fix values and use pandas map() to fix gender values 
        >>> gender = {'Male': 'M', 'Female': 'F', 'male': 'M', 'female': 'F', 'unknown': 'U', 'M': 'M', 'F': 'F', 'U': 'U'}
        >>> df['gender'] = df['gender'].map(gender)
        >>> df['gender'].head()

            # use pandas round() to round off salary values
        >>> df['salary'] = df['salary'].round()
        >>> df['salary'].head()
            0    50001.0
            1    60000.0
            2    70001.0
            3    55000.0
            4    65000.0
            Name: salary, dtype: float64

        >>> df.head()




------------------------------------------------------
2.7 Labeling Data Using AWS Services


  Importance of Data Labeling
    Data labeling is an exercise of assigning relevant and meaningful tags to raw data observation

  Business Cases
    Image Classification
      - need to label objects (e.g. various fruit, animals, etc) in the images
    Spam Detection
      - need to label emails as spam or not spam
    Medical Diagnosis
      - need to manually classify medical diagnosis like reading X-rays and diagnosing lab results to classify if 
        a specific patient's sample has an underlying medical condition or not.
    Sentiment Analysis
      -  manual intervention to determine if a piece of text is positive or negative comment.

  Value proposition of Data Labeling
     Better prediction accuracy
     Improved performance
       - Label data helps minimize or eliminate the noise and inconsistencies in the dataset, which eventually 
         improves performance.
     Better generalization on new data
       - By learning from a diverse set of label data, the model can generalize better on new and unseen data
     Faster time to market

  SageMaker Ground Truth
    - a labeling services that leverages the human feedback to improve prediction accuracy

    Self Serve Model
                           |------------------------------|
       Input Data (S3) --> |labeling Workforce    Label UI|  --> Output data (S3)
                           |------------------------------|
                               SageMaker Ground Truth     

     Type of Workforce
       Amazon Mechanical Turk
         - vendor-managed workforce
         - anyone can sign up to this service and become an MTurk worker
       Private workforce
         - if you have a highly confidential data, then you can create your own private workforce to 
           review and label the dataset
       AI application
         - you can use any AI applications like Amazon Recognition, Textract, Comprehend and Transcribe.

      Managed Service Model
        Amazon SageMaker Ground Truth Plus
           - branded as a turnkey service that leverages their expert workforce to deliver high quality 
             training dataset, which also costs 40% cheaper.  
           flow:
           - With this service, once the data is uploaded to an S3 bucket,
           - once the data is uploaded to an S3 bucket, Ground Truth Plus will set up a data labeling 
             workflow that meets customer privacy and security requirements and operates on behalf of the customer.
           - These workflows are analyzed either by Amazon employed workforce or a third party vendor who 
             are expertly trained on a variety of data labeling jobs.
           - The customer can monitor the progress, offer feedback and review as the data is getting labeled

                           |------------------------------|
       Input Data (S3) --> |workflow   Workforce   Monitor|  --> Output data (S3)
                           |------------------------------|
                             SageMaker Ground Truth Plus     




  SageMaker Ground Truth Demo:
    - classify different types of food from a set of images

     AWS Console -> S3 -> Create Bucket ...... -> create -> Upload images (to be labeled)
     AWs Console -> SageMaker -> Ground Truth -> Labeling Jobs -> Create Labeling Job -> 
        Job Name: FoodImageLabeling, Input data setup: Automated data setup, S3 location for input dataset: <S3 bucket>,
        Data Type: Image, IAM Role: <Use Existing Role> , 
        Task Type: Task Category: IMage, Task Selection: Image Classification (single label) -> Next ->
        Select workers and configure tool:
          Workers: worker types: Amazon Mechanical Turks, use default task timeout/task experation time, 
          Price per task: $0.012, check: not adult content, agree to terms boxes, 
          Image classification (Single label) labeling Tools 
           task description: identify the fruit is pineapple, apple, or orange, 
           Select an option [labels]: enter: Pineapple, Apple, and Orange
           -> create


           -> labeling task will be 'in progress' till worker picks up this task, then status changed to 'completed'


          -> select <Labeling job> -> View Labeling tool -> Shows instructions


  SageMaker Ground Truth
     - with Ground Truth, customers can create cost-effective high-quality training data effectively


------------------------------------------------------
2.8 Hands-on Lab: Preparing Data Using Amazon Athena and AWS Glue


About this lab

Imagine you are the data engineer and you have been assigned the task to prepare the data and get it ready for the 
machine learning engineers to create a highly predictable model. Your corporation has been working with AWS and you 
have been encouraged to use AWS services.

Your raw data has been uploaded to an input folder in an S3 bucket. You will use a Glue crawler to detect the schema 
structure. You will then upload the data to a database that will be queried using SQL to detect discrepancies. Then, 
you will use the Visual ETL tool from AWS Glue to check for any missing or duplicate data and upload the processed data 
to the output folder.

Learning objectives
  - Create a Storage Area to Store the Input Files
  - Read the Raw Data to a Database
      - Create a Glue Crawler.
      - Configure the S3 input folder as the data source.
      - When prompted to create a new IAM role, add the suffix mlsc01 to the predefined role name so it's easy to find later.
      - Create and add a database.
      - Run the crawler and write the raw data to this database.
  - Run SQL Queries and Detect Data Discrepancies
      - Launch Amazon Athena and run SQL queries to detect null values against the age feature, check the number of observations 
      that fall outside $250,000, and determine the format of first-name and last-name features.
  - Fix the Data Discrepancies
      - Use AWS Glue Visual ETL to configure an input S3 bucket to read the raw data, change the schema, assign proper data types, 
        fill missing values against the age feature, filter data, and ignore rows whose salary is greater than $250,000.
      - Run a SQL query to convert the first and last names to lower case and remove the blank spaces in the fields.
      - Finally, write the formatted data to the output folder of the S3 bucket.

   ------------------------------------------------------


Solution

    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 Region.
    Download the input dataset Employee_lab.csv from the lab GitHub repo

Create a Storage Area to Store the Input Files

    From the console, navigate to S3.
    Click Create bucket and enter a globally unique bucket name.
    Scroll down and click Create bucket.
    Click on the newly created bucket. Choose Create folder, name the folder input and click on Create folder.
    Repeat the process and create another folder named output.
    From the list of folders, choose the input folder, click Upload, Add files, select the Employee_lab.csv file that you previously downloaded and click Open.
    Scroll down to the bottom of the console page and click Upload.
    Click the Close button to close the upload screen.

Read the Raw Data to a Database

    Search for Athena and select the service in the AWS console.
    Click on Launch query editor.
    Click Edit Settings to set up a query result location.
    Choose Browse S3 and click on the name of the bucket you previously created.
    Select the radio button for the output folder, click Choose, and Save.
    Choose Amazon Athena from the breadcrumbs at the top so that you are back on the Athena main page.
    Click Launch query editor again.
    From the Tables and views section, click to expand the Create dropdown menu.
    Select AWS Glue Crawler, give the crawler a name, and click Next.
    Click Add a data source.
    Under the S3 path section, click on Browse S3, and click on the name of the bucket you previously created.
    Select the radio button for the input folder, click Choose, and Save. NOTE: Ensure there is a forward slash at the end of the path (input/). If the slash is present but you still see an error message, click anywhere outside of the path input field and it should disappear.
    Keep all default options and click Add an S3 data source and Next.
    On the IAM role configuration settings page, click on Create new IAM role.
    Add the suffix mlsc01 to the predefined role name and click Create. Make sure the role is selected in the dropdown. If not, click the refresh button. Click Next.
    Click on Add database. Provide a name for the database and click Create database. NOTE: This action will complete in a new tab.
    Go back to the previous AWS Glue Set output and scheduling tab, refresh the Target database section, and select the new database you just created from the dropdown. Click Next.
    Scroll down and click Create crawler.
    Once the crawler is created successfully, click on Run Crawler.
    Scroll down, and under the Crawler runs tab, you will see the crawler in Running state turning into Completed state after successful completion.
    Expand the Data Catalog section from the left-hand menu.
    Under Databases click Tables to see a new table and the database you just created. If you don't see it, please click the refresh button.

Run SQL Queries and Detect Data Discrepancies

    From the Athena main page, click on Launch query editor. Refresh the Data section and make sure the database you created is selected and the new input table is listed under the Tables and Views section.

    Click the + next to the input table to see the data types of all the columns.

    Copy the following SQL query to the query window and click the Run button to execute. You should see the query output displaying 100 records in the table.

    select count(*) from input

    Next, follow the same process and run the following query. You will see there are two records where age is null.

    select count(*) from input where age is null

    Next, run the following query to display all the columns whose age is null.

    select * from input where age is null

    Now that we have detected records containing null values, let's check for outliers. Run the following query to check if employees make over $250,000. You will see two records that match this criteria.

    select * from input where salary > 250000

Fix the Data Discrepancies

    Search for AWS Glue and launch the service from the AWS console.

    Click on Set up roles and users. Click Choose roles, click the checkbox next to the role ending in mlsc01 that you created in a previous step. Click Confirm.

    Click Next. Accept the default values in the Grant Amazon S3 access screen. Click Next.

    Accept the default values in the Choose a default service role screen. Click Next.

    Review all the values and click Apply changes.

    From the main AWS Glue page, click Author and edit ETL jobs.

    Under the Create job section, click Visual ETL.

    From the Sources tab, click Amazon S3 to add that node to the visual panel.

    Click on the Amazon S3 node from within the panel to set its properties.

    Click Browse S3, click the name of the S3 bucket you created previously, and click the radio button next to the input folder. Click Choose.

    From the Data format dropdown, select CSV. The input data will be previewed under the Data preview section.

    At the top of the screen, give this job a name and click Save at the top right to save your work thus far.

    Click the blue + button in the panel view to add the next node.

    Switch to the Transforms tab, click the Change Schema node. If it didn't connect automatically, connect the output of the S3 node to the input of the Change Schema node.

    Under the Change Schema (Apply Mapping) section, change the data type of employee_id, age, and salary from string to int.

    Save the project.

    Click the blue + button to add the next node. Choose Fill missing values from the Transforms section. Click the resulting node in the panel. Under the Data field with missing values section, select age.

    You might see a data preview failure, which is a known issue with Glue version 4.0. If you don't see the error, AWS has addressed this issue, and you can skip to step 20. If you see the error, continue to the next step.

    Switch to the Job details tab. From the Glue version drop-down menu, select Glue version 2.0 and click Save in the top right.

    Click Run to rerun the job. Switch to the Runs tab to monitor the job. It may take close to 3 minutes for the job to complete successfully.

    After the run status shows Succeeded, switch back to the Visual tab.

    Scroll to the right in the Data preview section. You will see a new column named age_filled. As you scroll down, you will see that the null age values are no longer present in this new column.

    Click the blue + button to add the next node. From the Transforms tab, click Filter.

    Select that node, from the Transform panel on the right, click Add condition.

    From the Key dropdown menu, select salary.

    From the Operation dropdown, select the > operation. Enter 250000 in the Value textbox. The data preview will display the two outliers we saw when we ran the SQL query.

    From the Operation dropdown, select the < operation. The Data preview is updated to list all the values except the two outliers.

    Save the project.

    Click the blue + button to add the next node. From the Transforms section, choose SQL Query.

    Paste the following query under the SQL query section:

    select lower(replace(first_name, ' ','')) as first_name, lower(replace(last_name, ' ','')) as last_name, age_filled as age, gender, department, salary from myDataSource

    Ensure the Data preview section displays the column names as expected.

    Click the blue + button to add the final target node. Switch to the Targets tab, and click Amazon S3. If the nodes are not connected, connect the output of the SQL query node to the target S3 bucket node.

    Click the resulting node to configure it. From the Format dropdown, select CSV.

    From the drop-down under Compression Type select None.

    Under S3 Target Location section, click Browse S3, click the name of your bucket, select the radio button next to the output folder, and click Choose.

    Click Save to save the job and click Run to execute it.

    Switch to the Runs tab and ensure a new job has been triggered and is in the Running state. The job may take close to 4 minutes to complete.

    Once the job completes, navigate to S3 in the console and click the bucket you created previously. Choose the output folder, and click the file that starts with run and ends with 00000.

    Click the Download button at the top. Open the downloaded file to confirm that the data has been preprocessed.


   ------------------------------------------------------

   bucket name: data-prep-athena-glue-pa
   Athena Crawler: data-prep-crawler
   Athena DB: data-prep-db
   Glue Job Name: 


------------------------------------------------------
2.9 Santize and Prepare Data For Modeling Review


  AWS ML examine  Task statement 2.1 Santize and prepare data for modeling:

    Identify and handle missing data, corrupt data, and stop words

      Missing data categories
        
        MCAR (Missing Completely At Random)
          - missing data is unrelated to both observed and unobserved data
          - Missing data values do not relate to any other data in the dataset and there is no pattern 
            to the actual values of the missing data themselves
          - strategies to address: Use mean, median, and KNN Imputation
    
        MAR (Missing At Random)
          - missing data do have a relationship with other variables in the dataset. However, the actual 
            values that are missing are random.
          - missing data is dependent on the observed data
          - strategies to address: Use MICE
          Multivariate Imputation By Chained Equation (MICE)
            - a statistical method of imputing missing data
            - runs multiple imputation threads parallelly
            - combines the results to produce the final dataset
    
        MNAR (Missing Not At Random)
          - The pattern of missingness is related to other variables in the dataset, but in addition, 
            the values of the missing data are not random.
          - missing data is related to both observed and unobserved data
          - strategies to address: Use selection models and shared parameter models

        Missing data visualization
          - use missingno library to visualize data
            example, use:
            missingno.matrix
               - msno.matrix nullity matrix is a data-dense display which lets you quickly visually pick out patterns 
                in data completion.

       
       SPM (Shared Parameter Model):
         https://typeset.io/questions/what-is-shared-parameter-model-27tc1e3uqi
         - a technique used in statistical modeling to account for missing data in longitudinal studies. 
         - It is particularly useful when the missing data is not at random (MNAR). 
         - The model incorporates explanatory variables and estimates parameters that capture the relationship 
           between the observed data and the missing data process
         - The model is typically implemented in the Bayesian framework and can be used to assess the predictive 
           performance of the fitted model 
 

       Imputing data
         - no one-size-fits-all approach

         Use pandas mode() to impute categorical data
           
           pandas.DataFrame.mode(axis=0, numeric_only=False, dropna=True) 
             - Get the mode(s) of each element along the selected axis.
             - The mode of a set of values is the value that appears most often. It can be multiple values, if each 
               value appear the same number of time.

         Use [pandas] mean(), [sklearn] KNNImputer to impute numerical data

           sklearn.impute.KNNImputer(missing_values=nan, n_neighbors=5, ...)
             - Imputation for completing missing values using k-Nearest Neighbors.
             - Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found 
               in the training set. Two samples are close if the features that neither is missing are close.

      Stop words
        - commonly used words in our communication that carry very little meaning
        - removing them will reduce the dataset size which will improve the training time and performance
        NLTK and SpaCy libraries
          - used to remove stop words
          - include a default list of stop words
          - can customize the list of stop workds by appending to the default list


    Formalize, normalize, augment, and scale data

      Data Formatting
        - data formatting is required to fix corrupted data
        - user errors, data synchronization errirs, and processing application errors are some of the 
          factors that contribute to data corruption
        NumPy and Pandas libraries
        - can use NumPy and Pandas libraries to address data formatting issues
        - can be used to address the spacing issues, converting the case, and replacing error values.
        - You may need to write Python functions or custom maps to handle more complex error scenarios

      Outlier
        - a data point that differs signficantly in value for other data points
        outlier detection:
          - use 'z-score' and 'boxplot' to detect outliers

            Z-score
              - the distance of an observation from the median in terms of standard deviation

              z-score = x  - mean / std_deviation   where x is a datapoint

              z-score outliers:
                    -3  < datapoint > +3


            Boxplot (also called wisker plot)

              - The box extends from the first quartile (Q1) to the third quartile (Q3) of the data, with a 
                line at the median. 
              - The whiskers extend from the box to the farthest data point lying within 1.5x the inter-quartile range (IQR) 
                from the box. 

                Interquartile range (IQR) : the distance between the 3rd quartile and 1st lower quartile

                    IQR = Q3 − Q1 = q_n( 0.75 ) − q_n ( 0.25 ) 

                    minunum = Q1 - 1.5 * IQR
                    maximum = Q3 + 1.5 * IQR
                      - any value less than the mininum value or greater than the maximum values is considered an outlier


                           minimum         median        maximum
                           Q1-1.5IQR   Q1     :    Q3    Q3+1.5IQR
                                        |-----:-----|
                        o      |--------|     :     |--------|    o  o
                                        |-----:-----|
                    outlier             <----------->            outliers
                                             IQR
        outlier errors:
          - not all outliers are errors - some maybe valid values
          - discuss with domain experts before replacing outliers

        fixing outliers options:

          - delete the outliers
             - not every data point that falls outside the range can be blindly treated as an outlier.  Sometimes it 
               could be a valid data element
             - discuss with domain expert before deleting them to determine if outliers are genine 
      
          - Log transformation
            - applying log transformations to compress the range and minimize the impact of extreme values
      
          - Replace the outlier with the median value
            - imputing or replacing the outlier with a median value or a predetermined higher or a lower threshold value.

    Determine whether there is sufficient labeled data:
      - identify mitigation strategies
      - Use labeling tools

      imbalanced dataset
        - uneven distribution where one of the categorical feature value is significantly higher than the rest
        - the main challenge with imbalanced data is a biased model
        undersampling and oversampling techniques:
          - have signficant side effects
        SMOTE (Synthetic Minority Over-sampling TEchnique)
          - common approach used to generate synthetix minority class samples
          - synthetic samples are generated from the minority class, instead of just duplicating them
          - avoids overfitting by oversampling the minority class without duplicating the data

      Data Labeling
        - provides the context and meaning to the raw data
        SageMaker Ground Truth
          - self-serve model labeling service 
          - labeling service that uses the human feedback in labeling the data.  
          SageMaker Ground Truth Plus
            - a managed service which is promoted as a turnkey service that leverages their expert workforce 
              to deliver high-quality training dataset and also costs 40% cheaper

          

------------------------------------------------------
2.10 Quiz: Santize and Prepare Data for Modeling


Question 4

  During your data analysis, you found that some features are "missing data completely at random (MCAR)". 
  Which metric would you use to measure it?

Choices:
  Mode

  Multivariate Imputation by Chained Equation

  There is no metric since the data is missing completely at random

  KNN Imputation                                                      <--- Correct Answer
Good work!

  This is a good metric to use when the feature is missing data completely at random.


Notes:
        
  Missing data categories

    MCAR (Missing Completely At Random)
      - missing data is unrelated to both observed and unobserved data
      - Missing data values do not relate to any other data in the dataset and there is no pattern 
        to the actual values of the missing data themselves
      - strategies to address: Use mean, median, and KNN Imputation

    MAR (Missing At Random)
      - missing data do have a relationship with other variables in the dataset. However, the actual 
        values that are missing are random.
      - missing data is dependent on the observed data
      - strategies to address: Use MICE
      Multivariate Imputation By Chained Equation (MICE)
        - a statistical method of imputing missing data
        - runs multiple imputation threads parallelly
        - combines the results to produce the final dataset

    MNAR (Missing Not At Random)
      - The pattern of missingness is related to other variables in the dataset, but in addition, 
        the values of the missing data are not random.
      - missing data is related to both observed and unobserved data
      - strategies to address: Use selection models and shared parameter models


  Multivariate Imputation By Chained Equation (MICE)
    - a statistical method of imputing missing data
    - runs multiple imputation threads parallelly
    - combines the results to produce the final dataset
    - The reasoning behind running multiple threads is to maintains the relationship between the variables 
      in the original data and reduces the bias introduced by the imputed values.
    - effective for datasets with large amounts of missing data
    - computationally intensive because of the mutliple parallel runs
    - Overall MICE has been proven as a valuable tool for addressing missing data that is more reliable and accurate



------------------------------------------------------

Chapter 3 Perform Feature Engineering

------------------------------------------------------
3.1 Identifying Feature Types

  AWS ML Exam Guide - 2.2 Perform feature engineering.
    - this lesson, we will attempt to cover all the topics that are part of Task Statement 2.2 of the exam guide,
      that is Perform Feature Engineering

  Framework
                        --> Preparing Instances 
     Data Preparation
                        --> Preparing Features 

      Preparing Instances includes: - chapter 2
        - removing duplicates
        - Handling outliers
        - Imputing missing values
        - resampling imbalanced data

      Preparing Instances includes:  - chapter 3
        - Feature selection
        - encoding techniques
        - normnalizing and scaling techniques
        - binning and transforming techniques


  Feature Engineering
    - involves selecting, extracting, and transforming the variables from raw data to effectively train
      your model

  Feature Selection
    What?
    - Choosing a subset of relevant features from the raw data
    - the purpose is to identify most informative features and remove redundant features
    Why (goal):
    - improves model performance, reduces computational costs, and enhances interpretability
    Example:
      - when predicting the house prices, the owner's name feature might not add value
         to training the model, and so it can safely remove it without affecting the model performance


  Feature Extraction
    What?
    - Process of creating new features from existing once to provide more relevant information
    Why?
    - reduces the dimensionality of the data and makes it easier for the algorithms to process the data
    Example
      - when predicting the house prices, you have owner name, total sq ft and price. 
      - can derive a new feature, cost per square feet, by dividing the price and the total square feet.
     

  Feature Transformation
    What?
    - process of transforming the features into a more suitable representation for the ML model
    why
    - mitigates the effects of skewnewss of the data and improves the model performance
    Example
      - when predicting the house prices, you have owner name, number of bedrooms, and price. 
      - where bedrooms would range between 2 to 10, but the price may range anywhere from 100,000 to 
        multi-million dollars 


  Types of Features
    Qualitative (categorical)
      Nominal
        - labeled variables with no order
        - example: house Heating type: gas, electric, central
      Ordinal
        - labeled variables with order [and ranking]
        - example: House Quality: poor, fair, good, very good
      Boolean
        - a feature with a binary value
        - example: house for sale? yes or no values

    Quantitative (numerical)
      Discrete
        - a feature with countable items
        - example: Number of Bedrooms
      Continuous
        - infinite numeric values within a specific range
        - example: House price


  Feature Engineering Challenges
    - Requires deep domain expertise
    - time-consuming and resource-intensive process especially for large datasets
    - requires a mastery of a wide array of topics
      - no one-size-fits-all approach.
      - need to understand which features to create, and what techniques to apply to them.
      - In short, feature engineering requires the right balance between domain expertise and 
        technical expertise

  Feature Engineering Value Proposition
    - improves generalization 
      - reduces the risk of overfitting
    - generates better predictions
    - helps make better business decisions

------------------------------------------------------
3.2 Normalizing, Standardizing, and Transforming Numerical Data


  Resources:

    Github Link -  tranforming data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/transformingdata

    Transforming data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\transformingdata\featuretransformation.ipynb


  Feature Transformation
    - process of transforming the features to mitigate the effects of skewnewss in the data distribution
    - involves modifying the features to create new features
    - common techniques include: polynomial transformation, logarithmic transformation, exponential transformation, 
      and box-cox transformation.

  Feature Scaling
    - changes the feature's scale in the dataset
    - does not create a new feature
    - common techniques include: MinMaxScaler, StandardScalar, RobustScaler, and MaxAbsScaler

  Feature Scaling - why
    - ensures that the model gives the same weightage to all the features
    Example: 
      - in employeed data set, the age column ranges from 25 to 45, whereas the salary goes beyond 130,000
      - so that a model gives the same weight to age and salary, feature scaling needs to be performed


  StandardScaler (z-score normalization or standardization)
    - assumes the data is normally distributed (most of the data is centered around the mean, and the tail
      tails extend symmetrically on both the sides
      
    - uses z-score to scale the data
      Z-score
        - the distance of an observation from the median in terms of standard deviation

          z-score = (x  - mean) / std_deviation   where x is a datapoint

              where:
                    std_deviation = [ (1/N) SUM (x_i - mean)**2 ]**1/2 
                         SUM is from i=1 to i=N



  Demo: Use Sklearn Standscalar

    Code: Demo: Use Sklearn Standardscalar to scale the employee salaries 

        >>> import pandas as pd
        >>> import numpy as np
        >>> from sklearn.preprocessing import StandardScaler
            # create a sklearn StandardScaler object 'scaler'
        >>> scaler = StandardScaler()

        >>> employee_df = pd.read_csv('Employee.csv')
        >>> employee_df.head()

        >>> print(f"salary min: {min(employee_df['salary'])}, salary max: {max(employee_df['salary'])}")
            salary min: 50000, salary max: 132000

            # fit the employee salary data with StandardScaler
            # Note: reshape is need so fit 2D Pandas dataframe instead of 1D Pandas Series 
        >>> scaler.fit(employee_df['salary'].values.reshape(-1,1))

            # fit and transform can be performed in step using .fit_transform
            # transform/scale salary data using standardScaler
        >>> employee_df['scaled_salary'] = scaler.transform(employee_df['salary'].values.reshape(-1,1))
        >>> employee_df[['scaled_salary']].describe()

         	        scaled_salary
            count 	1.010000e+02
            mean 	2.385331e-16
            std 	1.004988e+00
            min 	-1.672671e+00
            25% 	-7.182091e-01
            50% 	-1.879524e-01
            75% 	5.544070e-01
            max 	2.675434e+00


  MinMaxScaler [normalization] (or rescaling)
    - by default, rescales the data between 0 and 1
    - can specify maximum and minumum values of our choice

      MinMax = (X - X_min) / (X_max  -  X_min)

  Demo: Use Sklearn MinMaxScaler

    Code: Demo: Use Sklearn MinMaxScaler to scale the employee salaries between -1 and 1

        >>> from sklearn.preprocessing import MinMaxScaler
            # create a sklearn MinMaxcaler object 'mmscaler'
            #  setting the value of clip to true will clip the scaled data to match the featured range
        >>> mmscaler = MinMaxScaler(clip=True)

            # fit the employee salary data with MinMaxScaler
            # Note: reshape is need so fit 2D Pandas dataframe instead of 1D Pandas Series 
        >>> mmscaler.fit(employee_df['salary'].values.reshape(-1,1),)

            # fit and transform can be performed in step using .fit_transform
            # transform/scale salary data using minmaxScaler
        >>> employee_df['salary_minmax_scaled'] = mmscaler.transform(employee_df['salary'].values.reshape(-1,1))
        >>> employee_df[['salary_minmax_scaled']].describe()
                	salary_minmax_scaled
            count 	101.000000
            mean 	0.384690
            std 	0.231132
            min 	0.000000
            25% 	0.219512
            50% 	0.341463
            75% 	0.512195
            max 	1.000000

        >>> employee_df.head()



  Other Scaling Strategies
    MaxAbsScaler
      - takes the absolute value of a feature and divides it by the maximum value
      - scales the features between the range of -1 and +1

      MaxAbs = X  / (max|X|)

    RobustScaler
      - scales the data using IQR
      - good for data containing outliers because this technique is not influenced by mean, 
        max, or min values.

         X_robust = (X - Q1) / (Q3  -  Q1)
          where Q1 is the first quartile and Q3 is third quartile 



  Why Feature transformation:
    - helps interpret non-linear relationships in the data
    - effective against skewed data
      - It helps reduce the skewness of the data and makes it more symmetric.
    - study hidden patterns in the data
      - By transforming features to a different scale, we can often study the hidden patterns in 
        the data that were not readily apparent.


  Logarithmic transform
    - Logarithmic transformation helps us reduce the impact of two high values in our data while training our model



  Demo: Use Numpy log to logarithic transform the salary data

    Code: Use Numpy log to logarithic transform the salary data

        >>> import pandas as pd
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt

        >>> employee_df = pd.read_csv('Employee_transformation.csv')
        >>> employee_df.head()

        >>> np.min(employee_df['salary'])
            52000

        >>> np.max(employee_df['salary'])
            500000

            # transform salarys logarithicly
        >>> employee_df['log_salary'] = np.log(employee_df['salary'])
        >>> employee_df.head()

        >>> np.min(employee_df['log_salary'])
            10.858998997563564

        >>> np.max(employee_df['log_salary'])
            13.122363377404328

            # histogram plot of salaries with 100 bins
            # histogram shows salary data is right skewed.
        >>> plt.hist(employee_df['salary'], bins=100)

            # histogram plot of log_salaries with 100 bins
            # histogram shows log_salary data is relatively less skewed and more normally distributed
        >>> plt.hist(employee_df['log_salary'], bins=100)



  StandardScaler vs MinMaxScaler vs MaxAbsScaler:
    https://codefinity.com/courses/v2/a65bbc96-309e-4df9-a790-a1eb8c815a1c/1fce4aa9-710f-4bc9-ad66-16b4b2d30929/79d587a4-bba9-45c8-878f-f2948f0b0c7e
    - A StandardScaler is less sensitive to outliers, so it is a good default scaler.
    - If you don't like the StandardScaler, between MinMaxScaler and MaxAbsScaler, it comes down to personal 
      preferences, scaling data to the [0,1] range (with MinMaxScaler) or to [-1,1] (with MaxAbsScaler). 


  other Transformation Strategies
    Box-Cox Transformation
      - Used to convert a skewed target [non-normal dependent] variables into a normal distribution

      https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/
      - At the core of the Box Cox transformation is an exponent, lambda (λ), which varies from -5 to 5. 
      - All values of λ are considered and the optimal value for your data is selected; The “optimal value” is 
        the one which results in the best approximation of a normal distribution curve. 

          y(λ) = (y**λ - 1) / λ          if λ not equal 0
                  log(y)                 if λ equal 0

      - This test only works for positive data. However, Box and Cox did propose a second formula that 
        can be used for negative y-values


    Polynomial Transformation
      - creates new features by raising the powers of the original features
      -  primarily used to capture the non-linear relationship between the features and the target variables.

    Exponential transformation 
      - used when the relationship between the features, and the target variable exhibits an exponential pattern,
      - example: like what you see in stock prices, which show exponential growth or decay.


------------------------------------------------------
3.3 Applying Binning Techniques to Nunerical Data


  Resources:

    Github Link -  data binning
       https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/databinning

    Data Binning Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\databinning\databinning.ipynb


  Data Binning (data discretization or data bucketing)
    - Data binning is a process of transforming a continuous variable into discrete values grouped 
      into smaller number of bins or intervals.  

  Data Binning Benefits
    - reduce training time
       - Many machine learning algorithms perform better with discrete values because the 
         more value a feature has, the longer the training time will be.
       - Grouping these features into distinct features will considerably reduce the training time.
    - removes noise in the dataset
       - reduces noise by grouping similar values together.  
       - For example, your transformed feature can be just the minimum and the maximum value of the bin. 
         This can make it easier to detect meaningful patterns or trends.  
    - addresses data skewness
       - It can also help address the skewness of the data by grouping the long tail into a corresponding bin.

  Data Binning Drawbacks
    - Possible information loss
      - Since we group the data into separate bins, there is a possibility for information loss.
      - Fine grain data present in the actual data might be lost when they are grouped into bins.
    - bin sizes will impact detecting the underlying patterns
      - Selecting the bin sizes have significant impact in detecting the underlying patterns.
    - wrong bin sizes may introduce bias


  Binning Strategies

    Equal-width Strategy (uniform-width binning)
      - spread the data across multiple bins, each of them having the same width.
      - This strategy is suitable if the dataset is symmetric and evenly distributed

        bin_width =  data_Range / (no_of_bins)

    Equal-frequencey Strategy (quantile binning)
      - data is divided into bins, such that each bin contains the same number of observations.
      - all the bins have the same frequency, meaning they'll have the same number of data elements.
      - ideal for skew datasets

        bin_width =  (no_of_elements) / (no_of_bins)


  sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', ...)
    - Bin continuous data into intervals.
    Parameters:
      n_bins: int or array-like of shape (n_features,), default=5
        - The number of bins to produce. Raises ValueError if n_bins < 2.

      encode: {‘onehot’, ‘onehot-dense’, ‘ordinal’}, default=’onehot’
       - Method used to encode the transformed result.
            ‘onehot’: Encode the transformed result with one-hot encoding and return a sparse matrix. 
               Ignored features are always stacked to the right.
            ‘onehot-dense’: Encode the transformed result with one-hot encoding and return a dense array. 
               Ignored features are always stacked to the right.
            ‘ordinal’: Return the bin identifier encoded as an integer value.
      strategy: {‘uniform’, ‘quantile’, ‘kmeans’}, default=’quantile’
        - Strategy used to define the widths of the bins.
            ‘uniform’: All bins in each feature have identical widths.
            ‘quantile’: All bins in each feature have the same number of points.
            ‘kmeans’: Values in each bin have the same nearest center of a 1D k-means cluster.
      dtype: {np.float32, np.float64}, default=None
        - The desired data-type for the output. If None, output dtype is consistent with input dtype. 


  Demo: Data Binning Strategies

    Code: Use sklearn KBinsDiscreter to create quantile (equal frequency) bins and uniform width (equal-width) bins 

        >>> import numpy as np
        >>> import pandas as pd
        >>> from sklearn.preprocessing import KBinsDiscretizer
        >>> import matplotlib.pyplot as plt

            # create numpy array with 15 elements between 10 and 55
        >>> data = np.array([10, 11, 12, 13, 14, 15, 16, 20, 25, 30, 35, 40, 45, 50, 55]).reshape(-1, 1)

            # create quartile bins object, 'kbins' containing 5 bins where each will have roughly equal number of elements
            #   Note: encode='ordinal' returns and bin identifier as an integer
        >>> kbins = KBinsDiscretizer(n_bins=5, strategy='quantile', encode='ordinal')
            # transform numpy 'data' array into quartile (equal-frequency) bins, 'bin_EF'
        >>> bins_EF = kbins.fit_transform(data)
            # bins_EF array shows 3 elements in each array bin 0 to 4
        >>> bins_EF
            array([[0.],
                   [0.],
                   [0.],
                   [1.],
                   [1.],
                   [1.],
                   [2.],
                   [2.],
                   [2.],
                   [3.],
                   [3.],
                   [3.],
                   [4.],
                   [4.],
                   [4.]])

            

            # histogram shows 3 elements in each bin
        >>> plt.hist(bins_EF)
        >>> plt.show()

            # create uniform bins object, 'kbins_width' containing 5 bins of equal width
            #   Note: encode='ordinal' returns and bin identifier as an integer
            #   Note: subsample=None, means that all the training samples are used when computing the quantiles that 
            #         determine the bin thresholds
        >>> kbins_width = KBinsDiscretizer(n_bins=5, strategy='uniform', encode='ordinal', subsample=None)

            # transform numpy 'data' array into uniform (equal-width) bins, 'bin_EW'
        >>> bins_EW=kbins_width.fit_transform(data)

            # bins_EF array shows 7 elements in bin 0, 2 elements each in bins 1 to 4
        >>> bins_EW
            array([[0.],
                   [0.],
                   [0.],
                   [0.],
                   [0.],
                   [0.],
                   [0.],
                   [1.],
                   [1.],
                   [2.],
                   [2.],
                   [3.],
                   [3.],
                   [4.],
                   [4.]])

            # histogram shows 7 elements in bin 0, 2 elements each in bins 1 to 4
        >>> plt.hist(bins_EW)
        >>> plt.show()


  Other Binning Strategies
    K-Means binning
      - uses K-means clustering algorithm to partition continuous data
      - used when data distribution is not uniform
    Decision tree binning
      - builds a decision tree model on the continuous variable, and in the process, the algorithm 
        will determine the split points best separate the data into different branches of the tree
      - This technique is effective when the relationship between the continuous variable and the 
        target variable is non-linear


------------------------------------------------------
3.4 Encoding Categorical Data


  Resources:

    Github Link -  encoding text data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/encoding%20text%20data

    Encoding text data Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\encoding-text-data\encoder.ipynb


  Encoding
    - the process of transforming categorical data into numerical data


  Why Encode Categorical Data
    - most machine learning models accept only numerical data
    - helps us maintain compatibility with the algorithm
    - elevates model quality
      - encoding process allows these algorithms to learn the data patterns quickly and 
        elevate the model quality.
    - Improves accuracy by reducing the dimensionality of the data
      - Proper encoding techniques can help reduce the dimensionality of a data set, which eventually 
        improves model accuracy and performance.
    - Prevents model bias by ensuring all features have equal weigtage
      - By ensuring all features have equal weightage, encoding helps prevent bias in the model.
   - This transformation process varies based on the feature type,

  Ordinal Data
    - ordinal data represents features that have a clear order or ranking

  Ordinal Encoding
    Example: Job Title: Developer, Sr Developer, Manager, VP
    ordinal coding Developer: 1, Sr Developer: 2, Manager: 3, VP: 4


  Label Encoder
    - another version of ordinal encoder
    - be used for both ordinal and nominal feature, because it does not maintain the order 
      during the encoding process.

  Nominal data 
    - represents features that don't have any order or ranking

    one-hot encoding
      - popular technique used to encode nominal features
      - a column is created for each unique category
      - For every observation: 
           1 indicates the presence of a category
           0 indicates its absence

     one-hot encoding Benefits and limitations
       Benefits
         - improves model performance
          - great option to transform nominal values when there are fewer number of categories.
       Limitations
         - increase dimensionality, make the model complex to train
            - if you have a larger number of categories, It's not practical to create an individual feature for each 
              category, though a main advantage of One-hot Encoding is improving the model performance, it can lead 
              to increased dimensionality, which can make the model more complex to train.
         - creates a lot of sparse data that consumes memory
            -  It can also lead to sparse data, which means a dataset filled with a lot of zeros, and consumes a lot 
             of memory during the training process.

  Binary Encoding
    - nominal feature is passed through a label encoder and converted into numerical data
    - this numerical data is then converted into binary digits
    - the digits are split into individual columns (1 column per power of 2)

       example:          -> Label           -> binary encoding
          Department            Department Label         Department  Col1 Col2 Col3
             Marketing          Marketing   1            Marketing    0    0    0
             Sales              Sales       2            Sale         0    0    1
             It                 It          3            IT           0    1    0
             Finance            Finance     4            Finance      1    0    0
             HR                 HR          5            HR           1    0    1
          

  sklearn.preprocessing.LabelEncoder
    - Encode target labels with value between 0 and n_classes-1.
    - This transformer should be used to encode target values, i.e. y, and not the input X.

  Demo: Encoding Categorical Features

    Code: Use Sklearn LabelEncoder to encode the employee Titles (4 titles, so 0 to 3)

        >>> import pandas as pd
        >>> import numpy as np

        >>> employee_df = pd.read_csv('Employee_encoding.csv')
        >>> employee_df.head()

            # create a labelEncoder object, 'title_encoder'
        >>> from sklearn.preprocessing import LabelEncoder
        >>> title_encoder = LabelEncoder()

            # 4 unique titles
        >>> employee_df['title'].unique()
            array(['developer', 'senior developer', 'manager', 'vp'], dtype=object)

            # fit the employee titles
        >>> title_encoder.fit(employee_df['title'])

            # display title_encoder classes - shows the order the titles will be encoded 
        >>> title_encoder.classes_
            array(['developer', 'manager', 'senior developer', 'vp'], dtype=object)

            # reorder the encoding classes
        >>> title_encoder.classes_ = np.array(['developer','senior developer','manager','vp'])

            # call transform and assigned the output to 'encoded_title' column
        >>> employee_df['encoded_title'] = title_encoder.transform(employee_df['title'])

        >>> employee_df[['title', 'encoded_title']]
                title 	        encoded_title
            0 	developer 	        0
            1 	senior developer 	1
            2 	manager 	        2
            3 	vp 	                3
            4 	developer 	        0
            ... 	... 	...
            95 	vp 	                3
            96 	vp 	                3
            97 	vp 	                3
            98 	vp 	                3
            99 	vp 	                3
            
            100 rows × 2 columns
        



    Code: Use Sklearn OneHoteEncoder to encode the employee geneder

            # create a OneHotEncoder object, 'geneder_encoder'
        >>> from sklearn.preprocessing import OneHotEncoder
        >>> gender_encoder = OneHotEncoder()

            # 2 unique genders
        >>> employee_df['gender'].unique()
            array(['Male', 'Female'], dtype=object)

            # fit the employee genders
        >>> gender_encoder.fit(employee_df['gender'].values.reshape(-1,1))

            # show gender_encoder categories
        >>> gender_encoder.categories_
            [array(['Female', 'Male'], dtype=object)]

            # call transform and assigned the output to 'transform' object
            #  Note: you need to reshape data because fit() expect data to be in 2D format
        >>> transform = gender_encoder.transform(employee_df['gender'].values.reshape(-1,1))
        >>> transform
            <100x2 sparse matrix of type '<class 'numpy.float64'>'
	            with 100 stored elements in Compressed Sparse Row format>

            # create a new Pandas Dataframe called 'employee_df1'
            #  Note: SciPy todense() returns a dense representation of this sparse array/matrix
        >>> employee_df1 = pd.DataFrame(transform.todense(), columns=gender_encoder.categories_)
        >>> employee_df1
             	Female 	Male
            0 	0.0 	1.0
            1 	1.0 	0.0
            2 	0.0 	1.0
            3 	1.0 	0.0
            4 	0.0 	1.0
            ... 	... 	...
            95 	1.0 	0.0
            96 	0.0 	1.0
            97 	1.0 	0.0
            98 	0.0 	1.0
            99 	1.0 	0.0
            
            100 rows × 2 columns

            # join employee_df and employee_df1
        >>> combined_df = employee_df.join(employee_df1)
        >>> combined_df
             	employee_id 	first_name 	last_name 	age 	gender 	department 	salary 	title 	        encoded_title 	(Female,) 	(Male,)
              0 	1 	John 	        Doe 	        35 	Male 	Marketing 	50000 	developer 	        0 	0.0 	        1.0
              1 	2 	Jane 	        Smith 	        28 	Female 	Sales 	        60000 	senior developer 	1 	1.0 	        0.0
              . . .


------------------------------------------------------
3.5 Extracting Features from Text


  Extracting Features from Text
    - Extracting features from text refers to converting raw text data into a structured numerical format 
      to feed ML algorithms
    - very common for the input data to be unstructured
    - feature extraction from text is a crucial step in ML pipeline that captures information about the content, 
      structure, context during this transformation process

  Bag of Words (BoW)
    - common text processing technique
    - before applying BoW:
       - preprocess (clean) the raw text
         - Text cleaning strategies include: converting to lower case, removing punctuation, and removing stop words
    - technique tokenizes the text data into multiple words using a whitespace as a default delimiter
       - once cleaned, it passed as tokens
    - represents each token in the form of a matrix where each column represents a feature in the global vocabulary
    - measure the frequency of known words
      - [after text is tokenized,] measure the frequency of the known words (also called a 'Bag of Words')

    - technique only focuses on if known words occur in the document, and if so how frequent they occur 
      - does NOT focus on the sequence or order of the words
      - all ordering of words will be discharded

    - any new document is represented as a numerical vector where each element of the vector corresponds to the 
      frequency of the word in the vocabulary of the document 

    - Using this technique against large datasets [with large vocabulary] will result in high-dimensionality features


  Bag of Words (BoW) example:

       
    I am preparing for the AWS
    Machine Learning certification
    exam                               Data
                                       cleaning
    I am preparing for the AWS                    preparing, aws, machine, learning,
    Solution Architect Professional     ----->      certification, exam, solution,
    certification exam                             architect, professional, associate 
         
    I am preparing for the AWS
    Solution Architect Associate 
    certification exam


        Data cleaning included: converting to lower case, removing punctuation, and removing stop words

                                 
    I am preparing for the AWS                     preparing aws machine learning certification exam solution  architect professional associate 
    Machine Learning certification                    1       1     1        1           1        1     0          0          0           0
    exam                               Scoring
                                       
    I am preparing for the AWS                    
    Solution Architect Professional     ----->         1       1     0        0           1        1     1          1          1           0
    certification exam                             
         
    I am preparing for the AWS                         1       1     0        0           1        1     1          1          0           1
    Solution Architect Associate 
    certification exam

  Bag of Words (BoW) Scoring Strategy:
     Binary strategy
       - assigns '1' if the word exists in the global vocabulary and '0' if not
     Count strategy
       - counts the number of times each feature appears in the document
     Frequency strategy
       - calculates the frequency of each feature out of all the features in the document

  N-Gram
    - buids on the Bag of Words (BoW) technique
    - produces groups of words of n size
    - an N-gram with size of 1 is the same as a BoW technique
    Steps:
      - tokenize the individual words based on the n-gram size
      - split the text into consecutive sequences of items
      - count the frequency of each n-gram in the text
        - the frequencies are used as features for further analysis 



  Bi-gram (N=2) technique:


    I am preparing for the AWS
    Machine Learning certification
    exam                               2-grams
                                       
    I am preparing for the AWS                    preparing aws, aws machine,
    Solution Architect Professional     ----->     machine learning, learning  
    certification exam                             certification, certification exam,  
                                                   exam solution, solution architect, 
    I am preparing for the AWS                     architect professional, professional 
    Solution Architect Associate                   associate
    certification exam


  BoW and N-gram limitations
    - inability to capture contectional information
       - because each word is considered independently and assigned equal importance with 
         considering it symantic significants
     - handling out-of-vocabulary words
        - since techniques rely on predefined vocabulary, encountering words not in the vocabulary 
          may lead to information loss or misinterpretted data
     - handling large datasets can be computationally expensive
       - may require additional techniques like dimensionality reduction

  TF-IDF (Term Frequency-Inverse Document Frequency)
    - Statitical technique that Measures the importance of a word (or feature) in the document
    - Computed by multiplying the Term Frequence (TF) and Inverse Document Frequency (IDF) scores
    - the higher the TF-IDF score, the more important the word is in the document
    - a relatively high value means the term is more significant that other words

  Term Frequency
    TF = (no. of occurrences of a term in the document ) / (total number of terms in the document)

    example: if document has 100 terms and AWS occurs 5 times, the TF_aws = 5 / 100 = 0.05

  IDF (Inverse Document Frequency)
    - measures how unique or rare a term is across all the documents
    -  "no of docments containining the term" is incremented by (t+1) to avoid division by zero

      IDF = log [(total number of documents in the corpos) / (no of documents containing the the term t+1)]

      example: have 1000 documents and 100 documents contain AWS, then:

           IDF_aws = log (1000 / (100 + 1)) = 0.99


  TF-IDF Example Scenarios:
       TF-IDF_aws = TF_AWS  *  IDF_aws =  0.05  * 0.99 = 0.049 


  TF-IDF Example 2 Scenarios:
        750 document contain AWS (instead of 100 out of a 1000)

           TF_aws = 5 / 100 = 0.05             IDF_aws = log (1000 / (750 + 1)) = 0.124

           TF-IDF_aws = TF_AWS  *  IDF_aws =  0.05  * 0.124 = 0.006 

           - the lower value returns the term 'aws' is less important in this document because it appears
             in a large number of other documents

  -------
  from: O'Reilly Hands-ON Machine Learning with Scikit-Learn, Keras, and TensorFlow  Chapter 13 pages 471 - 473


      tf_idf: term-frequency x inverse-document-frequency (TF-IDF): This is similar to the count encoding, but words 
          that occur frequently in the training data are downweighted, and conversely, rare words are upweighted 

      TF-IDF variant used by TextVectorization
        - multiplies each word count by a weight equal to log(1 + d/(f + 1)) where d: total number of sentences in the
          training data; and f: counts how many of these training sentences contain the given word
  -------



  Word Embedding
    - captures semantic and syntactic similarity along with the context of a word in a document
    - every word is represented as a numerical vector
      - word embedding is vector representiation a word in a document 
    - a vector can have any number of dimensions
       - example if 'car' is represented as 'color', and 'model' and 'year', then it is a 3D vector
    - word embedding creates a dense vectory of a fixed length


    Syntactic: relating to the grammatical arrangement of words in a sentence:


  Word Embedding Techniques: Continuous Bag of Words (CBOW)
    - takes a fixed-sized windows of words around the target word
    - learns to predict the target word based on the context words
    - preliminary used for tasks like language modeling and text generation


  Word Embedding Techniques: Skip-Gram
    - predicts the context word based on the target word
       - takes the target word and predicts the surounding context words
    - effective technique for tasks such as word similarity and word analogy


  Stemming
    - powerful text processing technique used in NLP
    - lowers the inflection in words to their base (or root) form
       - example: learn, learning, learned are converted to 'learn'
    - normalizes the words with similar meanings but varying inflections
    - improves text analysis by reducing the number of unique words

------------------------------------------------------
3.6 Extracting Features from Image and Speech


  Resources:

    Github Link -  encoding text data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/extracting%20image%20features

    Extracting Image Features Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\extracting-image-features\image processing.ipynb



 Image Processing
   - most algorithms require data in numerical format
   - the most common method of storing images is using raster graphics, where the image is a represented 
     as pixel grid



  Extracting Features from Images techniques
    Traditional  Computer vision techniques
      - use Pixel intensity values
      - or edge detection algorithms
    Deep learning techniques
      - Convolution neural netwworks
      - transfer learning
      - auto encoders


  Grayscale Pixel values as Features
    - range if values from 0 to 255 (where 0 is white, 255 is black, and intermediate values
      represent different shades of gray
    - each pixel's value captures its brightness level providing information about the image's
       overall brightness distribution, contrast, and texture.
    - The number of features extracted from a grayscale image is equal to the total number of 
       pixels in the image.
    - This technique is commonly used in various image processing tasks like object detection
      and image classification.

  Mean Pixel value of Channels
    - used with color images.  
    - For a color image, each pixel stores multiple values representing the intensities of red, green,
      and blue channels.
    - The challenge with this approach is that the number of features will be very high, so a new matrix 
      is generated by computing the mean values of all the three channels, which retains the same number 
      of features as grayscale pixel value.
    - It's used in image segmentation, image classification, and image enhancement.

  extracting edge features 
    - used to identify and highlight edges of objects within an image.
    - Human eyes detect the edges of an object because there is a sharp change in the object color.
    - We learned before that an image is represented in the form of a matrix made of pixels, where each 
      pixel value identifies the image density at that point.
    - to identify if a pixel is an edge or not, we simply need to subtract the values on either sides of 
      the pixel.  A higher number will indicate that the pixel is an edge.
    - In this technique, we use the Prewitt Kernel, which is a simple three by three matrix as shown below,
      and multiply it with values surrounding a pixel to identify the edges in the x and y directions.

        -1    0    1                         -1     -1    -1
        -1    0    1                          0      0     0
        -1    0    1                          1      1     1
        Prewitt Kernel X direction            Prewitt Kernel Y direction
          3 x 3 matrix                          3 x 3 matrix

 
    Extracting Image Demo

      Code:  Use prewitt verical and horizontal filters to identify edges 

            # load Sci-kit image processing library
        >>> import numpy as np
        >>> from skimage.io import imread, imshow

            # read in image with 'A', 'B', and 'C' blue blocks  (1000 x 1000 pixels)
            #  convert image to grayscale (as_gray=True)
        >>> image = imread('blocks-letters_Blue.png', as_gray=True) 

        >>> height, width = image.shape
            (1000, 1000)

        >>> imshow(image)

            # features = pixel height x  pixel width 
        >>> featurses = height * width
        >>> features
            1000000

            # re-read image as a color image
        >>> image = imread('blocks-letters_Blue.png') 

            # shape: height, width, channels
            #   4 channels: red, green, blue, & alpha
            #   alpha channel represents the transparency of each pixel in the image
        >>> image.shape
            (1000, 1000, 4)

            # import horizontal and vertical Prewitt Kernels
        >>> from skimage.filters import prewitt_h,prewitt_v

            # re-read image as a grayscale image the Prewitt Kernel won't accept multidimensional array
        >>> image = imread('blocks-letters_Blue.png', as_gray=True) 

            # let's pass the image through both the horizontal and vertical kernels
        >>> edges_prewitt_horizontal = prewitt_h(image)
        >>> edges_prewitt_vertical = prewitt_v(image)

            # display veritical edges from prewitt veritical filter
        >>> imshow(edges_prewitt_vertical, cmap='gray')


  Extracting Features from Speech
    - speech recognition is a field of converting audio signals to text
    - requires transforming raw audio signal to feature vectors

  Extracting Features from Speech techniques
    Tranditional speech processing techniques
      - Mel Frequency Cepstral Coefficient (MFCC)
      - Linear Predictive Coding (LPC)
    Deep speech processing techniques
      - Long Short-Term Memory (LSTM)
      - Gated Recurring Unit (GRU)

  Limitations of Extracting Features from Speech
    - Speech Signal variability
      - speech signal varies significantly based on gender, age, and emotional state of a person.
    - Background noise and interference
      - The background noise and environmental interference can significantly contaminate speed 
        signals, making it harder to interpret and extract features.
    - High-Dimensionality data
       - may need dimensionality reduction techniques or high computational power.



  -------
  from: O'Reilly Hands-ON Machine Learning with Scikit-Learn, Keras, and TensorFlow  Chapter 15 pages 537 - 538

      Recurrent Neural Networks (RNNs):
      - a class of net that can predict the future (well, up to a point)
      - RNNs can analyze time series data, such as the number of daily active users on your website, the hourly temperature
        of your city, your home's power consumption, etc.
      - RNNs can work on sequences of arbitrary lengths, rather than on fixed-size inputs
      ARMA family of models
        - often used to forecast time series, and use them as baselines to compare with our RNNs
      RNNs Main Difficulties
        - Unstable Gradients
          - can be alleviated using various techniques include 'recurrent dropout' and 'recurrent layer normalization'
        - Limited short-term memory
          - can be extended using LSTM and GRU cells. 
       WaveNet
         - a CNN architecture capable of handling sequences of tens of thousands of tiem steps


  -------
  from: O'Reilly Hands-ON Machine Learning with Scikit-Learn, Keras, and TensorFlow  Chapter 15 pages 568 - 571


   Tackling the Short-Term Memory Problem (pages 568 - 576)

     RNN Short-term memory problem
       - due to transformation that goes through when traversing an RNN, some information is lost each time step
       - after a while, the RNN's state contains virtually no trace of first inputs
       - to tackle this problem, cells with long term memory have been introduced

   LSTM cells (pages 568 - 571)

     Long Short-term memory cell (LSTM):
       - if you consider the LSTM cell as a black box, it can be used like a basic cell, except it performs better,
         training converge faster, and it will detect longer-term patterns in the data
       - in Keras, simply use 'LSTM' layer instead of the 'SimpleRNN' layer


     How does LSTM cell work:
     - architecture is shown in Figure 15-12. An LSTM cell (page 569)
     - like a regular cell, except its state is split into two vectors h_(t) and c_(t) where h_(t) is short-term state
       and c_(t) is the long term state
     - as c_(t) tranverses the network, it goes through the 'forget gate' dropping some memories and then its adds some memories
       via addition operations (which adds the memories that were selected by the 'input gate')
     - after the addition operation, the long-term state is copied and passed through the 'tanh' function, and then
       filtered by the 'output gate'


     
   Gated Recurrent Unit (GRU)
     - architecture is shown in Figure 15-13. An GRU cell (page 571)
     - popular variant of the LSTM cell 
     - simplified version of the LSTM cell, and it seems to perform just as well
     main GRU simiplifications from LSTM cell:
       - both state vectors are merged into a single vector h_(t)
       - single gate controller z_(t) controls both the 'forget gate' and the 'input gate'
         - if the gated controller outputs a '1', the 'forget gate' is open (= 1) and the 'input gate' is closed (1 - 1 = 0)
         - if it outputs a '0', the opposite happens
         - that is, whenever a memory must be stored, the location where it will be stored is erased first
       - there is no 'output gate'; the full state vector is output at every time step


  -------
   


------------------------------------------------------
3.7 Performing Feature Selection and Dimensionality Reduction

  
  Resources:

    Github Link -  encoding text data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/pca

    Extracting Image Features Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\pca\pca.ipynb


  Dimensionality Reduction
    - Real-life datsets have a large number of features
      - some have 100's or even 1000's of features
    - As the number of features increases:
      - Complexity and sparsity of the data increases exponentially
      - ability to generalize decreases exponentially
    Curse of Dimensionality
      - more information starts to get detrimental instead of beneficiald


  Dimensionality reduction
    - technique of reducing the number of features of a dataset in such a way that the information loss
      is kept to a minimum.

  Dimensionality reduction - value proposition
    Saves Storage Costs
      - Reducing the features helps save the storage cost considerably, because a number of redundant features 
        have been removed from the original dataset.  
    Improves model performance and accuracy
      - With fewer number of features to process, the machine learning model can perform better and predict 
        accurately.
    Requires fewer resources to run the model
      - Reducing the number of features also have a direct impact on the amount of resources required to 
        run the machine learning model.
    Easier to visualize, interpret, and understand
      - simplifying the dataset makes it easier to visualize, understand and interpret.  
    Prevents overfitting
      - dimensionality reduction helps overcome this problem by focusing only on the essential features and 
        getting rid of the noise.

  Dimensionality reduction Techniques
    Feature Selection
      - select a subset of features by discarding irrelevant or redundant features.
      Filter method
         - takes only the subset of the relevant features.
        Techniques:
          Variance Thresholding
          Chi-square test
      Wrapper method 
        - uses the performance of an ML model to determine the relevance of features subsets
        - evaluates subsets of features by training a machine learning model on different combinations of 
          features and selecting the subset that results in the best performance.
        - wrapper methods are computationally intensive compared to filter methods, because we need to train 
          and evaluate a model.
        Techniques:
          Forward Selection
          Backward Elimination
          Recursive Feature Elimination
      Embedded method
        - selects features as part of the model training process
        - incorporates feature selection as part of the model training process itself.
        - Instead of evaluating feature subsets independently, these methods select features while training 
          the model.
        techniques:
          Lasso Regression 
          Ridge Regression,
          Gradient Boosting Machines 
          Elastic Net

    Feature Extraction
      - transforms the original features into lower dimensional space while preserving the most important 
        information present in the dataset
      Linear dimensionality reduction subcategory
        - projects data to lower dimensionality using linear transformation
        Techniques
          Principal Component Analysis (PCA)
          Linear Discriminant Analysis
      Nonlinear dimensionality reduction subcategory
        - captures complex relationships in a non-linear fashion
        Techniques
          T-Distributed Stochastic Neighbor Embedding
          Isometric Mapping


  Covarinace Matrix:
    - A covariance matrix is used to describe the relationships between multiple variables in the dataset.
    - The element in the Nth row and Mth column represents the covariance between Nth and Mth variable in 
      the dataset.

  Numpy Convariance:
    numpy.cov(m, y=None, rowvar=True, bias=False, ...)
      - Estimate a covariance matrix, given data and weights.
      - Covariance indicates the level to which two variables vary together. 
      - If we examine N-dimensional samples, X = [x_1, x_2, ...x_n]**T, then the covariance matrix 
        element C_ij is the covariance of x_i and x_j. The element C_ii is the variance of x_i

  Eigenvectors
    - The eigenvectors represents the direction of maximum variance
  EigenValues
    - eigenvalue indicates a magnitude of the variance.

  
  numpy.linalg.eig() Method 
   https://www.geeksforgeeks.org/numpy-linalg-eig-method-in-python/
   - computes the eigenvalues and right eigenvectors of a given square array 
   - take a square array as a parameter and it will return two values first one is eigenvalues of the 
     array and second is the right eigenvectors of a given square array.


  Principal Component Analysis (PCA) steps?
    https://builtin.com/data-science/step-step-explanation-principal-component-analysis

   1. Standardize the range of continuous initial variables
   2. Compute the covariance matrix to identify correlations
   3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components
   4. Create a feature vector to decide which principal components to keep
   5. Recast the data along the principal components axes

  Principal Components:
    - Principal components are new variables that are constructed as linear combinations or mixtures of the 
      initial variables. 
    - These combinations are done in such a way that the new variables (i.e., principal components) are 
      uncorrelated and most of the information within the initial variables is squeezed or compressed into 
      the first components. 
    - the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible 
      information in the first component, then maximum remaining information in the second and so on, 


  Demo: Dimensionality Reduction using PCA technique

    Code: Reduce Iris dataset dimensional for 4 to use PCA (via standardization, covariance matrix,
          selecting principal components via eigenvalues/eigenvector, and dot product of standardized & 
          principal components)
  
        >>> import pandas as pd
        >>> import numpy as np
        >>> from sklearn.datasets import load_iris
            
            # load the sklearn Iris dataset
            # then pass Iris data to X
        >>> iris = load_iris()
        >>> X = iris.data

            # standardize (or z-score normalization the iris petal and sepal lengths/widths)
        >>> X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

            # Use Numpy cov() to calculate the covariance matrix
        >>> covariance_matrix = np.cov(X_std.T)

        >>> covariance_matrix
            array([[ 1.00671141, -0.11835884,  0.87760447,  0.82343066],
                   [-0.11835884,  1.00671141, -0.43131554, -0.36858315],
                   [ 0.87760447, -0.43131554,  1.00671141,  0.96932762],
                   [ 0.82343066, -0.36858315,  0.96932762,  1.00671141]])

            # calculate the eigenvalues and eigenvectorys using Numpy eig
        >>> eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

            #  sort the eigenvectors based on the corresponding eigenvalues in descending order
            #  [start:stop:index] -> [::-1] -> reverses the order
        >>> sorted_indices = np.argsort(eigenvalues)[::-1]
        >>> sorted_eigenvalues = eigenvalues[sorted_indices]
        >>> sorted_eigenvectors = eigenvectors[:, sorted_indices]

            # set desired dimensionality to 2
        >>> k = 2  
        >>> principal_components = sorted_eigenvectors[:, :k]

            # transform the data by multiplying the original standardized data and the selected principal 
            # components to uptime the new lower dimensional representation of the data.
        >>> transformed_data = np.dot(X_std, principal_components)

            # plot the PCA transformed data
        >>> import matplotlib.pyplot as plt
        >>> plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=iris.target)

             # scatter plot shows that we have effectively reduced the number of features from four to two 
             # could have achieved this using the PCA library provided by scikit-learn as well,



------------------------------------------------------
3.8 Preprocessing Data Using Amazon SageMaker Processing


  Resources:

    Github Link -  encoding text data
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/preprocessing

    Extracting Image Features Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\preprocessing\preprocessing.ipynb


  Challenges with Data Preprocessing
    - Resource availability
    - tool selection and configuration
    - lack of reusability
       - require an efficient way where the pre-processing tasks can be built once and reused multiple 
         times as new data come in.

  SageMaker Processing - Value Proposition
    - Pay-as-you-go pricing model
    - tight integration with other SageMake components
      - SageMaker Processing is seamlessly integrated with other SageMaker components, enabling us to 
        perform pre-processing to modeling within one environment
    - support multiple programming languages, libraries, and frameworks
    - tracks changes to processing scripts by setting up version control

  SageMaker processing:
    - refers to SageMaker capabilities to run data pre and post-processing, feature engineering, 
      and model evaluation tasks on SageMaker fully managed infrastructure.

  SageMaker Architectural Diagram
    Input Data:
      - S3 data that fed into SageMaker Processing job
    Preprocessing step:
      - runs within a processing container.
      - performs multiple pre-processing steps like removing duplicates, encoding data, detecting 
        and handling outliers, scaling data, all of them using the SageMaker Processing APIs.
    Output
      - The process data is eventually returned in another folder of an S3 bucket or in a separate 
        bucket altogether.

  SageMaker - Processing Options
    SKLearnProcessor
      - process scikit-learn scripts
    PySparkProcessor
      - run PySpark scripts as processing jobs
 
  Demo: Preprocessing data using SageMaker processing

    Code:

            # install 'sagemaker' package if using script outside SageMaker
        >>> get_ipython().system('pip install -U sagemaker')

        >>> import boto3
        >>> import sagemaker
        >>> from sagemaker import get_execution_role
        >>> from sagemaker.sklearn.processing import SKLearnProcessor

        >>> region = boto3.session.Session().region_name

        >>> role = get_execution_role()
            # set execution role,  processor type ("ml.m5.xlarge"), instance count (1)
        >>> sklearn_processor = SKLearnProcessor(
        >>>     framework_version="0.20.0", role=role, instance_type="ml.m5.xlarge", instance_count=1)

        >>> import pandas as pd

            # load SageMaker example 'census-income' dataset
        >>> input_data = "s3://sagemaker-sample-data-{}/processing/census/census-income.csv".format(region)
        >>> df = pd.read_csv(input_data, nrows=10)
        >>> df.head(n=10)

            # create "preprocessing.py" script to be run with the Sklearn 'run' function
        >>> %%writefile preprocessing.py
        >>> 
        >>> import argparse
        >>> import os
        >>> import warnings
        >>> 
        >>> # import required libraries
        >>> import pandas as pd
        >>> import numpy as np
        >>> from sklearn.model_selection import train_test_split
        >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer
        >>> from sklearn.preprocessing import PolynomialFeatures
        >>> from sklearn.compose import make_column_transformer
        >>> 
        >>> from sklearn.exceptions import DataConversionWarning
        >>> 
        >>> warnings.filterwarnings(action="ignore", category=DataConversionWarning)
        >>> 
        >>> # specify the features
        >>> columns = [
        >>>     "age",
        >>>     "education",
        >>>     "major industry code",
        >>>     "class of worker",
        >>>     "num persons worked for employer",
        >>>     "capital gains",
        >>>     "capital losses",
        >>>     "dividends from stocks",
        >>>     "income",
        >>> ]
        >>> # specify label values
        >>> class_labels = [" - 50000.", " 50000+."]
        >>> 
        >>> 
        >>> def print_shape(df):
        >>>     negative_examples, positive_examples = np.bincount(df["income"])
        >>>     print(
        >>>         "Data shape: {}, {} positive examples, {} negative examples".format(
        >>>             df.shape, positive_examples, negative_examples
        >>>         )
        >>>     )
        >>> 
        >>> 
        >>> if __name__ == "__main__":
        >>>     parser = argparse.ArgumentParser()
        >>>     parser.add_argument("--train-test-split-ratio", type=float, default=0.3)
        >>>     args, _ = parser.parse_known_args()
        >>> 
        >>>     print("Received arguments {}".format(args))
        >>> 
        >>>     input_data_path = os.path.join("/opt/ml/processing/input", "census-income.csv")
        >>> 
        >>>     # read data to dataframe and drop NAN rows, drop duplicates, and convert labels to 0,1
        >>>     print("Reading input data from {}".format(input_data_path))
        >>>     df = pd.read_csv(input_data_path)
        >>>     df = pd.DataFrame(data=df, columns=columns)
                # drop rows with missing values
        >>>     df.dropna(inplace=True)
                # drop rows with duplicate values
        >>>     df.drop_duplicates(inplace=True)
        >>>     df.replace(class_labels, [0, 1], inplace=True)
        >>> 
        >>>     negative_examples, positive_examples = np.bincount(df["income"])
        >>>     print(
        >>>         "Data after cleaning: {}, {} positive examples, {} negative examples".format(
        >>>             df.shape, positive_examples, negative_examples
        >>>         )
        >>>     )
        >>> 
        >>>     # split data to train, test, and validation
        >>>     split_ratio = args.train_test_split_ratio
        >>>     print("Splitting data into train and test sets with ratio {}".format(split_ratio))
        >>>     X_train, X_test, y_train, y_test = train_test_split(
        >>>         df.drop("income", axis=1), df["income"], test_size=split_ratio, random_state=0
        >>>     )
        >>> 
        >>>     # usin KBinDiscretizer to create 10 one-hot bins for "age" and "num persons worked..."
        >>>     # uses standardScaler for 'capital gains" ... "dividends .."
        >>>     # uses one-hot encoder for 'education" ...  "class of worker .."
        >>>     preprocess = make_column_transformer(
        >>>         (
        >>>             ["age", "num persons worked for employer"],
        >>>             KBinsDiscretizer(encode="onehot-dense", n_bins=10),
        >>>         ),
        >>>         (["capital gains", "capital losses", "dividends from stocks"], StandardScaler()),
        >>>         (["education", "major industry code", "class of worker"], OneHotEncoder(sparse=False)),
        >>>     )
        >>>     # transform data
        >>>     print("Running preprocessing and feature engineering transformations")
        >>>     train_features = preprocess.fit_transform(X_train)
        >>>     test_features = preprocess.transform(X_test)
        >>> 
        >>>     # return transform data to CSV file
        >>>     print("Train data shape after preprocessing: {}".format(train_features.shape))
        >>>     print("Test data shape after preprocessing: {}".format(test_features.shape))
        >>> 
        >>>     train_features_output_path = os.path.join("/opt/ml/processing/train", "train_features.csv")
        >>>     train_labels_output_path = os.path.join("/opt/ml/processing/train", "train_labels.csv")
        >>> 
        >>>     test_features_output_path = os.path.join("/opt/ml/processing/test", "test_features.csv")
        >>>     test_labels_output_path = os.path.join("/opt/ml/processing/test", "test_labels.csv")
        >>> 
        >>>     print("Saving training features to {}".format(train_features_output_path))
        >>>     pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)
        >>> 
        >>>     print("Saving test features to {}".format(test_features_output_path))
        >>>     pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)
        >>> 
        >>>     print("Saving training labels to {}".format(train_labels_output_path))
        >>>     y_train.to_csv(train_labels_output_path, header=False, index=False)
        >>> 
        >>>     print("Saving test labels to {}".format(test_labels_output_path))
        >>>     y_test.to_csv(test_labels_output_path, header=False, index=False)

        >>> from sagemaker.processing import ProcessingInput, ProcessingOutput

            # use sklearn 'run' function to run 'preprocessing.py' script
            # as arguments, passing input data path, output data path, train/test split
        >>> sklearn_processor.run(
        >>>     code="preprocessing.py",
        >>>     inputs=[ProcessingInput(source=input_data, destination="/opt/ml/processing/input")],
        >>>     outputs=[
        >>>         ProcessingOutput(output_name="train_data", source="/opt/ml/processing/train"),
        >>>         ProcessingOutput(output_name="test_data", source="/opt/ml/processing/test"),
        >>>     ],
        >>>     arguments=["--train-test-split-ratio", "0.2"],
        >>> )

            # describe() function writes the response of the processing job execution to the variable 
            #  'preprocessing_job_description'
        >>> preprocessing_job_description = sklearn_processor.jobs[-1].describe()

        >>> output_config = preprocessing_job_description["ProcessingOutputConfig"]
        >>> for output in output_config["Outputs"]:
        >>>     if output["OutputName"] == "train_data":
        >>>         preprocessed_training_data = output["S3Output"]["S3Uri"]
        >>>     if output["OutputName"] == "test_data":
        >>>         preprocessed_test_data = output["S3Output"]["S3Uri"]


 Demo Output
    - write input 'preproessing' script and outputs to default SageMaker S3 bucket

 Demo Preparation issues:

   AccessDeniedExpection error ... when calll the DescribeLogStreams operation
     - due to IAM role not have CloudWatch access
     - fix: add CloudWatchLogsReadOnlyAccess polity to IAM role

   ResourceLimitExceeded Exception
      - due to 'ml.m5.xlarge' quota was the default '0'
      - fixed by: AWS console -> Service Quotas -> SageMaker -> 'ml.m5.xlarge for preprocessing jobs usage'
         -> request increase to '1'

 

------------------------------------------------------
3.9 Performing Feature Engineering Using Amazon SageMaker Lab


Perform Feature Engineering Using Amazon SageMaker

About this lab

Imagine you are the data engineer, and you have been assigned the task of preprocessing the data and getting it 
ready for the machine learning engineers to create a highly predictable model. Your data contains both text and 
numerical data. The numerical data is of different ranges, and some text features require proper ordering.

In this hands-on lab, you will learn how to encode, scale, and bin the data using scikit-learn.
  - Learning objectives
  - Launch SageMaker Notebook
  - Load Libraries and Prepare the Data
  - Apply Encoding Techniques
  - Apply Scaling Techniques
  - Apply Binning Techniqu


    ------------------------------------------------------

Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. If you are prompted to select a kernel, please choose conda_tensorflow2_p310.
    In the search bar, navigate to Amazon SageMaker.
    Under the Notebooks section in the left menu, click Notebook Instances.
    Confirm the notebook is marked as InService. If so, click the Open Jupyter link under Actions.
    Click on the Feature_Engineering.ipnyb file.

Load Libraries and Prepare the Data

    Click the first cell that imports the required Python libraries to highlight it, and use the Run button at the top to execute the code.

    An asterisk inside the square braces, (In [*]), indicates the code is running. You will see a number, (In[1]), once the execution is complete. This first cell uses the Pandas library and reads the raw data from Employee_encoding.csv.

    The second cell requires you to write the code to read the input file and load the dataframe. Paste the following Python code and click Run.

    employee_df = pd.read_csv('Employee_encoding.csv')
    employee_df.head()

Apply Encoding Techniques

    In the next cell under 3.1) Ordinal ENcoding, you will initialize the OrdinalEncoder and perform the fit operation. Highlight this cell and click Run.

    The next cell uses the categories_ attribute to display the encoder's sequence. Highlight this cell and click Run.

    In the third cell, insert and Run the following Python code to assign the transformed values to a new feature named encoded_title.

    employee_df['encoded_title'] = ordinal_encoder.transform(employee_df['title'].values.reshape(-1,1))
    employee_df.head()

    In the first cell, under 3.2) One-hot Encoding, the code initializes the OneHotEncoder. Highlight this cell and Run it.

    In the next cell, use fit_transform to perform fit and transform in a single function call. Update the cell with the following Python code and Run it.

    transform = gender_encoder.fit_transform(employee_df['gender'].values.reshape(-1,1))

    In the third cell, use the todense function to address the sparse nature of the data and join the output with the parent dataframe. Highlight this cell and Run it.

    In the first cell, under 3.3) Label Encoding, paste and Run the following code to initialize LabelEncoder.

    department_encoder = LabelEncoder()

    The next cell applies fit and transform on the department feature and assigns the output to encoded_department. Highlight this cell and click Run.

Apply Scaling Techniques

    In the first cell under 4) Scaling Techniques, the code scales the salary feature using MinMaxScaler. Highlight this cell and click Run.

    In the next cell, paste and Run the below code to invoke the describe function on the salary_minmax_scaled feature and ensure the value ranges between 0 and 1.

    employee_df[['salary_minmax_scaled']].describe()

Apply Binning Techniques

    In the first cell under 5) Binning Techniques, paste and Run the following Python code to initialize KBinDiscretizer with ten bins.

    kbins = KBinsDiscretizer(n_bins=10, strategy='quantile', encode='ordinal')

    Highlight the next cell and Run it to invoke fit_transform on the Kbins discretizer.

    Highlight and Run the code in the last cell to visualize the new age_bin feature using Matplotlib's histogram function.


    ------------------------------------------------------

------------------------------------------------------
3.10 Performing Feature Engineering Review


  this section focus:
    - focused exclusively on task statement 2.2 of the certification.



  Exam Guile task 2.2 Perform feature engineering.
    subtask 1:
    - identify and extract features from data sets, including from data sources such as text, 
      speech, image, and public datasets.
         Feature Engineering
          - Feature Engineering involves feature selection, extraction, and transformation.
          Types of Features
            Qualitative (categorical)
              Nominal
                - labeled variables with no order
                - example: house Heating type: gas, electric, central
              Ordinal
                - labeled variables with order [and ranking]
                - example: House Quality: poor, fair, good, very good
              Boolean
                - a feature with a binary value
                - example: house for sale? yes or no values
        
            Quantitative (numerical)
              Discrete
                - a feature with countable items
                - example: Number of Bedrooms
              Continuous
                - infinite numeric values within a specific range
                - example: House price

         Feature Extraction from text
           Techniques:
             Bag of Words (BoW)
             n-gram
             TF-IDF (Term Frequency-Inverse Document Frequency)
               - Statitical technique that Measures the importance of a word (or feature) in the document
               - Computed by multiplying the Term Frequency (TF) and Inverse Document Frequency (IDF) scores
               - the higher the TF-IDF score, the more important the word is in the document
               - a relatively high value means the term is more significant that other words

                Term Frequency
                  TF = (no. of occurrences of a term in the document ) / (total number of terms in the document)
              
                  example: if document has 100 terms and AWS occurs 5 times, the TF_aws = 5 / 100 = 0.05
              
                IDF (Inverse Document Frequency)
                  - measures how unique or rare a term is across all the documents
                  -  "no of ducments containining the term" is incremented by (t+1) to avoid division by zero
              
                    IDF = log [(total number of documents in the corpos) / (no of documents containing the the term t+1)]
              
                    example: have 1000 documents and 100 documents contain AWS, then:
              
                         IDF_aws = log (1000 / (100 + 1)) = 0.99

          Word Embedding
            - captures the syntatic and semantic similarity in the words
            Word Embedding Techniques:

              Word Embedding Techniques: Continuous Bag of Words (CBOW)
                - takes a fixed-sized windows of words around the target word
                - learns to predict the target word based on the context words
                - preliminary used for tasks like language modeling and text generation
            
            
              Word Embedding Techniques: Skip-Gram
                - predicts the context word based on the target word
                   - takes the target word and predicts the surounding context words
                - effective technique for tasks such as word similarity and word analogy
            
            
              Stemming
                - powerful text processing technique used in NLP
                - lowers the inflection in words to their base (or root) form
                   - example: learn, learning, learned are converted to 'learn'
                - normalizes the words with similar meanings but varying inflections
                - improves text analysisy by reducing the number of unique words

         Extracting Features from Images techniques
           Traditional  Computer vision techniques
             - use Pixel intensity values
             - or edge detection algorithms
           Deep learning techniques
             - Convolution neural netwworks
             - transfer learning
             - auto encoders
       
       
         Grayscale Pixel values as Features
           - range if values from 0 to 255 (where 0 is white, 255 is black, and intermediate values
             represent different shades of gray
           - each pixel's value captures its brightness level providing information about the image's
              overall brightness distribution, contrast, and texture.
           - The number of features extracted from a grayscale image is equal to the total number of 
              pixels in the image.
           - This technique is commonly used in various image processing tasks like object detection
             and image classification.
       
         Mean Pixel value of Channels
           - used with color images.  
           - For a color image, each pixel stores multiple values representing the intensities of red, green,
             and blue channels.
           - The challenge with this approach is that the number of features will be very high, so a new matrix 
             is generated by computing the mean values of all the three channels, which retains the same number 
             of features as grayscale pixel value.
           - It's used in image segmentation, image classification, and image enhancement.
       
         extracting edge features using Prewitt Kernels 
           - used to identify and highlight edges of objects within an image.
           - Human eyes detect the edges of an object because there is a sharp change in the object color.
           - We learned before that an image is represented in the form of a matrix made of pixels, where each 
             pixel value identifies the image density at that point.
           - to identify if a pixel is an edge or not, we simply need to subtract the values on either sides of 
             the pixel.  A higher number will indicate that the pixel is an edge.
           - In this technique, we use the Prewitt Kernel, which is a simple three by three matrix as shown below,
             and multiply it with values surrounding a pixel to identify the edges in the x and y directions.
       
               -1    0    1                         -1     -1    -1
               -1    0    1                          0      0     0
               -1    0    1                          1      1     1
               Prewitt Kernel X direction            Prewitt Kernel Y direction
                 3 x 3 matrix                          3 x 3 matrix
       



    subtask 2:
      - Analyze and evaluate feature engineering concepts (binning, tokenization, outliers, 
        synthetic features, one-hote encoding, reducing dimensionality of data)
      Feature Scaling
        - Feature scaling ensures that the model gives the same weightage to all the features.

        Standard scaler (also known as Z-score normalization)
          - assumes the data is normally distributed (most of the data is centered around the mean, and the tail
            tails extend symmetrically on both the sides

        MinMaxScaler
          - by default, rescales the data between 0 and 1
          - can specify maximum and minumum values of our choice
      
            MinMax = (X - X_min) / (X_max  -  X_min)

      Feature transformation
        - is very effective against skewed data

      Feature transformation techniques
        Box-Cox
          - Used to convert a skewed target [non-normal dependent] variables into a normal distribution
        Polynomial Transformation
          - creates new features by raising the powers of the original features
          -  primarily used to capture the non-linear relationship between the features and the target variables.
        Exponential transformation 
          - used when the relationship between the features, and the target variable exhibits an exponential pattern,
          - example: like what you see in stock prices, which show exponential growth or decay.

      Data Binning (data discretization or data bucketing)
        - Data binning is a process of transforming a continuous variable into discrete values grouped 
          into smaller number of bins or intervals.  

        Data Binning Strategies

          Equal-width Strategy (uniform-width binning)
            - spread the data across multiple bins, each of them having the same width.
            - This strategy is suitable if the dataset is symmetric and evenly distributed
      
              bin_width =  data_Range / (no_of_bins)
      
          Equal-frequencey Strategy (quantile binning)
            - data is divided into bins, such that each bin contains the same number of observations.
            - all the bins have the same frequency, meaning they'll have the same number of data elements.
            - ideal for skew datasets
      
              bin_width =  (no_of_elements) / (no_of_bins)

          K-Means binning
            - uses K-means clustering algorithm to partition continuous data
            - used when data distribution is not uniform
          Decision tree binning
            - builds a decision tree model on the continuous variable, and in the process, the algorithm 
              will determine the split points best separate the data into different branches of the tree
            - This technique is effective when the relationship between the continuous variable and the 
              target variable is non-linear


      Encoding
         - a process of transformationg categorical data into numerical data
         Ordinal Encoding
           - used to encode ordinal data
         one-hot Encoding
           - used to encode nominal data
         Binary Encoding
           - used when there are a large number of features

      Dimensionality Reduction
        - technique of reducing the number of features with minimum information loss

        Dimensionality reduction Techniques
          Feature Selection
            - select a subset of features by discarding irrelevant or redundant features.
            Filter method
               - takes only the subset of the relevant features.
              Techniques:
                Variance Thresholding
                Chi-square test
            Wrapper method 
              - uses the peformance of an ML model to determine the relevance of features subsets
              - evaluates subsets of features by training a machine learning model on different combinations of 
                features and selecting the subset that results in the best performance.
              - wrapper methods are computationally intensive compared to filter methods, because we need to train 
                and evaluate a model.
              Techniques:
                Forward Selection
                Backward Elimination
                Recursive Feature Elimination
            Embedded method
              - selects features as part of the model training process
              - incorporates feature selection as part of the model training process itself.
              - Instead of evaluating feature subsets independently, these methods select features while training 
                the model.
              techniques:
                Lasso Regression 
                Ridge Regression,
                Gradient Boosting Machines 
                Elastic Net

          Feature Extraction
            - transforms the original features into lower dimensional space while preserving the most important 
              information present in the dataset
            Linear dimensionality reduction subcategory
              - projects data to lower dimensionality using linear transformation
              Techniques
                Principal Component Analysis (PCA)
                Linear Discriminant Analysis
            Nonlinear dimensionality reduction subcategory
              - captures complex relationships in a non-linear fashion
              Techniques
                T-Distributed Stochastic Neighbor Embedding
                Isometric Mapping

      SageMaker - Processing Options
        SKLearnProcessor
          - process scikit-learn scripts
        PySparkProcessor
          - run PySpark scripts as processing jobs

------------------------------------------------------
3.11 Quiz: Performing Feature Engineering

Question 8

  Your company has approached you for advice on extracting features from a large set of images. There are no limits on 
  the hardware or storage. Which technique would you recommend?

choices:
  - The Mean Pixel Value of Channels.     -> incorrect answer
  - Convolution Neural Networks.          -> Correct Answer
  - Extracting Edge Features.
  - GrayScale Pixels Values as Features.
Sorry!

   This is recommended to extract features from color images. Since this is a traditional computer vision technique, 
   it may consume a lot of resources.

Correct Answer
   Though this is a deep learning technique, we don’t have a limit on resource usage.


Question 2  (redo 1)

  You are analyzing a dataset and have discovered that the data is sparse and uneven. You have been asked to convert 
  the continuous values into discrete values. Which binning strategy will you use, and why?

Choices:
  Equal-binning strategy.

  Equal-frequency strategy.                                                            <--- Correct Answer

  Equal-width strategy.

  Use the equal-width strategy by turning on the handle outlier option.
Good work!

  Equal frequency will instead guarantee that every bin contains roughly the same amount of data, which is usually preferable.


Question 4 (redo 1)

  What is an important limitation of using the bag-of-words technique to extract features from a large dataset containing 
  many unique values?

  Using this technique will result in a model with poor accuracy.

  The bag-of-words technique is a scaling technique and cannot be used to extract features.

  The bag-of-words technique captures only contextual information and not the frequency of words.

  The bag-of-words technique will result in high-dimensional features.                    <--- Correct Answer
Good work!

  This is a common limitation we must consider while using this technique.

Question 5 (redo 1)

  You are currently working on a dataset, and upon initial analysis, you find that the data contains outliers. How would you approach scaling this dataset?

  Use RobustScaler and turn on the detect outlier option.

  Use maxAbsScaler and turn on the detect outlier option.

  Use RobustScaler and scale the data using IQR.                                           <--- Correct Answer

  Use MinMaxScaler and scale the data using IQR.
Good work!

  Since the data contains outliers, using IQR is the better option.

------------------------------------------------------

Chapter 4 Analyze and Visualizeing Data for ML

------------------------------------------------------
4.1 Understanding Probability Distributions


  Chapter 4 focuses on AWS ML Exam task:
    2.3 Analyze and visualize data for machine learning.

  probability distribution
    - a mathematical function that gives the probabilities of occurrence of different possible outcomes 
      for an experiment.

  Why Probability Distribution
    - Find the hidden patterns in the dataset
    - Express uncertainity
      - Many ML algorithms like Bayesian neural networks uses probability distribution to express uncertainty
    - Determine goodness-of-it
      - Probability distribution also expresses how well the observed data fits against the model predictions.
      - Techniques like cross-validation and hypothesis testing rely on probability distributions.
    - integral aspects in performing Monte Carlo simulations
      - Monte Carlo methods are computational techniques used extensively in the field of finance and engineering, 
        and probability distributions are integral aspect of Monte Carlo methods.


  Transformation Types
    Types of data
      Discrete
        - a limited number of possible outcomes
        - examples: rolling the dice or picking a car
        Discrete Distribution 
          common types:
            Bernoulli distribution
            Binomial distribution
            Poisson distribution
      Continuous
        - an infinite number of outcomes
        - examples: measuring the temperature over a period of time or the weight of a person.
        Continuous Distribution
          common type
            Normal distribution
            log-Normal distribution
            Exponential distribution


  Discrete Distributions:
    Bernoulli Distribution
      - an event with a single trial with exactly two possible outcomes
      - The graph of a Bernoulli distribution is a simple bar chart with two values.
        The first bar indicates the outcome 1 (value of P).  The second bar indicates the outcome 2 (value of 1 - P).
  
    Binomial distribution
      https://medium.com/swlh/binomial-vs-bernoulli-distribution-dd9197c418d
      - repetition of multiple Bernoulli events
      - If Bernoulli distribution is an event with a single trial with exactly two possible outcomes, then binomial 
        distribution is nothing but repetition of multiple Bernoulli events.
      - coin toss example: if you repeat the trial n number of times where each trial is independent, the probability 
        of heads or tails is same for all the trials.
      - A binomial distribution is better represented as a histogram.
  
    Poisson distribution
      - the probability that an event will occur within a specific time
      - The rate of occurrence is known, but the actual timing of the occurrence is unknown.
      -  example: Predicting a hospital receiving emergency call. They know on an average they receive two 
         calls per day, but they cannot predict the exact timings.
      - This distribution relies on one parameter, X, which is the mean number of events.


  Continuous Distributions:
    Normal distribution
      - measure and visualize symmetrically distributed data with no skew
      - example:students' scores: most of the students' scores might range between 70 and 90, which forms the cluster at the center.
        Some of the top and bottom performers contribute to the tail at both the ends.  Parting the students' score will result 
        in a bell-shaped curve representing the normal distribution of the scores.

    Log-Normal distribution
      - derived from a normally distributed data and represents its logarithmic values
      - often used in financial data to understand future stock prices based on past trends.
      - lognormally distributed data does not form a symmetric shape but rather slants or skews more towards the right.

    Exponential distribution
      - models the time elapsed between the occurences of two events
      - reusing hospital receiving an emergency call example: exponential distribution models a time interval between two calls.
      - Exponential distribution relies on one parameter, R, which is the rate of occurrence.


------------------------------------------------------
4.2 Visualizing Distributions and Relationships Using Graphs


  Visualizing relationships
    - Visualizing the relationship between various features in our dataset helps uncover patterns, 
      identify trends, and determine the dependencies between the features.
    - provides a correlation between the various features and help find outliers

  Scatter Plot
    - shows the relationship between two numerical features plotted on the x and y access
    - graphs plot points along the x and you axis for two values

  Scatter Plot - Benefits and Limitations
    Benefits
      - Can detect non-linear patterns
      - effectively spot outliers
    Limitations
      - showing more than two features may make the chart less readable

  Bubble Plot (bubble charts)
    - effective to present the relationship betwe three features
    - the x and y axes represents the first two numerical features, and the bubble size represents the 3rd.
    - graphs plot points along the x and y axis for 3 values where bubbles size is the 3rd value


  Bubble Plot (bubble Chart) - Benefits and Limitations
    Benefits
      - can compare 3 or more features
      - the bubble size can highlight the patterns and trends in the data
    Limitations
      - it is difficult to distinguish individual data points, especially when they overlap
      - lacks interactivity like zooming, filtering, or drill-down capabilities

  Data Distributions
    - Studying the data distribution provides insights like the skewness, mean and median of a specific 
      feature in the dataset. 
    - shows how our data is grouped or clustered over certain intervals

  Distribution graph Types

    Histogram
      - shows the magnitude of a value in the form of a line or bar format
      - they group the data into bins or buckets and perform counts on top of them
      - these graphs put values into buckets or bins and determine a measurement (amount, frequency,
        duration, density, etc)
         - example: quantity vs price with 50 bins

    Histogram - Benefits and Limitations
      Benefits
        - allows you to see the frequency distribution quickly
        - can present categorical data
      Limitations
        - the number of bins determines the granularity of the data
        - not well-suited for detecting outliers
           - outliers can completely distort the distribution and affect the interpretation of the data.

    BoxPlot (box-and-whisker plot or whisker plot)
      - shows the minimum, first quartile (25% of data), meadian, third quartile (75% of data), and the maximum 
        values of a feature
      - the box represents the interquartile range (IQR) which is the range between the first and the third quartiles.
      - The whiskers extend from the top and bottom of the box to the highest and the lowest values that are within 1.5 
        times the IQR.
      - Any values that fall outside the top or bottom range are considered outliers


                      IQR = Q3 − Q1 = q_n (0.75) − q_n(0.25) 
      
                      Q1-1.5IQR   Q1 median  Q3   Q3+1.5IQR
                                  |-----:-----|
                  o      |--------|     :     |--------|    o  o
                                  |-----:-----|
                   
                  ^      ^        ^     ^     ^         ^      ^
                  |      |        |     |     |         |      |
            outlier(s)   min      |   median  |       max      extreme values
                         value    |   value   |       value    outlier(s)
                                lower        upper
                             quartile        quartile
                               value         value

       - Boxplots are ideally used to summarize the data distribution of a specific numeric feature.


    Boxplot - Benefits and Limitations
      Benefits
        - can present a clear summary of large amounts of data
        - can be used to quickly detect outliers
      Limitations
        - the exact data point value is not known,
        - not good at representing the skewness or the asymmetric nature of the data.

    Heatmap
      - a visualization technique to represent the numerical data distribution with the color intensity
        representing the magnitude of its value

    Heatmap - Benefits and Limitations
      Benefits
        - great way to identify the patterns in the data quickly
        - color gradient makes it easy to spot the high and low values
        - very effective in communicating large amounts of data in a compact fashion
      Limitations
        - users may find it challenging to interpret the data, especially if unfamiliar with the color legends
          used in the visualization
        - not an effective technique to visualize sparse data or unevenly distributed data
        - primarily designed for numerical data and may not be well suited for presenting categorical information

------------------------------------------------------
4.3 Visualizing Comparisons and Compositions Using Graphs
   

  Visualizing Comparisons
    - Visualizing comparisons in our data can provide a static snapshot of how different variables in our 
      dataset compare with one another and show how they change overtime.

  Comparison Charts

    Bar Chart
      - A graphical representation of data the uses bars to represent the frequency, count, or value of 
        different categories present in the feature
      - graphs that use lines (bars) to mark single variable values. 
      - provides a way to lookup and compare values

      Barchart vs Histogram:
        - histograms are used to represent the continuous data,
        - bar charts are used to represent the categorical data

      Bar Chart example1:
        US muscle cars (camaro, covette, mustang) vs price
        - take average price vs car name and show 1 bar for each car
         - bar charts can be vertical (car - x-axis; price y-axis) or horizontal (price - x-axis; car y-axis) 

      Bar Chart example2 :
        - comparing the number of star ratings for a mobile application in the app store
        - a bar for each star rating (1-star, 2-star, ..., 5-star)

      Bar Chart - Benefits and Limitations
        Benefits
          - easy to compare values of different categories visually
          - viewers don't need any prior knowledge about data to interpret the visualization
        Limitations
          - Cannot be used to represent continuous data
          - as the number of categories increase, the chart can quickly be cluttered

    Line Chart
      - graphs that use lines to show one or more variables changeing over time
      - that is, the x-axis shows when we measure the data, and the y-axis shows the actual value 
        during the measurement.

      Line Chart example1:
        US muscle cars (camaro, covette, mustang) vs price over time
        - price on Y-axix, date on x-axis, and 1 line per car for price over time created by
          plotting the car price points over time and connecting points with different color lines 
          for each car

      Line Chart example2 :
        - Ploting the price of Bitcoin over the past 3 years

      Line Chart vs Bar chart Summary 
        - can use bar charts to represent a single point in time, or a lookup value for our data,
        - can use line charts to show how our data changes over time.

      Line Chart - Benefits and Limitations
        Benefits
          - simple and easy to interpret
          - can be leveraged to compare trends in multiple categories at the same time
        Limitations
          - cannot be used to visualize categorical data
          - not effective against sparse datasets with missing values
          - an outliers in the data can easily distort the chart making it difficult to interpret

  Visualizing Compositions
    - Visualizing the composition of our data shows the individual elements that our data is made of.
    - For example, we may want to learn about the sales per region or the geographical location of all 
      the employees in a department.

  Composition charts

    Pie Chart
      - used to express a part-to-whole relationship in our data
      - example: total number of employees in each region dataset.  The pie chart visually can shows the employee 
        distribution across their geography, and each pie shows a percentage of the total number of employees.

      Pie Chart - Benefits and Limitations
        Benefits
          - simple and intuitive representation of data that is easy to understand
          - helps distinguish the smallest and largest categories that make up the data
        Limitations
          - as the number of categories increase, the chart can easily become cluttered
          - note well suited for visualizing time series data or any data that changes over time

    Stacked Bar Chart (Stack Column Chart)
      - a stacked column chart not only shows the magnitude of the value but the individual categories that
        make up the total value

       Stacked Bar Chart example:
         - dataset shows the number of employee for each department in each region
         - stacked bar chart where the size of each bar represents the total number of employees in each department,
           and the individual segments (in different colors) that make up the bar represents the number of employees 
           in each region.

      Stack Bar Chart - Benefits and Limitations
        Benefits
          - effectively communicates the contribution of each subcategory that make up the category
          - the height of each bar clearly communicates the magnitude of importance of a category
        Limitations
          - as the number of categories increase, the chart can easily become cluttered
          - not effective to represent data with negative values

------------------------------------------------------
4.4 Understanding Descriptive Statistics

 Descriptive statistics
   - Descriptive statistics deals with collecting, organizing, and interpreting the data.  
   - This is not about making predictions, but to understand and describe the data.

    
    descriptive statistics vs inferential statistics
      - inferential focuses on making inferences about a population based on a sample and makes predictions, 
        generalizations, and estimations.


  Descriptive statistics Types
    Measures of Frequency
      - The first one is a measures of frequency that is used to summarize the values in a dataset.
      Frequency:
        - number of occurrences of a specific value in a dataset.
        - For example, considering the sample set of data showing the student scores in a class, we can group 
          all the students with scores 95 and above as grade A, 85 to 95 as grade B, and less than 85 as grade C.
      Mode
        - the value that occurs the most in a dataset.
        multimodel dataset
          - If a dataset has more than single value that occurs frequently, it is called a multimodal dataset
    Measures of Central Tendency
      - the concept revolves around finding a central data that can best summarize the entire data set.
      Mean
        - average of all the values in the dataset
        - a mean value may get skewed in the presence of an outlier, 
        Outlier 
          - data science term for one-off data
      Median
        - the value in the  middle of a sorted dataset
    Measures of variability
      - shows the spread or dispersion of values in a dataset.  
      - Some of the commonly used metrics include the range
      Range
        - the difference between the maximum and the minimum value in the dataset.
      Standard Deviation
        - Average distance of each value from the mean of a dataset

        std deviation =  ( (1/n) SUM (x_i - x^)**2 ) ** 1/2    Where SUM is from i=1 to i=n  

            X_i  = individual observations
            x^ = mean value

      Variance
        - average squared deviation of each value from the mean

        variance  =  (1/n) SUM (x_i - x^)**2     Where SUM is from i=1 to i=n  

            X_i  = individual observations
            x^ = mean value

  Data Visualization related Metrics

    Skewness
      - a measure of the asymmetrical nature of the data distribution
      no skewed data
        - normally distributed data whose median value is at the center of the curve has no skew
      negatively skewed data
        - data is skewed to the left which means the tail on the curve's left side is longer.
        - The mean value of a negatively skewed data will be less than the mode.
        Note: mode is the value that occurs the most in the dataset
      positively skewed data 
        - the data is skewed to the right, which means the tail on the curve's right side is longer.
        - The mean of a positively skewed data is greater than the mode

    Skewness Formulas

      Pearson first coefficient of skewness  (Pearson Mode Skewness)
        https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/pearsons-coefficient-of-skewness/
        - If the mean is greater than the mode, the distribution is positively skewed.
        - If the mean is less than the mode, the distribution is negatively skewed. 

            ( Mean - Mode ) / ( standard deviation)

      Pearson second coefficient of skewness
        - If the mean is greater than the median, the distribution is positively skewed.
        - If the mean is less than the median, the distribution is negatively skewed. 

            3( Mean - median ) / ( standard deviation)

        - A negative value indicates that the data is negatively skewed
        - a positive value indicates that the data is positively skewed.


      Kurtosis
        https://www.scribbr.com/statistics/kurtosis/
        - a measure of the tailedness of a distribution. Tailedness is how often outliers occur. 
        - a measurement of the outliers existing in the data
        - A large tail mean that the data likely has a large outliers and a small or thin tail means that 
          there are not many outliers in the data.
        Excess kurtosis 
          - the tailedness of a distribution relative to a normal distribution
          Based on the data distribution, there are three broad categories:
            leptokurtic distribution
              - used to refer a heavy tail distribution, is where kurtosis is more than the normal distribution.
              - Distributions with high kurtosis (>3) (fat tails) 
              - high outlier frequency 
            Mesokurtic distribution
              - used to refer a normally distributed data
              - Distributions with medium kurtosis (3) (medium tails)
              - medium outlier frequency 
            Platykurtic distribution 
              - used to refer a short tail distribution, is where kurtosis is less than the normal distribution.
              - Distributions with low kurtosis (<3) (thin tails) 
              - low outlier frequency

      Correlation
        - defines how strongly two features are related.
        Correlation Coefficient
          - used to measure the correlation between two features using the letter 'r' that ranges between -1 to +1.
             -1 <  r  < +1
          - A value of zero indicates no relationship between the two features.
          positive correlation
          - When r is leaning towards positive one, the relationship is referred as a positive correlation 
            where one feature gets larger as the other feature increases its value.
          - example: previously, how the salary increased as the age of the employees went up.
          negative correlation
          - When r is leaning towards minus one, the relationship is referred as a negative correlation, which 
            means as one feature gets smaller, the other feature increases its value.
          - example: people who are paid more money tend to pay off their credit card bill and have less debt.
  

------------------------------------------------------
4.5 Performing Cluster Analysis Using the Elbow Method

  Cluster analysis 
    - a statistical technique that groups similar observations or data points into clusters or groups 

  Cluster analysis Use Cases
    Market Segmentation
      - grouping customers based on their preferences, behaviors and tailor marketing strategies to suit them
    customer Segmentation
      - Group customers based on their purchase history and customize the offerings
    Detect abnormal behaviors
      - fraud detection and network intrusion


  Cluster analysis Technique Categories
    - three major categories based on the number of observations, presence of outliers, and the features in the dataset
    Paritioning clustering
      - user must specify the number of custers indicated by a variable 'K'.
      - each cluster will contain at least one observation and they will be unique
      k-means
        - popular partitioning clustering algorithm
    Hierarchical clustering
      - user is not required to specify the number of clusters 
      - cluster assignments are determined by creating a hierarchy and cutting the tree at a specified depth
      - creating a hierarchy either in a bottom-up approach or a top-down approach, and cutting the tree at 
        a specified depth
      Agglomerative clustering 
        - a popular hierarchical clustering bottom-up approach algorithm
    Density-Based clustering
      - user is not required to specify the the number of clusters
      - cluster assignments are determined based on the density of the data points
      - This technique works based on a distance-based parameter that determines the distance between 
        the two points.
      DBScan 
        - popular density-based clustering algorithm

       DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 
         https://www.newhorizons.com/resources/blog/dbscan-vs-kmeans-a-guide-in-python
         - It is a density-based algorithm that groups together points that are close to each other based on a density criterion. 
         - Points that are not part of any cluster are considered noise. 
         - DBSCAN is particularly useful when dealing with datasets that have irregular shapes and different densities.

  K-means clustering algorithm
    - determine the number of cluster - the value of 'k'
    - K number of centroids are randomly selected from the dataset.
      - a centroid represents the center of a cluster and is calculated as a mean position of all the 
        data points within the cluster.
    - Next, compute Euclidean distance between the data point and the nearest centroid.
    - If needed, the data point is reassigned to the nearest cluster.
    - After reassignment, the new centroid is computed again.
    - This computation and reassignment is repeated until the centroid positions no longer change.
    k-means Challenge
      - determining the optimal number of clusters, and we will use the elbow method for that purpose.

  Elbow Method
    - start with a value of K (number of clusters) = 1
    - Compute Within-cluster sum of squares (WCSS)
        - WCSS is a sum of the square distance between each data point and the centroid in a cluster.
        - Typically, WCSS is high when K is one.
    - Repeat  the process by increasing the value of 'k' and computing the WCSS
       - As the number of cluster increases, the value of WCSS decreases.
       - plot 'number of clusters' (x-axis) vs 'WCSS' values (y-axis)
       - should see a rapid decline in the WCSS value upto the 'elbow' point
       - beyond the 'elbow point', the WCSS value will be almost parallel to the 'x-axis'
    Optimal number of clusters ('k' value)
      - 'elbow point' is the optimal number of clusters
    Elbow Method heuristic method
      - this is a heuristic method and there are scenarios where we may have multiple potential 
        elbow points, so it is important to use this value in conjunction with domain knowledge.

------------------------------------------------------
4.6 Performing Cluster analysis Using the Hierarchical Technique


  Resources:

    Github Link - clustering
      https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning---Specialty-MLS-C01-Exploratory-Data-Analysis/tree/main/clustering

    Extracting Image Features Jupyter Notebook:
      \pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-exploratory-data-analysis-course\notebooks\clustering\agglomerative.ipynb


  Hierarchical clustering technique
    - a clustering technique that groups data together based on similarities and builds a hierarchy of clusters.
    - These clusters are merged into larger clusters, forming a tree-like structure called a dendrogram.
    dendrogram
      - A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical 
        clustering calculation.

    Bottom-up Approach (agglomerative hierarchical clustering)
      - each data point is considered a cluster
      - clusters are merged recursively based on the distance between the two data points
      linkage methods
         - strategies for calculating the closest distance between two clusters
         Single Linkage
           - shortest distance between any two points in the two clusters
         Complete Linkage
           - the maximum distance between any two points in the two clusters



  SciPy library
    - an open source Python library used in linear algebra, interpolation, and image processing.

  scipy.cluster.hierarchy.dendrogram
   dendrogram(Z, p=30, truncate_mode=None,..)
     - Plot the hierarchical clustering as a dendrogram.
     - The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton 
       cluster and its children. 
     - The top of the U-link indicates a cluster merge. The two legs of the U-link indicate which clusters were merged. 
       The length of the two legs of the U-link represents the distance between the child clusters. It is also the 
       cophenetic distance between original observations in the two children clusters

     Parameters:

       Z : ndarray
         - The linkage matrix encoding the hierarchical clustering to render as a dendrogram. 
         - See the 'linkage' function for more information on the format of Z.
     
  scipy.cluster.hierarchy.linkage
    linkage(y, method='single', metric='euclidean', optimal_ordering=False)
      - Perform hierarchical/agglomerative clustering.
      - The input y may be either a 1-D condensed distance matrix or a 2-D array of observation vectors.

  Demo: Performing bottom-up hierarchical clustering

    Code:

            # import libraries include SciPy
        >>> import pandas as pd
        >>> import matplotlib.pyplot as plt
        >>> import scipy.cluster.hierarchy as sci

            # read in employee dataset
        >>> employee_df = pd.read_csv('Employee.csv')
        >>> employee_df.head()

            # reduce dataset to 'age' & 'salary'
        >>> employee_df = employee_df[['age','salary']]
        >>> employee_df.head()

            # use agglomerative hierarchical clustering with 'single' linkage, then plot the dendrogram
        >>> dendrogram = sci.dendrogram(sci.linkage(employee_df, method='single'))
        >>> plt.show()

            # use agglomerative hierarchical clustering with 'complete' linkage, then plot the dendrogram
        >>> dendrogram = sci.dendrogram(sci.linkage(employee_df, method='complete'))
        >>> plt.show()


  Dendrogram diagrams
   - The vertical axis of the dendrogram represents a similarity between the clusters.
   - The height at which the branches merge indicates the distance at which the clusters are merged.
     - Longer the vertical lines, greater the distance between them.
   cluster
     - Each branch in the dendrogram represents a cluster,
   node
     - each node represents a point at which the clusters are merged.
   number of clusters
     - To determine the number of clusters, we can cut the dendrogram at a specific height 
       determined by the business.
     - The horizontal line across the dendrogram defines the number of clusters.

  Single Linkage vs Complete Linkage
    - the single linkage method emphasizes the proximity between the clusters and creates long, 
      unchaining clusters
    - complete linkage method emphasizes the overall structure of clusters and produces more compact ones.
    - If your data doesn't contain outliers, then complete linkage method is preferred over 
      single linkage method.

  Top-down hierachical cluster approach (Divisive technique)
    - opposite of agglomerative hierarchical clustering
    - considers all the data points as a single cluster at the start
    - computes the measure of dissimilarity between the data points
    - the one with the highest dissimilarity is split into two clusters
    - the process is repeated recursively until each data point is in its own cluster

  Hiearchical Clustering issue
    - demands high computational power and memory requirements especially if the dataset is large

------------------------------------------------------
4.7 Building a Visualization Dashboard with Amazon QuickSight

  QuickSight
    - self-service BI solution powered by AWS that allows users to visualize and analyze data.
      irrespective of their technical expertise

  QuickSight Features
    - seamless integration with with various data sources like S3, Redshift, Athena, SaaS 
      applications like Salesforce, and third party databases like MySQL and PostgreSQL
    - allows you to analyze data from multiple sources simultaneously
    - no infrastructure management
      - it runs on a serverless architecture and scales dynamically to meet the growing demand
    - automatic data refresh
      - visualizations and dashboards are automatically updated with the latest data.
    - supports many data formats including: CSV, JSON, Parquet, and Excel.
    - collaboration and sharing
       - The visualizations created with QuickSight can be shared with other team members or 
         stakeholders by setting up proper access control mechanisms.
    - security and compliance
      - supports encryption at rest and is compliant of various industry standards like GDPR and HIPAA.
      - enables robust security features to ensure confidentiality, integrity, and availability of data.

  QuickSight Benefits
    - low cost of ownership
    - Blazing fast performance at scale with SPICE (Super Fast Parallel In-memory Calculation) engine.
       - SPICE allows thousands of users to parallely perform interactive analysis,
       - (SPICE) automatically replicates data for high availability and enhanced performance.
    - data insights is powered by machine learning 
       - allows organizations to find hidden trends and detect outliers.
    - support mobile applications (both iOS and Android platforms)
    - Programmatic access to BI assets

  QuickSight Terminologies
    Datasets
      - A dataset is a subset of a data source that is used as a basis for creating the visualizations.
      - once you create datasets, we create an analysis
    Analysis
      - An analysis is a basic workspace for creating visualizations.
    Sheet
      - An analysis consists of one or more sheets, which are the individual pages that display visualizations.
    Visuals
      - A visual is a single chart or a graph within a sheet.
    Dashboards
      - dashboards are read-only snapshots of analysis that are designed for sharing insights and interactivity.


  Demo: Building a dashboard using QuickSight

    AWS Console -> Quicksight
       -> Sign up for quicksight (note: 4 free users for 30 days, $250+/mo)
       -> after QS account is created

       QuickSight -> Datasets (left tab)  -> includes 4+ sample datasets
           # if you choose to upload your own data sources
              -> New Dataset -> Source: s3 -> bucket -> ...
           # already upload 'Employees_encoding.csv' dataset
            -> select "Employee_encoding.csv" -> Edit -> 
               -> From "focus" (left tab) -> All Fields -> select "employee_id" <right-click> -> exclude
           # note you can change the type of data, but not the actaul data (read only)
           -> click "Save & Publish" <upper right>
           -> click "Publish & Visualize" <upper right>
              # creates a new sheet with Autograph
              # AutoGraph: automatic visualization feature that leverages machine learning algorithms
              #      to generate the most appropriate and effective visualizations for users' data.
              -> drag and drop gender
              # since only 2 unique values, it creates a bar chart with 'male' and 'female' both with count=50
              -> drag and drop 'age'
              # creates a clustered bar chart with male and female age clusters
              #   male employees' age range from 31 to 42 and the female employees' age range from 26 to 32
              -> select 'filter' icon <upper left>, under filter tab -> add 'age' -> 
                 -> Filter condition: greater than, Minumum Value: 40 -> Apply
                 # dataset has only 1 employee older than 40
                 -> Filter condition: less than, Minumum Value: 40 -> Apply
              -> select 'visualize' icon <upper left>, select 'pie chart' icon 
                   under Visual -> Pie Chart tab -> add 'department' to "Group/Color"
                   # Pie chart show employees grouped by departments
                   under Visual <tab> -> add Box plot -> add 'salary' to "value"
                   # displays box plot of salaries which are between 50k and 128K
                   under Visual <tab> -> add Scatter plot -> add 'age' to 'x-axis and 'salary' to "y-axis"
                   # Scatter plot shows a drastic difference in the salaries for the same age
                   -> click "Publish" <upper right> -> Publish new Dashboard as: Employee Dashboard -> Publish
                   # to share:
                   -> click 'share'

------------------------------------------------------
4.8 Gaining Deeper Insights Using Amazon QuickSight ML Insights 


  QuickSight ML Insights - Value Proposition
   Two major features:
     Forecasting
       - predict business metrics, perform interactive analysis and discover hidden insights
       - uses Random Cut Forest algorithm to to detect trends and impute missing values
     Anomaly Detection
       - detects the presense of outliers in the data
       - uses Random Cut Forest algorithm to gain deep insights.
   Autonarratives
     - Effectively share the story behind the data
        - helps you build powerful dashboard with embedded narratives to share the story behind your data.
      - with ML insights, you no longer need to rely on manual expertise to analyze the data to 
        find deeper insights

  QuickSight ML Insights - Data Requirements
    - Must have at least one metric dimension
      - example: like the total number of employees, total revenue, etc.
    - Must have at least one categorical dimension
      - example: like the gender or department
    - Must have at least date dimension to perform anomaly detection and forecasting
      - example: like employee join date, their date of birth, etc.
    - Must contain a minimum of 15 observations for effective model training
    - Must contain historical data for optimal forecasting results



  Demo: QuickSight ML Insights

       QuickSight -> Analyses (left tab)  -> select "Employee_encoding.csv analysis" # from previous lesson
           -> select "Insight" icon <top left> -> Suggested insights <tab> 
           # QuickSight immediately shows some not-so-obvious details about our dataset.
           #   Behind the scenes, QuickSight has run the ML algorithm on our data to produce these additional 
           #   insights.  you can see that sales, finance and marketing have more employees compared to other 
           #   departments.  There are more employees aged 31, 33 and 29. ...
           -> select "top 3 salary" insight -> click "+" to add insight to "sheet 1"
           -> select "top 3 departments" insight -> click "+" to add insight to "sheet 1"
           # which to anamoly detection
           -> select "Insights" icon <top left> -> Visual <tab> -> + add -> Computation type: Anamoly detection 
              -> select
           # reports: "you need to add or remove fields" -> insight requires 1 dimension in Time and
           #           1 measured in values and 1 dimension in Categories
           #   missing date dimension
           -> in upper right corner of new visualization, select "delete" 
           # publish updated dashboard
           -> click "Publish" <upper right> -> select "Replace an existing Dashboard": Employee Dashboard -> Publish
           
       QuickSight -> Analyses (left tab)  -> select "People Overview Analysis" # sample analysis
         # shows the monthly compensation data of various job levels.  This dataset has a date feature.
         -> select "Insights" icon <top left> -> Visual <tab> -> + add -> Computation type: Anamoly detection 
           -> select
           -> in "Visual" -> Insight" tab -> drap and drop "date" to "Time" field,  
                 "monthly compensation to "Values" Field
                 -> in "insight" sheet -> click "get started"
                 -> save -> run now
                  -> click "explore Anomalies"
                  # one anomaly detected on November 20th, 2016.
                  -> switch back to analysis, publish results and replace the existing dashboard.
                  -> re-open the "People Overview analysis
                  # see how see how autonarratives works
                  -> Insight sheet -> upper right corner, right click on the "3 dots" -> select "Customize narrative"
                  # shows: dynamic computation of narratives using decision trees and for loops

                  # if want to add your organization name to this information instead of showing the expected value.
                  # You can remove the expected value and add the name of organization directly to the narrative,
                  -> save changes
                  -> In "edit narrative" -> computations <tab> 
                     # ML insights gives you an easy option to choose from the list of computations to customize 
                     # and enhance the narrative even further.  You can also use additional function to enhance 
                     # and format the narrative.
                     -> cancel
                     -> publish and replace exiting
                       # new anamoly detection is included in the sheet with a customize narrative


------------------------------------------------------
4.9 Visualizing data with Python with Python Libraries and Amazon SageMaker

About this lab

Imagine you are the data engineer, and you have been assigned to produce visualization charts and answer the 
following business questions using the dataset provided. Your data contains both categorical and numerical data.

 1. Distribution of all employee ages.
 2. Distribution of employee ages in each department.
 3. Compare the salary ranges of employees by department.
 4. Relationship between employee age and salary.
 5. Distribution of employee salary across all departments.

In this hands-on lab, you will create charts using Seaborn and matplotlib Python libraries.
  - Learning objectives
  - Launch SageMaker Notebook
  - Load Libraries and Prepare the Data
  - Perform Univariate Analysis
  - Perform Bivariate Analysis

   ------------------------------------------------------
Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. If you are prompted to select a kernel, please choose conda_tensorflow2_p310.
    In the search bar, navigate to Amazon SageMaker.
    Under the Notebooks section in the left menu, click Notebook Instances.
    Confirm the notebook is marked as InService. If so, click the Open Jupyter link under Actions.
    Click on the visualization.ipnyb file.

Load and Prepare the Data

    If this is your first time running a notebook, each cell contains Python commands you can run independently.
    Click the first cell to highlight it, and use the Run button at the top to execute the code. An asterisk inside the square braces, (In [*]), indicates the code is running. You will see a number, (In[1]), once the execution is complete. This cell uses the Pandas library and reads the raw data from Employee.csv.
    Highlight and Run the second cell to read the input file into a dataframe.

Perform Univariate Analysis

    Highlight and Run the first cell under 3) Visualizing Univariate data.

    In the next cell, paste the following Python code below the last line and Run.

    plt.xlabel('Department')
    plt.ylabel('Employee Count')
    plt.title('Distribution of employees by department')
    plt.show()

    In the next cell, insert the following code above the last line and Run.

    sns.countplot(data=employee_df, x='department', hue='gender')

Perform Bivariate Analysis

    In the first cell under 4) Visualizing Bivariate Data, insert the following code above the last line and Run.

    sns.barplot(data=employee_df, x='department', y='salary')

    Our next step is to show the descriptive statistics of employee's salaries across all the departments. We will use a boxplot instead of a barplot. Insert the following code above the last line and Run.

    sns.boxplot(data=employee_df, x='department', y='salary')

    Finally, to show the relationship between an employee's age and salary, insert the following code above the last line and Run.

    sns.scatterplot(data=employee_df, x='age', y='salary')

   ------------------------------------------------------
  seaborn.countplot
    seaborn.countplot(data=None, *, x=None, y=None, hue=None, ...)
      - Show the counts of observations in each categorical bin using bars.
      - A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. 
        The basic API and options are identical to those for barplot(), so you can compare counts across nested variables.

      Parameters:

       data : DataFrame, Series, dict, array, or list of arrays
        -  Dataset for plotting. If x and y are absent, this is interpreted as wide-form. Otherwise it is expected to be long-form.
     
        x, y, hue ; names of variables in data or vector data

        Inputs for plotting long-form data. See examples for interpretation.
      

    Code: Visualization Lab code

            # Visualizing Data with Python Libraries and Amazon SageMaker

            # Introduction
            # In this lab, you will learn how to visualize data using various charts like histogram, bar chart, 
            # scatter plot and box plot. The provided dataset contains a list of employees with their gender, age, 
            # salary, and the department that contains both numerical and categorical data.

            # # How to Use This Lab
            # We have already imported the required libraries and imported the dataset for you. You need to use 
            # the seaborn library to visualize the data. You need to choose the right type of chart to address the 
            # given business problem. Please pay attention to the cells with a #TODO header where you need to enter
            # the code. You can always use our lab guide if you are stuck.

            # # 1) Import the Libraries

        >>> # import required libraries.

        >>> import pandas as pd
        >>> import seaborn as sns
        >>> import matplotlib.pyplot as plt

            # 2) Read the Data

        >>> # Read the dataset and display first few rows.

        >>> employee_df = pd.read_csv('Employee.csv')
        >>> employee_df.head()


            # 3) Visualizing Univariate Data

        >>> # Show the distribution of all the employee's age in your oganization
        >>> sns.histplot(data=employee_df, x='age')
        >>> plt.show()

        >>> # Show the number of employees in each department in your organization.
        >>> # TODO: Use the matplotlib's "xlabel", "ylabel", and "title" function 
        >>> # to set the x-axis as "Department", y-axis as "Employee Count" and title as "Distribution of Employees by Department"
        >>> sns.countplot(data=employee_df, x='department')
        >>> plt.xlabel('Department')
        >>> plt.ylabel('Employee Count')
        >>> plt.title('Distribution of employees by department')
        >>> plt.show()

        >>> # Show the number of male and female employees in ech department in your organization
        >>> # TODO: Modify the countplot function to show the gender. 
        >>> # Tip: Use the hue attribute that will create separate bars for each category on the x-axis.

        >>> sns.countplot(data=employee_df, x='department', hue='gender')
        >>> plt.show()


        >>> # # 4) Visualizing Bivariate Data
        >>> # bivariate analysis of a numerical and categorical features.
        >>> # Show the salaries of all the employees by their departments and check if there are any salary discrepancies.

        >>> sns.barplot(data=employee_df, x='department', y='salary')
        >>> plt.show()

        >>> # Comparison of continuous and categorical features
        >>> # Show the descriptive statistics of employee's salary across all the departments
        >>> sns.boxplot(data=employee_df, x='department', y='salary')
        >>> plt.show()

        >>> # Relationship between two continuous features.
        >>> # Show the relationship between an employee's age and their salary.

        >>> sns.scatterplot(data=employee_df, x='age', y='salary')
        >>> plt.show()

   ------------------------------------------------------

------------------------------------------------------
4.10 Analyze and Visualize Data for ML Review


  Chapter 4 focuses on AWS ML Exam task:

    2.3 Analyze and visualize data for machine learning.
      Create graphs
        - example: scatter plots, time series, histgrams, boxplots
      Interpret descriptive statistics
        - example: correlation, summary statistics, p-value
      Perform cluster analysis
        - example: hierarchical, diagnosis, elbow plot, cluster size

  Create graphs
    Visualizing relationships
      - visualizing the relationship between two features helps uncover patterns and identify trends
      - scatter plots and bubble charts can be used to visualize the relationships
        by plotting each feature on the X and Y axis.
        - Bubble charts are very effective if you need to visualize the relationship between three or more features.
    Visualizing data distributions
      - visualizing the data distribution provides insights into the mean, median, and skewness
      - histograms, boxplots, and heatmaps can be used to visualize the data distributions
      - boxplots are an effective technique to visualize outliers
    Visualizing comparisons
      - Visualizing comparisons in our data can provide a static snapshot of how different variables in our 
        dataset compare, and their changes over time.
      - Bar charts and line charts are commonly used techniques to visualize comparisons.
    Visualizing composition of our data
      - Visualizing composition of our data shows the individual elements that our data is made of, 
        - pie chart and stacked bar chart are commonly used techniques to visualize composition 
        - pie chart is very effective in showing the part to whole relationship.

  Interpret descriptive statistics
    - descriptive statistics deal with collecting and interpreting the data.
       - Descriptive statistics is not the same as inferential statistics that deals with making prediction.
       - descriptive statistics include mean, median, variance, and standard deviation
    Data Visualization related Metrics
      - skewness, kurtosis, and correlation are the commonly used metrics in visualizing the data

  Perform cluster analysis
    - Cluster analysis is a statistical technique that groups similar data points into clusters.
        - it's commonly used when a target variable has not been identified.
    Three clustering techniques,
      Partition clustering 
      Hierarchical clustering
      Density-based clustering
    K-means clustering,
      - partition clustering technique
      - need to explicitly specify the number of clusters.
    Elbow Method
      - The elbow method can be used to make an educated decision on the number of clusters.
    hierarchical clustering,
     - don't need to explicitly specify the number of clusters.
      - hierarchical cluster groups data based on their similarities and builds a hierarchy of clusters
      - The resulting cluster forms a three leg structure, called a dendrogram.
     hierachical clustering approaches
       bottom up approach (agglomerative hierarchical technique)
         - In this technique, each data point is initially considered as an individual cluster, and eventually 
           merged together based on the similarities.
         Linkage Methods
           - Linkage methods are used to calculate the distance between any two points in the two clusters,
           - single linkage method is a commonly used technique.
       top-down approach
        divisive technique
          - a top-down approach which is the opposite of the bottom-up approach
          - in this technique, all the data is considered one cluster to begin with and then split into 
            multiple clusters based on the dissimilarities.
      hierarchical limitations
        - hierarchical techniques are computationally intensive and has high memory requirements, 
          especially for large data sets.

  QuickSight
    - is a BI tool, powered by AWS, to visualize and analyze the data.
    - It integrates with multiple data sources and supports many data formats.
    ML insights
      - a powerful feature provided by QuickSight to detect anomalies and perform forecasting using 
        machine learning algorithms.


------------------------------------------------------
4.11 Quiz: Analyze and Visualize Data for ML Review

Question 2

Imagine you are working for a global company, and your management is interested in knowing the revenue generated 
by each Region. Which visualization technique will you use to accomplish this?

choices:
  - Pie chart     <- correct answer
  - Bar chart     <- incorrect answer
  - Bubble chart
  - Scatter plot
Sorry!
  A bar chart is used to compare two features in a dataset.
Correct Answer
  A pie chart can be used to visualize the data composition



Question 7

Imagine you are a data engineer working for a company. Your management has asked you to visualize the purchasing patterns of 
customers based on their age. Which visualization technique will you use for this purpose?

choices:
  - Heatmap
  - Box plot
  - Pie chart     <- incorrect answer
  - Scatter plot  <- correct answer
Sorry!
  A pie chart is used to visualize the composition of the data.
Correct Answer
  A scatter plot can be used to visualize the relationship between two features in a dataset.



Question 3 (redo 1)

  You are working with a dataset and found that your data is positively skewed. Your team lead has asked you to measure 
  this skewness. How will you achieve this?

Choices:  
  Using the positive correlation technique

  Using kurtosis

  Using the Pearson coefficient formula                     <--- Correct Answer

  Using the negative correlation technique
Good work!

  Pearson's formula helps measure the skewness of data.

  Notes: 
      Pearson first coefficient of skewness

            ( Mean - Mode ) / ( standard deviation)

      Pearson second coefficient of skewness
        - A negative value indicates that the data is negatively skewed
        - a positive value indicates that the data is positively skewed.
    
    'kurtosis' is also about skewness, but it is more about of the outliers in the data based on the tailedness


Question 5 (redo 1)

  You are working on applying a clustering technique to a dataset. One of the requirements is to generate a dendrogram. 
  Which clustering technique will you use?

Choices:  
  Density-based clustering

  Default clustering

  K-means clustering

  Hierarchical clustering                     <--- Correct Answer
Good work!

  Both bottom-up and top-down techniques create a dendrogram.

Notes:

  Cluster analysis Technique Categories
    - three major categories based on the number of observations, presence of outliers, and the features in the dataset
    Paritioning clustering
      - user must specify the number of custers indicated by a variable 'K'.
      - each cluster will contain at least one observation and they will be unique
      k-means
        - popular partitioning clustering algorithm
    Hierarchical clustering
      - user is not required to specify the number of clusters 
      - cluster assignments are determined by creating a hierarchy and cutting the tree at a specified depth
      - creating a hierarchy either in a bottom-up approach or a top-down approach, and cutting the tree at 
        a specified depth
      Agglomerative clustering 
        - a popular hierarchical clustering bottom-up approach algorithm
    Density-Based clustering
      - user is not required to specify the the number of clusters
      - cluster assignments are determined based on the density of the data points
      - This technique works based on a distance-based parameter that determines the distance between 
        the two points.
      DBScan 
        - popular density-based clustering algorithm


Question 6 (redo 1)

  What is the secret behind Amazon QuickSight's blazing-fast performance at scale?

Choices:  
  SPICE engine                     <--- Correct Answer

  Mobile app support

  QuickSight's API capabilities

  Serverless infrastructure
Good work!

  Super-fast, Parallel, In-memory Calculation Engine (SPICE) automatically replicates data for high availability 
  and enhanced performance.

Notes:
  QuickSight Benefits
    - low cost of ownership
    - Blazing fast performance at scale with SPICE (Super Fast Parallel In-memory Calculation) engine.
       - SPICE allows thousands of users to parallely perform interactive analysis,
       - (SPICE) automatically replicates data for high availability and enhanced performance.
    - data insights is powered by machine learning 
       - allows organizations to find hidden trends and detect outliers.
    - support mobile applications (both iOS and Android platforms)
    - Programmatic access to BI assets


Question 7 (redo 1)

  You are currently analyzing a large dataset and have been assigned a task to detect anomalies. Your team lead 
  wants you to use Amazon Quicksight to accomplish this. What feature will you use?

Choices:  
  Datasets

  Dashboard

  ML Insights                     <--- Correct Answer

  AutoGraph
Good work!

  One of the major features offered by ML Insights is anomaly detection.

Notes: 
  QuickSight ML Insights - Value Proposition
   Two major features:
     Forecasting
       - predict business metrics, perform interactive analysis and discover hidden insights
       - uses Random Cut Forest algorithm to to detect trends and impute missing values
     Anomaly Detection
       - detects the presense of outliers in the data
       - uses Random Cut Forest algorithm to gain deep insights.
   Autonarratives
     - Effectively share the story behind the data
        - helps you build powerful dashboard with embedded narratives to share the story behind your data.
      - with ML insights, you no longer need to rely on manual expertise to analyze the data to 
        find deeper insights


------------------------------------------------------

Chapter 5: Conclusion

------------------------------------------------------
5.1 Course Summary

  Exam Domain 2: Exploratory Data Analysis
    Exploratory data analysis
      Sanitize and prepare data for modeling
      Perform Feature Engineering
      Analyze and visualizae data for ML

  Next Steps
    Practice hands-on exercises
     - open a Jupyter Notebook and trying out some variations of what we covered in the lessons
     Attempt the labs that are part of this course

  Other Resources:
    AWS Account
    Amazon QuickSight
      - A Cloud Guru course: Amazon QuickSight Deep Dive
    Jupyter Notebooks
      - A Cloud Guru course: Introduction to Jupyter Notebooks
    Preparing Data
      - A Cloud Guru course: Data Preparation (Import and Cleaning) for Python
    Jupyter Notebooks


------------------------------------------------------
5.2 Creating a TensorFlow Lab
------------------------------------------------------
5.3 Creating and MXNet Ima
------------------------------------------------------
5.4 Creating a scikit-learn
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
