------------------------------------------------------

O'Reilly

Hands-ON Machine Learning with Scikit-Learn, Keras, and TensorFlow

------------------------------------------------------
Chapter Preface
------------------------------------------------------
 Object and Approach

   Sckit-Learn: 
      https://scikit-learn.org
   TensorFlow
      https://www.tensorflow.org/
   Keras
     https://keras.io/

 Code Examples
    All code examples are at:
      https://github.com/ageron/handson-ml3

 Prerequistes

     Python:
        https://www.learnpython.org/
     Official Python Tuturial 
        https://docs.python.org/3/tutorial

    Python NumPy 
      https://numpy.org/
       - NumPy is centered around a powerful N-dimensional array object, and it also contains useful linear algebra, 
         Fourier transform, and random number functions
       - Python library that provides a multidimensional array object, various derived objects (such as masked arrays 
         and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, 
         shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical 
         operations, random simulation and much mor

    Python Pandas 
      https://pandas.pydata.org/
       - pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,
         built on top of the Python

      install:  pip3 install pandas


    Python Matplotlib
      https://matplotlib.org/
      - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python

      install:  pip3 install matplotlib
      
      example:
++++++++++++++
import matplotlib.pyplot as plt
plt.hist(np.random.rand(100000), density=True, bins=100, histtype="step", color="blue", label="rand")
plt.hist(np.random.randn(100000), density=True, bins=100, histtype="step", color="red", label="randn")
plt.axis([-2.5, 2.5, 0, 1.1])
plt.legend(loc = "upper left")
plt.title("Random distributions")
plt.xlabel("Value")
plt.ylabel("Density")
plt.show()
++++++++++++++

    NumPy, Pandas, & Matplotlib tutorials - Machine Learning Prerequistes
      https://homl.info/tutorials
      -> Links to:
        https://colab.research.google.com/github/ageron/handson-ml3/blob/main/index.ipynb

     install: pip3 install numpy
       


  Other Resources:

     Machine Learning resources

       Andrew Ng's ML course on Coursera 
         - Supervised Machine Learning: Regression and Classification Class
         https://homl.info/ngcourse 
         -> links to: https://www.coursera.org/learn/machine-learning/

       Scikit-Learn User Guide
         https://homl.info/skdoc
         -> links to:  https://scikit-learn.org/stable/user_guide.html

       Dataquest (interactive ML tutorials)
         https://dataquest.io

       Quora (ML Blogs)
         https://homl.info/1
         -> Links to: https://www.quora.com/What-are-the-best-artificial-intelligence-blogs-newsletters

How to Contact Us

   https://homl.info/oreilly3
   Links to: https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/
   -> includes links to Errata and Code Examples

   Download code examples
   https://github.com/ageron/handson-ml3



------------------------------------------------------
Chapter 1 The Machine Learning Landscape
------------------------------------------------------

  What is machine learning?
    
    Machine Learning is the field of study that gives computers the ability to learn without explicitly programmed


    A computer programm is said to lear from experience 'E' with respect to some task 'T' and some performance 
    measure 'p', if the performance on 'T', as measured by 'P' improves with experience.


  Example of Applications

    Analyzing images of products on a production line to automatically classify them

    Detecting Tumors in brain scans

    Automatically classify new articles

    Automatically flagging offensive comments in discussion forums

    Summarizing long documents automatically

    Creating a chatbot or a personal assistant

    ....

  Training Supervision

    Supervised Learning
      - the  training set you feed to the algorithm includes the desired solutions called labels

      Classification
        - a typical supervised learning task. Example spam filter

      Target numerica value
        - predict target value (e.g. car price) based on a give set of features (mileage, age, brand, etc)
        - a regression task
           - predict a value, given input features

    Unsupervised Learning
      - the training data is unlabeled. 
      - the system tries to learn without a teacher

        Visualization algorithms
          - you feed them a lot of complex, unlabeled data, and they output a 2D or 3D presentation of your
            data that can be easily plotted
        Dimensional reduction
          - goal is to simplify the data without losing too much information
          Feature extraction
            - dimensionality reduction whereby strongly corelated features are merge into one feature
        Anonaly Detection
          - examples: detecting unusual credit card transactions to prevvent fraud, catching manufacturing defects,
            or automatically removing outliers from a dataset before feeding it to another learning algorithm
        Novelty Detection
          - detect new instances that look different from all instances in the training set
        Association Rule Learning
          - the goal is to dig into a large amount of data and discover interesting relationships between attributes
          - example: supermarket sales logs show people who purchase BBQ sauce and potato chips tend to purchase steaks,
            so you may want to place them close together

    Semi-supervised Learning
      - when you have many unlabeled instance and some labeled instances 
      - algorithm that can deal with partially labeled data
      - example: photo hosting services: automatically recognizing the same person in photos based persons in prevouly 
        labeled photos

    Self-supervised Learning
      - generating a fully labeled dataset from a fully unlabeled dataset
      - once the dataset is fully labeled, supervised learning algorthm can be used

    Reinforcement Learning
      - the learning system, called an agent, can observe the environment, select and perform actions, and get rewards
        in returns (or penalties in the form or negative rewards)
      - it must then learn by itself what is the best strategy, called a policy, to get the must rewards
      - a policy defines what action the agent should choose when it is given a situation
      - example: robots learning how to walk

  Batch Versus Online Learning

    Batch learning
      - the system is incapable of learning incrementally; it must be trained using all the available data
      Offline learning
        - First the system is trained, and then it is launched into production and runs without learning any more
      Model Rot or Data Drift
        - model's performance tends to decay over time simply because the world continues to evolve while the
          mode remains unchanged

    Online learning (incremental learning)
      - you train the system incrementally by feeding it data instances sequentially, either individually or in 
        small groups called mini-batches
      Learning rate
        - how fast they should adapt to changing data
        - if you set a high learning rate, your system will quickly adapt to new data, but it will also tend to 
          quickly forget old data

  Instance-Based Versus Model-Based Learning
    generalize
      - one way to categorize ML systems is by how they generalize
      - given a number of training examples, the ML system needs to be able to make a good predictions for 
        (generalize to) examples it has never seen before
    - two main approaches to generalization: instance-based learning and model-based learning

    Instance-based learning 
      - the system learns the examples by heart, then generalizes to the new cases by using a 'similarity measure' to
        compare them to the learned examples (or a subset of them)
      - the model learns from examples and stores them in memory. The model can then make predictions by comparing new 
        input data to the stored examples. 
        - example K-Nearest Neighbors (knn)

    Model-based learning
      - build a model from a set of  examples, and then use that model to make predictions
      - the model learns the underlying relationships and patterns in the data by creating a mathematical representation or a model.
        - example Linear Regression
      Performance measures
        utility or fitness function: measures how good your model is
        Cost function:               measures how bad your model is
      Training model
        - running an algorithm to find the model parameters that will make it best fit the training data

  Exercises:

     1. How would you define machine learning?
       - Machine Learning is the field of study that gives computers the ability to learn without explicitly programmed
       answer:
       - Machine Learning is about building systems that can learn from data. Learning means getting better at some task, 
         given some performance measure.

     2. Can you name 4 types of applications where it [ML] Shines?
        - Analyzing images of products on a production line to automatically classify them
        - Detecting Tumors in brain scans
        - Automatically classify new articles
        - Automatically flagging offensive comments in discussion forums
        - detecting credit card fraud
        answer:
        - Machine Learning is great for:
          - complex problems for which we have no algorithmic solution, 
          - to replace long lists of hand-tuned rules, 
          - to build systems that adapt to fluctuating environments, and 
          - finally to help humans learn (e.g., data mining).

     3. What is a labeled training set?
        - a supervised learning training set
        answer:
        - A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance.

     4. What are the 2 most common supervised tasks?
        - classification (e.g. spam filter)
        - regression: i.e: predict a target numeric value (e.g. car price) based on given set of features (e.g. mileage, age, brand) 
        answer:
        - The two most common supervised tasks are regression and classification.

     5. Can you name 4 common unsupervised tasks?
        - clustering - e.g. detect similarity between groups
        - Visualization - output 2D & 3D representation of your data that can be plotted 
        - dimensional reduction - goal is to simplify data without losing too much information
        - anonaly detection - detection outliers such as unusual credit card transactions
        - association rule learning - goal is to dig into large amount of data and discover interesting relationships

        answer:
        - Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.

     6. What type of algorithm would you use to allow a robot to walk in various unknown terrains?
       - reinformation learning 
       answer:
       - Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains, 
         since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express 
         the problem as a supervised or semi-supervised learning problem, but it would be less natural.

     7. What type of algorithms would you use to segment your customers into multiple groups?
       - clustering algorithm
       answer:
       - If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment i
         your customers into clusters of similar customers. 
       - However, if you know what groups you would like to have, then you can feed many examples of each group to a 
         classification algorithm (supervised learning), and it will classify all your customers into these groups.

     8. Would you frame the problem of span detection as supervised learning problem or an unsupervised learning problem?
       - initially as a supervised learning, for example, using a classification algorithm.
       - but it may need to include unsupervised learning over time
       answer:
       - Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their 
         labels (spam or not spam).

     9. What is an online learning system?
       - you train the system incrementally by feeding it data instances sequentially, either individually or in 
         small groups called mini-batches
       answer:
       - An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of 
         adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data.

     10. What is out-of-core learning?
       - when a huge dataset cannot fit in a machine's memory.
       - for out-of-core learning, the algorithm loads part of the data, runs a train step on that data, and repeats the
         process until it has run all the data
       answer:
       - Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. 
       - An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn 
         from these mini-batches.

     11. What type of algoritm relies on a similarity measures to make predictions?
       - instance based algorithm:  the system learns the examples by heart, then generalizes to the newcases by using 
         a 'similarity measure' to compare them to the learned examples (or a subset of them)
        - example K-Nearest Neighbors (knn)
       answer:
       - An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a 
        similarity measure to find the most similar learned instances and uses them to make predictions.

     12. What is the difference between a model parameter and a model hyperparameter?
       - a hyperparameter is a parameter of a learning algorithm (not of the model)
       - a model parameter is a parameter of the model
       answer:
       - A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope 
         of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes 
         well to new instances. 
       - A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply).

     13. What do model based algorithms search for? 
         What is the most common strategy they use to succeed?
         How do they make predictions?
        - example Linear Regression
      answer:
      - Model-based learning algorithms search for an optimal value for the model parameters such that the model will 
        generalize well to new instances. 
      - We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions 
        on the training data, plus a penalty for model complexity if the model is regularized. 
      - To make predictions, we feed the new instance's features into the model's prediction function, using the parameter 
        values found by the learning algorithm.

     14. Can you name 4 main challenges in ML?
       - Insufficient Quantity of Training data
       - Nonrepresentative training data
       - Poor Quality Data
       - Irrelevant Features
       - Overfitting the training data (model performs well on training data, but it does not generalize well)
            - happens when the model is too complex relative to the amount and noisiness of the training data
       - Underfitting the training data (model is too simple)
       answer:
       - Some of the main challenges in Machine Learning are:
         - the lack of data, poor data quality, nonrepresentative data, 
         - uninformative features, 
         - excessively simple models that underfit the training data, 
         - and excessively complex models that overfit the data.

     15. If your model performs great on the training set, but generalizes poorly to new instances, what is happening?
       - Overfitting the training data
       Can you name 3 possible solutions?
       - simplify the model by selecting one with fewer parameters or by reducing the number attributes in the training data
       - gather more training data
       - reduce the noise in the training data (e.g. fix data errors and remove outliers)
       answer:
       - If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting 
         the training data (or we got extremely lucky on the training data). 
       - Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing 
         the number of parameters or features used, or regularizing the model), or reducing the noise in the training data.

     16. What is a test set, and why would you want to use it?
       - test set is used to test [validate] your model  after you trained it using the training set
       - it is used to estimate the error rate on new cases called the generalization error (or out-of-sample-error)
       answer:
       - A test set is used to estimate the generalization error that a model will make on new instances, before the model is 
         launched in production.

     17. What is the purpose of a validation set?
       - validation set (or development set or dev set) is hold out part of the training set to evaluate serveral candidate models
         and select the best one
       answer
       - A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.

     18. What is a train-dev set, when you need it, and how do you use it? 
        - when the training data is not fully representative of the production data
     
        answer:
        - The train-dev set is used when there is a risk of mismatch between the training data and the data used in the 
          validation and test datasets (which should always be as close as possible to the data used once the model is in production).
        - The train-dev set is a part of the training set that's held out (the model is not trained on it). The model is trained 
          on the rest of the training set, and evaluated on both the train-dev set and the validation (dev) set. 
        - If the model performs well on the training set but not on the train-dev set, then the model is likely overfitting the 
          training set. 
        - If it performs well on both the training set and the train-dev set, but not on the validation set, then there is 
          probably a significant data mismatch between the training data and the validation + test data, and you should try 
          to improve the training data to make it look more like the validation + test data.

     19. What can go wrong if you tune hyperparameters using the test set?
        - overfitting
        answer:
        - If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error 
          you measure will be optimistic (you may launch a model that performs worse than you expect).

------------------------------------------------------
Chapter 2 End-to-End Machine Learning Project
------------------------------------------------------


Working with Real Data:
  Popular open data repositories:
    - OpenML.org (https://openML.org)
    - Kaggle.com (https://kaggle.com/datasets)
    - OpenML.org (https://openML.org)
    - PapersWithCode.com (https://PapersWithCode.com/datasets) 
    - AWS datasets (https://regristry.opendata.aws)
    ....

Select a Performance Measure (page 43 - 45):
 - a typical performance measure for regression problems is 'root mean square error (RMSE)'

    RMSE (root mean squared error): 
       RMSE(X,h) = [ (1/m) SUM (h(xi) - yi)**2 ]**1/2 where SUM is from i=1 to i=m

       m: number of instances in thd dataset
       xi: a vector of all the features values of the ith instances of the dataset, and
           yi is the label (desired output)
       X: is a matrix containing all the features excluding labels of all instances in the dataset
       h: your systems prediction's function, also called the 'hypothesis'
       RMSE(X,h): is the cost function measurce on the set of examples using your 'hypothesis 'h'

    MAE (mean absolute error):
       MAE(X,h) =  (1/m) SUM |(h(xi) - yi)|  where SUM is from i=1 to i=m

    RMSE: corresponds to the 'Euclidean norm'; this is the notation of distance. It also called the 'l2 norm',
          noted || . ||2 (or just || . ||)

     MAE: corresponds to the 'l1 norm', noted || . ||. 
     
     lk norm: more generally, the lk norm of a vectory 'v' containing 'n' elements is defined as the
       ||v||**k = (|v1|**k + |v2|**k + ... + |vn|**k)**1/k . 'l0' gives the the number of non-zero elements in
        the vector , and linf gives the max absolute value in the vectoro

     The higher the norm index, the more it is focused on large values and neglects same ones. This is my RMSE
     is more sensitive to outliers than MAE. But when outliers are exponentially rare (like in bell-shaped curve),
     the RMSE performs very well and is perferrred.


Look for Correlations (page 63 - 66)

  - Since the dataset is not too large, you can easily compute the 'standard correlation coefficient' (also called 
    'Pearson's r) between every pair of attributes using the corr() method: 
        >> corr_matrix = housing.corr()
    Now you can look  at how much each attribute correlates with the median house values.

    correlation cofficient:
      - ranges from -1 to 1
      - when it is close to 1, it means there is a strong positive correlation
      - when it is close to -1, it means there is a strong negative correlation
      - when it is close to 0, it means there is no linear correlation
      Note: the correlation coefficient only measures linear correlations (as 'x' goes up, y generally goes up/down.
            It may completely miss out  on nonlinear relationships (e.g. as x approaches 0, y goes up)


Clean the data

  In the book 3 options are listed to handle the NaN values:

     1. remove instances (rows) with missing [e.g. total_bedrooms] data:
        >>> housing.dropna(subset=["total_bedrooms"], inplace=True)    # option 1

     2. remove column/feature with some missing [e.g. total_bedrooms] data:
        >>> housing.drop("total_bedrooms", axis=1)       # option 2

     3. replacing missing data with some value (zero, mean, median, most_frequent, constant, etc. This is called 'imputation'
        >>> median = housing["total_bedrooms"].median()  # option 3
        >>> housing["total_bedrooms"].fillna(median, inplace=True)

    Powerfull Imputers (both only for numerical features):
      KNNImputers:  replaces each missing value withe the mean of the k-nearest neigher values for that feature.
                    The distances is based on the available features
      IterativeImputers:  trains a regression model per feature to predict the missing values base on all the available
                    features.

   Next, drop outliers

Handling Text and Categorical Attributes (pages 71 - 75)

  - most machine learning algorithms perfer to work with numbers, so let's convert categories from text
    to numbers. 
    >>> from sklearn.preprocessing import OrdinalEncoder

    >>> ordinal_encoder = OrdinalEncoder()
    >>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)


   - one hot
     - ML alogorithms will assume that 2 nearby values are more simular than 2 distant values.
     - to fix, use one-hot encoding
     >>> from sklearn.preprocessing import OneHotEncoder

     >>> cat_encoder = OneHotEncoder()
     >>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)


Feature Scaling and Transformation (page 75 - 79)
  - ML algorithm generally don't perform well when the input numberical attributes have very different scales
  - two common wayt to get all attributes to have the same scale: min-max scaling and standardization scaling

    min-max scaling (also called normalization): 
       - the values are shifted and rescaled so they end up ranging from 0 to 1
       - you can use sklearn MinMaxScaler. It has a feature_range hyperparameter that lets you change the range

        >>> from sklearn.preprocessing import MinMaxScaler
        >>> min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        >>> housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)

    standardization scaling:
      - first it subtracts the 'mean value' (so standardize values have a mean 0), then it divides the the standard
        deviation (so the standarized values have a standard deviation equal to 1)
      - standardization is much less affected by outliers

        >>> from sklearn.preprocessing import StandardScaler
        >>> std_scaler = StandardScaler()
        >>> housing_num_std_scaled = std_scaler.fit_transform(housing_num)

    heavy tail:
      - when values fare from the mean are not exponentially rare
      - both min-max scaling and standardization scaling will squash most [heavy tail] values into small range 
      - first, you need to tranform to shrink the heavy tail, and if possible, make the distribution roughly symmetrical
      - transform heavy tail options:
         replace with its square root:
           - replace feature with its square root (or raise feature to power between 0 and 1)
         replace with its logarithm:
           - replace feature with its logarithm (for really long tails suche as power law distribution)
         bucketizating the feature
           - chopping features distribution into roughly equal size buckets, and replacing feature value with
             the index of the bucket it belongs to

    multimodal distributions (two or more clear peaks, called modes):
      - tranform options:
        bucketize
          - replace with bucketize the feature
        similarity between modes
          - add a feature for each modes (or at least main ones) with a similar between the feature and the mode
          - similarity mearch is typically computed uwing a 'radial basis function' (any function that only depends
            on the input value and a fixed point)
          - Guassian RBF is the most common RBF. Its output decays exponentially as the input moves away from a fixed point
             Guassion RBF = ln (-gamma(x - fixedPoint)**2) 

             >>> from sklearn.metrics.pairwise import rbf_kernel
             >>> age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)

      target transformation          
        - targets may also be transformed, example if they have a heavy tail.
        - if so, you may to inverse transform the algoritms predicted values
        options
         inverse_transform() method
         TransformedTargetRegressor class:
           >>> from sklearn.compose import TransformedTargetRegressor
           >>> model = TransformedTargetRegressor(LinearRegression(),
           >>> model.fit(housing[["median_income"]], housing_labels)
           >>> predictions = model.predict(some_new_data)

Custom Transformers (page 79 - 83):


   sklearn.preprocessing.FunctionTransformer

   class sklearn.preprocessing.FunctionTransformer(func=None, inverse_func=None, *, validate=False, accept_sparse=False, 
       check_inverse=True, feature_names_out=None, kw_args=None, inv_kw_args=None)[source]

       Constructs a transformer from an arbitrary callable.

       A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns 
       the result of this function. This is useful for stateless transformations such as taking the log of frequencies, doing 
       custom scaling, etc.

       Note: If a lambda is used as the function, then the resulting transformer will not be pickleable.

   Example: Create a log (ln) custom transfomer (includes and inverse_function):
   >>> from sklearn.preprocessing import FunctionTransformer
   >>> log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
   >>> log_pop = log_transformer.transform(housing[["population"]])

   Example: Create a Gaussian RBF custom transfomer:
   >>> from sklearn.preprocessing import FunctionTransformer
   >>> rbf_transformer = FunctionTransformer(rbf_kernel, kw_args=dict(Y=[[35.]], gamma=0.1))
   >>> age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])

  TrnaformerMixin and BaseEstimator
    - FunctionTransformer is very handy in trainable use cases 
    - if you like your transformer to be trainable, and learning some parameters in the 'fit()' method
      and using them later in the 'transform()' method, then you need a custom class
       - for this, you need three methods: fit() (which returns 'self'), transform(), and fit_transform()
       - you get 'fit_transform()' by adding 'TransformerMixin' as a base class; the default implementation
         will just call fit() and 'transform()'
       - if you add 'BaseEstimator' as a base class (and avoid using *args and **kwargs in your constructor),
         you will also get tow extrea methods: 'get_param()' and 'set_params()'. These will be useful for
         automatic hyperparameter tunning.


    Example: Custom Transformer that acts much like StandardScalar

    >>> from sklearn.base import BaseEstimator, TransformerMixin
    >>> from sklearn.utils.validation import check_array, check_is_fitted
    >>> 
    >>> class StandardScalerClone(BaseEstimator, TransformerMixin):
    >>>     def __init__(self, with_mean=True):  # no *args or **kwargs!
    >>>         self.with_mean = with_mean
    >>> 
    >>>     def fit(self, X, y=None):  # y is required even though we don't use it
    >>>         X = check_array(X)  # checks that X is an array with finite float values
    >>>         self.mean_ = X.mean(axis=0)
    >>>         self.scale_ = X.std(axis=0)
    >>>         self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
    >>>         return self  # always return self!
    >>> 
    >>>     def transform(self, X):
    >>>         check_is_fitted(self)  # looks for learned attributes (with trailing _)
    >>>         X = check_array(X)
    >>>         assert self.n_features_in_ == X.shape[1]
    >>>         if self.with_mean:
    >>>             X = X - self.mean_
    >>>         return X / self.scale_


Transformation Pipelines (pages 83 - 88):

  example: pipeline for numerical attributes:
    >>> from sklearn.pipeline import Pipeline
    >>> num_pipeline = Pipeline([ ("impute", SimpleImputer(strategy="median")), ("standardize", StandardScaler()), ])

  - if you don't want to name the transformers, you can use the 'make_pipeline()' function instead:

  example: make_pipeline for numerical attributes:
    >>> from sklearn.pipeline import make_pipeline
    >>> num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())


  example: set_config(display='diagram') to visualize the pipeline (e.g. 'num_pipeline')
    >>> from sklearn import set_config
    >>> set_config(display='diagram')
    >>> num_pipeline
        Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),
                      ('standardscaler', StandardScaler())])

  'ColumnTransformer' can be used to combine transforming categorical columns with numerical columns 
  Example: Using ColunmTransformer:
    >>> from sklearn.compose import ColumnTransformer

    >>> num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income"]
    >>> cat_attribs = ["ocean_proximity"]

    >>> cat_pipeline = make_pipeline( SimpleImputer(strategy="most_frequent"), OneHotEncoder(handle_unknown="ignore"))
    >>> preprocessing = ColumnTransformer([ ("num", num_pipeline, num_attribs), ("cat", cat_pipeline, cat_attribs), ])

   Alternative, you can select colunms by data types using 'make_column_selector' with 'make_column_transformer'
    >>> from sklearn.compose import make_column_selector, make_column_transformer

    >>> preprocessing = make_column_transformer(
    >>>     (num_pipeline, make_column_selector(dtype_include=np.number)),
    >>>     (cat_pipeline, make_column_selector(dtype_include=object)),
    >>> ):
  
      >>> housing_prepared = preprocessing.fit_transform(housing)

Train and Evaluate Training Set (pages 88 - 89):

  Example: Using RMSE to evaluation prediction results via 'mean_squared_error()' function:
    >>> from sklearn.metrics import mean_squared_error
    >>> lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
    >>>  lin_rmse
    68687.89176589991

   underfitting:
     - linear regression model is underfitting the data. 
       This could be due to:
         - features do not provide enough information to make good predictions
         - model is not powerful enough
       Potential underfitting fixes:
         - use more powerful model
         - feed training algorithm with better features
         - reduce constraints on the model
 

Better Evaluation with Cross-Validation (pages 89 - 91):

    k-foled cross validation feature
    Example: randomly split the training set into '10' nonoverlapping subsets called 'folds', then train 
      and evaluation the decision tree model 10 times, picking a different fold for evaluation every time and
      using 9 folds for training. The result is an array containing 10 evaluation scores:

    >>> from sklearn.model_selection import cross_val_score
    >>> tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

    >>> pd.Series(tree_rmses).describe()
        count       10.000000
        mean     66868.027288
        std       2060.966425
        min      63649.536493
        25%      65338.078316
        50%      66801.953094
        75%      68229.934454
        max      70094.778246
        dtype: float64

    Note: Scikit-Learn's 'cross-validation' features expect a 'utility function' (greater is better) rather than a 
         cost function (lower is better), so the scoring function is actually the opposite of the RMSE. It's a negative
         value so you need to swithc the sign on f the output to get the RMSE error.


Grid Search (pages 91 - 93):

   GridSearchCV class
     - used to experiment with hyperparameters
     - you tell it the hyperparameter values to experiment with
     - it uses cross-validation to evaluate all the possible combinations of hyperparameters

    Example: Used GridSearchCV to experiment with 'preprocessing' and 'random_forest' hyperparameters:
    >>> from sklearn.model_selection import GridSearchCV
    >>> 
    >>> full_pipeline = Pipeline([
    >>>     ("preprocessing", preprocessing),
    >>>     ("random_forest", RandomForestRegressor(random_state=42)),
    >>> ])
    >>> param_grid = [
    >>>     {'preprocessing__geo__n_clusters': [5, 8, 10],
    >>>      'random_forest__max_features': [4, 6, 8]},
    >>>     {'preprocessing__geo__n_clusters': [10, 15],
    >>>      'random_forest__max_features': [6, 8, 10]},
    >>> ]
    >>> grid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')
    >>> grid_search.fit(housing, housing_labels)
    >>>
    >>> grid_search.best_params_
        {'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}

Randomized Search (pages 93 - 94):

   RandomizedSearchCV class:
     - grid search is fine when you are exploring relatively few hyperparameter combinations
     - RandomizedSearchCV is often preferrable when the hyperparameter search space is large
     - instead of trying out all possible combinations, it evaluates a fixed number of combinations,
       selecting a random value for each hyperparameter at every iteration
     - for each hyperparameter, you provide either a list of possible values, or a probability distribution

    Example: Using RandomizedSearchCV to search hyperparameter across a range of values:

    >>> from sklearn.model_selection import RandomizedSearchCV
    >>> from scipy.stats import randint
    >>> 
    >>> param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
    >>>                   'random_forest__max_features': randint(low=2, high=20)}
    >>> 
    >>> rnd_search = RandomizedSearchCV(
    >>>     full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    >>>     scoring='neg_root_mean_squared_error', random_state=42)
    >>> 
    >>> rnd_search.fit(housing, housing_labels)

    How to choose the sampling distribution for a hyperparameter

     - scipy.stats.randint(a, b+1): for hyperparameters with discrete values that range from a to b, and all 
       values in that range seem equally likely.
     - scipy.stats.uniform(a, b): this is very similar, but for continuous hyperparameters.
     - scipy.stats.geom(1 / scale): for discrete values, when you want to sample roughly in a given scale. 
       E.g., with scale=1000 most samples will be in this ballpark, but ~10% of all samples will be <100 and ~10% will be >2300.
     - scipy.stats.expon(scale): this is the continuous equivalent of geom. Just set scale to the most likely value.
     - scipy.stats.loguniform(a, b): when you have almost no idea what the optimal hyperparameter value's scale is. 
       If you set a=0.01 and b=100, then you're just as likely to sample a value between 0.01 and 0.1 as a value between 10 and 100.

Ensemble Methods:
  - try a combination of models that perform best. The group (or 'ensemble') will often perform better than
    the best individual models


Analyzing the Best Model and Their Errors:

  - RandomForestRegressor can indicate the relative importannce of each attribute for making accurate predictions
  - with the feature importance scores, you may want to drop some of the less useful features.
  Example: Examining features importance:
    >>> final_model = rnd_search.best_estimator_  # includes preprocessing
    >>> feature_importances = final_model["random_forest"].feature_importances_
    >>> feature_importances.round(2)
        array([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, 0.04, 0.01, 0.  , ... ])
    >>> 
    >>> sorted(zip(feature_importances,
    >>>            final_model["preprocessing"].get_feature_names_out()),
    >>>            reverse=True)
        [(0.18694559869103852, 'log__median_income'),
        (0.0748194905715524, 'cat__ocean_proximity_INLAND'),
        . . .
        (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),
        (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]


  SelectFromModel:
    - the sklearn.feature_selection.SelectFromModel transformer can automatically drop the least userful features
      for you: when you fit it, it trains a model (typically a random forest), looks at the feature_importance_ 
      attribute, and selects the most useful features. Then, you call 'transform()', it drops the other features


Evaluate Your System on the Test Set:

You can using the scipy t.interval compute 95% confidence interval
   interval(confidence, df, loc=0, scale=1)
       Confidence interval with equal areas around the median.

   Example: Calcute the 95% confidence interval:
    >>> final_predictions = final_model.predict(X_test)
    >>> final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
    >>> print(final_rmse)
        41424.40026462184
    >>> from scipy import stats
    >>> confidence = 0.95
    >>> squared_errors = (final_predictions - y_test) ** 2
    >>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
    >>>                  loc=squared_errors.mean(),
    >>>                  scale=stats.sem(squared_errors)))
        array([39275.40861216, 43467.27680583])



Chapter 2 Exercises:

    -> see exercise_notebooks/02_exercises.ipynb

Exercise 1:

Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel="linear" (with 
various values for the C hyperparameter) or kernel="rbf" (with various values for the C and gamma hyperparameters). Note 
that SVMs don't scale well to large datasets, so you should probably train your model on just the first 5,000 instances 
of the training set and use only 3-fold cross-validation, or else it will take hours. Don't worry about what the 
hyperparameters mean for now (see the SVM notebook if you're interested). How does the best SVR predictor perform?

++++++++++++++++++++++
python code
++++++++++++++++++++++
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR

pipeline_svr = Pipeline([ ("preprocessing", preprocessing), ("svr", SVR()) ])

param_grid_svr = [
        {'svr__kernel': ['linear'], 'svr__C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},
        {'svr__kernel': ['rbf'], 'svr__C': [1.0, 3.0, 10., 30., 100., 300., 1000.0], 'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
    ]
grid_search_svr = GridSearchCV(pipeline_svr, param_grid_svr, cv=3, scoring='neg_root_mean_squared_error')
grid_search_svr.fit(housing.iloc[:5000], housing_labels.iloc[:5000])


grid_search_svr_rmse = -grid_search_svr.best_score_
grid_search_svr_rmse

grid_search_svr.best_params_
++++++++++++++++++++++


Exercise 2:
Try replacing the GridSearchCV with a RandomizedSearchCV.


++++++++++++++++++++++
python code
++++++++++++++++++++++
# see https://docs.scipy.org/doc/scipy/reference/stats.html
# for `expon()` and `loguniform()` documentation and more probability distribution functions.

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import expon, loguniform
#from sklearn.svm import SVR

pipeline_svr = Pipeline([ ("preprocessing", preprocessing), ("svr", SVR()) ])

param_distribs_svr = {
        'svr__kernel': ['linear', 'rbf'],
        'svr__C': loguniform(20, 200_000),
        'svr__gamma': expon(scale=1.0),
    }
rnd_search_svr = RandomizedSearchCV(pipeline_svr, param_distributions=param_distribs_svr, n_iter=50, cv=3, scoring='neg_root_mean_squared_error', random_state=42)
rnd_search_svr.fit(housing.iloc[:5000], housing_labels.iloc[:5000])


rnd_search_svr_rmse = -rnd_search_svr.best_score_
rnd_search_svr_rmse

rnd_search_svr.best_params_
++++++++++++++++++++++

Exercise 3:


+++++++++++++
python code
+++++++++++++
from sklearn.feature_selection import SelectFromModel

selector_pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),
                                 threshold=0.005)),  # min feature importance
    ('svr', SVR(C=rnd_search_svr.best_params_["svr__C"],
                gamma=rnd_search_svr.best_params_["svr__gamma"],
                kernel=rnd_search_svr.best_params_["svr__kernel"])),
])


selector_rmses = -cross_val_score(selector_pipeline,
                                  housing.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  scoring="neg_root_mean_squared_error",
                                  cv=3)
pd.Series(selector_rmses).describe()
+++++++++++++
------------------------------------------------------
Chapter 3 Classification
------------------------------------------------------

Fixing MNIST ssl certificate error when downloading it from sklearn:
  https://stackoverflow.com/questions/66032023/question-about-fetch-openml-in-scikit-learn

  add:
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context


Performance Measures:

Measuring Accuracy Using Cross-Validation

https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html
 sklearn.model_selection.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, 
      n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)[source]

    Evaluate a score by cross-validation.

  Code:
    from sklearn.model_selection import cross_val_score

    cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

  Equivalent code to "cross_val_score()"

    from sklearn.model_selection import StratifiedKFold
    from sklearn.base import clone
    
    skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not
                                           # already shuffled
    for train_index, test_index in skfolds.split(X_train, y_train_5):
        clone_clf = clone(sgd_clf)
        X_train_folds = X_train[train_index]
        y_train_folds = y_train_5[train_index]
        X_test_fold = X_train[test_index]
        y_test_fold = y_train_5[test_index]
    
        clone_clf.fit(X_train_folds, y_train_folds)
        y_pred = clone_clf.predict(X_test_fold)
        n_correct = sum(y_pred == y_test_fold)
        print(n_correct / len(y_pred))

Sklearn DummyClassier:
https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html
from sklearn.dummy import DummyClassifier

  Code:
    from sklearn.dummy import DummyClassifier
    dummy_clf = DummyClassifier()
    dummy_clf.fit(X_train, y_train_5)
    print(any(dummy_clf.predict(X_train)))
    cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring="accuracy")
 
Confusion Matrix:

sklearn.model_selection.cross_val_predict

    sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, 
       fit_params=None, pre_dispatch='2*n_jobs', method='predict')[source]

    Generate cross-validated estimates for each input data point.

    The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed 
    with an estimator fitted on the corresponding training set.

    Passing these predictions into an evaluation metric may not be a valid way to measure generalization performance. Results can 
    differ from cross_validate and cross_val_score unless all tests sets have equal size and the metric decomposes over samples.



  Code: 
    from sklearn.model_selection import cross_val_predict
    y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)


        Confusion Matrix:

                        |  Predicted Class     |     Predicted Class
                        |  Negative            |     Positive
                        |  (NOT FRAUD)         |     (FRAUD)
          --------------|----------------------|----------------------
          Actual Class  | True Negative (TN)   |  False Positive (FP)       
                        |                      |                             
          Negative      | NOT FRAUD was        | FRAUD was incorrectly      
          (NOT FRAUD)   | correctly predicted  | predicted as NOT FRAUD    ^
                        | as NOT FRAUD         |                           |
          --------------|----------------------|----------------------     |
          Actual Class  | False Negative (FN)  |  True Positive (TP)       |
                        |                      |                           |
          Postive       | FRAUD was incorrectly| FRAUD was correctly       |
          (FRAUD)       | predicted as         | predicted as FRAUD       Precision
                        | NOT FRAUD            |                             
          --------------|----------------------|----------------------      
                                                  <-------- Recall

        Metric for Classification Problems

           Accuracy: (TP + TN)  / (TP + FP + TN + FN)
              - percentage of predictions that were correct:
              - less effective with a lot of true negatives
                 - example: predicting fraud with little to no fraud data

           Precision: (TP)  / (TP + FP)
              - accuracy of positive predictions
              - percentage of positive predictions that were correct:
              - Use when the cost of false positives is high
                 - example: an email is flagged and deleted as spam when it really isn't

           Recall: (TP)  / (TP + FN)
              - also called sensitivity or true positive rate (TPR)
              - percentage of actual positive predictions that were correctly identified:
              - Use when the cost of false negatives is high
                 - example: someone has cancer, but screening does not find it

           F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
             - combined precision and recall score
             - harmonic mean of the precision and recall 
             - regular mean treats all values equally, the harmonic mean give more weight to low values
             - classifiers will only get high F1 Score if both recall and precision values are high

           Equation 3-3: F1 score:

           F1 = 2 / [ (1/precision) + (1/recall)]  =  2 x [( precision x recall) / (precision + recall)] 

              = (TP)  / [TP + ((FN + FP) / 2)]


https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
 sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)

    Compute confusion matrix to evaluate the accuracy of a classification.

    By definition a confusion matrix 'C' is such that 'Ci,j' is such that is equal to the number of 
     observations known to be in group 'i' and predicted to be in group 'j'

     Thus in binary classification, the count of true negatives is 'C0,0', , false negatives is 'C1,0' , 
       true positives is 'C1,1' and false positives is 'C0,1' .

     output: array([TN, FP], 
                   [FN, TP]])

  Confustion Matrix code:
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_train_5, y_train_pred)
    cm


Precision and Recall:


https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html
sklearn.metrics.f1_score

sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')[source]

    Compute the F1 score, also known as balanced F-score or F-measure.

    The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its 
    best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. 
    The formula for the F1 score is:

    F1 = 2 * (precision * recall) / (precision + recall)  = TP / (TP + (FN + FP)/2)

    In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting 
    depending on the average parameter.

  Code:
    from sklearn.metrics import f1_score
    f1_score(y_train_5, y_train_pred)
    
    # extra code - this cell also computes the f1 score
    cm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)

Notes: 
   - Increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off.
   - a classififier will get a high F1 score if both 'recall' and 'precision' are high

The Precision/Recall Trade-off:

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html
sklearn.metrics.precision_recall_curve

sklearn.metrics.precision_recall_curve(y_true, probas_pred, *, pos_label=None, sample_weight=None, drop_intermediate=False)[source]

    Compute precision-recall pairs for different probability thresholds.

    Note: this implementation is restricted to the binary classification task.

    The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. 
    The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.

    The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. 
    The recall is intuitively the ability of the classifier to find all the positive samples.

    The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This 
    ensures that the graph starts on the y axis.

    The first precision and recall values are precision=class balance and recall=1.0 which corresponds to a classifier 
    that always predicts the positive class.

  Code:
    y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,
                                 method="decision_function")
    from sklearn.metrics import precision_recall_curve
    precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
    
    plt.figure(figsize=(8, 4))  # extra code - it's not needed, just formatting
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)
    plt.vlines(threshold, 0, 1.0, "k", "dotted", label="threshold")
    
    # extra code - this section just beautifies and saves Figure 3-5
    idx = (thresholds >= threshold).argmax()  # first index >= threshold
    plt.plot(thresholds[idx], precisions[idx], "bo")
    plt.plot(thresholds[idx], recalls[idx], "go")
    plt.axis([-50000, 50000, 0, 1])
    plt.grid()
    plt.xlabel("Threshold")
    plt.legend(loc="center right")
    save_fig("precision_recall_vs_threshold_plot")
    
    plt.show()


The ROC Curve:

  The receiver operating characteristic (ROC) curve plots the True Positive rate (TPR) (aka recall) against 
    the False Positive Rate (FPR).
  The FPR (aka fall-out) is the ratio of negative instances incorrectly classified as positive.
  The TNR (true negative rate) (aka specificity) is the ration of negative instances that are correctly classified as negative. 
   
           ROC Curve: plots  TPR (aka recall)  versus   FPR 
                      -> equivalent: sensitivity (recall) versus 1 - specificity

           TPR or Recall or sensitivity:       TP  / (TP + FN)

           TNR or specificity:                 TN / (FP + TN)   

           FPR (or fall-out):                  FP  / (FP + TN) = 1 - TNR

           FNR              :                  FN  / (TP + FN) 


https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html
sklearn.metrics.roc_curve

sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)

    Compute Receiver operating characteristic (ROC).

    Note: this implementation is restricted to the binary classification task.

  Code:

    from sklearn.metrics import roc_curve
    fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)
    
    idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()
    tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]
    
    plt.figure(figsize=(6, 5))  # extra code - not needed, just formatting
    plt.plot(fpr, tpr, linewidth=2, label="ROC curve")
    plt.plot([0, 1], [0, 1], 'k:', label="Random classifier's ROC curve")
    plt.plot([fpr_90], [tpr_90], "ko", label="Threshold for 90% precision")
    
    # extra code - just beautifies and saves Figure 3-7
    plt.gca().add_patch(patches.FancyArrowPatch(
        (0.20, 0.89), (0.07, 0.70),
        connectionstyle="arc3,rad=.4",
        arrowstyle="Simple, tail_width=1.5, head_width=8, head_length=10",
        color="#444444"))
    plt.text(0.12, 0.71, "Higher\nthreshold", color="#333333")
    plt.xlabel('False Positive Rate (Fall-Out)')
    plt.ylabel('True Positive Rate (Recall)')
    plt.grid()
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="lower right", fontsize=13)
    save_fig("roc_curve_plot")
    
    plt.show()


AUC (Area Under the Curve):
  - one way to compare classifiers is to measure the AUC.
  - a perfect Classifier will have and ROC AUC equal to 1.
  - a purely random Classifier will have and ROC AUC equal to 0.5.

  Code:
    from sklearn.metrics import roc_auc_score
    roc_auc_score(y_train_5, y_scores)

ROC (receiver operating characteristic) curve vs PR (Precision/recall) curve:

  As a rule of thumb, we should prefer the PR curve whenever the positive class is rare or when we care more 
  about the false positives than the false negatives. 
  Otherwise, use the ROC curve.

ROC Curve and AUC
  https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc

  ROC curve (receiver operating characteristic curve) 
    - is a graph showing the performance of a classification model at all classification thresholds. 
    - This curve plots two parameters:
        True Positive Rate (TPR)   on Y-axis
        False Positive Rate (FPR)  on X-axis

    True Positive Rate (TPR) (as called recall) 
       TPR = TP / (TF + FN)

    False Positive Rate (FPR):
       FPR = FP / (FP + TN)
 

  AUC: Area Under the ROC Curve
    - measures the entire two-dimensional area underneath the entire ROC curve (think integral 
      calculus) from (0,0) to (1,1).
    - provides an aggregate measure of performance across all possible classification thresholds. 
    - One way of interpreting AUC is as the probability that the model ranks a random positive 
      example more highly than a random negative example. 
    - AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; 
      one whose predictions are 100% correct has an AUC of 1.0.`


Understanding the ROC and AUC Intuitively
  https://medium.com/@shaileydash/understanding-the-roc-and-auc-intuitively-31ca96445c02

  Accuracy :  (TP + TN)  / (TP + FP + TN + FN)
    - probability that the test yeilds a correct result
    - how accurate is our model in making predictions correctly.
    - less effective with a lot of true negatives
        - example: predicting fraud with little to no fraud data

  Precision: (TP)  / (TP + FP)
    - accuracy of positive predictions (percentage of positive predictions that were correct)
    - use Precision when we need to check the accuracy of our forecast and when the forecast is being used 
       for an action
    - Use when the cost of false positives is high
         - example: an email is flagged and deleted as spam when it really isn't
         - example: correctly predict male vs female visitors based on browing history to make geneder specific offers

  Recall: (TP)  / (TP + FN)
    - also called sensitivity or true positive rate (TPR)
    - percentage of actual positive predictions that were correctly identified:
    - used when you seek to minimize a false negatives.
    - Use when the cost of false negatives is high
       - example: someone has cancer, but screening does not find it
           - extensively used in medical testing and imaging false negatives often need to be minimized


  F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
    - combines precision and recall score
    - weighted harmonic mean of the precision and recall 
    - uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.
       - that is, Harmonic means is more robust to outliers
    - regular mean treats all values equally, the harmonic mean give more weight to low values (non outliers)
      - classifiers will only get high F1 Score if both recall and precision values are high

           Equation 3-3: F1 score:

           F1 = 2 / [ (1/precision) + (1/recall)]  =  2 x [( precision x recall) / (precision + recall)] 

              = (TP)  / [TP + ((FN + FP) / 2)]


   ROC ( Receiver Operating Curve ):
     - An ROC is a system which seeks to determine how accurately medical diagnostics tests can distinguish 
       between positive and negative cases
     - created by plotting the true positive rate (TPR) (y-axis) against the false positive rate (FPR) (x-axis) 
       at various threshold settings. 
     - Some key metrics used by ROC are:

       Accuracy                      =  (TP + TN)  / (TP + FN + FP + FN)       
          - probability that the test yeilds a correct result

       TPR, Recall, Sensitivity, TPF =  TP  / (TP + FN)       
          - probability that a true positive case will be positive
          - Use when the cost of false negatives is high
          - also called: TPF (True Positive Fraction)
          - example: someone has cancer, but screening does not find it

       Specificity, TNR, TNF     =  TN  / (TN + FP)         
          - probability that a true negative case will be negative
          - also called: TNR (True Negative Rate) and TNF (True Negaive Fraction)
          - example: Out of all the people that do not have the disease, how many got negative results?

       FPR, (1 - Specifity), FPF   =  FP  / (FP + TN)
          - probability that a true negative case will be positive

       Precision, PPV                =  (TP)  / (TP + FP)
         - probability that a positive test result will truly be actual positive
         - accuracy of positive predictions (percentage of positive predictions that were correct)
         - also called: PPV (Positive Predictive Value)
         - Use when the cost of false positives is high
           - example: an email is flagged and deleted as spam when it really isn't
           - example: correctly predict male vs female visitors based on browing history to make geneder specific offers

       NPV (Megative Predictive Value) =  (TN)  / (TN + FN)
          - probiliity of a negative test results will be actual negative 


   Example 

                     Actual Disease status 
                       Pos         Neg
      predicted Pos    27 (TP)     173  (FP)
                Neg    73 (FN)     727  (TN)



      Sensitivity:   27 / (27 + 73)    = .27

      Specifity  :   727 / (727 + 173) = .808

      FPR        :     1 - specificity = 1 - .81 = .19
                  or:  173 / (173 - 727)         = 0.19 

      Accuracy  :   27 + 727 / (27 + 73 + + 173 + 727) = .754

      Precision:   27 / (27 + 173) = 0.135


      NPv:         727 / (727 + 73) = .909


  ROC Curvers Use and Interpretation:
    - plot of TPR (sensitvity) on y-axis and FPR (1 - specificity) on x-axis
    - visual display of how sensitivity and specifity vary as our threshold varies

  Area under the Curve (AUC)
    - total area of the grid represented by an ROC is 1, since but TPR and FPR range from 0 to 1
    - the portion of this total area that falls below the ROC curve is known as the Area under the Curve (AUC)

  Area under the Curve (AUC) Interpretation
    - The AUC serves as a quantitative summary of the strength of association between the underlying test statistic and disease status
    - An AUC of 1.0 would mean that the test statistic could be used to perfectly discriminate between cases and controls 
    -An AUC of 0.5 (reflected by the diagonal 45 line) is equivalent to simply guessing

    

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
sklearn.ensemble.RandomForestClassifier

class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]

    A random forest classifier.

    A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the 
    dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled 
    with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.

    For a comparison between tree-based ensemble models see the example Comparing Random Forests and Histogram Gradient Boosting models.

  Code:

from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state=42)
y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method="predict_proba")
y_scores_forest = y_probas_forest[:, 1]
precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve( y_train_5, y_scores_forest)

Multiclass Classification

  - binary Classifiers disinguish betwen 2 classes
  - Multiclass classifiers (aka multinomial classifiers) distinguish between more than 2 classes
  - when you run an scikit-learn binary classification algorithm (e.g. SVC) for a multiclass classification task,
    it automatically runs OvR or OvO, depending on the algorithm
  - if you want to run a one-versus-one (OvO) or one-versus-rest (OvR), you can use the OneVsOneClassifier or
    ONveVsRestClassifier classes

  OvR (one-versus-the-rest) strategy (aka one-versus-all - OVA)
   - OvR strategy splits a multi-class classification into one binary classification problem per class.
   - for binary classification algorithms, OvR strategy is preferred

  OvO (one-versus-one) strategy 
    - train binary classifier for every pair of digits (e.g. 0s vs 1s, 0s vs 2s , .... , 8s vs 9s)
    - if there are N classess, this means you need to train: N x (N - 1) / 2 (e.g. for MNIST: 10 x (10 - 1) / 2 = 45
    - advantage is that you only have to train on a part of the training set data containing the two classes

MultiLabel Classification

  - classification system that outputs multiple classes (multiple binary tags) for each instance

  Code:
    import numpy as np
    from sklearn.neighbors import KNeighborsClassifier
    
    y_train_large = (y_train >= '7')
    y_train_odd = (y_train.astype('int8') % 2 == 1)
    y_multilabel = np.c_[y_train_large, y_train_odd]
    
    knn_clf = KNeighborsClassifier()
    knn_clf.fit(X_train, y_multilabel)

https://numpy.org/doc/stable/reference/generated/numpy.c_.html
numpy.c_ = <numpy.lib.index_tricks.CClass object>

  Translates slice objects to concatenation along the second axis.

  Examples

  np.c_[np.array([1,2,3]), np.array([4,5,6])]
  array([[1, 4],
         [2, 5],
         [3, 6]])


MultiOutput Classification:

  multioutput-multiclass classification (or just multioutput classification) - a generalization of multilabel
     classification where each label can be multiclass (have more than 2 possible values)

  Code:
     np.random.seed(42)  # to make this code example reproducible
     noise = np.random.randint(0, 100, (len(X_train), 784))
     X_train_mod = X_train + noise
     noise = np.random.randint(0, 100, (len(X_test), 784))
     X_test_mod = X_test + noise
     y_train_mod = X_train
     y_test_mod = X_test
     
     knn_clf = KNeighborsClassifier()
     knn_clf.fit(X_train_mod, y_train_mod)
     clean_digit = knn_clf.predict([X_test_mod[0]])
     plot_digit(clean_digit)
     save_fig("cleaned_digit_example_plot")  # extra code  saves Figure 313
     plt.show()

https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html
numpy.random.randint
random.randint(low, high=None, size=None, dtype=int)

  Return random integers from low (inclusive) to high (exclusive).

  Return random integers from the "discrete uniform" distribution of the specified dtype in the 
  "half-open" interval [low, high). If high is None (the default), then results are from [0, low).

Chapter 3 Exercises:

    -> see exercise_notebooks/03_exercises.ipynb

Exercise 1:

knn_clf1 = KNeighborsClassifier()
knn_clf1.fit(X_train, y_train_mod)


------------------------------------------------------
Chapter 4 Training Models
------------------------------------------------------

4.1 Linear Regression (pages 132 - 137)

     Linear model:
      - makes a prediction by simple computing a weighted sum of the input features pus a constanct 
        called the 'bias term' (also called the intercept)

  Linear Regression (pages 132 - 134)
      Equation 4-1 Linear regression model prediction:

      pred-y = theta0 + theta1 * x1 + theta2 * x2 + ... + theta-n * xn
        pred-y: the predicted value
        n:      the number of features
        xi:     the ith feature value
        theta-j: the jth model parameter, including the bias term, theta0

    training linear regression model:
      - find the value of theta that minimizes the RMSE
      - but it is simplier to minimize the MSE (mean square error), but leads to the same result

    equation 4-3: MSE cost functon for a linear regression model

       MSE (X, h-theta) = (1/m) SUM (theta-T * xi - yi)2  where SUM is from i = 1 to m


  Normal Equation (pages 134 - 137)

    Normal Equation:
      - a closed-form solution to find theta that minimizes the MSE

    Equation 4-4 Normal Equation

     pred-theta = (XT @ X)**-1  @ XT @ y

        in this equation:
         pred-theta: the value of theta that minimizes the cost function
         y: the vector of the target values containing y1 to ym
         XT: matrix X transposed

      >>> theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

    Example: Using Linear Regression classifer

     >>> from sklearn.linear_model import LinearRegression
     >>> 
     >>> lin_reg = LinearRegression()
     >>> lin_reg.fit(X, y)
     >>> lin_reg.intercept_, lin_reg.coef_

    LinearRegression Classifier:
      - separates the 'bias term' (intercept_) from the feature weight coefficient (coef_)
      - based on the scipy.linalg.lstsq() function 

  Computation Complexity (pages 137 - 138):
    - normal equation computes the inverse of XT * X, which is an (n + 1) x (n + 1) matrix (where n is the
      number of features.
      - computation complexity of inverting such a matrix  is typically about O(n**2.4) to O(n**3)
      - if you double the number of features, you multiple the computation time by roughly 2**2.4 = 5.3 to 2**3 = 8
      - the SVD approach used by the Scikit-Learn's LinearRegression class is about O(n**2)

4.2 Gradient Descent (pages 138 - 149)

  Gradient Descent (pages 138 - 142)
    - general idea it tweak parameters iteratively in order to minimize the cost function`
    - measures the local gradient of the error function with regard to the parameter vector theta,
      and it goes in the direction of the descending gradient
    - once the gradient is zero, you have reached a minimum
    - in practice, you start by filling theta with random values (called 'random initialization'). Then you
      improve gradually, attempting to decrease the cost function (e.g. MSE) until the algorithm has 
      converged on a minimum
    - training a model means searching for a combination of model parameters that minimize the cost function
      (over the training set). It is a search of the 'parameter space'. 
      - The more parameters model has, the more dimensions this space has, and the harder the search is: 
        searching for a needle in a 300 parameter haystack is much tricker than in 3 dimensions

      learning rate:
        - hyperparameter controling the size of the gradient descent step
        - if too small, then algorithm will have to go through many steps to converge
        - if too high, you might jump across valleys and up on the other side, possibly even higher up

      MSE cost function:
        - MSE is a convex function for linear regression model which means that if you pic any 2 points
          on the curve, the line segment joining them is never below the curve
         - this implies that there are no local minima, just one global minium

     Scale:
       - when using gradient descent, you should ensure that all features have a similar scale (e.g.
         StandScalar), or elase it will take a long time to converge

  Batch Gradient Descent (pages 142 - 144)

    Partial Derivative
      - to implement Gradient Descent, you to compute the gradient of the cost function with regards to
        each model parameter theta-j. 
      - In other workds, you need to calculate to calculate how mouch the cost function will change if you
        change theta-j just a little bit. This is called the 'partial derivative'

        Equation 4-5 Partial Derivative of this cost function

            (d/d-theta-j) MSE(theta) = (2/m) SUM (theta-T * xi - yi) xi-j

               where: d/d-theta-j) : partial derivative relative to theta-j
                       SUM    : sum from i = 1 to i = m
                       theta-T: theta transposed
                       xi-j:  : jth value in the xi vector, e.g. x[i,j] 


       - instead of calculating the partial derivatives individually, you can use the equation 4-6 to
         calculate them all at once.
       - the gradient vector,  grad MSE(theta), contains all the partial derivatives of the cost function
         (one for each model parameter)

         Equation 4-6 Gradient vector of the cost function

                                     | (d/d-theta-0) | 
             grad-theta MSE(theta) = | (d/d-theta-1) | =  (2/m) X-T(X*THETA - y) 
                                     |      .        | 
                                     |      .        | 
                                     |      .        | 
                                     | (d/d-theta-n) | 


                    where: X-T   : X matrix transpose
                           X     : X matrix
                           THETA : matrix


    batch gradient descent:
      -  gradient vector of the cost function involves calculation over the full training
         set X, at each gradient descent. That is why the algorithm is called 'batch gradient descent'
      - it uses the whole batch of training data at every step (full gradient descent would be
         at better name)
      - as a result, it is terribly slow on very large datasets
      - however, it scales well with the number of features: training a linear regression model with
        hundreds of features is much faster with gradient descent that using then Normal Equation or
        SVD decomposition


      Gradient Descent step
        - once you have the gradient vector which points up uphill, just go in the opposite direction
          to go downhill. This means subtracting the grad-theta MSE(THETA) from THETA.
          - this is where the learning rage, eta, comes into play: multiple the gradient vectory by
            eta to determine the size of the downhill step

         Equation 4.7 Gradient step:

            theta-next_step = THETA - (eta) grad-theta MSE(THETA)

      Example: [batch] Gradient Descent algorithm

        >>> eta = 0.1  # learning rate
        >>> n_epochs = 1000
        >>> m = len(X_b)  # number of instances
        >>> 
        >>> np.random.seed(42)
        >>> theta = np.random.randn(2, 1)  # randomly initialized model parameters
        >>> thetalist = np.empty((1,2), float)
        >>> gradientslist = np.empty((1,2), float)
        >>> for epoch in range(n_epochs):
        >>>     gradients = 2 / m * X_b.T @ (X_b @ theta - y)
        >>>     theta = theta - eta * gradients
        >>>     gradientslist = np.append(gradientslist, gradients.T,axis=0)
        >>>     thetalist = np.append(thetalist, theta.T, axis=0)

      Epoch:
        - each iteration over the training set is called an 'epoch'

       finding a good learning rate, eta:
         - you can use a grid search to find a good learning rate
         - however, you may want to limit the number of 'epochs' so the grid search can eliminate models
           that take to long

       Setting the number of Epochs:
         - a simple solution is to set a very large number of epochs but interrupt the algorithm when the 
           gradient descent vector becomes tiny - that is, when it s norm becomes smaller that a tiny 
           number 'epsilon' (called 'tolerance') because this happens when gradient descent has almost
           reached its minimum.

  Stochasitc Gradient Descent (pages 145 - 148)

     Batch gradient descent limitation:
       - the main problem with batch gradient descent is that fact that it uses the whole training set to
         compute the gradients at every step, which is very slow when the training set is large

     Stochastic gradient descent:
       - picks a random instance in the train set at every step and computes the gradients based only a
         single instance
       - working on a single instance at a time makes the algorith very fast
       - stochastic GD can be implemented as an out-of-core algorith
       - using a single instance makes the algorithm much less regular than batch GD: instead of gently 
         decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing
         only on average
       - Once the algorithm stops, the final parameter values will be good but not optimal
       - When the cost function is very irregular, this can actually help the algorithm jump out of local
         minima, so stochastic GD has a better chance of finding the global miniumum than batch GD
         - that is, randomness is good to escape local optima, but bad because it means the algoritm can 
           never settle at the minimum

      Stochastic GD randomness solution:
         - one solution is to gradually reduce the learning rate
         - the steps start out large (which helps make quick progress and escape local minima), then get smaller
           and smaller, allowing the algorithm to settle at the global minimum

      Stochastic GD learning schedule
        - learning schedule is the function that determines the learning rate at each iteration
          - reducing learning rate too quickly, you may get stuck in a local minimum, or even frozen halfway
            to the minimum
          - reducing learning rate too slowly, you may jump around the minimum for a long time and end up with
            a suboptimal solution

          - by convention, you iterate by rounds of 'm' interactions
             - each round is called an 'epoch'

        Code: Stochastic GD simple learning schedule

        >>> n_epochs = 50
        >>> t0, t1 = 5, 50  # learning schedule hyperparameters
        >>> 
        >>> def learning_schedule(t):
        >>>     return t0 / (t + t1)
        >>> 
        >>> np.random.seed(42)
        >>> theta = np.random.randn(2, 1)  # random initialization
        >>> 
        >>> for epoch in range(n_epochs):
        >>>     for iteration in range(m):
        >>>         random_index = np.random.randint(m)
        >>>         xi = X_b[random_index : random_index + 1]
        >>>         yi = y[random_index : random_index + 1]
        >>>         gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m
        >>>         eta = learning_schedule(epoch * m + iteration)
        >>>         theta = theta - eta * gradients

      Stochastic GD instances identically distributed
        - when using stochastic GD, the training instances must be independent and identically distributed (IID)
          to ensure parameters get pulled towards a global minimum, on average
             - for example, you do NOT want instances sorted by labels!
        - a simple way to ensure this is to shuffle the instances during training (e.g. pick each instance
          randomly, or shuffle the training set at the beginning of each epoch

      SGDRegressor class
        - scikit-learn SGDRegressor class can be used to perform linear regression using stochastic GD
        - default to the MSE cost function

        Code: SGDRegression example:

        >>> from sklearn.linear_model import SGDRegressor
        >>> 
        >>> sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,
        >>>                        n_iter_no_change=100, random_state=42)
        >>> sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets

  Mini-Batch Gradient Descent (pages 148 - 149)

    Mini-Batch Gradient Descent 
      - computes the gradient on a small random sets of instances called 'mini-batches'
      - main advantage of mini-batch GD over stochastic GD is hta yuo can ge a performance boost
        from hardware optimization of matrix operations, especially when using GPUs
      - algorithm's progress is less erratic than with stochastic GD, especially with fairly large
        mini-batches
      

    Table 4-1   Comparison of algorithms for Linear Regression    

    Algorthm         Large m   Out-of-Core   Large n    Hyperparams   Scaling     Scikit-Learn
                                support                               Required
    ---------------  --------   ----------   --------   -----------   ---------    ----------
    Normal Equation    Fast        No          Slow         0           No           N/A
    SVD                Fast        No          Slow         0           No           LinearRegression
    Batch GD           Slow        No          Fast         2           Yes          N/A
    Stochastic GD      Fast        Yes         Fast        >= 2         Yes          SGDRegressor
    Mini-Batch GD      Fast        Yes         Fast        >= 2         Yes          N/A

4.3 Polynomial Regression (pages 149 - 151)

  Polynomial Regression
    - making linear models fit non-linear data by adding powers of each feature as new features,
      then train the linear model on the extended feature set

  PolynomialFeature class
    - PolynomialFeatures(degree=d) transforms an array containing 'n' features into an array containing
       (n + d)!/d!n! features where n! is the factorial of n1, equal to 1 x 2 x 3 x ... x n. 
    - beaware of the combinatorial explosion of the number of features
    - example PolynomialFeatures of degree=3 would not only add the features a**2, a**3, b**2, b**3, 
      but also ab, (a**2)b, and a(b**2).

    Code: PolynomialFeature class used with LinearRegression class to solve a 'y = a(x**2) + bx + c'
          quadratic equation 

        >>> from sklearn.preprocessing import PolynomialFeatures

        >>> # create quadratic equation
        >>> np.random.seed(42)
        >>> m = 100
        >>> X = 6 * np.random.rand(m, 1) - 3
        >>> y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)

        >>> # add PolynomialFeatures
        >>> poly_features = PolynomialFeatures(degree=2, include_bias=False)
        >>> X_poly = poly_features.fit_transform(X)
        >>> X[0]  
        >>> array([-0.75275929])
        >>> X_poly[0]

        >>> # use with LinearRegression to solve
        >>> array([-0.75275929,  0.56664654])
        >>> lin_reg = LinearRegression()
        >>> lin_reg.fit(X_poly, y)
        >>> lin_reg.intercept_, lin_reg.coef_
        >>> (array([1.78134581]), array([[0.93366893, 0.56456263]]))


4.4 Learning Curves (pages 151 - 155)

   High Degree Polynomial vs plain linear model
     - with high degree Polynomial regression, you will like fit the training data much better than
       plain linear regression, but this may be due to overfitting while the linear regression may
       be under fitting

    Cross-Validation 
       - can be used to estimate a model's generization performance
       - overfitting: if a model performs well on the training data but generizes poorly according to 
         the cross-validation metrics
       - underfitting: if a model performs poorly on both the training data and generalization according 
         to the cross-validation metrics

    Learning Curve
      - plots of the model's training error and validation error as a function of the training iteration
      - that is, just evaluate the model at regular intervals during training on both the training set 
        and validation set, plot the results 
      - if the model cannot be trained incrementally (does not support partial_fit() or warm_start(), then
        you must train it several times on gradually large subsets of the training set

    Scikit-Learn learning_curve() function
      - trains and evaluates the model using cross-validation
      - by default it retrains the model on growing subsets of the training set, but if the model support
        incremental learning, you can set 'exploit_increment_learning=True', and it will train the model
        incrementally instead

        underfitting learning_curves plots:
          - training error and validation error both plateau at similar straight lines with high error rates

        overfitting learning_curves plots:
          - the training errors and validation errors plateau with a significant gap where the training
            error is lower than the validation errors

      Improving underfitting
        - use a better model
        - come up with better features

      Improving Overfitting
         - feed more training data until the validation error reaches the training error

     The Bias / Variance Trade-off
       - a model generalization error can be expressed at the sum of three very different errors:
       Bias:
         - the part of the generalization to wrong assumptions, such as assuming the data is linear
           when it is actually quadratic
         - a high-bias model is most likely underfitting the training data
       Variance:
         - this part is due to the model's excessive sensitivity to small variation in the training data
         - a model with many degrees of freedom (such as a high-degree polynomial model) is likely  to
           have high-variance and thus 'overfit' the training data
       Irreduclibe Error:
         - due to the noiseness of the data
         - the only way to reduce this part of the error is to clean-up the data (fix the data sources, such
           as a broken sensor, or detect and remove outliers)
        Note:
          - Increasing a model's complexity will typically increases its variance and reduce its bias
          - Decreasing a model's complexity will typically increases its bias and reduce its variance
       
4.5 Regularized Linear Models (pages 155 - 164)

  Regularizing Linear models
  - a good way to reduce overfitting is to regularize model (constrain it); the fewer degrees of freddom
    it has, the harder it will be to overfit the data
  - a simple way to regularize a polynomial model is to reduces the number of polynomial degrees
  - for linear models, regularization is typically achieved by weight of the models

  Ridge Regression (pages 156 - 158)
    - a regularized version of linear regression with a regularization term equal to:
       (alpha/m) SUM theta-i**2  (where SUM is from i=1 to i=n) is added to the MSE
    - this forces the learning algorithm to not fit the data but also keep the model weights as small
      as possible
    - regularization term should not be added to cost function during training
    - that is, evaluate the model's performance using unregularized MSE (or RMSE)

    The hyperparameter 'alpha':
        - controls how much you want to regularize the model
        - if alpha = 0, then ridge regression is just linear
        - if alpha is large, then all weights end up near zero and the result is a flat line going
          through the data's mean
        - increasing alpha leads to flatter (i.e. less extreme, more reasonable) predictionss, thus
          reducing the model's variance, but increasing it bias

      equation 4-8 Ridge Regression cost function:

          J(THETA) = MSE(THETA) + (alpha/m) SUM theta-i**2  (where SUM is from i=1 to i=n)

          Notes:
             - the bias term theta-0 is not regularized (SUM starts at i = 1, not i = 0)
             l2 norm:
             - if you define 'w' as the vector features of weights theta-1 to theta-n, then the
               regularization term is equal to [alpha(||w||2)**2 / m] where ||w||2 represents the
               'l2 norm' of the weighted vector
             for batch gradient descent:
               - just add (2*alpha*w / m) to the MSE gradient vector that corresponds to the feature 
                 weights without adding anything to gradient of the bias term
             scaling:
              - it is important to scale the data (e.g. using StandardScaler) before performing 
                ridge regression, as it is sensitive to the scale of the input features. This is
                true for most regularized models
          

        Ridge Regression closed-form equation or performing gradient descent
          - the pros and cons are the same for both
          - Closed-form Equation

          equation 4-9 Ridge Regression Closed-form solution

                pred-THETA = (X-transposed * X + alpha * A)**-1 X-transpose * y
                  where:
                     alpha * A: added term to normal equation

      Code: Ridge Regression Using Ridge class that matrix factorization technique ty Andre-Louis Choleesky) 

        >>> from sklearn.linear_model import Ridge
        >>> 
        >>> ridge_reg = Ridge(alpha=0.1, solver="cholesky")
        >>> ridge_reg.fit(X, y)
        >>> ridge_reg.predict([[1.5]])
            array([[1.55325833]])

      Code: Stochastic Gradient Descent with Ridge Regression Regularization (via 'penalty="l2")

        >>> from sklearn.linear_model import SGDRegressor
        >>> sgd_reg = SGDRegressor(penalty="l2", alpha=0.1 / m, tol=None,
        >>>                        max_iter=1000, eta0=0.01, random_state=42)
        >>> sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets
        >>> sgd_reg.predict([[1.5]])
            array([1.55302613])

      Code: Ridge Regression Using Stochastic Gradient Descent (via solver="sag")
        >>> # extra code - show that we get roughly the same solution as earlier when
        >>> #              we use Stochastic Average GD (solver="sag")
        >>> ridge_reg = Ridge(alpha=0.1, solver="sag", random_state=42)
        >>> ridge_reg.fit(X, y)
        >>> ridge_reg.predict([[1.5]])

  Lasso Regression (pages 158 - 161)

    Least Absolute Shrinkage and Selection Operator (LASSO) regression
      - adds a regularization term to the cost function, but adds an l1 norm of the weight factor
        (instead of square of the l2 norm added by the ridge regression)
      - l1 norm is multipled by [2 * alpha] (where ridge l2 norm is multipled by [alpha / m] ) 
      - lasso tends to eliminate the weights of the least important features (i.e. sets them to 0)
      - lasso automatically performs feature selection and outputs a sparse model with few non-zero
        feature weights


      equation 4-10 Lasso Regression cost function:

          J(THETA) = MSE(THETA) + (2* alpha) SUM |theta-i|  (where SUM is from i=1 to i=n)

          Notes:
             - the bias term theta-0 is not regularized (SUM starts at i = 1, not i = 0)

     Learn rate:
       - to keep gradient descent from bouncing around the optimum at the end when using the 
         lasso regressions, you need to gradually reduce the learning rate during training.
       - it will still bounce aroudn the optimum, but the steps will get smaller and smaller, so
         it will converge

      Lasso is not differentiable 
         - Lasso is not differentiable at theta-i = 0 (for i = 1, 2, ..., n), but gradient descent
           still works if you the 'subgradient vecotr 'g' instead when theta-i = 0
         - 

      Equation 4-11 Lasso regression subgradient vector
                                                                 /               \
                                                                 | sign(theta-1) |
           g(theta, J) = gradient-theta * MSE(theta) + 2 * alpha | sign(theta-2) | 
                                                                 |     .         |
                                       /                  \      |     .         |
                                       |-1 if theta-i < 0 |      |     .         |
                where sign (theta-i) = | 0 if theta-i = 0 |      | sign(theta-n) |
                                       |+1 if theta-i > 0 |      \               /
                                       \                  /

                 Notes: 
                       g:     subgradient vector
        

      Code: Lasso Regression - Linear model trained with L1 (lasso regularization)
        >>> from sklearn.linear_model import Lasso
        >>> 
        >>> lasso_reg = Lasso(alpha=0.1)
        >>> lasso_reg.fit(X, y)
        >>> lasso_reg.predict([[1.5]])
            array([1.53788174])

  Elastic Net Regression (pages 161 - 162)

    Elastic Net Regression
      - regularization term is a weighted sum of both ridge and lasso's regularization term,
        and you can control the mix ratio 'r'.
        - when 'r = 0', elastic net is equivalent to ridge regression
        - when 'r = 1', elastic net is equivalent to lasso regression

     Equation 4-12 Elastic net cost function


          J(THETA) = MSE(THETA) + r(2* alpha) SUM |theta-i|) + (1 - r) (alpha/m) SUM theta-i**2 
                                                      (where SUM is from i=1 to i=n)

           Note: r: lasso / ridge regularization mix ratio

      elastic net vs ridge vs lasso vs plain linear regression
         - preferable to have at least a little bit of regularization
         - Ridge is the default
         - if only a few features are useful, you should prefer lasso or elastic net because they
           tend to reduce useless features
         - in general elastic net is preferred over lasso behaves erractically when the number of features
            greater than the number of training instances or when several features are strongly 
            correlated.

      Code: Elastic Net Regression - Linear model trained with ridge (l2) & lasso (l1) regularization
        >>> from sklearn.linear_model import ElasticNet
        >>> 
        >>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
        >>> elastic_net.fit(X, y)
        >>> elastic_net.predict([[1.5]])
            array([1.54333232])

  Early stopping (pages 162 - 164)

     Early Stopping
       - regularization by stop training as soon as the validation error reaches a minimum

     Early stopping with Stochastic GD and mini-batch GD
       - with stochastic GD and mini-batch GD, validation error & training error curves are not so smooth,
         and it may be hard to know whether you have reached a mimimum or not
         - one solution is top only after validation error has been above the minimum for awhile, then
         roll back the parameters to to the point when the validation error was at a minimum

      Code: Stochastic GD with Early stopping regularization example code
            Note: does not actual stop training, but lets you revert to best model

        >>> from copy import deepcopy
        >>> from sklearn.metrics import mean_squared_error
        >>> from sklearn.preprocessing import StandardScaler
        >>> 
        >>> # extra code - creates the same quadratic dataset as earlier and splits it
        >>> np.random.seed(42)
        >>> m = 100
        >>> X = 6 * np.random.rand(m, 1) - 3
        >>> y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)
        >>> X_train, y_train = X[: m // 2], y[: m // 2, 0]
        >>> X_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]
        >>> 
        >>> # add polynomial features to data and standardize
        >>> preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),
        >>>                               StandardScaler())
        >>> X_train_prep = preprocessing.fit_transform(X_train)
        >>> X_valid_prep = preprocessing.transform(X_valid)
        >>>
        >>> sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)
        >>> n_epochs = 500
        >>> best_valid_rmse = float('inf')
        >>> train_errors, val_errors = [], []  # extra code - it's for the figure below
        >>> 
        >>> for epoch in range(n_epochs):
        >>>     sgd_reg.partial_fit(X_train_prep, y_train)
        >>>     y_valid_predict = sgd_reg.predict(X_valid_prep)
        >>>     val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)
        >>>     if val_error < best_valid_rmse:
        >>>         best_valid_rmse = val_error
        >>>         best_model = deepcopy(sgd_reg)
        >>> 
        >>>     # extra code - we evaluate the train error and save it for the figure
        >>>     y_train_predict = sgd_reg.predict(X_train_prep)
        >>>     train_error = mean_squared_error(y_train, y_train_predict, squared=False)
        >>>     val_errors.append(val_error)
        >>>     train_errors.append(train_error)


           Note: SGDRegressor.partial_fit(X, y[, sample_weight])
                    - Perform one epoch of stochastic gradient descent on given samples.

4.3 Logistic Regressions (pages 164 - 173)

    Logistic Regressions 
      - logistic regressions (also called logit regression) is used to estimate the probability that an
        instance belongs to a particular class 
      - if the probability estimate is greater than the threshold (typicall 50%), then it predicts the
        instance belongs to the class (called 'positive label')
      - if the probability estimate is less than the threshold (typicall 50%), then it predicts the
        instance does not belong to the class (called 'negative label')
      - this is a 'binary classifier'

  Estimating Probabilities (pages 164 - 165)

    logistic regression model
      - computes the weighted sum of the input features (plus a bias term), but instead of outputing the 
        results directly like the linear regression model does, it outputs the 'logistic of this result'

     Equation 4-13 Logistic regression model estimated probability (vectorized form)

        pred-p = h-theta(x) = sigma(theta-transpose * x)


     logistic function:
        - a sigmoid function (i.e. S-shared) that a number between 0 and 1
        - it is defined in Equation 4-14

     Equation 4-14 Logistic Function

        sigma(t) = 1 / (1 + exp(-t))

           t: score
         
          where exp:  e exponential function

      logit:
        - the score 't' is often called the 'logit'
        - the name comes from the logit function, defined as logit(p) = log(p/(1 - p)),
          is the inversed of the logistic function
        - if you compute the 'logit' of the estimated probability 'p', you will find the result is 't'

  Training and Cost Functions (pages 165 - 167)

    Equation 4-16. Cost function of a single training instance
      
                  /
       c(theta) = | -log(pred-p)     if y = 1
                  | -log(1 - pred-p) if y = 0
                  \

                  p: probability

     Cost function
       - the objective of training is to set the parameter vector theta so that the model estimates high
         probabilities for positive instances (y = 1) and low probabilitys for negative instances (y = 0) 
       - this cost function makes sense because -log(t) grows large when 't' approaches '0', so the cost
         will be large if the models estimates a proabability close to '0' for a positive instance
       - -log(t) is close to '0' when 't' (score) is close to '1', so the cost will be close to '0' if the 
         estimated probability is close to '0' for a negative instance or close to '1' for a 'positive instance'
     
     Cost function over the whole training set
       - is the average cost over all the training instances
       - single expressioin called the 'log loss'

       Equation 4-17 Logistic regression cost function (log loss)

         J(theta) = (1/m) SUM [yi * log(pred-pi) + (1 - yi) * log(1 - pred-pi)]
                 where SUM is from i = to i = m


     Log loss assumption
       - minimizing the log loss will result in the model to maximumum likelihood of being optimal
         ASSUMING the instances follow a Gaussian distribution around the mean of their class
       - when using the log loss, you are assuming a Gaussian distribution
       - the more wrong this assumption is, the more biased the model will be

     Linear regression assumption
       - when you use MSE to train a linear regression model, you are implicitly assumping the data
         is purely linear, plus some Gaussian noise
       - if the data is NOT linear (e.g. if its quadratic) or if the noise is not Gaussian noise
         (e.g. if outliers are not exponentially rare), then the model will be biased


     Equation 4-18. Logistic cost function partial derivative

            (d/d-theta-j) J(theta) = (1/m) SUM [sigma((theta-T * xi) - yi) xi-j

               where: d/d-theta-j) : partial derivative relative to theta-j
                       SUM    : sum from i = 1 to i = m
                       theta-T: theta transposed
                       xi-j:  : jth value in the xi vector, e.g. x[i,j] 

  Logistic Training using Logistics cost function partial derivative
    - equation 4-18 is very similar to equation 4-5 ([Batch GD] Partial derivative of the cost function)
      - main difference is the 'sigma' factor
    - Thus, the logistic gradient vector cost function will be similar and can be plugged into batch GD:
          (d/d-theta-j) MSE(theta) = (2/m) SUM (sigma * theta-T * xi - yi) xi-j
    - once you have the gradient vector containing all the partial derivatives, you can use it
      in the batch GD algorithm - that is, you know how to train a logistic regression
    - for stochastic GD, you would take one instance at a time, and for mini-batch GD, you would
      use a mini-batch at a time

  Decision Boundardies (pages 167 - 170)

     - using Iris dataset to illustrate logistic regression

    Iris dataset
      - iris dataset contains the sepal and petal length and width of 150 iris flower of three different
        species: Iris setosa, Iris vericolor, and Iris virginica (50 of each)

    Code: loading Iris dataset

        >>> from sklearn.datasets import load_iris
        >>> 
        >>> iris = load_iris(as_frame=True)
        >>> list(iris)
        ['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']
        
        >>> print(iris.DESCR)  # extra code - it's a bit too long
           .. _iris_dataset:
           Iris plants dataset
           --------------------
           . . .
        >>> 
        >>> iris.data.head(3)
           sepal length (cm) 	sepal width (cm) 	petal length (cm) 	petal width (cm)
           0 	        5.1 	             3.5 	              1.4              	0.2
           1 	        4.9 	              3.0 	              1.4 	        0.2
           2 	        4.7 	              3.2 	              1.3 	        0.2

        >>> 
        >>> iris.target.head(3)  # note that the instances are not shuffled
        0    0
        1    0
        2    0
        Name: target, dtype: int32

        >> iris.target_names
           array(['setosa', 'versicolor', 'virginica'], dtype='<U10')

    Code: Using LogisticRegression with Iris dataset to predict Iris virginica based on petal width

        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.model_selection import train_test_split
        >>> 
        >>> X = iris.data[["petal width (cm)"]].values
        >>> y = iris.target_names[iris.target] == 'virginica'
        >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
        >>> 
        >>> log_reg = LogisticRegression(random_state=42)
        >>> log_reg.fit(X_train, y_train)        
        >>> 
        >>> X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector
        >>> y_proba = log_reg.predict_proba(X_new)
        >>> decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]
        >>> decision_boundary
            1.6516516516516517
        >>> log_reg.predict([[1.7], [1.5]])
            array([ True, False])

    decision boundary
      - the point where the model estimates a 50% probability

     LogisticRegression predict() method vs predict_prob() method
        - predict(): returns which ever class is the most likely
           - if two classes have over 50% probability, it will return the class with the highest probability
        - predict_prob(): returns a 2D vector with the probabilities for each class where the sum of
          the probabilities equals 1

     LogisticRegression regularization
       - the hyperparameter controling the regularization strength of a Scikit Learn LogisticRegression
         is NOT 'alpha' (as in other linear models), but its inverse 'C'. 
       - the higher the value of 'C', the less the model is regularized
       - logisticRegression models can regularized using l1 or l2 penalities. l2 penalty is by default.


  Softmax Regressions (pages 170 - 173)

    Softmax regression or multinomial logistic regression
      - logistic regression models can be generalized to support multiple classes directly without having
        to train and combine multiple binary classifiers
      - when given an instance 'x', the model first computes sk(x) for each class 'k', then estimates the
        probability of each class by apply the softwmax function (also called the normalized exponential)
        to the scores

        Equation 4-19. Softmax score for class 'k'. page 170

           sk(x) = (theta-k)-T * x
                where: 
                     (theta-k)-T:  the transpose of theta-k
                     theta-k:     parameter vector for class 'k' (each class has its own paramter vector)
                      
        Probabilities using Softmax function:
           - once you have computed the score of every class for the instance 'x', you can estimate the
             probabilities prob-k that the instance belongs to the class 'k' by running the scores
             through the 'softmax function'
           - softmax function computes the exponential of every score, the normalizes them (dividing by the
             sum of all the exponentials). 
             - the scores are generally called logits or log-odd (although they are actually unnormalized
             log-odds)

        Equation 4-20. Softmax function. page 170

        prob-k = sigma(s(x))k = exp (sk(x)) / [ SUM exp(sj(x)) ]

             where: SUM is from j = 1 to j = K
                    K is the number of classes
                    s(x) is a vector containing the scores of each class for the instance x
                    sigma(s(x))k:  is the estimated probability that the instance 'x' belongs to the class 'k',
                        given the scores of each for that instance
                    sigma() is the logistic function

       https://www.pinecone.io/learn/softmax-activation/
       Softmax function:
         - you can think of the softmax function as a vector generalization of the sigmoid activation
         - The softmax activation function takes in a vector of raw outputs of the neural network and returns a vector of probability scores.
         - All entries in the softmax output vector are between 0 and 1.
         - In a multiclass classification problem, where the classes are mutually exclusive, notice how the entries of the softmax 
           output sum up to 1: 0.664 + 0.249 + 0.087 = 1. (see example in above website)


        equation of the softmax function
            softmax(z)_i = exp (z_i)) / [ SUM_j exp(z_j)) ]
                  where:
                    - SUM_j is from j = 1  to j = N
                    - z is the vector of raw outputs from the neural network
                    - The value of e ~= 2.718
                    - The i-th entry in the softmax output vector softmax(z) can be thought of as the predicted probability 
                      of the test input belonging to class i.
                    - N: number of classes 

    Code: python Softmax function and some example calculations

         >>> import numpy as np
         >>> 
         >>> def softmax(z):
         >>>   '''Return the softmax output of a vector.'''
         >>>   exp_z = np.exp(z)
         >>>   sum = exp_z.sum()
         >>>   softmax_z = np.round(exp_z/sum,3)
         >>>   return softmax_z

         >>> z = [0.25, 1.23, -0.8]
         >>> softmax(z)
             array([0.249, 0.664, 0.087])      
             
         >>> z4 = [0,0.9,0.1]
         >>> softmax(z4)
             array([0.219, 0.539, 0.242])      
             # since softmax assigns some probability to less likely classes (e.g. 0, 0.1) so it consider 
             # softer version of the argmax function



        Equation 4-21. Softmax regression classifer prediction. page 171


        pred-y = argmax-k sigma(s(x))k  = argmax-k sk(x) = argmax-k ((theta-k)-T * X)

           Note: 
               - default softmax regression classifier predicts the class with the highest probability
                 (which is simply the class with the highest score()
               - argmax-k: returns the value 'k' that maximimizes the estimated probability of sigma(s(x)k
               - softmax regression classifier predicts only one class at a time, so it should be used
                 only with mutually classes.


        Equation 4-21. Cross Entropy cost function. page 171

        J(THETA) = - (1/m) SUM-m  SUM-K yk-i * log(pred-pk-i)
            where: 
                 - SUM-m is from i=1 to i=m
                 - SUM-k is from k=1 to i=K where 'K' is the number of classes
                 - yk-i is the target probability that the ith instance belongs to the 'k' class 
                   (generally, it is either 1 or 0 depending on whether the instance belongs to the class or not
                 - pred-pk-i  is the pred-probabality for k-class at the ith  instance
                 - when there are just two classess (K=2), this cost function is equivalent to the logistic
                   regression cost function (log loss; see equation 4-17)

     Cross Entropy
       - measures the average number of bits you actually send per option
       - that is, how to most efficient send the information based on the probability of each option
         occurring. This is, if option 1 occurs 50% of the time, then it would be more efficient to send
         this option on it own separate bit, that encode with the other options are shared option bits

        Equation 4-21. Cross Entropy gradient vector for class 'k'

           grad-theta-k * J(THETA) = (1/m) SUM-m (pred-k-i - yk-i)xi
               where:
                    grad-theta-k: gradient vector of theta for class 'k'
                    SUM-m is from i=1 to i=m
                    pred-k-i: predicted class k value for the ith instance
                    yk-i:  actually class 'k' for ith instance
                    xi: ith  instance


     Code: Use logisticRegression to predict the probability of Iris classes (Note: predict_proba uses softmax function)

        >>> X = iris.data[["petal length (cm)", "petal width (cm)"]].values
        >>> y = iris["target"]
        >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
        >>> 
        >>> softmax_reg = LogisticRegression(C=30, random_state=42)
        >>> softmax_reg.fit(X_train, y_train)
        >>> # predict the probability for a 5cm long and 2cm with iris petal
        >>> softmax_reg.predict([[5, 2]])
            array([2])
        >>> softmax_reg.predict_proba([[5, 2]]).round(2)
            array([[0.  , 0.04, 0.96]])
        >>> #  96% for Iris virgincia (class 2) and 4% for Irisversicolor (class 1) 


       Notes:
          logisticRegression:
            parameters:
              C : float, default=1.0
                Inverse of regularization strength; must be a positive float. Like in support vector machines, 
                smaller values specify stronger regularization.
            methods:
             predict_proba(X)[source]
               Probability estimates.

               The returned estimates for all classes are ordered by the label of classes.

               For a multi_class problem, if multi_class is set to be "multinomial" the softmax function is used 
               to find the predicted probability of each class. Else use a one-vs-rest approach, i.e. calculate the 
               probability of each class assuming it to be positive using the logistic function. and normalize these 
               values across all the classes.
              

ML | Normal Equation in Linear Regression
https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/
-> shows how Normal equation is derived from Least Square
  Normal Equation is an analytical approach to Linear Regression with a Least Square Cost Function. We can 
  use the normal equation to directly compute the parameters of a model that minimizes the Sum of the squared 
  difference between the actual term and the predicted term. This method is quite useful when the dataset is small


numpy.random.randn
random.randn(d0, d1, ..., dn)
  Return a sample (or samples) from the "standard normal" distribution.

  Parameters:
    d0, d1, ...  dn : int, optional
        The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.
Returns:

    Zndarray or float

numpy.random.rand
random.rand(d0, d1, ..., dn)
  Random values in a given shape.
  Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).

  Parameters:

    d0, d1, ..., dn  : int, optional
        The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.

  Returns:
    outndarray, shape (d0, d1, ..., dn)

        Random values.


Cross Entropy video (from note on pat 172):
   https://homl.info/xentropy

Chapter 4 Exercises:

    -> see exercise_notebooks/04_exercises.ipynb

1. Which linear regression training algorithm can you use if you have a training set with millions of features?

  based on table 4-1 (page 149): 
    Normal Equation and SVD are slow for "large n" (features) so they would not work with millions of features.
    Normal Equation, SVD, and Batch GD do not have "out of core support" so they would not work with millions of features.
    That leaves: Stochastic GD and Mini-batch GD.

  1 book answer: If you have a training set with millions of features you can use Stochastic Gradient Descent or 
  Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot 
  use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) 
  with the number of features.

2. Suppose the features in your training set have very different scalses. Which algorithms migh suffer from this and how?
   What can you do about it? 

  page 141: While the cost function has the shape of a bowl, it can be an elongated bowl if the features 
  have different scale. ... [the elongated bowl] It will eventually reach the minimum, but it will take a long time

  The note on page 141 states: "When using gradient descent, you should ensure that all features have
  similar scale (e.g. using Scikit-Learns StandardScaler class) or else it will take much longer to
  converge."

  The note on page 156 states: "It is important to scale the data (e.g. using StandardScaler) before preforming
  ridge regression, as it is sensitive to the scale of the input features. This is true of most regularized
  models
  -> thus, input scaling should also be used with LASSO Regression, Elastic Net Regression, and LogisticRegression


  2 book answer: If the features in your training set have very different scales, the cost function will have 
  the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve 
  this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work 
  just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features 
  are not scaled: since regularization penalizes large weights, features with smaller values will tend to be 
  ignored compared to features with larger values.

3. Can gradient descent algorithms get stuck in a local minimum when training a logistic regression model?

   -> No, because the MSE cost fucntion is a 'convex function'

   page 141: Fortunately, the MSE cost function for linear regression model [e.g. gradient descent] happens 
   to be a 'convex function' which means that if you pick any two points on the cureve, the segment joining 
   them is never below the curve.  This implies that here are no local minima , just a global minimum.

   page 166: But the good news is that this [logistic regression] cost function [log loss] is convex, so gradient
   descent (or any other algorithm is guaranteed to find the global minimum (if the learning rate is not too
   large and you wait long enough).

  3 book answer: Gradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because 
  the cost function is convex. _Convex_ means that if you draw a straight line between any two points on the curve, 
  the line never crosses the curve.

4. Do all gradient descent algorithms lead to ths same model, provide you let them run long enough?

  page 144: [referencing figure 4-8] On the right, the learning rate is too high: the algorithm diverages, jumping
  all overthe place and actually getting further and further away from the solution at every step.

  page 145: [referencing stochastic Gradient Descent] Over time it will end of very close to the minimum, but
  once it gets there it will continue to bounce around never settling down (see Figure 4-9). Once the algorithm
  stops, the final parameter values will be good, but not optimal.

  page 148: The [mini-batch GD] algorithm's progress in parameter space is less erratic that with stochastic GD
  especially with fairly large mini-batches. As a result, mini-batch GD will end up walking a bit closer to the
  minimum that stochastic GD - but it may be harder for it to escape from local minima (in the case of problems
  that suffer from local minima, unlike linear regression with the MSE cost function).

  4 book answer: If the optimization problem is convex (such as Linear Regression or Logistic Regression), and 
  assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum 
  and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD 
  and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. 
  This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce 
  slightly different models.

5. Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice
   that the validation error consistently goes up, what is likely going on? How can you fix it?

  page 144: [referencing figure 4-8] On the right, the learning rate is too high: the algorithm diverages, jumping
  all overthe place and actually getting further and further away from the solution at every step.
  -> if learning rate is too high, reduce the learning rate

  page 154: There is gap between the [validation error and training error] curves [in figure 4-16]. This means
  the model performs signficantly better on the validation data which is the hallmark of 'overfitting model'.

  5 book answer: If the validation error consistently goes up after every epoch, then one possibility is that the 
  learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly 
  the problem and you should reduce the learning rate. However, if the training error is not going up, then your 
  model is overfitting the training set and you should stop training.

6. Is it a good idea to stop minibatch gradient descent immediately when the validation error goes up?

   -> No, it may be bouncing around early on

  6 book answer: Due to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent 
  is guaranteed to make progress at every single training iteration. So if you immediately stop training when the 
  validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save 
  the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never 
  beat the record), you can revert to the best saved model.

7. Which gradient descent algorighm (among those we discussed) will reach the vicinitiy of the optimal solution the 
   fastest? Which will actually converge? How can you make the others converge as well?

   -> Stochistic GD will reach the vicinity of the optimal solution the fastest because it only calcuates on
   one instance at a time, but it will be a less optimal solution. With small mini-batch size, the mini-batch
   may provide similar performance.

  7 book answer: Stochastic Gradient Descent has the fastest training iteration since it considers only one training 
  instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with 
  a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. 
  As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.

8. Suppose you are using Polynomial regressions. You plot the learning curves and you notice a large gap between
   the training error and the validation error. What is happening? What are three ways to solve this?

  page 154: There is gap between the [validation error and training error] curves [in figure 4-16]. This means
  the model performs signficantly better on the validation data which is the hallmark of 'overfitting model'.

  page 152: This high degree polynomial regression model is severely overfitting the training data while
  the linear model is underfitting it.

  -> The polynomial model degrees does fit the data. If it overfitting, then polynomial number of degrees needs
  to be reduced.

  page 156: Ridge regression (. . .) is regularized version of linear regression: a regulation term equal to
  [l2 equation shown] is added to the MSE. This forces the learning algorithm to not fit the data but also keep 
  the model weightsas small as possible. Note that the regularization term should only be added to the cost 
  function during training.

  page 158: Least Absolute shrinkage and selection operator regression (usually simply called LASSO regression)
  is another regularization term to the cost function, but it uses the l1 norm of the weight vector instead
  of the l2 norm.

  8 book answer: If the validation error is much higher than the training error, this is likely because your model 
  is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer 
  degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model-for example, by 
  adding an l2 penalty (Ridge) or an l1 penalty (Lasso) to the cost function. This will also reduce the degrees of 
  freedom of the model. Lastly, you can try to increase the size of the training set.

9. Suppose you are using ridge regression and you notice that the training error and the validation error are 
   almost equal and fairly high. Would you say tha the model suffers from high bias or high variance? Should 
   you increase the regularization hyperparameter 'alpha' or reduce it?

   -> Fairly equal and high training and validation error indicates unfitting the data which is 
      normally due to high bias. With Ridge Regression, you would reduce bias by increasing the 'alpha'. 
      (this conflicts with the book answer but note the book quotes)

   page 154: The learning curves are typical of a model that's underfitting. Both [validation errors &
   training errors] curves have reached a plaateau; they are close and fairly high.

   page 154: If your model is underfitting the training data, adding more training examples will not help. 
   You need to use a better model or come up with better features.

   page 155:
   Bias: 
     This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear
     when it is acutally quadradtic. A 'high-bias' model is most likely to underfit the training data.
   Variance:
     This part is due to the model's excessive sensitivity to small variation in the training data. A model with
     with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance and 
     thus overfit the training data.
   Irreduciblw error:
     This part is due to the noiseness of the data itself. The only way to reduce this part of the error is 
     to clean up the data (e.g. fix the data sources, such as broken sensors, or detect and remove outliers

   page 157: Note how increaing [ridge regression] 'alpha' leads to flatter (i.e. less extreme, more reasonable
   predictions, thus reducing the model's variance, but increasing it its bias.

   9 book answer:  If both the training error and the validation error are almost equal and fairly high, the 
   model is likely underfitting the training set, which means it has a high bias. You should try reducing 
   the regularization hyperparameter 'alpha'.


10. Why would your want to use:
  a. Ridge regression instead plan linear regression (i.e. without any regularization)?
  b. Lasso instead of ridge regression.?
  c. Elastic net instead of lasso regression?

  -> Generally, Use Ridge Regression instead of plan linear regression since it is almost always preferable 
     to have at least a little bit of regularization, 
  -> Use Lasso (or net elastic) instead of ridge regression when only a few features are userful, because they 
     tend to reduce the useless features weights down to zero.
  -> In general, 'elastic net' is preferred over 'lasso' because lasso may behave erractically when the number 
     of features is greater than the number of training instances or when several features are strongly correlated.

  page 156: Ridge regression (. . .) is regularized version of linear regression: a regulation term equal to
  [l2 equation shown] is added to the MSE. This forces the learning algorithm to not fit the data but also keep 
  the model weightsas small as possible. Note that the regularization term should only be added to the cost 
  function during training.

  page 158: Least Absolute shrinkage and selection operator regression (usually simply called LASSO regression)
  is another regularization term to the cost function, but it uses the l1 norm of the weight vector instead
  of the l2 norm.

  page 159: An important characteristc of lasso regression is the it tends toe eliminate the weights of the 
  least import features (i.e. set them to zero). . . . In other words, lasso regression automatically performs 
  feature selection and outputs a sparse model with few nonzero feature weights

  page 161: Elastic regression is the middle ground between ridge regression and lasso regression. The 
  regularization is a weighted sum of both ridge and lasso's regularization terms, and you can control the 
  mix ratio 'r'. When r = 0, elastic net is equivalent to ridge regression, and when r = 1, it is equivalent 
  to lasso regression.

  page 161 - 162: So when should you use elastic net regression, or ridge, or lasso, or plan linear regression 
  (i.e. without any regularization)? It is almost always preferable to have at least a little bit of regularization, 
  so generally you should avoid plain linear regression. 'Ridge regression' is a good default, but if you suspect
  that only a few features are userful, you should prefer lasso or elastic net because they tend to reduce the
  useless features weights down to zero, as discussed earlier. In general, 'elastic net' is preferred over
  'lasso' because lasso may behave erractically when the number of features is greater than the number of 
  training instances or when several features are strongly correlated.

  10 book answer: Let's see:
     * A model with some regularization typically performs better than a model without any regularization, so you 
       should generally prefer Ridge Regression over plain Linear Regression.
     * Lasso Regression uses an l1 penalty, which tends to push the weights down to exactly zero. This leads to 
       sparse models, where all weights are zero except for the most important weights. This is a way to perform 
       feature selection automatically, which is good if you suspect that only a few features actually matter. 
       When you are not sure, you should prefer Ridge Regression.
     * Elastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several 
       features are strongly correlated or when there are more features than training instances). However, it does 
       add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use 
       Elastic Net with an `l1_ratio` close to 1.

11. Suppose you want to classify pictures as outdoor/indoor and daytime/nightime. Should you implement two logistic
    regression classifiers or one softmax regression classifier?
    
   -> You should implement two logistic regression classifiers because softmax regression classifier predicts
      one class at a time, and the pictures could be two classes (e.g. outdoor and daytime)

   page 170: The logistic regression model can be generalized to support multiple classes directly without having
   to train and combine multiple binary classifiers (as discussed in Chapter 3). This is called softmax regression
   or multinominal logistic regression.

   The idea is simple: when give an instance 'x', the softmax regression first computes a score sk(x) for each 
   class 'k', then estimates the probability of each class by applying softmax function (also called normalized
   exponential) to the scores.

   page 171: The softmax regression classifier predicts only one class at a time (i.e. it is multiclass, not 
   multioutput) so it should be used only with mutually exclusive classes, such as different species of plants.
   You cannot use it to recognize multiple people in one picture.


  11 book answer: If you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not 
  exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers.

12. Implement batch gradient descent with early stopping for a softmax regression without using SciKit-Learn, 
    only NumnPY. Use it on a classification task such as the iris dataset.



------------------------------------------------------
Chapter 5 Support Vector Machines
------------------------------------------------------

https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html
sklearn.datasets.make_moons

  sklearn.datasets.make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None)[source]
    Make two interleaving half circles.

    A simple toy dataset to visualize clustering and classification algorithms. Read more in the User Guide.

https://www.sciencedirect.com/topics/veterinary-science-and-veterinary-medicine/radial-basis-function
Gaussian RBF (Radial Bias Function)

  Radial basis function (RBF) is a function whose value depends on the distance (usually Euclidean distance) 
  to a center (xc) in the input space. The most commonly used RBF is Gaussian RBF. It has the same form as 
  the kernel of the Gaussian probability density function and it is defined as
     phi(x,xc)=exp(-||x  xx||**2 / 2*sigma**2)

  where ||x - xc|| is the Euclidean distance between x and xc. The larger this distance, the smaller the value 
  of RBF. Hence, an RBF with center xc can be used to approximate the local characteristics of a nonlinear 
  function close to xc. The smoothness of a Gaussian RBF is controlled by the magnitude of 'sigma'.

https://en.wikipedia.org/wiki/Radial_basis_function
 Commonly used types of radial basis functions include (writing r = || x  xi || and using 'epsilon' to 
 indicate a shape parameter that can be used to scale the input of the radial kernel.

   - Gaussion:
       phi(r) = exp(epsilon*r**2)


Radial Basis Function (RBF) Kernel (Note: excellent RBF description):
https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a

  The RBF kernel function for two points X1 and X2 computes the similarity or how close they are to each other. 
  This kernel can be mathematically represented as follows:
 
     K(x1, x2) = exp (-||X1 - X2||**2 / 2 * sigma**2)
  where,
  1. sigma is the variance and our hyperparameter 
  2. ||X1 - X2|| is the Euclidean (L2-norm) Distance between two points X1 and X2

  The maximum value that the RBF kernel can be is 1 and occurs when d12 (distance between x1 and x2)
  is 0 which is when the points are the same, i.e. X1 = X2  (maximum similarity).


python code provided 05 Jupyter Notebook:
def gaussian_rbf(x, landmark, gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)


numpy axes, explained:
https://www.sharpsightlabs.com/blog/numpy-axes-explained/

  for 2d array:
  axis=0   (   row axis - e.g. sum the values in each colunm along the row axis)
  axis=1   (column axis - e.g. sum the values in each row along the column axis)

  np.sum:
  np_array_2d = np.arange(0, 6).reshape([2,3])
  print(np_array_2d)
  [[0 1 2]
   [3 4 5]]

  np.sum(np_array_2d, axis = 0)
  array([3, 5, 7])

  np.sum(np_array_2d, axis = 1)
  array([3, 12])


  np.concatenate:
  np_array_1s = np.array([[1,1,1],[1,1,1]])
  np_array_9s = np.array([[9,9,9],[9,9,9]])

  print(np_array_1s)
  array([[9, 9, 9],
       [9, 9, 9]])

  print(np_array_9s)
  array([[1, 1, 1],
       [1, 1, 1]])


  np.concatenate([np_array_1s, np_array_9s], axis = 0)
  array([[1, 1, 1],
       [1, 1, 1],
       [9, 9, 9],
       [9, 9, 9]])

  np.concatenate([np_array_1s, np_array_9s], axis = 1)
  array([[1, 1, 1, 9, 9, 9],
       [1, 1, 1, 9, 9, 9]])

Chapter 5 Exercises:

    -> see exercise_notebooks/05_exercises.ipynb

1. What is the fundamental idea behind support vector machines?

  -> identify the widest possible 'street' (hyperplane) between 2 classes 

  page 175: The two classes can clearly be separated easily with a straight line (they are linearly
  separable). ... You can think of an SVM classifier as fitting the widest possible street .... 
  This is called 'large margin classification'.

  https://www.spiceworks.com/tech/big-data/articles/what-is-support-vector-machine/
  Technically, the primary objective of the SVM algorithm is to identify a hyperplane that distinguishably 
  segregates the data points of different classes. The hyperplane is localized in such a manner that the 
  largest margin separates the classes under consideration.

  page 177: ... The objective is to find a good balance between keeping the street as large as possible and
  limiting the 'margin violations (i.e. the instance that end up in the middle of the street or even on the
  wrong side). This is call 'soft margin classification'.

  book answer: The fundamental idea behind Support Vector Machines is to fit the widest possible "street" between 
  the classes. In other words, the goal is to have the largest possible margin between the decision boundary that 
  separates the two classes and the training instances. When performing soft margin classification, the SVM searches 
  for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few 
  instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets. SVMs 
  can also be tweaked to perform linear and nonlinear regression, as well as novelty detection.

2. What is a support vector?
   -> the instances located on street (hyperplane separating classes) including instances located on the 
   edge of the street

  page 176: Notice that adding more training instances "off the street" will not affect the decision boundary
  at all: it fully determined (or "supported") by the instance located on the edge of the street. These instances
  are called the "support vectors" (they are circled in figure 5-1).

  book answer: After training an SVM, a support vector is any instance located on the "street" (see the previous 
  answer), including its border. The decision boundary is entirely determined by the support vectors. Any instance 
  that is not a support vector (i.e., is off the street) has no influence whatsoever; you could remove them, add 
  more instances, or move them around, and as long as they stay off the street they won't affect the decision 
  boundary. Computing the predictions with a kernelized SVM only involves the support vectors, not the whole training set.

3. Why is it important to scale the inputs when using SVMs?

  -> when features are not scaled, the larger valued features may be over weighted when determing the
  decision boundary. this can result in a smaller street between the classes.

  page 176: SVMs are sensitive to the feature scales, as you can see in Figure 5-2. IN the left plot, the
  vertical scale is much larger than the horizontal scale, so the widest possible street is close to 
  horizontal, After feature scaling (e.g. using Scikit-Learn's Standscalar) the decision boundary in the right plot 
  looks better.

  book anwser: SVMs try to fit the largest possible "street" between the classes (see the first answer), so if 
  the training set is not scaled, the SVM will tend to neglect small features (see Figure 5-2)

4. Can an SVM classifier output a confidence score when it classifies an instance?

  -> LinearSVC decision_function returns 'confidence scores' based on the signed distance between the instance
  and decision boundary.

  -> SVC predict_proba() returns probabities scores and predit_log_proba() returns log probability scores
  provided 'probability=True' when training at a cost of slower training

  page 178: The first plant is classified as an 'Iris virginica' while the second is not. Let's look at the
  scores that the SVM used to make these predictions. These measure the signed distance between each instance
  and the decision boundary:
    >>> svm_clf = make_pipeline(StandardScaler(), LinearSVC(C=1, dual=True, random_state=42))
    >>> svm_clf.fit(X, y)
    >>> X_new = [[5.5, 1.7], [5.0, 1.5]]
    >>> svm_clf.predict(X_new)
        array([ True, False])
    >>> svm_clf.decision_function(X_new)
        array([ 0.66163411, -0.22036063])

   page 178: Unlike LogisiticRegression, LinearSVC doesnt have a predict_proba() method to estimate the class
   probabilities. That said, if you use the SVC class (discussed shortly) instead of LinearSVC, and if you 
   set its probability hyperparameter to 'True', then the model will fit an extra model at the end of training
   to map the SVM decision function scores to probabilities. Under the hood this requires using 5-fold-cross-
   validation to generate out-of-sample predictions for every instance in the training set, then training
   a LogisticRegression model, so it will slow does training considerably. After that, the 'predict_proba()'
   and 'predict_log_proba()' methods will be avaiable.

    https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.decision_function
    sklearn.svm.LinearSVC
      ...
      decision_function(X)[source]

      Predict confidence scores for samples.

      The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.

    https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    sklearn.svm.SVC
      ... 
      predict_log_proba(X)
         Compute log probabilities of possible outcomes for samples in X.
      predict_proba(X)
         Compute probabilities of possible outcomes for samples in X.

         The model needs to have probability information computed at training time: fit with attribute 
         'probability' set to 'True'.

  book answer:  You can use the `decision_function()` method to get confidence scores. These scores represent 
  the distance between the instance and the decision boundary. However, they cannot be directly converted into 
  an estimation of the class probability. If you set `probability=True` when creating an `SVC`, then at the end 
  of training it will use 5-fold cross-validation to generate out-of-sample scores for the training samples, 
  and it will train a `LogisticRegression` model to map these scores to estimated probabilities. The 
  `predict_proba()` and `predict_log_proba()` methods will then be available.

5. How can you choose between 'LinearSVC', 'SVC', and 'SGDClassifer'?

   -> all 3 can be used for large margin linear classification
   -> Use LinearSVC for large linear training sets which can fit in memory
   -> Use SVC for small or medium-sized nonlinear training sets
      - SVC does not scale well with large number of instances (e.g. 100Ks)
   -> Use SGDClassifier for large linear training sets which cannot fit in memory

  page 183: 
    LinearSVC class: 
     - based on liblinear liblinear (optimized algorithm for linear SVM).
     - training complexity is roughly O(m x n)
     - scales almost linearly with training instances and number of features
     - algorithm controlled by the tolerance hyperparameter 'epilson' (called 'tol' in Scikit-Learn

    SVC class: 
     - based on liblinear libsvm 
     - supports the 'kernel trick'
     - training complexity is between O(m**2 x n) and O(m**3 x n)
     - gets dreadfully slow when the number of training instances get large (100ks of instances)
     - best for small or medium-sized nonlinear training sets
     - scales well with the number of features, specially with sparse features (i.e. when each instance 
       has few non-zero features)

    SGDClassifier class: 
     - performs large margin classification by default
     - its hyperparameters - especially the regulization hyperparameters ('alpha' and 'penalty') and the
       'learning rate' can be adjusted to produce similar results as the linear SVMs
     - training complexity is O(m x n) 
     - for training, it uses stochastic gradient descent which allows incremental learning and uses little memory, 
       so you can use it to train a model on a large dataset that does not fit in memory ('out-of-core learning)

  book answer:  All three classes can be used for large-margin linear classification. The `SVC` class also 
  supports the kernel trick, which makes it capable of handling nonlinear tasks. However, this comes at a cost: 
  the `SVC` class does not scale well to datasets with many instances. It does scale well to a large number 
  of features, though. The `LinearSVC` class implements an optimized algorithm for linear SVMs, while 
  `SGDClassifier` uses Stochastic Gradient Descent. Depending on the dataset `LinearSVC` may be a bit faster 
  than `SGDClassifier`, but not always, and `SGDClassifier` is more flexible, plus it supports incremental learning.

6. Say you've trained an SVM classifier with and RBF kernel, but it seems to underfit the training set.
   Should you increase or decrease 'gamma'? What about 'C'?

   -> For overfitting, you should increase 'gamma' and similarly, you should increase 'C'.

  page 182: So 'gamma' acts like a regularization hyperparameter: if your model is overfitting, you 
  should reduce 'gamma'; if it is underfitting, you should increase 'gamma' (similar to the 'C' hyperparameter).

  book answer: If an SVM classifier trained with an RBF kernel underfits the training set, there might be 
  too much regularization. To decrease it, you need to increase `gamma` or `C` (or both).

7. What does it mean for a model to be 'epsilon' insensitive?

  -> SVR Regression model is 'epsilon' insensitive when adding more training instances within the
  margin does not affect the model's prediction.

  page 184: To use SVMs for regression instead of classification, the trick is to tweek the objective:
  instead of trying to fit the largest possible street between two classes while limiting the margin 
  violations, SVM regression tries to fit as many instances as possible 'on the street' while limiting
  margin violations (i.e. instances 'off the street'). The width of the 'street' is controlled by a
  hyperparameter, 'epsilon' ...

  page 184: Reducing 'epsilon' increases the number of support vectors, whuich regularizes the model.
  Moreover, if you add more training instances within the margin, it will not affect the model's predictions;
  thus, the model is said to be 'epsilon'-insensitive.

  book answer:  A Regression SVM model tries to fit as many instances within a small margin around its 
  predictions. If you add instances within this margin, the model will not be affected at all: it is said 
  to be 'epsilon'-insensitive.

8 What is the point of using the 'kernel' trick?

  -> allows SVMs to handle non-linear datasets

  page 180: Fortunately, when using SVMs you can apply an almost miraculous mathematical technique caled 
  the 'kernel trick' (...). The 'kernel trick' makes it possible to get the same results as if you had
  added many polynomial features, even with a high degree, without actually having to add them. This
  means there's no combinatorial explosion of the number of features. This trick is implemented by the SVC
  class. 

    >>> from sklearn.svm import SVC
    >>> poly_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=3, coef0=1, C=5))
    >>> poly_kernel_svm_clf.fit(X, y)

  page 183: ... As a rule of thumb, you should always try the linear kernel first. The 'LinearSVC' class
  is much faster thans SVC(kernel="linear"), especially if the training set is very large. If it is not
  too large, you should also try kernelized SVMs, starting with the Gaussian RBF Kernel; if often works
  really well.

   book answer: The kernel trick is mathematical technique that makes it possible to train a nonlinear SVM model. 
   The resulting model is equivalent to mapping the inputs to another space using a nonlinear transformation, 
   then training a linear SVM on the resulting high-dimensional inputs. The kernel trick gives the same result 
   without having to transform the inputs at all.

9. Train a 'LinearSVC' on a linearly separable dataset. Then train an SVC and a 'SGDClassifier' 
   on the same dataset See if you can get them to produce roughly the same model.

10. Train an SVM classifer on the wine dataset, which you can load using 'sklearn.datasets.load_wine()'. 
   This dataset contains the chemical analyses of 178 wine samples produced by 3 different cultivators: 
   the goal is to train a classifaction model capable of predicting the cultivator based on the wine's
   chemical analysis. Since SVM classifiers are binary classifiers, you will need to use one-versus-all
   to classify all three classes. What accuracy can you reach?

   sklearn.model_selection.RandomizedSearchCV

     class sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, 
     n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, 
     return_train_score=False)[source]

    Randomized search on hyper parameters.

    RandomizedSearchCV implements a "fit" and a "score" method. It also implements "score_samples", "predict", 
    "predict_proba", "decision_function", "transform" and "inverse_transform" if they are implemented in the 
    estimator used.

11. Train and fine-tune an SVM regressor on the California housing dataset. You can use the original
    dataset rahter than the tweaked version whe usind in Chapter 2, which you cna load using 
    'sklean.datasets.fetch_california_housing()'.  The targets represent hundreds thousands of dollars.
    Since there are over 20,000 instances, SVMs can be slow, so for hyperparater tuning you should use
    far fewer instances (e.g. 2000) to test many more hyperparameter combinations. What is your best
    model's RMSE?


------------------------------------------------------
Chapter 6 Decision Trees
------------------------------------------------------


https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html
sklearn.tree.export_graphviz

sklearn.tree.export_graphviz(decision_tree, out_file=None, *, max_depth=None, feature_names=None, class_names=None, 
label='all', filled=False, leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, 
rounded=False, special_characters=False, precision=3, fontname='helvetica')[source]

    Export a decision tree in DOT format.

    This function generates a GraphViz representation of the decision tree, which is then written into out_file. 
    Once exported, graphical renderings can be generated using, for example:

    $ dot -Tps tree.dot -o tree.ps      (PostScript format)
    $ dot -Tpng tree.dot -o tree.png    (PNG format)


Note: Must install graphviz first:
   conda install anaconda::graphviz

   For python without anaconda:
   pip install graphviz


https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c
Decision Trees Explained



Chapter 6 Exercises:

    -> see exercise_notebooks/06_exercises.ipynb

https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html
sklearn.model_selection.ShuffleSplit

class sklearn.model_selection.ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)[source]

    Random permutation cross-validator.

    Yields indices to split data into training and test sets.

    Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be 
          different, although this is still very likely for sizeable datasets.

 https://scikit-learn.org/stable/modules/generated/sklearn.base.clone.html
 sklearn.base.clone
 sklearn.base.clone(estimator, *, safe=True)[source]

    Construct a new unfitted estimator with the same parameters.

    Clone does a deep copy of the model in an estimator without actually copying attached data. It returns 
    a new estimator with the same parameters that has not been fitted on any data.

numpy.empty
numpy.empty(shape, dtype=float, order='C', *, like=None)

   Return a new array of given shape and type, without initializing entries.

Scipy package:
SciPy is a scientific computation library that uses NumPy underneath. SciPy stands for Scientific Python. 
It provides more utility functions for optimization, stats and signal processing.

scipy.stats.mode
scipy.stats.mode(a, axis=0, nan_policy='propagate', keepdims=False)[source]

  Return an array of the modal (most common) value in the passed array.

  If there is more than one such value, only one is returned. The bin-count for the modal bins is also returned.



Skip to Main
06_exercises
Last Checkpoint: 2 hours ago
[Python 3 (ipykernel)]

Chapter 6: Decision Tree Exercises

1. What is the approximate depth of a decision tree trained (without) restriction on a training set with one million instances?

  -> by default, max_depth = 'None'. However, Decision tres generally are approximately balanced, so traversing the 
  decision tee requires going through roughly O(log2(m)) nodes, where log2(m) is the 'binary logarithm of 'm', 
  equal to log(m) / log(2). 'm' is the number of instances in the dataset. For 1M instances, approximate depth is: 
  log2(1M) = 19.9 or depth of ~20.

   page 202: To avoid overfitting the training data, you need to restruct the decision tree's feedom during training. 
   As you know by now, this is called regularization. The regularization hyperparameters depend on the algoritym used, 
   but generally you can at least restrict the maximum dpth of the decision tree. In Scikit-Learn, this controlled by the 
   'max_depth' hyperparameter. The default value is 'None', which means unlimited. Reducing the 'max_depth' will regularize 
   the model and thus reduce the risk of overfitting.

   page 200 - 201: Making predictions requires traversing the decision from the root to a leaf. Decision tres generally 
   are approximately balanced, so traversing the decision tee requires going through roughly O(log2(m)) nodes, where 
   log2(m) is the 'binary logarithm of 'm', equal to log(m) / log(2). Since each node only requires checking the value 
   of one feature, the overals prediction complexity is O(log2(m)), independent of the number of features. So predictions 
   are very fast, even when dealing with large training sets.

   book answer: The depth of a well-balanced binary tree containing m leaves is equal to log2(m), rounded up. log2 is the 
   binary log; log2(m) = log(m) / log(2). A binary Decision Tree (one that makes only binary decisions, as is the case with 
   all trees in Scikit-Learn) will end up more or less well balanced at the end of training, with one leaf per training 
   instance if it is trained without restrictions. Thus, if the training set contains one million instances, the Decision 
   Tree will have a depth of log2(106) ~= 20 (actually a bit more since the tree will generally not be perfectly well balanced).

2. Is a node's Gini impurity generally lower or higher that its parent's? Is it generally lower/higher, or always lower/higher?

  -> The Gini impurity equation is: Gi = 1 - sum(pi,k)**2 (where sum is k=1 to k=n). A node's Gini impurity generally lower 
   than its parent. . This is due to the CART training algorithm's cost function, which splits each node in a way that 
   minimizes the weighted sum of its children's Gini impurities (see cost function below)

  page 197: ... Finally, a node's 'gini' attribute measures its 'Gini impurity': a node is 'pure' (gini=0) if all training 
  instances it applies to belong to the same class. For example, since the depth-1 lef node applies to the "Iris setosa" 
  training instances, it is pure, and its 'Gini impurity' is 0. Equation 6-1 shows how the training algorithm computes the 
  'Gini impurity Gi of the ith node'. The depth-2 left node has a Gini impurty equal to 
  1 - (0/54)**2 - (49/54)**2 - (5/54)**2 ~= 0.168.

   Equation 6-1 Gini impurity Gi = 1 - sum(pi,k) (where sum is k=1 to k=n)

  page 199 - 200: Scikit-Learn uses the 'Classification and Regression Tree (CART)' algorithm to train decision trees 
  (also called 'growing trees'). The algorithm works by first splitting the training set into two subsets using a single 
  feature 'k' and a threshold tk (e.g. "petal length <= 2.45cm"). How does it choose k and t ? It searches for the 
  pair (k,t) that produces the purest subsets, weighted by their size. Equation 6-2 gibves the cost function that the 
  algorithm tries to minimize.

  Equation 6-2 CART cost function for classification

    J(k,t) = (m_left/m) * G_left + (m_right/m) * G_right where: G_left/_right measures the impurity of the left/right subset; 
    m_left/_right is the number of instances in the left/right subset

  Once the CART algorithm has successfully split the training set in two, it splitsthe subsets using the same logic, then 
  the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth 
  hyperparameter) or it cannot find a split that will reduce impurity. ...

  book answer: A node's Gini impurity is generally lower than its parent's. This is due to the CART training algorithm's 
  cost function, which splits each node in a way that minimizes the weighted sum of its children's Gini impurities. However, 
  it is possible for a node to have a higher Gini impurity than its parent, as long as this increase is more than compensated 
  for by a decrease in the other child's impurity. For example, consider a node containing four instances of class A and one 
  of class B. Its Gini impurity is 1 - (1/5) - (4/5) = 0.32. Now suppose the dataset is one-dimensional and the instances 
  are lined up in the following order: A, B, A, A, A. You can verify that the algorithm will split this node after the 
  second instance, producing one child node with instances A, B, and the other child node with instances A, A, A. The first 
  child node's Gini impurity is 1 - (1/2) - (1/2) = 0.5, which is higher than its parent's. This is compensated for by 
  the fact that the other node is pure, so its overall weighted Gini impurity is 2/5  0.5 + 3/5  0 = 0.2, which is lower 
  than the parent's Gini impurity.

3. If a decision tree is overfitting the training set, is it a good idea to try decreasing max_depth?

  -> Yes, reducing max_* hyperparameters such as max_depth regularize decision tree models (e.g. reduce overfitting) better.

  page 202: [in addition to max_depth] The DecisionTreeClassifier class has a few other parameters that simularly restrict 
  the shape of the decision tree:

    max_features: max number of features that are evaluation for splitting at each node

    max_leaf_nodes: maximum number of leaf nodes

    min_sample_split: Min number of samples a node must have before it can be split

    min_sample_leaf: Min number of samples a leaf node must have to be created

    min_weight_fraction_leaf: Same ans min_sample_leaf, but expressed as a fraction of the total number of weighted instances

  Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model

  page 203: The unregularized model on the left [Figure 6-3] is clearly overfitting, and the regularized model 
  [min_sample_leaf=5] on the right will probably generalize better.

  book answer: If a Decision Tree is overfitting the training set, it may be a good idea to decrease max_depth, since 
  this will constrain the model, regularizing it.

4. If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?

  -> No, decision trees don't require scaling. so scaling will be a waste of time.

  page: 197: One of the many qualities of 'decision trees' is that the rquire very little data preparation. In fact, they 
  don't require scaling or centering at all.

  book answer: Decision Trees don't care whether or not the training data is scaled or centered; that's one of the nice 
  things about them. So if a Decision Tree underfits the training set, scaling the input features will just be a waste of time.

5. If it takes one hour to train a decision tree on a training set containing one million instances, roughly how much time 
  will it take to train another decision tree on a training set containining ten million instances? Hint: consider the CART 
  algorithm's computational complexity.

  -> The CART algorithm's computational complexity is: O(n  m log2(m)). If it takes 1 hour for 1M instances (m=1000000), 
  then it for 10M instances (m=10000000), it will take (n * 10M * log2(10M)) / (n * 1M * log2(1M)) 
  = (10 x 23.25) / 19.93 = 11.67 = X hours / 1 hour -> X hours = 11.66

  page 200 - 201: Making predictions requires traversing the decision from the root to a leaf. Decision tres generally are 
  approximately balanced, so traversing the decision tee requires going through roughly O(log(m)) nodes, where log2(m) is 
  the 'binary logarith of 'm', equal to log(m) / log(2). Since each node only requires checking the value of one feature, 
  the overal prediction complexity is O(log(m)), independent of the number of features. So predictions are very fast, 
  even when dealing with large training sets.

  page 201: The training algorithm compares all features (or less if max_features is set) on all samples at each node. 
  Comparing all features on all samples ate ach node results in training complexity of O(n x m log2(m)).

  book anwser: The computational complexity of training a Decision Tree is O(n  m log2(m)). So if you multiply the 
  training set size by 10, the training time will be multiplied by K = (n  10 m  log2(10 m)) / (n  m  log2(m)) 
  = 10  log2(10 m) / log2(m). If m = 10 , then K ~= 11.7, so you can expect the training time to be roughly 11.7 hours.

6. If it takes one hour to train a decision tree on a given training set, roughly how much time will it take if you 
   double the number of features?

  -> The CART algorithm's computational complexity is: O(n  m log2(m)). Thus, if the number of features (n) doubles, 
  the the training time will approximately double.

  page 201: The training algorithm compares all features (or less if max_features is set) on all samples at each node. 
  Comparing all features on all samples ate ach node results in training complexity of O(n x m log(m)).

  book answer: If the number of features doubles, then the training time will also roughly double.

7. Train and fine-tune a deision tree for the 'moons' dataset by following these steps:

   a. Use 'make_moons (n_samples=10000, noise=0.4)'' to generate a moons dataset

    b. Use 'train_test_split()'' to split the datset into a training set and a test set.

    c. Uses grid search and cross-validation (with the help of the 'GridSearchCV' class) to find a good hyperparameter 
       values for a DecisionTreeClassifer. Hint: try various values for max_leaf_nodes.

    d. Train it on the full training set using these hyperparamseters and measure your model's performance on the test set. 
       You should get roughly 85% to 87% accuracy

8. Grow a forest by following these steps:

    a. Continuning the previous exercise generate 1000 subsets of the training set, each containing 100 instances 
       selected randomly. Hint: you can use Scikit-Learn's ShuffleSplit class for this.

    b. Training one decision tree on each subset, using the best hyperparameter values found in the previous exercise. 
       Evaluate these 1000 decision trees on the test set. Since they were trained on smaller sets, these decision trees 
       will likely perform worse that the first decision tree, achieving only about 80% accuracy.

    c Now comes the magic. For each test set instance, generate the predictions of the 1000 decision trees, and keep only 
      the most frequent prediction (you can use SciPy's 'mode()' function for this). This approach gives you 'majority-vote 
      predictions' over the test set.

    d. Evaluate these predicitions on the test set: you should obtain a slightly higher accuracy than your first model 
      (about 0.5 to 1.5% higher). Congratulations, you have trained a random forest classifier!


------------------------------------------------------
Chapter 7 Ensemble Learning and Random Forest
------------------------------------------------------

sklearn.ensemble.VotingClassifier

class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)[source]

    Soft Voting/Majority Rule classifier for unfitted estimators.

VotingClassifier example:
    from sklearn.datasets import make_moons
    from sklearn.ensemble import RandomForestClassifier, VotingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC

    X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

    voting_clf = VotingClassifier(
        estimators=[
            ('lr', LogisticRegression(random_state=42)),
            ('rf', RandomForestClassifier(random_state=42)),
            ('svc', SVC(random_state=42))
        ]
    )
    voting_clf.fit(X_train, y_train)

    for name, clf in voting_clf.named_estimators_.items():
        print(name, "=", clf.score(X_test, y_test))

sklearn.ensemble.BaggingClassifier

class sklearn.ensemble.BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, 
bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, 
verbose=0)[source]

    A Bagging classifier.

    A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original 
    dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. 
    Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), 
    by introducing randomization into its construction procedure and then making an ensemble out of it.

    This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random 
    subsets of the samples, then this algorithm is known as Pasting [1]. If samples are drawn with replacement, then the 
    method is known as Bagging [2]. When random subsets of the dataset are drawn as random subsets of the features, then 
    the method is known as Random Subspaces [3]. Finally, when base estimators are built on subsets of both samples and 
    features, then the method is known as Random Patches [4].

BaggingClassifier example:
    from sklearn.ensemble import BaggingClassifier
    from sklearn.tree import DecisionTreeClassifier

    bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,
                                max_samples=100, n_jobs=-1, random_state=42)
    bag_clf.fit(X_train, y_train)

sklearn.ensemble.RandomForestClassifier

class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, 
   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, 
   min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, 
   warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)[source]

    A random forest classifier.

    A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples 
    of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the 
    forest use the best split strategy, i.e. equivalent to passing splitter="best" to the underlying 
    DecisionTreeRegressor. The sub-sample size is controlled with the max_samples parameter if 
    bootstrap=True (default), otherwise the whole dataset is used to build each tree.

RandomForestClassifier example:

    from sklearn.ensemble import RandomForestClassifier

    rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,
                                     n_jobs=-1, random_state=42)
    rnd_clf.fit(X_train, y_train)
    y_pred_rf = rnd_clf.predict(X_test)

    # A Random Forest is equivalent to a bag of decision trees:

    bag_clf = BaggingClassifier(
        DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
        n_estimators=500, n_jobs=-1, random_state=42)
    # extra code - verifies that the predictions are identical
    bag_clf.fit(X_train, y_train)
    y_pred_bag = bag_clf.predict(X_test)
    np.all(y_pred_bag == y_pred_rf)  # same predictions

sklearn mnist_784 dataset:
https://towardsdatascience.com/784-dimensional-quantum-mnist-f0adcf1a938c
  MNIST, a popular dataset of handwritten digits, the numbers 0 through 9. The dataset is popular because of 
  its quality, which allows you to focus on training and testing your model without worrying about cleaning 
  your data.

https://www.openml.org/search?type=data&sort=runs&id=554&status=active

  The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. 
  It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples

  It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a 
  fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition 
  methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and 
  white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. 
  The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization 
  algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating 
  the image so as to position this point at the center of the 28x28 field.


sklearn.datasets.fetch_openml

sklearn.datasets.fetch_openml(name: str | None = None, *, version: str | int = 'active', data_id: int | None = None, 
  data_home: str | PathLike | None = None, target_column: str | List | None = 'default-target', 
  cache: bool = True, return_X_y: bool = False, as_frame: str | bool = 'auto', n_retries: int = 3, 
  delay: float = 1.0, parser: str = 'auto', read_csv_kwargs: Dict | None = None)[source]

    Fetch dataset from openml by name or dataset id.

    Datasets are uniquely identified by either an integer ID or by a combination of name and version (i.e. there 
    might be multiple versions of the iris dataset). Please give either name or data_id (not both). In case a 
    name is given, a version can also be provided.

mnist_784 example:

    from sklearn.datasets import fetch_openml

    X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False, parser='auto')

    rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
    rnd_clf.fit(X_mnist, y_mnist)

sklearn.ensemble.AdaBoostClassifier

class sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', 
    random_state=None)[source]

    An AdaBoost classifier.

    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset 
    and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly 
    classified instances are adjusted such that subsequent classifiers focus more on difficult cases.

AdaBoostClassifier example:
    from sklearn.ensemble import AdaBoostClassifier

    ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=30, learning_rate=0.5, random_state=42)
    ada_clf.fit(X_train, y_train)

 class sklearn.tree.DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, 
   min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, 
     min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)[source]

    A decision tree regressor.

    Parameters:

        criterion: {"squared_error", "friedman_mse", "absolute_error", "poisson"}, default="squared_error"

            The function to measure the quality of a split. Supported criteria are "squared_error" for the mean 
            squared error, which is equal to variance reduction as feature selection criterion and minimizes the 
            L2 loss using the mean of each terminal node, "friedman_mse", which uses mean squared error with 
            Friedmans improvement score for potential splits, "absolute_error" for the mean absolute error, 
            which minimizes the L1 loss using the median of each terminal node, and "poisson" which uses 
            reduction in Poisson deviance to find splits.

        splitter: {"best", "random"}, default="best

            The strategy used to choose the split at each node. Supported strategies are "best" to choose 
            the best split and "random" to choose the best random split.

        max_depth: int, default=None

            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all 
            leaves contain less than min_samples_split samples.


sklearn.ensemble.GradientBoostingRegressor

class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, ...)

    Gradient Boosting for regression.

    This estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of 
    arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of 
    the given loss function.

    sklearn.ensemble.HistGradientBoostingRegressor is a much faster variant of this algorithm for intermediate 
    datasets (n_samples >= 10_000).

GradientBoostingRegressor example1:

    from sklearn.ensemble import GradientBoostingRegressor

    gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,
                                     learning_rate=1.0, random_state=42)
    gbrt.fit(X, y)

GradientBoostingRegressor example2:

    gbrt_best = GradientBoostingRegressor(
        max_depth=2, learning_rate=0.05, n_estimators=500,
        n_iter_no_change=10, random_state=42)
    gbrt_best.fit(X, y)
    gbrt_best.n_estimators_


sklearn.ensemble.HistGradientBoostingRegressor

class sklearn.ensemble.HistGradientBoostingRegressor(loss='squared_error', *, quantile=None, learning_rate=0.1, 
    max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, ...)

    Histogram-based Gradient Boosting Regression Tree.

    This estimator is much faster than GradientBoostingRegressor for big datasets (n_samples >= 10 000).

    This estimator has native support for missing values (NaNs). During training, the tree grower learns at each 
    split point whether samples with missing values should go to the left or right child, based on the potential 
    gain. When predicting, samples with missing values are assigned to the left or right child consequently. If 
    no missing values were encountered for a given feature during training, then samples with missing values are 
    mapped to whichever child has the most samples.

example:

    HistGradientBoostingRegressor housing = load_housing_data()

    import pandas as pfrom sklearn.pipeline import make_pipeline
    from sklearn.compose import make_column_transformer
    from sklearn.ensemble import HistGradientBoostingRegressor
    from sklearn.preprocessing import OrdinalEncoder

    hgb_reg = make_pipeline(
        make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]),
                                remainder="passthrough"),
        HistGradientBoostingRegressor(categorical_features=[0], random_state=42)
    )
    hgb_reg.fit(housing, housing_labels)d
    from sklearn.model_selection import train_test_split
    import tarfile
    import urllib.request

    def load_housing_data():
        tarball_path = Path("datasets/housing.tgz")
        if not tarball_path.is_file():
            Path("datasets").mkdir(parents=True, exist_ok=True)
            url = "https://github.com/ageron/data/raw/main/housing.tgz"
            urllib.request.urlretrieve(url, tarball_path)
            with tarfile.open(tarball_path) as housing_tarball:
                housing_tarball.extractall(path="datasets")
        return pd.read_csv(Path("datasets/housing/housing.csv"))

    train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
    housing_labels = train_set["median_house_value"]
    housing = train_set.drop("median_house_value", axis=1)


sklearn.ensemble.StackingClassifier
class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', 
    n_jobs=None, passthrough=False, verbose=0)[source]

    Stack of estimators with a final classifier.

    Stacked generalization consists in stacking the output of individual estimator and use a classifier to 
    compute the final prediction. Stacking allows to use the strength of each individual estimator by using 
    their output as input of a final estimator.

    Note that estimators_ are fitted on the full X while final_estimator_ is trained using cross-validated 
    predictions of the base estimators using cross_val_predict.

StackingClassifier example:
    from sklearn.ensemble import StackingClassifier

    stacking_clf = StackingClassifier(
        estimators=[
            ('lr', LogisticRegression(random_state=42)),
            ('rf', RandomForestClassifier(random_state=42)),
            ('svc', SVC(probability=True, random_state=42))
        ],
        final_estimator=RandomForestClassifier(random_state=43),
        cv=5  # number of cross-validation folds
    )
    stacking_clf.fit(X_train, y_train)
    stacking_clf.score(X_test, y_test)

Chapter 7 Exercises:

    -> see exercise_notebooks/07_exercises.ipynb

sklearn.neural_network.MLPClassifier

class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, 
batch_size='auto', learning_rate='constant', learning_rate_init=0.001, ...)

    Multi-layer Perceptron classifier.

    This model optimizes the log-loss function using LBFGS or stochastic gradient descent.

sklearn.preprocessing.LabelEncoder
class sklearn.preprocessing.LabelEncoder[source]

    Encode target labels with value between 0 and n_classes-1.

    This transformer should be used to encode target values, i.e. y, and not the input X.


1. If you trained five different models on the exact same training data, and they all achieve 95% precision, is there 
   any change that you can combine these models together to get better results? If so, how? If not, why?

  -> By aggregating the predictions of each of the 5 classifer: the class that gets the most votes is the ensemble's 
  prediction. Voting classification often achieves a higher accuracy that the best classifier in the ensemble especially 
  when the predictors are as independent from each other as possible.

  page 212: A very simple way to create an even better classifer is to aggregate the predictions of each classifer: the class 
  that gets the most votes is the ensemble's prediction. This majority-vote classifier is called 'hard voting classifier'.

  page 213: Somewhat surprisingly, this voting classification often achieves a higher accuracy that the best classifier in the 
  ensemble. In fact, enve if each classifier is a 'weak learner' (meaning it does only slightly better than random guessing), 
  the ensemble can still be a 'strong learner' (achieving high accuracy), provided there are a sufficient number weak learners 
  in the ensemble and they are sufficiently diverse.

  page 214: Ensemble method works best when the predictors are as independent from each other as possible. One way to get 
  diverse classifers is to train them using different algorithms. This increases the chance that they will make very different 
  types of errors, improving the ensemble's accuracy.

  page 214: Scikit-Learn provides a VotingClassifier clas that's quite easy to use: just give it a list of name/predictor pairs 
  and using it like a normal classifier. ...

  book answer: If you have trained five different models and they all achieve 95% precision, you can try combining them into 
  a voting ensemble, which will often give you even better results. It works better if the models are very different (e.g., 
  an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are 
  trained on different training instances (that's the whole point of bagging and pasting ensembles), but if not this will still 
  be effective as long as the models are very different.

2. What is the difference between hard and soft voting classifiers?

  -> 'hard voting' classifier give equal weight each predictor. That is, predicts the class with the highest probability 
  based on the majoring voting of the classifiers in the ensemble. Soft voting' predicts the class with the highest class 
  probability by average estimated class probability for each class and picks the class with the highest probability

  page 212: A very simple way to create an even better classifer is to aggregate the predictions of each classifer: the class 
  that gets the most votes is the ensemble's prediction. This majority-vote classifier is called 'hard voting classifier'.

  page 215: If all classifiers are able to estimate class probabilities (i.e. if they all have 'predict_proba()'' method), then 
  you can tell Scikit-Leasrn to predict the class with the highest class probability, averaged over all the individual classifiers. 
  This is called 'soft voting'. It often achieves higher performance than 'hard voting' because it gives more weight to highly 
  confident votes. All you need to do is set voting classifier's voting hyperparameter to 'soft', and ensure that all classifiers 
  can estimate class probabilities. This is not the case for the the SVC class by default, so you need to set it probability 
  hyperparameter to 'True' (this will make the 'SVC class' uses cross-valication to estimate class probabilities, slow down 
  training, and it will add a predict_proba method).

  book answer: A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets 
  the most votes. A soft voting classifier computes the average estimated class probability for each class and picks the class 
  with the highest probability. This gives high-confidence votes more weight and often performs better, but it works only if 
  every classifier is able to estimate class probabilities (e.g., for the SVM classifiers in Scikit-Learn you must set 
  probability=True).

3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting 
  ensembles, boosting ensembles, and random forests, or stacking ensembles?

  -> Both bagging and pasting ensembles predictors can be trained in parallel so they can be distributed across multiple servers. 
  -> Boosting trains classifiers sequentially, so training cannot be speed up by distributing it across multiple servers. 
  -> Random Forest is decison tree ensemble using bagging (or pasting), so it can be distributed across multiple servers. 
  -> Stacking ensembles train predictors and run cross-validation in parallel, so they can be distributed across multiple 
    servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.

  page 215: ... Another approach is to use the same training algorithm fore eavy predictor but train them on different random 
  subset of the training set. When sampling is performed 'with replacement', this method is called 'bagging' (...) (short for 
  'bootstrap aggregating'). When sample is performed 'without replacement', it is called 'pasting' (...). 

  note 1: [bagging] Imagine picking a card randomly from a deck of cards, writing it dlow, the placing it back in the deck before 
  picking the next card: the same card could be sample multiple times.

  page 216: As you can see in Figure 7-4 [Bagging and Pasting involve training several predictors on different random samples 
  of the training set], predictors can be trained in parallel, via different CPU cores or even different servers. Similarly, 
  predictions can be made in parallel. This is one reason bagging and pasting are popular methods: they scale well.

  page 220: As we have discussed, a 'random forest' (...) is an ensemble of decision trees, generally training via bagging method 
  (or sometimes pasting), typicall with 'max_samples' set to the size of the training set. Instead of building a 'BaggingClassifier' 
  class and passing it a 'DecisionTreeClassifiers', you can use the 'RandomForestClassifier' class, which is more convenient and 
  optimized for decision trees (similarly, there is a 'RandomForestRegressor' class for regression tasks).

  page 222: Boosting (originally called hypothesis boosting) refers to an ensemble method that can combine several weak learners 
  into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct 
  it predecessor. There are many boosting methods available, byt by far the most popular are 'AdaBoost' (...) (short for adaptive 
  boosting) and 'gradient boosting'.

  page 222 - 223: One way for a new predictor to correct it predecessor is to pay a bit more attention to the training instances 
  that the predecessor 'underfit'. This results in new predictors focusing more and more on the hard cases. This is the technique 
  used by 'AdaBoost'.

  For example, when training an 'AdaBoost classifer', the algorithm first trains a base classifier (such as 'decision tree') and 
  uses it to make predictions on the training set. The algorithm then increases the relative weight of the misclassified training 
  instances. Then it trains a second classifier, using the updated weights, and againg makes predictions on the traning set, update 
  the instance weights, and so on (see Figure 7-7 [AdaBoost sequential training with instanc weight updates])

  page 226: Another popular boosting algorithm is 'gradient boosting' (...). Just like AdaBoost, gradient boosting works by 
  sequentially adding predictors to an ensemble, each one correcting it predecessor. However, instead of tweaking the instance 
  weights at every interation like AdaBoost does, this method tring to fit the new predictor to the 'residual errors' make 
  by the predictor.

  page 232: The last ensemble method we will discuss in this chapger is calledd 'stacking' (short for 'stacked generalization' 
  (...). It is based on a simle idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of 
  all in an ensemble, why don't we train a model to perform this aggregation? Figure 7-11 [Aggregating preidtions using a 
  blending predictor] shows such an ensemble performing a regression task on a new instance. Each of the bottom three prdictors 
  predict a different value (3.1, 2.7, & 2.9), and then the final predictor (called a 'blender' or a 'meta learner') takes 
  these predictions as inputs and makes the final preiction (3.0).

  page 232 - 233: To train the blender, you first need to build the blender training set. You can use 'cross_val_predict()' in 
  the ensemble to get out-of-sample predictions fore ach instance in the original training set (Figure 7.12 [Training the blender 
  in a stacking ensemble]), and use these can be used as input features to traing the blender; and targets can simply be copied 
  from the original training set. Note that regardless of the number of features in the original training set (just one in this 
  example), the blending training set will contain one input feature per predictor (three in this example). Once the blender 
  is trained, teh base predictores are retrained one last time on the full original training set.

  book answer: It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since 
  each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the 
  same reason. However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily 
  sequential, and you will not gain anything by distributing training across multiple servers. Regarding stacking ensembles, all 
  the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, 
  the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.

4. What the benefit of out-of-bag evaluations?

  -> the out-of-bag evaluation, the OOB (out-of-bag - held out from the predictor training) can be used for validation instead 
  of creating validation set.

  page 218: With bagging, some training instances may be sample serveral times for any given predictor, while others may not 
  be sampled at all. By default a 'BaggingClassifier' samples 'm' training instances 'with replacement' (bootstrap=True), 
  where 'm' is the size of the training set. With this process, it can be show mathematically that only 63% of the training 
  instances are sampled on average fore each predictor. The remaining 37% of the training instances are not sampled are called 
  'out-of-bag' (OOB) instances. Note that are not the same 37% for all predictors.

  page 218: A bagging ensemble can be evaluated using OOB instances, without the need for a seperate validation set: indeed, 
  if there are enough estimators, then each instance in the training set will likely be an OOB instance of several estimators, 
  so these estimators can be used to make a fair ensemble prediciton for that instance. Once you have prediction fore each 
  instance, you can compute the ensemble's prediction accuracy (or any other metric)

  book answer: With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated using instances that it was not 
  trained on (they were held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the 
  need for an additional validation set. Thus, you have more instances available for training, and your ensemble can perform 
  slightly better.

5. What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees 
  classifiers slower or faster than regular random forests?

  -> extra-trees ensemble use random thresholds for each feature rather than searching for the best possible thresholds. 
  -> extra-trees trades more bias for a lower variance. It also makes 'extra-trees classifier' much faster to train than 
     regular 'random forests', because finding the best possible threshold for each feature at every node is one of the 
     most time-consuming task of growing a tree.

  page 208 (chapter 6): Luckily, by averaging predictions over many trees, it's possible to reduce variance significantly. 
  Such an 'ensemble of trees' is called a 'random forest', and it;s one of the most powerful types of models available today, 
  as you will see in the next chapter.

  page 220: The random forest algorithm introduces extra randomness when growing trees instead of search for the very best 
  features when splitting a node (Chapter 6), it searchs for the best feature amount a random subset of featurs. By default, 
  it samples n**1/2 features ('n' is the total number of features). The algorithm results in greater tree diversity, which 
  (again) trades a higher bias for a lower variance generally yielding a novera better model. So, the following 
  BaggingClassifier is equivalent to the previous RandomForestClassifier: 
    bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16), n_estimators=500, n_jobs=-1, random_state=42) 
    bag_clf.fit(X_train, y_train) y_pred_bag = bag_clf.predict(X_test)

  page 220: When you are growing a tree in a random foest, at each node only a random subset of features is considered for 
  splitting (as discussed earlier). It is posssible to make trees even more random by also using random thresholds for each 
  feature rather than search for the best possible thresholds (like regular decision trees do). For this, simples set 
  'splitter="random"' when creatng a DecisionTreeClassifier.

  page 221: A forest of such extremely random trees is called an 'extremely randomized trees' (...) (or 'extra-trees' 
  for short) ensemble. Once again, this technique trades more bias for a lower variance. It also makes 'extra-trees 
  classifier' much faster to train than regular 'random forests', because finding the best possible threshold for each 
  feature at every node is one of the most time-consuming task of growing a tree.

  book answer: When you are growing a tree in a Random Forest, only a random subset of the features is considered for 
  splitting at each node. This is true as well for Extra-Trees, but they go one step further: rather than searching for 
  the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra 
  randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform 
  better. Moreover, since Extra-Trees don't search for the best possible thresholds, they are much faster to train than 
  Random Forests. However, they are neither faster nor slower than Random Forests when making predictions.

6. If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak, and how?

  -> AdaBoost ensemble increases the weight of the 'unfitting instances' (misclassified) after each estimator stage, so 
  increasing the number of stages/estimators ('n_estimators' hyperparater) can be used to reduced underfitting. Reducing 
  the regularization of the estimators used, can also be used to reduce underfitting.

  page 222 - 223: One way for a new predictor to correct it predecessor is to pay a bit more attention to the training instances 
  that the predecessor 'underfit'. This results in new predictors focusing more and more on the hard cases. This is the 
  technique used by 'AdaBoost'.

  For example, when training an 'AdaBoost classifer', the algorithm first trains a base classifier (such as 'decision tree') 
  and uses it to make predictions on the training set. The algorithm then increases the relative weight of the misclassified 
  training instances. Then it trains a second classifier, using the updated weights, and againg makes predictions on the 
  training set, update the instance weights, and so on (see Figure 7-7 [AdaBoost sequential training with instanc weight updates])

  page 226: Another popular boosting algorithm is 'gradient boosting' (...). Just like AdaBoost, gradient boosting works by 
  sequentially adding predictors to an ensemble, each one correcting it predecessor. However, instead of tweaking the instance 
  weights at every interation like AdaBoost does, this method tring to fit the new predictor to the 'residual errors' make 
  by the predictor.

  page 226: If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more 
  strongly regularizing the base estimator

  book answer: If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or 
  reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.

7. If your gradient boosting ensemble overfits the training set should you increase or decrease the learning rate?

  -> If gradient boosting is overfitting, you can try to decrease the learning rate. Instead you may try to the reduce the 
  number of estimators and add early stopping.

  page 228: The 'learning_rate' hyperparameter scales the contributions of each tree. If you set to a low value, such as 0.05, 
  you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This 
  is a regularization technique called 'shrinkage'. Figure 7-10 [GBRT ensembles with not enough predictors (left) and just 
  enough (right)] shows two GBRT [Gradient Boosted Regression Trees] ensembles trained with different hyperparameters: the one 
  on the eft does not have enough trees to fit the training set, while the one on the righ has about the right amount. If we 
  added more trees, the GBRT would start to overfit the training set.

  page 229: To find the optimal number of trees, you could perform cross-validation using 'GridSearchCV' or 'RandomizedSearchCV', 
  as usual, buter there's a simpler way: if you set the 'n_iter_no_changer' hyperparameter to an integer value, say 10, the the 
  'GradientBoostingRegressor' will automatically stop adding more trees during training if it sees the last 10 trees did not help. 
  This is simply 'early stopping' ...

  page 229: If you set 'n_iter_no_change' too low, training may stop too earcy and the model will underfit. But if yest it too 
  high, it will overfit instead. We also set a fairly small learning rate and a high number of estimators, bu the actual number 
  of estimators in the trained ensemble is much lower, thanks to early stopping:

  book answer: If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You 
  could also use early stopping to find the right number of predictors (you probably have too many).

8. Voting Classifier

  Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g. 
  50,000 instances for training, 10,000 for validation, and 10,000 for testing), Then train various classifiers, such as a 
  random forest classifier, and extra-trees classifier, and SVM classifier. Next, try to combine them into an ensemble that 
  outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on 
  the test set. How much better does it perform compared to the individual classifiers?


9. Stacking Ensemble

  Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training 
  set with the resulting predictions: each training instance is a vecotr containing the set of predictions from all your 
  classifiers for an image, and target is the image's class. Train a classifier on this new training set. Congratulations - you 
  have trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. 
  For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get 
  the ensemble's predictions. How does it compare to the voting classifier you trainined earlier? Now try again using a 
  'StackingClassifier' instead. Do you get better performance? If so, why?

------------------------------------------------------
Chapter 8 Dimensionality Reduction
------------------------------------------------------

scipy.spatial.transform.Rotation.from_rotvec
classmethod Rotation.from_rotvec(cls, rotvec, degrees=False)

  Initialize from rotation vectors.

  A rotation vector is a 3 dimensional vector which is co-directional to the axis of rotation and whose norm gives the angle of rotation [1].


sklearn.decomposition.PCA
class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', 
    n_oversamples=10, power_iteration_normalizer='auto', random_state=None)[source]

    Principal component analysis (PCA).

    Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. 
    The input data is centered but not scaled for each feature before applying the SVD.

    It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending 
    on the shape of the input data and the number of components to extract.


Principal components:

import numpy as np

# X = [...]  # the small 3D dataset was created earlier in this notebook
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt[0]
c2 = Vt[1]

Note: in principle, the SVD factorization algorithm returns three matrices, U, Sigma and V, such that X = U @ Sigma @ V 
where U is an m  m matrix, Sigma is an m  n matrix, and V is an n  n matrix. But the 
svd() function returns U, s and Vt instead. s is the vector containing all the values on the main diagonal 
of the top n rows of Sigma. Since Sigma is full of zeros elsewhere, your can easily reconstruct it from s, like this:

# extra code - shows how to construct Sigma from s
m, n = X.shape
Sigma = np.zeros_like(X_centered)
Sigma[:n, :n] = np.diag(s)
assert np.allclose(X_centered, U @ Sigma @ Vt)

definitions:
    Unitary Matrix: 
       a square matrix of complex numbers. The product of the conjugate transpose of a unitary matrix, with the unitary 
       matrix, gives an identity matrix. 
    Conjugate
      In Algebra, the conjugate is where you change the sign (+ to , or  to +) in the middle of two terms.
      Examples:
       - from 3x + 1 to 3x  1

Chapter 8 Exercises:

    -> see exercise_notebooks/08_exercises.ipynb

 sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)[source]

    Accuracy classification score.

    In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly 
    match the corresponding set of labels in y_true.


sklearn.manifold.TSNE
class sklearn.manifold.TSNE(n_components=2, *, learning_rate='auto', n_iter=1000, init='pca', ...)[source]

    T-distributed Stochastic Neighbor Embedding.

    t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries 
    to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional 
    data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.

    n_components : int, default=2
      Dimension of the embedded space.
    
    learning_rate : float or "auto", default="auto"
       The learning rate for t-SNE is usually in the range [10.0, 1000.0]. 

    init : {"random", "pca"} or ndarray of shape (n_samples, n_components), default="pca"
      Initialization of embedding. PCA initialization cannot be used with precomputed distances and is usually more globally stable 
      than random initialization.`

sklearn.preprocessing.MinMaxScaler
class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)[source]

    Transform features by scaling each feature to a given range.

    This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.

    The transformation is given by:

    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
    X_scaled = X_std * (max - min) + min
      where min, max = feature_range.

    This transformation is often used as an alternative to zero mean, unit variance scaling.



1.  What are the main motivations for reducing the dataset's dimensionality? What are the main drawbacks?

  -> reducing data dimensionality is mainly used to speed up training, but it can be used for data visualization, and 
  occasionally it can reduce noise -> The drawbacks to reducing data dimensionality cause information loss which may 
  result in your system performing worse. It adds complexity to your data pipeline.

  page 237: Many mahcine learning problems involve thousands or even millions of features for each training instance. Not 
  only do all these features make training extremely slow, but they can also make it much harder to find a good solution, 
  as you will see. This problem is often referred to as the 'curse of dimensionality'.

  page 237: Reducing dimensionality does cause some information loss, just like compressing an image to JPEG can degrade its quality, 
  so even though it will speed up training, it may make your system perform slightly worse. It also makes your pipelines a bit more 
  complex and thus harder to maintain. Therefore, I recommend you first try to train your system with the original data before 
  considering using dimensionality reduction. In some cases, reducing the dimensionality of the training data may filter out some 
  noise and unnecessary details and thus results in higher performance, but in general it won't, it will just speed up training.

  page 238: Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization. Reducing the 
  number or dimensions dow to two (or three) make it possible to plot a condensed view of a high dimensional training set on a 
  graph and often gain some important insights by visually detecting patterns, such as clusters. Moreover, visualization is 
  essential to communicate your conclusions to people who are not data scientists - in particular, decision makers who will use 
  your results.

  page 249: After dimensionality reduction, the training set takes up much less space. For example, after applying PCA to the MNIST 
  dataset while preserving 95% of its variance, we are left with 154 features, instanc of the original 784 features. So the dataset 
  is a now less than 20% of its original size, and we only lost 5% of it varinace! This is a reasonable compression ration, and it's 
  easy to see how such as size reduction would speed up a classification algorithm tremendously.

  book answer: The main motivations for dimensionality reduction are: 
    - To speed up a subsequent training algorithm (in some cases it may even remove noise and redundant features, 
      making the training algorithm perform better) 
    - To visualize the data and gain insights on the most important features 
    - To save space (compression)

    The main drawbacks are: 
    - Some information is lost, possibly degrading the performance of subsequent training algorithms. 
    - It can be computationally intensive. 
    - It adds some complexity to your Machine Learning pipelines. 
    - Transformed features are often hard to interpret.

2. What is the curse of dimensionality?

  -> As the dimensionality increases, the number of data points required for good performance of any machine learning 
  algorithm increases exponentially. -> An increase in the dimensions can in theory, add more information to the data 
  thereby improving the quality of data but practically increases the noise and redundancy during its analysis.

  page 239: Here is a more trobulesome difference: if you pick two points randomly in a unit square, the distance between 
  these points , on average, roughly 0.52. If you pick two random points ina 3D unit cube, the average distance will be 
  roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional unit hypercube? The average distance, 
  believe it or not, will be 408.25 (roughly (1,000,000/6)**1/2)! This is counterintuitive: how can two points be so far 
  apart when they both liewithin the same unit hypercube? Well, there's just plenty of space in high dimensions. As a result, 
  high-dimensional datasets are at rist of being very sparse: most training instances will likely be far away from any training 
  instance, make predictions much less reliable that in lower dimensions, since they will be based on much larger extrapolations. 
  In short, the more dimensions the training set has, the greater risk of overfitting it.

  book answer: The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise 
  in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high-dimensional 
  vectors are generally far from one another, increasing the risk of overfitting and making it very difficult to identify 
  patterns without having plenty of training data.

3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?

  -> generally, you cannot reverse the operation the dimensionality reduction. -> however, with 
  PCA compression / dimensionality reduction, you can use the PCA inverse_transform method to recovered the original 
  data within the variance reduction of the PCA compresion.

  page 249: It is also possible to decompress the reduced dataset back to 784 dimensions by apply the inverse transformation 
  of the PCA projection. This won't give you back the original data, since the proejct lost a bit of information (within 
  the 5% variance that was dropped), but it will likely be close to the original data.

  page 249: the inverse_transform() method lets us decompress the reduced MNIST dataset back to 784 dimensions:

            X_recovered = pca.inverse_transform(X_reduced)

  book anwser: Once a dataset's dimensionality has been reduced using one of the algorithms we discussed, it is almost 
  always impossible to perfectly reverse the operation, because some information gets lost during dimensionality reduction. 
  Moreover, while some algorithms (such as PCA) have a simple reverse transformation procedure that can reconstruct a 
  dataset relatively similar to the original, other algorithms (such as t-SNE) do not.

4, Can PCA be used to reduce the dimensionality of a highly non-linear dataset?

  -> PCA can be used to significantly reduce the dimensionality of highly non-linear datasets. However, some dataset like 
  the Swiss roll need to be unrolled instead of projected on the a reduced data plane PCA projection would loss need information.

  page 241: However, projection is not always the best approach to dimensionality reduction. IN many cases the subspace may 
  twist and turn, such as the famous swiss roll toy dataset represented in figure 8-4 [Swiss roll dataset].

  Simply projecting onto a plane (e.g. by dropping x1) would squash different layers of the Swiss roll together, as show on 
  the left side of Figure 8-5 [Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)]. What 
  you probably want instead is to unroll the swiss roll to object the 2D dataset on the right side of Figure 8-5.

  page: 243: Principal component analysis (PCA) is by far the most popular dimensionality reduction algorithm. First, it 
  identifies the hyperplane that lies closest to the data, and then it projects the data onto it, just like 
  Figure 8-2 [A 3D dataset lying close to a 2D subspace].

  book answer: PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, 
  because it can at least get rid of useless dimensions. However, if there are no useless dimensions - as in the Swiss roll 
  dataset - then reducing dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it.

5. Suppose your perform PCA on a 1,000-dimensional dataset, settting the explained variance ratio to 95%. How many dimensions 
   will be the resulting dataset have?

  -> depends on the dataset.

  page 244: PCA identifies the axis that accounts for the largest amount of the variance in the training set. In Figure 8-7 
  [Selecting the subspace on which to project], it is the solid line. It also finds a second axis, orthogonal to the first one, 
  that aacounts for the largest amount of the remaining variance. In this 2D example, there is no choice:: it is the dotted line. 
  If it were a higher-dimensional dataset, PCA would find a third axis, orthogonal to both previous axes, and a fournt, a fifth, 
  and so on - as many axes as the number of dimesions in the dataset.

  page 246: Another user piece of information is the 'explained variance ratio' of each principal component, avaiable via the 
  'explained_variance_ratio_variable'. The ratio indicates the proportion of the dataset's variance that lies along the each 
  principal component. For example, let's look at the explained variance ratios of the first two components of the 3D dataset 
  represented in figure 8-2 [A 3D dataset lying close to a 2D subspace]

    pca.explained_variance_ratio_ array([0.7578477 , 0.15186921])

  The first dimension explains about 76% of the variance, while the second explains about 15%.

  book answer: That's a trick question: it depends on the dataset. Let's look at two extreme examples. First, suppose the 
  dataset is composed of points that are almost perfectly aligned. In this case, PCA can reduce the dataset down to just 
  one dimension while still preserving 95% of the variance. Now imagine that the dataset is composed of perfectly random points, 
  scattered all around the 1,000 dimensions. In this case roughly 950 dimensions are required to preserve 95% of the variance. 
  So the answer is, it depends on the dataset, and it could be any number between 1 and 950. Plotting the explained variance as 
  a function of the number of dimensions is one way to get a rough idea of the dataset's intrinsic dimensionality.

6. In what cases would you use regular PCA, incremental PCA, and randomized PCA, or random projection?

  -> Use regular PCA by default provided the dataset can fit in memory. 
  -> Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the 
     dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks, when you need to apply 
     PCA on the fly, every time a new instance arrives. 
  -> Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, 
     it is much faster than regular PCA. 
  -> Finally, Random Projection is great for very high-dimensional datasets.

  page 250: Randomized PCA. If you set the 'svd_solver' hyperparameter to 'randomized', Scikit-Learn uses a stochastic algorithm 
  called 'randomized PCA' that quickly finds and approximation of the first 'd' principal components. It computational complexity 
  is O(m x d2) O(d3), instead of O(m x n2) + O(n3) for the full SVD approach, so it is dramatically faster than for SVD when 'd' 
  is much smaller than n.

  page 250: Incremental PCA. One problem with the preceding implementation [randomized PCA] of PCA is that they require the whole 
  training set to fit in memory in order for the algorithm to run. Fortunately, 'incremental PCA (IPCA) algorithms' have been 
  developed that allow you to split the training set into mini-batches and feed these in one mini-batch at a time. This is useful 
  for large training set and for applying PCA online (i.e., on the fly, as new instance arrives)

  page 251: For very high dimensional datasets, PCA can be too slow. As you saw earlier, enve if you use randomized PCA, it 
  computational complexity is still O(m x d2) + O(d3), so that target number of dimensions 'd' must not be too large. If you are 
  dealing witha dataset with tens of thousands of features or more (e.g. images), the training may become much too slow: in this 
  cause you should consider random projections.

  page 252: Randomized Projection. As it name suggests, the 'random project algorithm' project the data to a lower dimensional 
  space using a random linear projection. This may sould crazy, but it turns out that such a raondom projection is actually very 
  likely to preserve distances fairly well, as demonstrated mathematically by William B. Johnson and Joram Lindenstrauss in a 
  famous lemma. So, two similar instances will remain similar after the projection, and two veryf different instances will 
  remain very different.

  Obviously, the more dimensions you drop, the more information is lost, and the more distances get distorted. So how can you 
  choose the optimial number of dimensions? Well, Johnson and Lindenstrauss came up with an equation that determines the minimum 
  number of dimensions to preserve in order to ensure - with high probability - that the distances won't change by more than a 
  given tolerance. For example, if you have a dataset containinig m = 5000 instances and n = 20,000 features each, and you don't 
  want the squared distance between any two instances to change by more than epsilon = 10%, then you should project the data 
  down to 'd' dimensions, with d >= ln(m) / (1/2 * e**2 - 1/3 * epsilon**3), which 7300 dimensions. That's quite a significant 
  dimensionality reduction! Note that the equation does not use 'n', it only relies on the 'm' and 'epsilon'. This equation is 
  implemented by the johnson_lindenstrauss_min_dim() function.

            >>> m, epsilon = 5_000, 0.1 
            >>> d = johnson_lindenstrauss_min_dim(m, eps=epsilon) 
            >>> d 
            7300

  page 254: In summary, random projection is a simple, fast, and memory-efficient, and surprisingly powerful dimensionality 
  reduction algorithm that you should keep in mind, especially wheu you deal with high-dimensional datasets.

  book answer: Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large 
  datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer 
  regular PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the fly, every time a new instance 
  arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this 
  case, it is much faster than regular PCA. Finally, Random Projection is great for very high-dimensional datasets.

7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?

  book answer: Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the 
  dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the 
  reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if 
  you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest 
  classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose 
  too much information, then the algorithm should perform just as well as when using the original dataset.

8. Does it make sense to chain two different dimensionality reduction algorithms?

  page 254: LLE. 'Locally Linear embedding (LLE)' is a non-linear dimensionality reduction (NLDR)' technique. It is a manifold 
  learning technique that does not rely on projections, unlike PCA and random projections. In a nutshell, LLE works by first 
  measuring each training instance linearly relates to its nearest neighbors, and then looking for a low-dimensional 
  representation of the training set where theses local relationships are best preserved (...). This approach makes it 
  particularly good at unrolling twisted manifolds, especially when there is not two much noise.

  book answer: It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is 
  using PCA or Random Projection to quickly get rid of a large number of useless dimensions, then applying another much slower 
  dimensionality reduction algorithm, such as LLE. This two-step approach will likely yield roughly the same performance as 
  using LLE only, but in a fraction of the time.

  9.  Load the MMIST dataset and split it into a training set and test set (take the first 60,000 instances for training, and the 
  remaining 10,000 for testing). Train a random forest classifier on the dataset and time how long it takes, the evaluate the result 
  model on the test set. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new 
  random forest classifier on the reduced dataset and see how long it takes. Was the training much faster? Next, evaluate the 
  classifier on the test set. How does it compare to the previous classifier? Try again with an SGDClassifier. How much does the 
  PCA help now?
    -> see exercise_notebooks/08_exercises.ipynb



10. Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to 2 dimensions and plot the results using Matplotlib. 
    You can use a scatterplot using 10 different colors to represent each image's target class. Alternatively, you can replace each 
    dot in the scatterplot with the corresponding instances's class (a digit from 0 to 9), or even plot scaled-down versions of the 
    digit images themselves (if you plot all digits the visualization will be too cluttered, so you should either draw a random sample 
    or plot an instance only if no other instances has already been plotted at a close distance). You should get a nice visualization 
    with well-separated clusters of digits. Try using other dimensionality reduction algorithms, such as PCA, LLE, or MDS and compare 
    the resulting visualization.

    -> see exercise_notebooks/08_exercises.ipynb


------------------------------------------------------
Chapter 9 Unsupervised Learning Techniques
------------------------------------------------------

sklearn.mixture.GaussianMixture
class sklearn.mixture.GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, ...)

    Gaussian Mixture.

    Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a 
    Gaussian mixture distribution.

    n_components : int, default=1
        The number of mixture components.
    
Voronoi Tessellation 
  Voronoi diagram
    In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. 
    It can be classified also as a tessellation.
  Tessellation
    A tessellation or tiling is the covering of a surface, often a plane, using one or more geometric shapes, called tiles, 
    with no overlaps and no gaps.
Centroid
  the center of mass of a geometric object of uniform density.

sklearn.cluster.KMeans
   class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, 
      random_state=None, copy_x=True, algorithm='lloyd')[source]

       K-Means clustering.


PIL.Image.open(fp, mode='r', formats=None)  Image[source]

  Opens and identifies the given image file.

  This is a lazy operation; this function identifies the file, but the file remains open and the actual image data is not 
  read from the file until you try to process the data (or call the load() method). 

numpy.asarray
  numpy.asarray(a, dtype=None, order=None, *, like=None)

  Convert the input to an array.

The K-Means Algorithm
  The K-Means algorithm is one of the fastest clustering algorithms, and also one of the simplest:
    - First initialize k-centroids randomly: e.g., k-distinct instances are chosen randomly from the dataset and 
      the centroids are placed at their locations.
    - Repeat until convergence (i.e., until the centroids stop moving):
       - Assign each instance to the closest centroid.
       - Update the centroids to be the mean of the instances that are assigned to them.

   The KMeans class uses an optimized initialization technique by default. To get the original K-Means algorithm 
   (for educational purposes only), you must set init="random" and n_init=1. 

K-Means Variability
  In the original K-Means algorithm, the centroids are just initialized randomly, and the algorithm simply runs a single 
  iteration to gradually improve the centroids, as we saw above.

  However, one major problem with this approach is that if you run K-Means multiple times (or with different random seeds), 
  it can converge to very different solutions, 

K-Means Inertia

  To select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an 
  unsupervised task, so we do not have the targets. But at least we can measure the distance between each instance and its 
  centroid. This is the idea behind the inertia metric:

  >>> kmeans.inertia_
  211.5985372581684

  Inertia is the sum of the squared distances between each training instance and its closest centroid

K-Means Multiple Initializations

  So one approach to solve the variability issue is to simply run the K-Means algorithm multiple times with different random 
  initializations, and select the solution that minimizes the inertia.

  When you set the n_init hyperparameter, Scikit-Learn runs the original algorithm n_init times, and selects the solution that 
  minimizes the inertia. By default, Scikit-Learn sets n_init=10.

  >>> kmeans_rnd_10_inits = KMeans(n_clusters=5, init="random", n_init=10, random_state=2)
  >>> kmeans_rnd_10_inits.fit(X)

K-Means Centroid initialization methods

  Instead of initializing the centroids entirely randomly, it is preferable to initialize them using the following algorithm, 
  proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii:

    - Take one centroid 'c1', chosen uniformly at random from the dataset.
    - Take a new center 'ci', choosing an instance xi with probability: D(xi)**2 / Sum [j=1, m] D(xj)**2 where D(xi) is the 
      distance between the instance and the closest centroid that was already chosen. This probability distribution ensures 
      that instances that are further away from already chosen centroids are much more likely be selected as centroids.
    - Repeat the previous step until all 'k' centroids have been chosen.

   The rest of the K-Means++ algorithm is just regular K-Means. With this initialization, the K-Means algorithm is much less 
   likely to converge to a suboptimal solution, so it is possible to reduce n_init considerably. Most of the time, this largely 
   compensates for the additional complexity of the initialization process.

   To set the initialization to K-Means++, simply set init="k-means++" (this is actually the default):

Accelerated K-Means

  The K-Means algorithm can sometimes be accelerated by avoiding many unnecessary distance calculations: this is achieved 
  by exploiting the triangle inequality (given three points A, B and C, the distance AC is always such that AC <= AB + BC) 
  and by keeping track of lower and upper bounds for distances between instances and centroids (see this 2003 paper by 
  Charles Elkan for more details).

  For Elkan's variant of K-Means, use algorithm="elkan". For regular KMeans, use algorithm="full". The default is "auto", 
  which uses the full algorithm since Scikit-Learn 1.1 (it used Elkan's algorithm before that).  

Mini-Batch K-Means

   Scikit-Learn also implements a variant of the K-Means algorithm that supports mini-batches (see this paper):

   >>> from sklearn.cluster import MiniBatchKMeans
   >>> minibatch_kmeans = MiniBatchKMeans(n_clusters=5, n_init=3, random_state=42)
   >>> minibatch_kmeans.fit(X)
   MiniBatchKMeans(n_clusters=5, random_state=42)

Finding the optimal number of clusters

  What if the number of clusters was set to a lower or greater value than 5?

  No, we cannot simply take the value of 'k' that minimizes the inertia, since it keeps getting lower as we increase 'k'. 
  Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower 
  the inertia will be. However, we can plot the inertia as a function of 'k' and analyze the resulting curve:

  ...

  As you can see, there is an elbow at k = 4, which means that less clusters than that would be bad, and more clusters would 
  not help much and might cut clusters in half. So is a pretty good choice. Of course in this example it is not perfect 
  since it means that the two blobs in the lower left will be considered as just a single cluster, but it's a pretty good 
  clustering nonetheless.

  Another approach is to look at the 'silhouette score', which is the mean 'silhouette coefficient' over all the instances. 
  An instance's 'silhouette coefficient' is equal to (b - a) / max(a, b) where 'a' is the mean distance to the other instances 
  in the same cluster (it is the mean 'intra-cluster' distance), and 'b' is the mean 'nearest-cluster' distance, that is the 
  mean distance to the instances of the next closest cluster (defined as the one that minimizes 'b', excluding the instance's 
  own cluster). The 'silhouette coefficient' can vary between -1 and +1: a coefficient close to +1 means that the instance is 
  well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster 
  boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster.

Mini-Batch K-Means

  Scikit-Learn also implements a variant of the K-Means algorithm that supports mini-batches 

  >>> from sklearn.cluster import MiniBatchKMeans
  >>> minibatch_kmeans = MiniBatchKMeans(n_clusters=5, n_init=3, random_state=42)
  >>> minibatch_kmeans.fit(X)

  >>> minibatch_kmeans.inertia_
  211.65239850433204

Using MiniBatchKMeans along with memmap (not in the book)

  If the dataset does not fit in memory, the simplest option is to use the memmap class, just like we did for 
  incremental PCA in the previous chapter.

Limits of K-Means

  Let's generate a more difficult dataset, with elongated blobs and varying densities, and show that K-Means 
  struggles to cluster it correctly:

Using Clustering for Semi-Supervised Learning

  Another use case for clustering is semi-supervised learning, when we have plenty of unlabeled instances and very 
  few labeled instances.

  Let's tackle the digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 88 images representing 
  digits 0 to 9.

Using Clustering for Image Segmentation

  Download the ladybug image:


sklearn.metrics.silhouette_score

sklearn.metrics.silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)[source]

    Compute the mean Silhouette Coefficient of all samples.

    The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster 
    distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the 
    distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient 
    is only defined if number of labels is 2 <= n_labels <= n_samples - 1.

    This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use silhouette_samples.

    The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally 
    indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.

    Read more in the User Guide.

    Parameters:

        X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric == "precomputed" or 
            (n_samples_a, n_features) otherwise

            An array of pairwise distances between samples, or a feature array.

        labels : array-like of shape (n_samples,)

            Predicted labels for each sample.



sklearn.cluster.DBSCAN

class sklearn.cluster.DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', 
   leaf_size=30, p=None, n_jobs=None)[source]

    Perform DBSCAN clustering from vector array or distance matrix.

    DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands 
    clusters from them. Good for data which contains clusters of similar density.

    The worst case memory complexity of DBSCAN is O(n**2), which can occur when the eps param is large and min_samples is low.

  Parameters:

    eps : float, default=0.5

       The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is 
       not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to 
       choose appropriately for your data set and distance function.

    min_samples : int, default=5

       The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. 
       This includes the point itself.
     
  Attributes:

    core_sample_indices_ : ndarray of shape (n_core_samples,)
        Indices of core samples.
    components_ : ndarray of shape (n_core_samples, n_features)
        Copy of each core sample found by training.
    labels_ : ndarray of shape (n_samples)
        Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.


sklearn.cluster.SpectralClustering

class sklearn.cluster.SpectralClustering(n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, 
    n_init=10, gamma=1.0, ...)

    Apply clustering to a projection of the normalized Laplacian.

    In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex, 
    or more generally when a measure of the center and spread of the cluster is not a suitable description of the 
    complete cluster, such as when clusters are nested circles on the 2D plane.

    If the affinity matrix is the adjacency matrix of a graph, this method can be used to find normalized graph cuts [1], [2].

    When calling fit, an affinity matrix is constructed using either a kernel function such the Gaussian (aka RBF) kernel 
    with Euclidean distance d(X, X):

    np.exp(-gamma * d(X,X) ** 2)

    or a k-nearest neighbors connectivity matrix.

  Parameters:

    n_clusters : int, default=8

        The dimension of the projection subspace.

    gamma : float, default=1.0

        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for affinity='nearest_neighbors'.

    
  Attributes:

    affinity_matrix_array-like of shape (n_samples, n_samples)

        Affinity matrix used for clustering. Available only after calling fit.

        Alternatively, a user-provided affinity matrix can be specified by setting affinity='precomputed'.



sklearn.cluster.AgglomerativeClustering

class sklearn.cluster.AgglomerativeClustering(n_clusters=2, *, metric='euclidean', memory=None, connectivity=None, 
    compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)[source]

    Agglomerative Clustering.

    Recursively merges pair of clusters of sample data; uses linkage distance.

  Parameters:

    n_clusters : int or None, default=2

        The number of clusters to find. It must be None if distance_threshold is not None.

    linkage : {ward, complete, average, single}, default=ward

        Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. 
        The algorithm will merge the pairs of cluster that minimize this criterion.

          - ward minimizes the variance of the clusters being merged.
          - average uses the average of the distances of each observation of the two sets.
          - complete or maximum linkage uses the maximum distances between all observations of the two sets.
          - single uses the minimum of the distances between all observations of the two sets.


sklearn.mixture.GaussianMixture

class sklearn.mixture.GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, 
   n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, ...)

    Gaussian Mixture.

    Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters 
    of a Gaussian mixture distribution.

  Parameters:

    n_components : int, default=1

        The number of mixture components.

    covariance_type : {full, tied, diag, spherical}, default=full

        String describing the type of covariance parameters to use. Must be one of:

            full: each component has its own general covariance matrix.
            tied: all components share the same general covariance matrix.
            diag: each component has its own diagonal covariance matrix.
            spherical: each component has its own single variance.
    
    n_init : int, default=1

        The number of initializations to perform. The best results are kept.

  Attributes:

    weights_ : array-like of shape (n_components,)

        The weights of each mixture components.

    means_ : array-like of shape (n_components, n_features)

        The mean of each mixture component.

    covariances_ : array-like

        The covariance of each mixture component. The shape depends on covariance_type:

            (n_components,)                        if 'spherical',
            (n_features, n_features)               if 'tied',
            (n_components, n_features)             if 'diag',
            (n_components, n_features, n_features) if 'full'

    converged_ : bool

        True when convergence was reached in fit(), False otherwise.

    n_iter_ : int

        Number of step used by the best fit of EM to reach the convergence.

  Methods
    aic(X)
        Akaike information criterion for the current model on the input X.
    bic(X)
        Bayesian information criterion for the current model on the input X.
    score(X[, y])
        Compute the per-sample average log-likelihood of the given data X.

	

Chapter 9 Exercises sklearn class info:


sklearn.datasets.fetch_olivetti_faces

sklearn.datasets.fetch_olivetti_faces(*, data_home=None, shuffle=False, random_state=0, download_if_missing=True, return_X_y=False)[source]

    Load the Olivetti faces data-set from AT&T (classification).

    >>> from sklearn.datasets import fetch_olivetti_faces
    >>> olivetti = fetch_olivetti_faces()
    >>> print(olivetti.DESCR)


sklearn.model_selection.StratifiedShuffleSplit

class sklearn.model_selection.StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)[source]

    Stratified ShuffleSplit cross-validator.

    Provides train/test indices to split data in train/test sets.

    This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. 
    The folds are made by preserving the percentage of samples for each class.

    Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, 
    although this is still very likely for sizeable datasets.


Chapter 9 Exercises:

    -> see exercise_notebooks/09_exercises.ipynb

1. How would define clustering? Can you name a few clustering algorithms?

  -> Clustering is an unsupervised learning technique used for grouping simular instances into clusters. 
  -> Similarity with respect to clustering is task specific. Some clustering algorithms look for instances centered 
     aroucnd a particular point, called a 'centroid'. Others look for continuous regions of densely packed instances: 
     these clusters can take on amy shape. Some algorithms are hierarchical, looking for 'clusters of clusters' 
  -> two popular clustering algorithms: 'k-means' and 'DBSCAN'

  page 260: Clustering: The goal is to group similar instances into clusters. Clustering is a great tool for data analysis, 
  customer segmentation, recommender systesm, search engines, image segmentation, semi-supervised learning, dimensionality 
  reduction, and more.

  page 260: ... You may need a botanist to tell you what species that is, but you certainly don't need an expert to identify 
  groups of similar-looking objects. This is called 'clustering': it is the task of identifying similar instances and assigning 
  them to 'clusters' or groups of similar instances.

  page 260: Just like in classification, each instance get assigned to a group. However, unlike classification, clustering 
  is unsupervised task. ...

  page 262: There is no universal definition of what a cluster is: it really depends on the context, and different 
  alogrithms will capture different kinds of clusters. Some algorithms look for instances centered aroucnd a particular 
  point, called a 'centroid'. Others look for continuous regions of densely packed instances: these clusters can take on 
  any shape. Some algorithms are hierarchical, looking for 'clusters of clusters'. And the list goes no.

  page 262: In this section, we will look at two popular clustering algorithms, 'k-means' and 'DBSCAN', and explore some of 
  their applications, such as nonlinear dimensionality reduction, semi-supervised learning, and anomaly reduction.

  book answer: In Machine Learning, clustering is the unsupervised task of grouping similar instances together. The notion 
  of similarity depends on the task at hand: for example, in some cases two nearby instances will be considered similar, 
  while in others similar instances may be far apart as long as they belong to the same densely packed group. Popular 
  clustering algorithms include K-Means, DBSCAN, agglomerative clustering, BIRCH, Mean-Shift, affinity propagation, and spectral :.

2. What are some of the main applications of clustering algorithms?

  -> Some main applications for clustering algorithm: data analysis, customer segmentation, recommender systesm, search 
     engines, image segmentation, semi-supervised learning, dimensionality reduction, and more

  page 261 - 262: Clustering is used in a wide variety of applications including: Customer segmentation: You can cluster 
  your customer based on their purchase and their activity on your website. ... Data analysis:
  When analyze a new dataset, it can be helpful to run a clustering algorithm, and analyze each cluster separately. 
  Dimensionality Reduction: Once a dataset has be clustered, it is usually possible to measure each instance's 'affinity' 
  with each cluster: 'affinity' is any measure of how well an instance fits intoa cluster. Each instance's feature vector 
  'x' can then be replaced with the vector of its cluster 'affinities'. If there a 'k-clusters', then this vector is 
  k-dimensional. The new vector is typically much lower-dimensional that the original feature vectors, but it can preserve 
  enough information for further processing. Feature Engineering: Cluster 'affinities' can also be usef as 'extra features'
  . ... Anomaly Detection (also called 'outlier detection'): Any instance that has 'low affinity' to all other 'clusters' 
  is likely to be an 'anomaly'. ... Semi-supervised learning: If you only have a few labels, you could perform clustering 
  and propagate the labels to all the instances in the same cluster. ... Search Engines: Some earch engines let your search 
  for images that are similar to a reference image. To build such a system, you would first apply 'clustering algorithm' 
  to all the images in your database: similar images would end up in the same cluster. .... Image Segmentation: By clustering 
  pixels according to their color, then replace each pixel's color with the 'mean color' of its cluster, it is possible to 
  considerably reduce the number of different colors in an images. ...

  book answer: The main applications of clustering algorithms include data analysis, customer segmentation, recommender 
  systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, anomaly detection, 
  and novelty detection.

3. Describe two techniques to select the right number of clusters when using k-means?

  -> a course approach for selecting the right number of clusters is plotting k-means inertia value as a function of 
     the 'k' cluster values. Then, use the 'k' value at the elbow inflexion point where the inertia drops significantly 
     slower as k increase.
  -> A more precise (but also computationaly expensive) approach is to use the 'silhouette score' which is the mean 
     'silhouette coefficient' over all the instances. An instance's 'silhouette coefficient' is equal to (b - a) / max(a,b), 
     where 'a' is the mean distance to the other instances in the same cluster (i.e. the mean intra-cluster distance) 
     and 'b' is the mean nearest-cluster distance (i.e. the mean distance to instances of the next closest cluster). If you 
     plot the silhouette score as a function of 'k' clusters, where the silhouette score peaks is likely near the 
     optimal 'k' cluster value.

  The 'silhouette coefficient' can vary between -1 and +1. A coefficient close to +1 means that the instance is well inside 
  its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary: 
  finally, a coefficient close to -1 means the instanc may have been assigned to the wrong cluster.

  page 265: The K-means algorithm. So, how does it work? ... But you are given neither the labels nore the centroids, so 
  how can you preceed? Start by placing the centroids randomly (e.g. by picking the k instances at random from the the dataset, 
  and using their locations as centroids). Then label the instances, updat4e the centroids, label the instances, update the 
  centroids, and so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps 
  (usually quite small). That's because the mean squared distance between the instances and their closest centroids cna only 
  go down at each step, and since cannot be negative, it's guaranteed to converge.

  page 267: .... It uses a performance metric! That metric is called the model's 'inertia' which is the sum of the squared 
  distances between the isntances and their closest centroids. It is roughly 219.4 for the model on the left in Figure 9-5 
  [Suboptimal solutions due to unlucky centroid initializations] , 258.6 for the model on the right in Figure 9-5, and 
  only 211.6 for the model in Figure 9-3 [K-means decision boundaries (Voronoi tessellation)]. The 'KMeans' runs the alogorithm 
  'n_init' times, and keeps the model with the lowest inertia. In this example, the Figure 9-3 will be selected (unless we 
  are very unlucky with 'n_init' consecutive initializations). If you are curious, model's inertia is accessible via the 
  'inertia_' instance variable.

  page 269: Finding the optimal number of clusters ... The 'inertia' is not a good performance metric when trying to choose 'k' 
  because it keeps getting lower as we increase 'k'. Indeed, the more clusters there are, the closer each instance will be 
  to its closest centroid, and therefore the low the inertia will be. Let's plot the inertia as a function of 'k'. When we do 
  this, the curve often contains an inflexion point cale the 'elbow' (see figure 9-8 [Plotting the inertia as a function of 
  number of clusters 'k'])

  page 270: As you can see, the inertia drops very quickly as we increase 'k' up to 4, but then it decreases much more 
  slowly as we keep increasing 'k'. This curve has roughly the shape of the arm, and there is an elbow at k = 4. So if we 
  did not know better, we migh thing 4 was a good choice: any lower value would be dramatic, whil any higher value would not 
  help much, and we might just be splitting perfectly goo clusters in half for no good reason.

  page 270: This technique for choosing the best value for the number of clusters is rather course. A more precise (but also 
  computationaly expensive) approach is to use the 'silhouette score' which is the mean 'silhouette coefficient' over all the 
  instances. An instance's 'silhouette coefficient' is equal to (b - a) / max(a,b), where 'a' is the mean distance to the other 
  instances in the same cluster (i.e. the mean intra-cluster distance) and 'b' is the mean nearest-cluster distance (i.e. the 
  mean distance to instances of the next closest cluster). The 'silhouette coefficient' can vary between -1 and +1. A coefficient 
  close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 
  means that it is close to a cluster boundary: finally, a coefficient close to -1 means the instanc may have been assigned to 
  the wrong cluster.

  book answer: The elbow rule is a simple technique to select the number of clusters when using K-Means: just plot the inertia 
  (the mean squared distance from each instance to its nearest centroid) as a function of the number of clusters, and find the 
  point in the curve where the inertia stops dropping fast (the "elbow"). This is generally close to the optimal number of 
  clusters. Another approach is to plot the silhouette score as a function of the number of clusters. There will often be a peak, 
  and the optimal number of clusters is generally nearby. The silhouette score is the mean silhouette coefficient over all 
  instances. This coefficient varies from +1 for instances that are well inside their cluster and far from other clusters, 
  to -1 for instances that are very close to another cluster. You may also plot the silhouette diagrams and perform a more 
  thorough analysis.

4. What is label propagation? Why whould you implement it, and how?

  -> Label propagation is identifying similar instances to the label instances and propagating labels from the labeled 
     instances to the similar instances. 
  -> Label propagation is implemented because labeling instances is expensive, time consuming, and often you have few labeled 
     instances 
  -> One label propagation approach is to use k-means or another clustering algorithm to identify clusters containing similar 
     instances and propagating the label from a labeled instance to some or all of the instances in the cluster containing 
     the labeled instance.

  page 275: Using Clustering for Semi-Supervised Learning. Another use case for clustering is in semi-supervised learning, 
  when we have plenty of unlabeled instances and very few labeled instances. In this section, we'll use the 'digit dataset' 
  which is a simple MNIST-like dataset containing 1797 grayscale 8 x 8 images representing th digits 0 to 9. ...

  page 276: The model's accuracy is just 74.8%. ... Let's see how we can do better. First, let's cluster the training set 
  into 50 clusters. Then, for each cluster, we'll find the image closest to the centroid. We call these images the 
  'representative images': ...

  Let's look at each image and manually label them. ... Now, we have a dataset with 50 labeled instances, but instead of 
  being random instances, each of them is a 'representative image' of its cluster.

            y_representative_digits = np.array([ 1, 3, 6, 0, 7, 9, 2, 4, 8, 9, 5, 4, 7, 1, 2, 6, 1, 2, 5, 1, 4, 1, 3, 3, 8, 
                8, 2, 5, 6, 9, 1, 4, 0, 6, 8, 3, 4, 6, 7, 2, 4, 1, 0, 7, 5, 1, 9, 9, 3, 7 ])

  Now we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a 
  representative image of its cluster. Let's see if the performance is any better:

  Wow! We jumped from 74.8% accuracy to 84.9%, although we are still only training the model on 50 instances. Since it's 
  often costly and painful to label instances, especially when it has to be done manually by experts, it's a good idea to 
  make them label representative instances rather than just random instances.

  page 277: But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same 
  cluster? This is called 'label propagation'

  page 277: Nice! With just 50 labeled instances (only 5 examples per class on average!) we go 90.9% accuracy, which is actually 
  higher thatn the performance we goo on the fully labeled digits dataset (90.7%). This is partly thanks to the fact that we 
  dropped some outliers, and partly because the propagated labels are actually pretty good - their accura y is 97.5% as the 
  following code shows: ...

  book answer: Labeling a dataset is costly and time-consuming. Therefore, it is common to have plenty of unlabeled instances, 
  but few labeled instances. Label propagation is a technique that consists in copying some (or all) of the labels from the 
  labeled instances to similar unlabeled instances. This can greatly extend the number of labeled instances, and thereby allow 
  a supervised algorithm to reach better performance (this is a form of semi-supervised learning). One approach is to use a 
  clustering algorithm such as K-Means on all the instances, then for each cluster find the most common label or the label of 
  the most representative instance (i.e., the one closest to the centroid) and propagate it to the unlabeled instances in the 
  same cluster.

5. Can you name two cluster algorithms that can scale to large datasets? And two that look for regions of high density?

  -> K-means and BIRCH scale well large datasets. -> DBSCAN and Mean-shift look for regions of high density.

  page 265: The computational complexity of the [k-means] algorithm is generally linear with regards to the number instances 
  'm', the number of clusters 'k', and the number of dimensions 'n'. However, this is only true when the data has a clustering 
  structure. If it does not, then in the worst-case scenario the complexity can increase exponentially with the number of 
  instances. IN practice, this is rarely happens, and k-means is generally one of the fastest algorithms.

  page 279: DBSCAN. The density-based spatial clustering of applications with noise (DBSCAN) algorithm defines clusters as 
  continuous regions of high density. Here is how it works:

   - For each instance,, the algorithm counts how many instances are located within a small distance 'epsilon' from it. The 
     region is called the instance's epsilon-neighborhood.
   - If an instance has at least 'min_samples' instances in its epsilon-neighborhood (including itself), then it is consider 
     a 'core instance'. In other workds, 'core instances' are those that are located in dense regions.
   - All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other 
     'core instances'; therefore, a long sequence of neighboring 'core instances' form a single cluster.
   - Any instance that is not a 'core instance' and does not have one in its neighborhood is considered an anomaly.

  The algorithm works well if all clusters are well separated by low-density regions. ...

  page 281: In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. 
  It is robust to outliers, and it has just two hyperparameters (eps and min_samples). If the density varies significantly 
  across the clusters, however, or if there's no sufficiently low-density region around some clusters, DBSCAN can struggle 
  to capture all the clusters properly. Moreover, its computational complexity is roughly O(m**2 x n), so it does not scale well 
  to large datasets.

  page 282: BIRCH. The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm was designe specifically 
  for very large datasets, and it can be faster thatn batch k-means with similar results, as long as the number of features is 
  not too large (<20). During training, it builds a treee structure conntaining just enough information to quickly assign each 
  new instance to a cluster, witout have to store all the instances in the tree: this approach allows it to use limited memory 
  while handling huge datasets.

  page 282: Mean-shift. This algorithm starts by placing a circle centered on each instance; then for each circle it computes the 
  mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this 
  mean-shifting step until all the circles stop moving. ... Mean-Shift shifts the circles in the direction of higher density, until 
  each of the has found a local density maximum. Finally, all the instances whose circles have settled in the same (or close enough) 
  are assigned to the same cluster. Mean-shift has some of the same features as DBSCAN, like how it can find any numnber of clusters 
  of any shape, it has very few hyperparameters (just one - the radius of the circles, called the 'bandwidth'), and it relieas on 
  local density estimatioins. But unlike DBSCAN, mean-shift tends to chop clusters into pieces when the have internal density 
  variations. Unfortunately, its computational complexity is O(m**2 x n), so it is not suited for large datasets.

  book answer: K-Means and BIRCH scale well to large datasets. DBSCAN and Mean-Shift look for regions of high density.

6. Can you think of a use case where active learning would be useful? How would you implement it?

  -> Active learning is useful when you have lots of unlabeled instances and labeling the instances is costly, time-consuming, etc. 
  -> One active learning strategy is called 'uncertainity sampling'. In this iterative strategy, the model is trained on the 
     currently labeled data and then makes predictions on the unlabeled data. The expert focuses on labeling the instances with 
     the lowest probability predictions.

  page 278: Active Learning. To continue improving your model and your training set, the next step could be to do a few rounds 
  of 'active learning', which is when human experts interacts with the learning algorithm, providing labels for specific 
  instances when the algorithm requests them. There are many different strategies for 'active learning', but one of the more common 
  ones is called 'uncertainity sampling'. Here is how it works

   1. The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the 
      unlabeled instances.
   2. The instances for which the model is most uncertain (i.e. where it estimated probability is lowest) are give to the 
      expert for labeling.
   3. You iterate this process until the performance improvement stops being worth the labeling effort. Ohter active learning 
     strategies include labeling the instance that would result in the largest model change or the largest drop in the model's 
     validation error, or the instances that different models disagree on (e.g. SVM and a random forest).

  book answer: Active learning is useful whenever you have plenty of unlabeled instances but labeling is costly. In this case 
  (which is very common), rather than randomly selecting instances to label, it is often preferable to perform active learning, 
  where human experts interact with the learning algorithm, providing labels for specific instances when the algorithm requests 
  them. A common approach is uncertainty sampling (see the Active Learning section in chapter 9).

7. What is the difference between anomaly detection and novelty detection?

  -> With 'anomaly detection', the algorithm is trained on a dataset that contains outliers, and the objective is to detect 
     'outliers' in the training dataset and new data. -> With 'novelty detection', the algorithm is trained on a presumed 
     'clean dataset', and the objective is to detect 'novelties' among the new instances.

  page 288: Using Gaussian Mixtures for Anomaly Detection. Using a Gaussian mixture model for anomaly detection is quite simple: 
  any instance localed in a low-density region can be considered an 'anomaly'. You must define what density threshold you want 
  to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually 
  well known. Say it is equal to 2%. You then set the density threshold to be the value that results in having 2% of the instances 
  located in areas below the threshold density. ...

  page 288: A closely related task is 'novelty detection': it differs from 'anomaly detection' in that the algorithm is assumed to 
  be trained on a 'clean dataset', uncontaminateed by outliers, whereas 'anomaly detection' does not make this assumption. Indeed, 
  outlier detection is of used to clean up a dataset.

  book answer: Many people use the terms anomaly detection and novelty detection interchangeably, but they are not exactly the 
  same. In anomaly detection, the algorithm is trained on a dataset that may contain outliers, and the goal is typically to 
  identify these outliers (within the training set), as well as outliers among new instances. In novelty detection, the algorithm 
  is trained on a dataset that is presumed to be "clean," and the objective is to detect novelties strictly among new instances. 
  Some algorithms work best for anomaly detection (e.g., Isolation Forest), while others are better suited for novelty detection 
  (e.g., one-class SVM).

8. What is a Gaussian mixture? What tasks can you use if for?

  -> A Gaussian mixture model (GMM) is probabilistic model that assumes that instances were generated from a mixture of several 
     Gaussian distributions whose parameters is unknown. The assumption is that the data is grouped in 'k' ellipsoidal shape 
     clusters with varying shapes, sizes, and density. -> Gaussian Mixture models can be used for density estimations, clustering, 
     and anomaly detection.

  page 283: New let's dive into Gaussian mixture models, which can be used for density estimations, clustering, and anomaly 
  detection.

  page 283: Gaussian Mixtures. A Gaussian mixture model (GMM) is probabilistic model that assumes that instances were generated 
  from a mixture of several Gaussian distributions whose parameters is unknown. All the instances generated from a single Gaussian 
  distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, 
  density, and orientation just like in Figure 9-11 [K-means fails to cluster these ellipsoidal blobs properly]. When you observe 
  an instance, you know it was generated from one of the Guassian distributions, but you are not told which one, and you do not know 
  what the parameters of these distributions are.

  There are several GMM variants. In the simplest variant, implemented in the GaussianMixture class, you must know in advance the 
  number of 'k' of Gaussian distribution. The dataset 'X' is assumed to have been generated through the following probabilistic process.

  book answer: A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a 
  mixture of several Gaussian distributions whose parameters are unknown. In other words, the assumption is that the data is grouped 
  into a finite number of clusters, each with an ellipsoidal shape (but the clusters may have different ellipsoidal shapes, sizes, 
  orientations, and densities), and we don't know which cluster each instance belongs to. This model is useful for density 
  estimation, clustering, and anomaly detection.

9. Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?

  -> Plot AIC and BIC values a function of the number 'k' clusters. Select the number of 'k' clusters where AIC or BIC have 
     the lowest value.

  page 289: Selecting the number of [Gaussian Mixture] Clusters. With k-means, you can use 'inertia' or 'silhouette score' to 
  select the appropriate number of clusters. But with Gaussian mixtures, it is not possible to use these metrics because they are 
  not reliable when clustering is not spherical or have different sizes. Instead, you can try to find the model that minimizes a 
  theoritical information criterion such as the 'Bayesian Information Criterion' (BIC) or the 'Akaike Information Criterion' (AIC) 
  defined in equation 9-1.

  Equation 9-1. 'Bayesian Information Criterion' (BIC) and 'Akaike Information Criterion' (AIC) 

     BIC = log(m)p - 2log(likelihood_fcn) 

     AIC = 2p - 2log(likelihood_fcn) 

     In these equations: - 'm' is the number of instances - 'p' is the number of parameters learned by the model - 
     'likelihood_fcn' is the maximized value of the 'likelihood function' of the model

  page 290: Both BIC and AIC penalize model that have more parameters to learn (e.g. more clusters) and reward models that fit 
  the data well. They often end up selecting the same model. When they differ, the model selected by the 'BIC' tends to be 
  simplier (fewer parameters) than the one selected by 'AIC', but tends to not fit the data quite as well (this is especially 
  true for larger datasets).

  page 290: Likelihood Function. The term 'probability' and 'likelihood' are often used interchangeable in everyday language, 
  but they have very different meanings in statistics. Given a statistical model with some parameter 'theta', the word 'probability' 
  is used to describe how plausible future outcome 'x' is (knowing the parameter value of 'theta'), while the word 'likelihood' is 
  used to describe how plausible a particular set of parameter values 'theta' are, after the outcome 'x' is known.

  page 291: Figure 9-20 [AIC and BIC for different number of clusters 'k']. As you can see, buth the BIC and AIC are lowest 
  when 'k = 3', so it is most likely the best choice.

  book answer: One way to find the right number of clusters when using a Gaussian mixture model is to plot the Bayesian information 
  criterion (BIC) or the Akaike information criterion (AIC) as a function of the number of clusters, then choose the number of 
  clusters that minimizes the BIC or AIC. Another technique is to use a Bayesian Gaussian mixture model, which automatically selects 
  the number of clusters.

10. Cluster the Olivetti Faces Dataset: 
    The classic Olivetti faces dataset contains 400 grayscale 64 x 64 pixel images of faces. Each image is flattened 1D vector 
    of size 4096. Forty different people were photographed (10 times each), and the usual task is to train a model that can predict 
    which person is represented in each picture. Load the dataset using the skelearn.dataset.fetch.olivetti_faces() function, the 
    split it into a training set, a validation set, and test set (note that the dataset is already scaled between 0 and 1). Since 
    the dataset is quite small, you will probably want to use stratified sampling to ensure that there are same number of images per 
    person in each set. Next, cluster the images using k-means, and ensure that hour have a good number of clusters (using one of the 
    technique discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?

    -> see exercise_notebooks/09_exercises.ipynb

11. Using Clustering as Preprocessing for Classification: 
    Continue with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate 
    it on the validation set. Next, use k-means as a dimensionality reduction tool, and train a classifier on the reduced set. Search 
    for the number clusters that allows the classifiere to get the best performance: what performance can you reach? What if you append 
    the features from the reduced set to the original features (again, searching for the best number of clusters)?

    -> see exercise_notebooks/09_exercises.ipynb

12. Train a Gaussian mixture model on the Olivetti faces dataset. To sped up the algorithm, you should proabably reduce the dataset's 
    dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the sample() method), 
    and visualize them (if you used PCA, you will need to use its inverse_transform() method). Try to modify some images (e.g. rotate, 
    flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal 
    images and for anomalies).

    -> see exercise_notebooks/09_exercises.ipynb

13. Using Dimensionality Reduction Techniques for Anomaly Detection: 
    Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset 
    and reduce with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of 
    the modified images you built in the previous exercise and look at their reconstruction error: notice how much larger it is. 
    If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face.

    -> see exercise_notebooks/09_exercises.ipynb



------------------------------------------------------
Chapter 10 Introduction to Artificial Neural Newtorks with Keras
------------------------------------------------------

From Biological to Artifical Neurons (pages 300 - 317)

    Artifical Neural Networks (ANNs)
      - inspired by brain networks, but now are very different
      - ANNs are at the core of deep learning

    Tensorflow's Keras API:
      - simple high-level API for building, training, and evaluating, and running neural networks

  Logical Computations with Neuron (pages 303 - 304)

    artificial neurons
      - it has one or more binary (on/off) inputs and one binary output
      - activate it soutput when more than a certain number of its inputs are active

    Artifical Neural Networks (ANNs) logical computations
      - include Identity (output = input), Logical AND (A ^ B), logical OR ( A v B),  
        logical OR with NOT (A v -B)
    
  The Perceptron (pages 304 - 309)

   Perceptron
     - based on threshold logic unit (TLU) (also called linear threshold unit (LTU))
     - The TLU computes a linear of its inputs: 
         z = w1*x1 + w2*x2 + ... + wn*xn + b
     - then it applies a step function to the result:
           hw(x) = step(z)
     - almost like logistic regression, except it uses a 'step function' instead of a 'logistic function'
     - a perceptron is composed of one or more TLUs organized in a single layer, whern every TLU is
       connected to every input
     -  perceptron with multiple TLUs can perform multilabel classification (where instances can be associated
        with multiple labels) and be a multiclass classification (assign one class per instance)
     - When the Perceptron finds a decision boundary that properly separates the classes, it stops learning. This means 
       that the decision boundary is often quite close to one class:

    TLU (Threshold Logic Unit) (also called linear threshold unit (LTU))
      - an artifical neuron that computes the weighted sum of the inputs  WT*X (W-Transpose) plus bias
        term, b, and applies a step function
     - a single TLU can be used for simple linear binary classification
     - equation 10-2 can be used to efficiently comput the outputs of a layer of artifical neurons for several
       instances at once


    Equation 10-1. Common step functions used in 'perceptron' (assuming threshold = 0)

                        /                             / -1 if z < 0
         heavyside(z) = | 0 if z < 0         sgn(z) = |  0 if z = 0
                        | 1 if z >= 0                 \  1 if z > 0
                        \

    fully connected layer (or dense layer): every input is connected to every TLU

    Equation 10-2. Computing the outputs of a fully connected layer

      hw,b(X) = phi(XW + b)

      where:
        X: matrix input features
        W: weight matrix with one row per input, and one column per neuron
        b: bias vector - contains all the bias terms, one per neuron
        phi: 'activation function': when the artificial neurons are TLUs, it is a step function
   
   Sum of matrix and vector:
     - in mathematics, the sum of a matrix and vector is undefined
     - in data science, the sum of a matrix and vector is called 'broadcasting' and means adding a vector to each
       row in the matrix

   Perceptron training
    - perceptron is fed one training instance at a time, and for each instance it makes its prediction
    - for every output that produced a wrong prediction, it reinforces the connection weights from the inputs that
      would have made a correct prediction
    - see equation 10-3.

    Equation 10-3. Perceptron learning rule (weight updates)

        wi,j(next step) =  wi,j + eta(yj + pred-yj)xi

        where: 
            wi,j:     connection weight between the ith input and jth neuron
            xi:       the ith input value of the current training instance
            pred-yj:  output of the jth output neuron for the current training instance
            yj:       the target output of the jth output neuron for the current training instance
            eta:      learning rate

    perceptron convergence theorem (page 307):
      - the 'decision boundary' of each output neuron is linear, so the perceptron are incapable of learning 
        complex patterns (just like logistic regression classifiers).
      - if the instances are 'linear separable', this algorithm would converge to a solution


    Code: Using Perceptron class to predict Iris setosa or NOT Iris Setoas

        >>> import numpy as np
        >>> from sklearn.datasets import load_iris
        >>> from sklearn.linear_model import Perceptron
        >>> 
        >>> iris = load_iris(as_frame=True)
        >>> X = iris.data[["petal length (cm)", "petal width (cm)"]].values
        >>> y = (iris.target == 0)  # Iris setosa
        >>> 
        >>> per_clf = Perceptron(random_state=42)
        >>> per_clf.fit(X, y)
        >>> 
        >>> X_new = [[2, 0.5], [3, 1]]
        >>> y_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers
        >>> y_pred
            array([ True, False])

    Perceptron and SGClassifier (Stochastic Gradient Descent):
       - The Sklearn Perceptron class is equivalent to a SGDClassifier with loss="perceptron", no regularization, and a constant 
         learning rate equal to 1:

    Code: Show how to build Perceptron using SGCLassifier to predict Iris setosa or NOT Iris Setoas
    
        >>> # extra code - shows how to build and train a Perceptron
        >>> 
        >>> from sklearn.linear_model import SGDClassifier
        >>> 
        >>> sgd_clf = SGDClassifier(loss="perceptron", penalty=None,
        >>>                         learning_rate="constant", eta0=1, random_state=42)
        >>> sgd_clf.fit(X, y)
        >>> assert (sgd_clf.coef_ == per_clf.coef_).all()
        >>> assert (sgd_clf.intercept_ == per_clf.intercept_).all()       


     Multilayer Perceptron
       - have input layer, TLUs hidden layer (in the middle), and TLUs output layer
       - solves some single layer Perceptron issues such as handline XOR

     Perceptron vs Logistic regression classifers (page 308)
       - perceptron do not output a class probability while Logisitic regression does
       - perceptron do not use any regularization by default
       - stop training as soon as there are no more prediction errors on the training set, so the model typically 
         does not generalize as well as logistic regression or a linear SVM classifier
       - perceptron may train a bit faster

  The multilayer Perceptron and Backpropagation (pages 309 - 313)

     Multilayer Perceptron
       - composed of: one input layer, one or more layers of TLUs called 'hidden layers', and one final layer
         of TLUs called the output layer
         - lower layer: (hidden layers) layer close to the input layers
         - upper layer: (hidden layers) layer close to the ouputs 
       - signals flow only in one direction (from inputs to the outputs) - called Feedforward Neural Network (FNN)

    Deep Neural networks (DNN)
      - when an ANN contains a deep stack of hidden layers
      - the field of deep learning studies DNNs

    Reverse-Model automatic differentiation (or reverse-mode autodiff)
      - in just two passes through the network (one forward, one backward), it is able to compute the gradients
        of the neural network's error with regard to every single parameter
      - in other words, it can find out how each connection weight and each bias should be tweaked in order to
        reduce the neural network's error
      - these gradients can be used to perform a gradient descent step
      - well suited when the function to differentiate has many variables (e.g. connection weights and biases)
        and few outputs (e.g. one loss)

    Backpropagation (or backprop): 
      - combines autodiff and gradient descent 
      - most popular training technique for neural networks
      - how it works:
         - handles one mini-batch at a time (for example, containing 32 instances each)
            - it goes through the full training set multiple times
            - each pass is called an 'epoch'
         - each mini-batch enters the network through the input layer
            - it computes the output for all the neurons on the 1st hidden layer, for every instance in the mini-batch
            - the results is passed on to the next layer, its outputs calculated, and passed on to the next layer,
              and so on until we get the output of the layer, the output layer
            - this is called 'forward pass'
         - algorithm measures the networks' output error (i.e. it uses a loss function that compares the desired
           output and the actual output, and returns a some measure of the error)
         - computes how much each output bias and each connection to output layer contributed to the error
           - done analytically by apply the [differential calculus] chain rule
         - reverse pass 
           - measure how of these error contributions came from the connection in the layer below, again using
             the 'chain rule', work backwards, until it reaches the input layer
           - this reverse pass efficiently measures the error gradient across all the connections weights and 
             biases by propagating the error gradient backwardk through the network
         - gradient descent step
           - performs a gradient descent step to tweak all the connections weights in the network using the
             error gradient it just computed

    Backpropagation Random initialization
      - it is important to initialize all hidden layers' connection weights randomly, or else the training
        will fail
        - For example, if you initialize all the weights and biases to zero, then all neuron in a give layer will be 
          perfectly identical, and thus backpropagation will affect them in exactly the same way, so they remain identical. 
        - In other words, despite have hundreds of neurons per layer, you model will act as if it had only one neuron per 
          layer: it won't be too smart. 
        - if instead you randomly initialize the weights, you break the 'synmmetry' and allow backpropagation
        to train a diverse team of neurons

    Backpropagation summary:
      - makes predictions for mini-batch (forward pass), 
      - measures the error, 
      - then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), 
      - and finally in the gradient descent step, tweaks the connection weights and bias to reduce the error

    Backpropagation activation function:
      - replaced the step function with the logistic function (sigmoid function)
      - step function only has flat segments, so there is no gradient to work with (gradient descent cannot work
        on a flat surface

      - sigmoid function sigma(z) = 1 / (1 + exp(-z))   =  exp(z)  / ( 1 + exp(z) )  
          where z: score
                exp:  e exponential function

          where: sigma(z) -> 0 as  z -> -infinity
                 sigma(z) -> 1 as  z ->  infinity

        Note: e = 2.718281828459 ....
        y = e**x and y = log_e(x) graph:
        http://amsi.org.au/ESA_Senior_Years/SeniorTopic3/3h/3h_2content_7.html
          -> for y = e**x    shows: y -> infinity  as x -> infinity   BUT y increases extremely slow after ln(e) which equals '1'
                                    y -> 0         as x -> 0  

          -> for y = log_e(x) shows: y -> 1  as x -> infinity  
                                     y -> 0  as x -> -infinity  

       https://www.pinecone.io/learn/softmax-activation/
       Sigmoid function:
         - The sigmoid function takes in any real number as the input and maps it to a number between 0 and 1. This is 
            exactly why its well-suited for binary classification
       Softmax function:
         - you can think of the Sigmoidsoftmax function as a vector generalization of the sigmoid activation
         - The softmax activation function takes in a vector of raw outputs of the neural network and returns a vector of probability scores.
       Argmax function:
         https://machinelearningmastery.com/argmax-in-machine-learning/
         - returns the index of the maximum value in the input array.
         - One limitation with using the argmax function is that its gradients with respect to the raw outputs of the neural 
           networks are always zero.  As a result, use cannot use the argmax function in training. Unless theres backpropagation 
           of gradients, the parameters of the neural network cannot be adjusted, and theres effectively no learning!

       code: Sigmoid function graph

         >>> import numpy as np
         >>> import seaborn as sns
         >>> 
         >>> def sigmoid(x):
         >>>   exp_x = np.exp(x)
         >>>   return np.divide(exp_x,(1 + exp_x))
         >>>   
         >>> x = np.linspace(-10,10,num=200)
         >>> sigmoid_arr = sigmoid(x)
         >>> 
         >>> sns.set_theme()
         >>> sns.lineplot(x = x,y = sigmoid_arr).set(title='Sigmoid Function')

    Backpropagation algorithm works with other activation function:
      - not works with sigmoid fuction, it also works with:
        The hyperbolic tangent function: tanh(z) = 2 * sigma * (2z) - 1
           - S-shaped continuous and differentiable, but its outpu value range is from -1 to 1 (instead of 0 to 1)
        The rectified linear unit function: ReLU(z) = max(0,z)
           - the ReLU function is continuous but unfortunately not differiable at z = 0
           - in practice, however, it works very well and has the advantage of being fast to compute, so
             it has be the default

     Why are activation functions needed?
       - if you chain several linear transformations, all you get is a linear transformation
       - if you don't have some nonlinearity between layers, then even deep stack of layers is equivalent to
         to a single layer, and you can't solve complex problems
       - conversely, a large enough DNN with nonlinear activation can theoretically approximate any
         continuous function

  Regression MLPs (MultiLayer Perceptrons) (pages 313 - 315)

    Regression MLPs:
      - MLPs can be used for regression task
      - you a single neuron to predict a single value
      - for multivariate regressions (i.e. to predict multiple values at once), you need one neuron per
        output dimension (center of 2D image - needs to neurons to predict the 2D coordinate

    fetch_california_housing function
      - sklearn simplified calfornia housing dataset containing only numeric values (i.e. no ocean_proximity_feature)
        and no missing values
      - This dataset was derived from the 1990 U.S. census
      - The target variable is the median house value for California districts, expressed in hundreds of thousands 
        of dollars ($100,000).
   
    sklearn MLPRegressor
      - Multi-layer Perceptron regressor.
      - This model optimizes the squared error using LBFGS, stochastic gradient descent, or 'adam' SGD option.
      - 'adam' is variant of gradient descent
      - when trained using gradient descent, features MUST be scaled because gradient descent does not converge
        well when features have different scales
      - includes activation function for hidden layers, but does NOT include output activation function
      - requires just a few lines of code so it is very convenient, but neural net features are limited
        (i.e. no output activation function) so book switches to Kera later in chapter
      - uses means squared error output loss function which is usually what you want for regressions

    Code: Train MLPRegressor to predict the housing values based on fetch_california_housing dataset
          Notes: MLPRegressor to use 3 hidden layers composed of 50 neurons each
                 MLPRegressor to use default activation='relu' (activation function for hidden layers)
                 MLPRegressor to use default solver='adam' (the solver for weight optimization)

        >>> from sklearn.datasets import fetch_california_housing
        >>> from sklearn.metrics import mean_squared_error
        >>> from sklearn.model_selection import train_test_split
        >>> from sklearn.neural_network import MLPRegressor
        >>> from sklearn.pipeline import make_pipeline
        >>> from sklearn.preprocessing import StandardScaler
        >>> 
        >>> housing = fetch_california_housing()
        >>> X_train_full, X_test, y_train_full, y_test = train_test_split(
        >>>     housing.data, housing.target, random_state=42)
        >>> X_train, X_valid, y_train, y_valid = train_test_split(
        >>>     X_train_full, y_train_full, random_state=42)
        >>> 
        >>> mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)
        >>> pipeline = make_pipeline(StandardScaler(), mlp_reg)
        >>> pipeline.fit(X_train, y_train)
        >>> y_pred = pipeline.predict(X_valid)
        >>> rmse = mean_squared_error(y_valid, y_pred, squared=False) 
        >>> rmse
            0.5053326657968978
            # found a RMSE 0.505 which is comparable to what you would get with the random forest classifier

      Measuring the output error (loss function);
        Mean Square Error (MSE)
         -  means squared error which is usually what you want for regressions
        Mean Absolute Error (MAE)
         - may prefer to use if you have a lot of outliers 
        Huber loss
          - combination of MSE and MAE
          - it is quadratic when the error is smaller than the threshold 'delta' (typically 1), but linear
            when the error is greater than 'delta'
          - the linear part makes it less sensitive to outliers than the MSE, and the quadratic part 
            allows it to converge  faster and more precise tha the MAE

    Output Activation functions
      - MLPRegressor does not have any output activation function
        ReLU activation function
          equation: The rectified linear unit function: ReLU(z) = max(0,z)
          - use if you want an guarantee an output is always positive
        softplus activation function
           equation: softplus(z) = log(1 + exp(z))
           - close to zero when z is negative
           - close to z when z is positive
        sigmoid function or hyperbolic tangent function
          - if you want to guarantee that the predictions will always fall within a given range of values,
            scale the targets to the appropriate range 0 to 1 for sigmoid and -1 to 1 for tanh 

   
   Table 10-1. Typical Regression MLP Architecture

     Hyperparameter                  Typical value
     ---------------                 -------------------------------
     # hidden layers                 Depends on problem, but typically 1 to 5
     # neurons per hidden layer      Depends on problem, but typically 10 to 100
     # output neurons                1 per prediction dimension
     hidden activation               ReLU
     output activation               None, or ReLU/softmax (if positive outputs) or sigmoid/tanh (if bounded outputs)
     Loss function                   MSE or Huber if outliers

  Classification MLPs (pages 315 - 316)
    
    Classification MLPs
      - MLPs can be used for classification
      - for binary classification, you need a single output neuron using a sigmoid activation function:
        the output will be between 0 and 1 which you can interpret as the estimated probability of the
        positive class and the negative class estimated probability is equal to 1 minus this number
      - for multilabel classification, you would dedicate one output neuron for each positive class 
        using a sigmoid activation functions
      - for multiclass classification (each instance belongs to only one class), you need one neuron 
        output neuron per class using the softmax activation function
          - the softmax function will ensure that all the estimated probabilities are between 0 and 1
            and they will add up to 1
      - for loss function, since we are predicting probability distributions, the 'cross-entropy loss 
        (or x-entryp or log loss) is generally a good choice
    
    sklearn MLPClassifier
      - almost identical to MLPRegressor, except it minimizes the cross entropy rather than MSE
      - 

    Table 10-2. Typical classification MLP Architecture

     Hyperparameter         Binary Classification    Multilabel Binary Classification   Multiclass Classification
     ---------------        ----------------------   ---------------------------------  -------------------------
     # hidden layers        typically 1 to 5 depending on the task
     # output neurons       1                         1 per binary label                 1 per class
     output activation      Sigmoid                   Sigmoid                            Softmax
     Loss function          X-entropy                 X-entropy                          X-entropy

    Code: Train MLPClassifier to predict the iris type based on sklearn load_iris dataset
          Notes: MLPClassifier to use 1 hidden layer composed of 5 neurons 
                 MLPClassifier to use default activation='relu' (activation function for hidden layers)
                 MLPClassifier to use default solver='adam' (the solver for weight optimization)
   
        >>> from sklearn.datasets import load_iris
        >>> from sklearn.model_selection import train_test_split
        >>> from sklearn.neural_network import MLPClassifier
        >>> 
        >>> iris = load_iris()
        >>> X_train_full, X_test, y_train_full, y_test = train_test_split(
        >>>     iris.data, iris.target, test_size=0.1, random_state=42)
        >>> X_train, X_valid, y_train, y_valid = train_test_split(
        >>>     X_train_full, y_train_full, test_size=0.1, random_state=42)
        >>> 
        >>> mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000, random_state=42)
        >>> pipeline = make_pipeline(StandardScaler(), mlp_clf)
        >>> pipeline.fit(X_train, y_train)
        >>> accuracy = pipeline.score(X_valid, y_valid)
        >>> accuracy
            1.0

Implementing MLPs with Keras (pages 317 - 344)

    Kera
      - TensorFlow's high-level deep learning API
      - it allows you to build, train, evaluate, and execute all sorts of neural networks
      - Keras is tensorFlow only
      - Keras was officially chosen as tensorFlows preferred API when tensorflow 2 came out

  Building Image Classifiers Using the Sequential API (pages 318 - 328)

    Using Keras to load the dataset (pages 318 - 319)

      Kera Fashion MMIST dataset
        - https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data
        - 60000 grayscale images of 28x28 pixels, with 10 fashion classes, test set of 10000 images
        - every image is represented by 28x28 array (rather than MNIST numbers using 1D array size 784)
        - pixel intensity represented as integers (from 0 to 255) rather floats (from 0.0 to 255.0)


     Code: Use tensorflow Keras to Load Fashion MNIST

        >>> import tensorflow as tf
        >>> 
        >>> fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
        >>> (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
        >>> X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
        >>> X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]     
        >>> X_train.shape
            (55000, 28, 28)
        >>> X_train.dtype
            dtype('uint8')
        # scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255
        >>> X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.
        >>> y_train
            array([9, 0, 0, ..., 9, 0, 2], dtype=uint8)
            # Here are the corresponding class names:
        >>> class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
        >>>    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
        >>> class_names[y_train[0]]
            'Ankle boot'


    Creating the model using Sequential API (pages 319 - 323)

     Sequential API
       - simplest kind of Keras model fro neural networks that just composed of a single stack of
         layers connected sequestially

     Creating Sequential API steps in below code;
       - set tensorflow random seed. 
           - You could used 'tf.keras.utils.set_random_seed() to set random seedds for Tensorflow,
             Python (random.seed()), and Numpy (np.random.seed())
       - create Sequential model
       - build first layer, the input layer, and added to the model
           - Keras needs to know the input shape so it can determine the shape of the connection
             weight matrix of the first hidden layer
       - add a flatten layer, and added to the model
           - its role is to convert each input image into a 1D array. It does not have any parameter
       - add a dense hidden layer with 300 neurons and ReLU activation function, and added to the model
           - each dense layer manages its own weight matrix, containing all the connection weights between
             the neurons and their inputs. It also manages a vector or bias terms (one per neuron)
       - add a 2nd dense hidden layer with 100 neurons and ReLU activation function, and added to the model
       - add dense output layer with 10 neurons (one per class) and softmax activation function, and added 
         to the model
           - using softmax activation function because the classes are exclusive

     Code: Use tensorflow Keras Sequential API for the fashion MNIST dataset - passing one layer at a time

        >>> tf.random.set_seed(42)
        >>> model = tf.keras.Sequential()
        >>> model.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))
        >>> model.add(tf.keras.layers.Flatten())
        >>> model.add(tf.keras.layers.Dense(300, activation="relu"))
        >>> model.add(tf.keras.layers.Dense(100, activation="relu"))
        >>> model.add(tf.keras.layers.Dense(10, activation="softmax"))

     Code: Use tensorflow Keras Sequential API for the fashion MNIST dataset - passing a list of layers when 
           creating the Sequential mode

        >>> # extra code - clear the session to reset the name counters
        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>> 
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     tf.keras.layers.Dense(300, activation="relu"),
        >>>     tf.keras.layers.Dense(100, activation="relu"),
        >>>     tf.keras.layers.Dense(10, activation="softmax")
        >>> ])
        >>> model.summary()
            Model: "sequential"
            _________________________________________________________________
             Layer (type)                Output Shape              Param #   
            =================================================================
             flatten (Flatten)           (None, 784)               0         
             dense (Dense)               (None, 300)               235500    
             dense_1 (Dense)             (None, 100)               30100     
             dense_2 (Dense)             (None, 10)                1010      
            =================================================================
            Total params: 266610 (1.02 MB)
            Trainable params: 266610 (1.02 MB)
            Non-trainable params: 0 (0.00 Byte)

    Dense layer parameters:
      - have lots of parameters (e.g. connection weights: connnections x neurons ; bias weights: 1 x neurons  
      - for example, the above 1st hidden layer has 784 x 300 connections weights plus 300 bias weights

    Layer naming
      - each layer in a model must have a unique name
      - you can set the layer names explicity using the constructors' name argument, but generally it's simplier
        to let Keras name the layers automatically

     Code: Get information on Sequential model:
            # using 'layers' attribute to list model's layers
        >>> model.layers
           [<keras.layers.core.flatten.Flatten at 0x7fb220f4d430>,
            <keras.layers.core.dense.Dense at 0x7fb282285af0>,
            <keras.layers.core.dense.Dense at 0x7fb282365b50>,
            <keras.layers.core.dense.Dense at 0x7fb282365fa0>]
        >>> hidden1 = model.layers[1]
        >>> hidden1.name
            'dense'
            # use get_layer() method to access layer by name
        >>> model.get_layer('dense') is hidden1
            True
            # access layer parameters by using get_weights() and set_weights()
        >>> weights, biases = hidden1.get_weights()
        >>> weights.shape
            (784, 300)
        >>> biases.shape
            (300,)

     Parameter Initialization
       - dense layers connection weights by default are randomly initialize (which is to break symmetry)
         - use "kernel_initializer() if you want to use a different method to initialize connection weights
         - 'kernel' is another name for matrix of connection weights
       - biases by default are initialized to zeros
         - use "bias_initializer() if you want to use a different method to initialize bias weights
       - information on Layer weight initialization
         https://keras.io/api/layers/initializers/

     Weight Matrix
       - shape of weight matrix is dependent on the number of inputs which is why you specify the input
         shape with 'input_shape' when creating the model
       - it is okay to not specify the input shape when creating the model - keras will simply wait until 
         it knows the input shape. This will happen when you feed it some data (during training) or when
         you call its 'build()'.

    Compiling the Model (pages 323 - 324)

      Compiling the model
        - after a model is created, you must call its 'compile()' method to specify the 'loss function'
          and the 'optimizer' to use
        - full list of Keras losses options:     https://keras.io/api/losses/
          losses functions: 
            - The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.
        - full list of Keras optimizers options: https://keras.io/api/optimizers/
        - full list of Keras metrics options:    https://keras.io/api/metrics/
          metric functions:
            - A metric is a function that is used to judge the performance of your model.
            - Metric functions are similar to loss functions, except that the results from evaluating a metric are 
              not used when training the model. Note that you may use any loss function as a metric.

      Code: Compile model 

        >>> model.compile(loss="sparse_categorical_crossentropy",
        >>>               optimizer="sgd",
        >>>               metrics=["accuracy"])

      Code: Compile model - equivalent to previous, but with full option names 

        >>> model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
        >>>               optimizer=tf.keras.optimizers.SGD(),
        >>>               metrics=[tf.keras.metrics.sparse_categorical_accuracy])

      Loss function:
        sparse_categorical_crossentropy 
          - Use when there are two or more label classes, and labels to be provided as integers.
          - loss function used because we have sparse labels (i.e. for each instance, the is just a target class 
            index from 0 to 9 in this case), and the classes are exclusive
        categorical_crossentropy 
          - Use when there are two or more label classes, and labels to be provided in one-hot representation .
          - used for one target probability per class for each instance (such as one-hot vector)
        binary_crossentropy 
          - Use this cross-entropy loss for binary (0 or 1) classification applications. 
          - used for binary classification or multilabel classification 
          - also would use 'signoid' activation function in the output layer instead of 'softmax' activation

     Optimizer 'sgd'
       - means the model will be trained with stochastic gradient descent
       - keras will perform backpropagation algorithm (i.e. reverse-mode autodiff plus gradient descent)
       - with SGD optimization, it is important to tune the 'learning rate', so we will generally want to use
        'optimizer=tf.keras.optimizers.SGD(learning_rate=__???__)' to set the learning rate

      Metrics:
        categorical_accuracy:
          Calculates how often predictions match one-hot labels.
        sparse_categorical_accuracy:
          Calculates how often predictions match integer labels.

    Training and evaluating the model (pages 324 - 327)

      To train:
        - call the model's fit() method
        - fit() Trains the model for a fixed number of epochs (dataset iterations).
        - each epoch approximately trains approximately: (dataset size / epochs) instances
        - during each epoch, it trains data in mini-batches based on the 'batch_size'

        fit() inputs:
          input features (X_train) and target classes (y_train) 
          batch_size [default: 32]:  (mini-batch size) Number of samples per gradient update
          epochs [default: 1]: number of epochs (30) [default: 1]
          validation_data [default: None]: 
             - Data on which to evaluate the loss and any model metrics at the end of each epoch.
          validation_split [default: None]: 
             - Float between 0 and 1. Fraction of the training data to be used as validation data.
          class_weight
            - Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the 
              loss function (during training only). 
            - This can be useful to tell the model to "pay more attention" to samples from an under-represented class
              and lower weight for overrepresented classes
          sample_weight
            - pre-instance weights (e.g. could be useful some instances were labeled by experts and others by crowdsourcin
            - Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only)

        fit() outputs:
          - at each epoch during training, Keras displays runtime, training loss (loss), training accuracy, validation
            loss (val_loss), and validation accuracy (val_<type>_accuracy:
        
            1719/1719 [==============================] - 5s 3ms/step - loss: 0.4885 - sparse_categorical_accuracy: 0.8297 - val_loss: 0.4639 - val_sparse_categorical_accuracy: 0.8322

      Code: Train model

        >>> history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))

      Shape errors:
         - shape errors are common especially when get started
         - can occur because input shape is incorrectly, specified, loss function does not match expected targets
           (e.g. sparse_categorical_crossentropy loss expect integer targets/outputs, and categorical_crossentropy loss
           expects one-hot targets/outputs), missing flatten layer, etc

         - example errors - incorrectly specified shape:
           ValueError: Input 0 of layer "sequential" is incompatible with the layer: expected shape=(None, 26, 26), found shape=(None, 28, 28)
         - example errors - incorrectly specified loss function for an integer target:
           ValueError: Shapes (None, 1) and (None, 10) are incompatible

      Code: Evaluate model using test data

        >>> model.evaluate(X_test, y_test)
            313/313 [==============================] - 1s 3ms/step - loss: 0.3247 - sparse_categorical_accuracy: 0.8864
            [0.3247327506542206, 0.8863999843597412]

    Using the model to make predictions (pages 324 - 327)

      using model to make predictions:
        - 'predict()' method makes predictioins on new instances
        - for each instance, the model estimates one probability per class (e.g. in this case, class for 0 to 9)
        - if you only care about the class with the highest probability, you can use argmax()

      Code: Use model to make predictions (just on original 3 instances)

        >>> X_new = X_test[:3]
        >>> y_proba = model.predict(X_new)
        >>> y_proba.round(2)
            array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97],
                   [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],
                   [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],
                  dtype=float32)
        >>> y_pred = y_proba.argmax(axis=-1)
        >>> y_pred
            array([9, 2, 1])
        

  Building Regression MLP Using the Sequential API (pages 328 - 329)

    Using Sequential API for Regression MLP
      Difference with previous Classification MLP
        - output layer has a single neuron since we want to only predict a value
        - it does not use an output layer activation function
        - the loss function is mean squared error
        - metric is RMSE
        - for this example, you don't need a flatten layer, and instead using a Normalization layer
          as the first layer
            - Normalization layer does the same thing as sklearn StandardScaler, but it must be fitted to the
              training using its 'adapt()' method before you the model's 'fit()' method

    Code: reload California Housing set

        >>> # extra code - load and split the California housing dataset, like earlier
        >>> housing = fetch_california_housing()
        >>> X_train_full, X_test, y_train_full, y_test = train_test_split(
        >>>     housing.data, housing.target, random_state=42)
        >>> X_train, X_valid, y_train, y_valid = train_test_split(
        >>>     X_train_full, y_train_full, random_state=42)

    Code: Create Regression MLP Uinsg Sequential API for the California Housing dataset

        >>> tf.random.set_seed(42)
        >>> norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])
        >>> model = tf.keras.Sequential([
        >>>     norm_layer,
        >>>     tf.keras.layers.Dense(50, activation="relu"),
        >>>     tf.keras.layers.Dense(50, activation="relu"),
        >>>     tf.keras.layers.Dense(50, activation="relu"),
        >>>     tf.keras.layers.Dense(1)
        >>> ])
        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        >>> model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
        >>> norm_layer.adapt(X_train)
        >>> history = model.fit(X_train, y_train, epochs=20,
        >>>                     validation_data=(X_valid, y_valid))
        >>> mse_test, rmse_test = model.evaluate(X_test, y_test)
        >>> X_new = X_test[:3]
        >>> y_pred = model.predict(X_new)
        
        >>> rmse_test
            0.5297096967697144

        >>> y_pred
            array([[0.4969182],
                   [1.195265 ],
                   [4.9428763]], dtype=float32)

  Building Complex Models Using the Functional API (pages 329 - 336)

    Keras Functional API
      https://keras.io/guides/functional_api/
      - Functional API is a way to create models that are more flexible than the keras.Sequential API. 
      - The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or output

    Wide and Deep neural networks using Functional API
      - it connects all or part of the inputs layer directly to the output layer (wide connections)
      - it also connects inputs indirectly via hidden layers to the outputs (deep connections)
      - this architecture makes it possible for the neural network to learn both deep patterns 
        (using deep path) and simple rules (using 'wide' short path)
      - a regular MLP forces all data to flow through the full stack of layers, thus simple patterns
        may be distorted by the sequence of transformations
      Concatentate() layer
        - a Concatenate() layer merges wide connections (direct from inputs) and deep connections
          (layers stack ouputs)
      Normalization() layer
        - Normalization layer does the same thing as sklearn StandardScaler, but it must be fitted to the
          training using its 'adapt()' method before you the model's 'fit()' method
        - This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. 
        - It accomplishes this by precomputing the mean and variance of the data, and calling (input - mean) / sqrt(var) at runtime.
        - The mean and variance values for the layer must be either supplied on construction or learned via adapt(). 
        Normalization.adapt()
          - adapt() will compute the mean and variance of the data and store them as the layer's weights. 
          - adapt() should be called before fit(), evaluate(), or predict().


    Code: Wide and Deep neural network example using Keras Functional API

        >>> # extra code - reset the name counters and make the code reproducible
        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>> 
        >>> # create normalization layer to standardize (normalize) the data
        >>> normalization_layer = tf.keras.layers.Normalization()
        >>> 
        >>> hidden_layer1 = tf.keras.layers.Dense(30, activation="relu")
        >>> hidden_layer2 = tf.keras.layers.Dense(30, activation="relu")
        >>>
        >>> # create concatentation layer to merge the wide and deep connections
        >>> concat_layer = tf.keras.layers.Concatenate()
        >>> output_layer = tf.keras.layers.Dense(1)
        >>> 
        >>> input_ = tf.keras.layers.Input(shape=X_train.shape[1:])
        >>> normalized = normalization_layer(input_)
        >>> hidden1 = hidden_layer1(normalized)
        >>> hidden2 = hidden_layer2(hidden1)
        >>> concat = concat_layer([normalized, hidden2])
        >>> output = output_layer(concat)
        >>> 
        >>> model = tf.keras.Model(inputs=[input_], outputs=[output])
        >>>
        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        >>>
        >>> # create model and specify 'mse' loss, 'Adam' optimizer, and 'RMSE' metrics functions
        >>> model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
        >>> # use adapt() to compute the data mean and variance and store as weights
        >>> normalization_layer.adapt(X_train)
        >>> history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))
        >>> mse_test = model.evaluate(X_test, y_test)
        >>> y_pred = model.predict(X_new)
        >>> y_pred
            array([[0.4832877],
                  [1.4687781],
                  [4.2974386]], dtype=float32)

    Wide and Deep neural networks sending some inputs to wide path and some inputs to deep path 
      - use to handle cases where some inputs are used on wide path (direct connections to ouputs) and some
        inputs on the deep path (via dense layers stack)
      - need to create multiple input layers (at least one for wide inputs and one for deep inputs)
      - need to create multiple normalization layers (at least one for wide inputs and one for deep inputs)

    Code: Wide and deep sending some inputs to wide path and some to deep path neural network example  
        - send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). 
        - Note that 3 features will go through both (features 2, 3 and 4).
        - example show how to create dense layer on 1 line (define with parameters and pass in previous layer)
        - Normalization() layers cannot be created on 1 line because you need access to its 'adapt()' to
          compute the mean and variance
        - "input_wide" and "input_deep" objectives are both passed to Model's 'inputs' argument

        >>> tf.random.set_seed(42)  # extra code
        >>> input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4
        >>> input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7
        >>> norm_layer_wide = tf.keras.layers.Normalization()
        >>> norm_layer_deep = tf.keras.layers.Normalization()
        >>> norm_wide = norm_layer_wide(input_wide)
        >>> norm_deep = norm_layer_deep(input_deep)
        >>> hidden1 = tf.keras.layers.Dense(30, activation="relu")(norm_deep)
        >>> hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
        >>> concat = tf.keras.layers.concatenate([norm_wide, hidden2])
        >>> output = tf.keras.layers.Dense(1)(concat)
        >>> model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])
        >>> 
        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        >>> model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
        >>> 
        >>> X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]
        >>> X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]
        >>> X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]
        >>> X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]
        >>> 
        >>> norm_layer_wide.adapt(X_train_wide)
        >>> norm_layer_deep.adapt(X_train_deep)
        >>>
        >>> # both inputs training datasets need to be passed to 'model.fit()'
        >>> history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,
        >>>                     validation_data=((X_valid_wide, X_valid_deep), y_valid))
        >>> mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)
        >>> y_pred = model.predict((X_new_wide, X_new_deep))

    Multiple Outputs Use Cases:
      - task may demand it 
        - example: you may want to both locate and classify the main object in an image. This is both
          a regression task and classification task
      - multiple independent tasks on the same data
        - example: peform multitask classification on pictures of faces - facials expressions, wearing glasses
      - Regularization technique
        - example: train a constant whose objective is to reduce overfitting

    Code: Wide and deep neural network with Auxilary output for regularization
          - hidden2 2nd dense layer objective is passed to both the Concatenation layer and
            the aux_output dense layer
          - both "output" and "aux_outputs" objectives are passed to the Model's 'outputs' argument
          - in Model 'compile()', a loss function and loss_weights ratio needs to be specified for 
            each output specified when creating the Model via the Model's 'outputs' argument 
          Note: tf.keras.Model has tf.keras.models.Model as an alias.

        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>>
        >>> input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4
        >>> input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7
        >>> norm_layer_wide = tf.keras.layers.Normalization()
        >>> norm_layer_deep = tf.keras.layers.Normalization()
        >>> norm_wide = norm_layer_wide(input_wide)
        >>> norm_deep = norm_layer_deep(input_deep)
        >>> 
        >>> hidden1 = tf.keras.layers.Dense(30, activation="relu")(norm_deep)
        >>> hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
        >>> concat = tf.keras.layers.concatenate([norm_wide, hidden2])
        >>> output = tf.keras.layers.Dense(1)(concat)
        >>> 
        >>> aux_output = tf.keras.layers.Dense(1)(hidden2)
        >>> model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output, aux_output])
        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        >>> model.compile(loss=("mse", "mse"), loss_weights=(0.9, 0.1), optimizer=optimizer, metrics=["RootMeanSquaredError"])

        >>> # alternative compile declaration using dict instead of tuple for loss argument
        >>> # model.compile(loss={"output": "mse", 'aux_output: "mse"), loss_weights=(0.9, 0.1), optimizer=optimizer, 
            #   metrics=["RootMeanSquaredError"])
        >>> 
        >>> norm_layer_wide.adapt(X_train_wide)
        >>> norm_layer_deep.adapt(X_train_deep)
        >>> 
        >>> # both inputs training datasets and both training labels need to be passed to 'model.fit()'
        >>> history = model.fit(
        >>>     (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,
        >>>     validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))
        >>> )

    Code: Wide and deep neural network with Auxilary output for regularization - evaluate() and predict()
         - Keras returns the weighted sum of the losses, as well as the individual losses and metrics
         - predict() will return predictions for each output

        >>> eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))
        >>> weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results
           162/162 [==============================] - 0s 2ms/step - loss: 0.3355 - dense_2_loss: 0.3297 - dense_3_loss: 0.3886 - 
              dense_2_root_mean_squared_error: 0.5742 - dense_3_root_mean_squared_error: 0.6234
        >>> 
        >>> y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))
        >>> 
        >>> y_pred_tuple = model.predict((X_new_wide, X_new_deep))
        >>> y_pred = dict(zip(model.output_names, y_pred_tuple))

  Building Subclassing API to Build Dynamic Models (pages 336 - 337)

    Declarative modeling:
      - Both Keras sequential API and functional API are declarative: you start by declaring which layers input want
        to use and how they are connected, only thn can you starting feeding the model some data for training and
        inference
        Declarative advantages:
          - models can be saved, shared, cloned
          - its structure can be analyzed; framework can infer shapes and check types, so errors can be caught early
          - whole mode is static graph layer
        Declarative disadvantages:
          - cannot handle dynamic behaviors - examples: loops, varying shapes, conditional branching

    Imperative (Dynamic) modeling:
      Implementation:
        - subclass the 'Model' calls, create layers you need in the __init__ constructor
        - use these layers in the 'call' method
        - essentially, you create class with includes the Keras 'Model' subclass with __init__ constructor to
          define the layers, and the required functionality in the 'call' method 
        - the 'call' method can include loops, if statements, low-level TensorFlow operations, etc
      Advantages:
        - flexibility
        - great API when experimenting with new ideas
      disadvantages:
        - your models architecture is hidden so keras cannot inspect it
        - cannot clone using tf.keras.models.clone_model()
        - when you call summary() method, you only get a list of layers without connectivity information
        - Keras cannot check types and shapes ahead of time
        
    Declarative modeling vs Imperative Modeling:
       - unless you really need the extra flexibility, you should stick to sequential API and functional API

    Code: Using the 'Model' Subclassing API to build Dynamic Model

        >>> class WideAndDeepModel(tf.keras.Model):
        >>>     def __init__(self, units=30, activation="relu", **kwargs):
        >>>         super().__init__(**kwargs)  # needed to support naming the model
        >>>         self.norm_layer_wide = tf.keras.layers.Normalization()
        >>>         self.norm_layer_deep = tf.keras.layers.Normalization()
        >>>         self.hidden1 = tf.keras.layers.Dense(units, activation=activation)
        >>>         self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        >>>         self.main_output = tf.keras.layers.Dense(1)
        >>>         self.aux_output = tf.keras.layers.Dense(1)
        >>> 
        >>>     def call(self, inputs):
        >>>         input_wide, input_deep = inputs
        >>>         norm_wide = self.norm_layer_wide(input_wide)
        >>>         norm_deep = self.norm_layer_deep(input_deep)
        >>>         hidden1 = self.hidden1(norm_deep)
        >>>         hidden2 = self.hidden2(hidden1)
        >>>         concat = tf.keras.layers.concatenate([norm_wide, hidden2])
        >>>         output = self.main_output(concat)
        >>>         aux_output = self.aux_output(hidden2)
        >>>         return output, aux_output
        >>> 
        >>> tf.random.set_seed(42)  # extra code - just for reproducibility
        >>> model = WideAndDeepModel(30, activation="relu", name="my_cool_model")
        >>> 
        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        >>> model.compile(loss="mse", loss_weights=[0.9, 0.1], optimizer=optimizer,
        >>>               metrics=["RootMeanSquaredError"])
        >>> model.norm_layer_wide.adapt(X_train_wide)
        >>> model.norm_layer_deep.adapt(X_train_deep)
        >>> history = model.fit(
        >>>     (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
        >>>     validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))
        >>>
        >>> eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))
        >>> weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results
        >>> y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))

  Saving and Restoring a Model (pages 337 - 338)

    Saving Kera Model
      - use "model.save("<modelSavedName>, format=<saveFormat>)
      Save Format
        save_format='tf': 
           - saves the model using TensorFlow's SavedModel format
           - this <modelSaveName> directory containing several files and subdirectories
           - 'saved_model.pb' contains the model's architecture and logic in serialized computation graphs
           - 'keras_metadata.pb' file contains extra information needed by kera 
           - 'variables' subdirectory contains all the parameter values including connection weights, the biases,
             normalization statistics, optimizer parameters
           - asset subdirectory contains extra files such as data samples, feature names, class names, etc.

        save_format='h5' (or use a filename that ends in .h5, hdf5, keras: 
          - saves model to a single file using the Keras specific format based on the HDF5 format
          - most TensorFlow deployments require the SaveModel format

    Code: save kera model
        >>> model.save("my_keras_model", save_format="tf")

    Loading Kera Model
      - use "tf.keras.models.load_model("<modelSavedName>")
      - with tensorflow v2 "tf" format is deprecated. To load tensorflow V1 tf formatted saved models, use:
        tf.saved_model.load(<model_path>)

    Code: Load kera model and make predictions
        >>> model = tf.keras.models.load_model("my_keras_model")
        >>> y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))

   Saving Weights and Loading Weights
      - saving weights is faster and uses less disk space than saving the whole model, so its perfect for quick
        checkpoints during training

   Code: Save model weights and load model weights
       >>> model.save_weights("my_weights")
       >>> model.load_weights("my_weights")

  Using Callbacks (pages 338 - 340)

   Callbacks
       https://keras.io/api/callbacks/
     - the model 'fit()' 'callback' argument lets you specify a list of objectives that Kera will call before
       and after training, before and after each epoch, and even before and after procesing each mini-batch
       ModelCheckpoint callback:
         - save a checkpoint of your model at regular intervals during training, by default at the end of each epoch
         - if using a validation set during training you can set 'save_best_only=True' when creating this callback
           it will only save your model when the performance on validation set is the best so far
         - this is one way to implement early stopping
       EarlyStopping callback:
         - it will interrupt training when it measures no progress on the validation set for a number of epochs
           (defined the 'patience' argument)
         - if you set 'restore_best_weights=True', it will roll back to the best model at end of training
       ModelCheckpoint callback plus EarlyStopping callback:
         - can combine both callbacks to save checkpoints of your model in case your computer crashes and interrupt
           training early when there is no progress
       Custom callbacks:
         - you can implement custom callbacks that can be called during evaluation and predictions
         - these include on_train_beging, on_train_end, on_epoch_beging, on_epoch_end, ... , on_test_begin, on_test_end,
          ..., on_predict_begin, no_predict_end, .... 

  Code: Checkpoint callback
        >>> checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("my_checkpoints",
        >>>                                                    save_weights_only=True)
        >>> history = model.fit(
        >>>     (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
        >>>     validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
        >>>     callbacks=[checkpoint_cb])

  Code: EarlyStopping callback
        >>> early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
        >>> history = model.fit(
        >>>     (X_train_wide, X_train_deep), (y_train, y_train), epochs=100,
        >>>     validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
        >>>     callbacks=[checkpoint_cb, early_stopping_cb])

  Code: custom callback
        >>> class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):
        >>>     def on_epoch_end(self, epoch, logs):
        >>>         ratio = logs["val_loss"] / logs["loss"]
        >>>         print(f"Epoch={epoch}, val/train={ratio:.2f}")
        >>> 
        >>> val_train_ratio_cb = PrintValTrainRatioCallback()
        >>> history = model.fit(
        >>>     (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
        >>>     validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
        >>>     callbacks=[val_train_ratio_cb], verbose=0)

  Using TensorBoard for Visualization (pages 340- 344)
    TensorBoard
      - interactive visualization tool
      - can view the learning curves during training, compare curves and metrics between multiple runs
        visualize the computation graphs, analyze training statistics, view images generated by your
        model, visualize complex multidimensional data projected down to 3D, profile your network 
        (measure its speed to identify bottlenecks)

    TensorBoard Installation
      - automatically installed when install TensorFlow
      - need to install TensorBoard plug-in to visualize profiling data

   Code: install TensorBoard plugings
      >>> %pip install -q -U tensorboard-plugin-profile

   Code: Create unique run directory for each model run
        >>> # cleanup previous tensorboard logs directory
        >>> shutil.rmtree("my_logs", ignore_errors=True)
        >>>
        >>> from pathlib import Path
        >>> from time import strftime
        >>> 
        >>> def get_run_logdir(root_logdir="my_logs"):
        >>>     return Path(root_logdir) / strftime("run_%Y_%m_%d_%H_%M_%S")
        >>> 
        >>> run_logdir = get_run_logdir()
           WindowsPath('my_logs/run_2024_03_08_17_15_13')

   Code: Create tensorboard callback and pass callback to model fit() method

        >>> tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,
        >>>                                                 profile_batch=(100, 200))
        >>> history = model.fit(X_train, y_train, epochs=20,
        >>>                     validation_data=(X_valid, y_valid),
        >>>                     callbacks=[tensorboard_cb])

   Code: Start TensorBoard inside Jupyter Notebook:

        >>> %load_ext tensorboard
        >>> %tensorboard --logdir=./my_logs

    Code: (in web browser window) open Tensorboard
         http://localhost:6006/

    Code: Get TensorBoard pid so that you stop (kill) TensorBoard
        >>> from tensorboard import notebook
        >>> notebook.list()
        Known TensorBoard instances:
         - port 6006: logdir ./my_logs (started 0:22:05 ago; pid 3636)

      Use the following command on Linux or MacOSX, replacing <pid> with the pid listed above:
         !kill <pid>
      On Windows:
         !taskkill /F /PID <pid>

Fine Tuning Neural Networks Hyperparameters (pages 344 - 353)
    Keras Tuner Library
      - hyperparameter tuning library
      - offers several tuning strategies and has integration with tensorboard
      - to install inside Jupyter Notebook:
           %pip install -q -U keras_tuner

    Use Keras Tuner Library To tune hyperparameters:
         Write a function (e.g. 'build_model' in below example) builds, compiles, and returns a Kera model
           - in first part of the function, defines the hyperparameters and specifies range of values possible values
              e.g. 'n_hidden': checks if already present in the HyperParameters object 'hp', if so returns the value. 
                   If not, it registers a new integer 'n_hidden' hyperparameter whose possible value range is 0 to 8
                   and default is '2' (if default is not specified, then min value is used for the default)
           - in 2nd part of the function, builds the model using the hyperparameters 
         Pass function to tuner's constructor and call tuner's search() method
           - for basic random search, you can create a kt.RandomSearch()' tuner
               - RandomSearch tuner first calls 'build_model' function with empty Hyperparameters object just
                 to gather the hyperparameter specifications
               - in below example, it runs 5 trials (max_trials=5), for each trial, it builds a model using 
                 hyperparameters sampled randomly within their respective ranges, and trains the model for 10 epochs 
                 (epocsh=10), and saves it in a "my_fashion_mnist/my_rnd_search" subdirectory (directory="my_fashion_mnist", 
                 project_name="my_rnd_search")
               - since overwrite=True, it will delete previous subdirectory. If overwrite=False, it will continue training
                 from where it left off.
               - since objective="val_accuracy", the tuner prefers models with the validation accuracy

    Code: create a function ('build_model') that specifies hyperparameters to and how to build the kera model

        >>> import keras_tuner as kt
        >>> 
        >>> def build_model(hp):
        >>>     n_hidden = hp.Int("n_hidden", min_value=0, max_value=8, default=2)
        >>>     n_neurons = hp.Int("n_neurons", min_value=16, max_value=256)
        >>>     learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2,
        >>>                              sampling="log")
        >>>     optimizer = hp.Choice("optimizer", values=["sgd", "adam"])
        >>>     if optimizer == "sgd":
        >>>         optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
        >>>     else:
        >>>         optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        >>> 
        >>>     model = tf.keras.Sequential()
        >>>     model.add(tf.keras.layers.Flatten())
        >>>     for _ in range(n_hidden):
        >>>         model.add(tf.keras.layers.Dense(n_neurons, activation="relu"))
        >>>     model.add(tf.keras.layers.Dense(10, activation="softmax"))
        >>>     model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
        >>>                   metrics=["accuracy"])
        >>>     return model

    Code: Pass function ('build_model') Kera Tuner's constructor (i.e. 'kt.RandomSearch') and call its 
          'search()' method to tune the Kera hyperparameters

        >>> random_search_tuner = kt.RandomSearch(
        >>>     build_model, objective="val_accuracy", max_trials=5, overwrite=True,
        >>>     directory="my_fashion_mnist", project_name="my_rnd_search", seed=42)
        >>> random_search_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
            Trial 5 Complete [00h 00m 44s]
            val_accuracy: 0.8361999988555908

            Best val_accuracy So Far: 0.8623999953269958
            Total elapsed time: 00h 03m 40s

        >>> top3_models = random_search_tuner.get_best_models(num_models=3)
        >>> best_model = top3_models[0]
        >>> 
        >>> top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)
        >>> top3_params[0].values  # best hyperparameter values
            {'n_hidden': 8,
             'n_neurons': 37,
             'learning_rate': 0.008547485565344062,
             'optimizer': 'sgd'}

        >>> best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]
        >>> best_trial.summary()
            Trial 3 summary
            Hyperparameters:
            n_hidden: 8
            n_neurons: 37
            learning_rate: 0.008547485565344062
            optimizer: sgd
            Score: 0.8623999953269958

        >>> best_trial.metrics.get_last_value("val_accuracy")
            0.8623999953269958

        >>> # continue training the best model over 5 more epochs
        >>> best_model.fit(X_train_full, y_train_full, epochs=10)
        >>> test_loss, test_accuracy = best_model.evaluate(X_test, y_test)
   
    Use Keras Tuner Library To fine-tune preprocessing hyperparameters or model.fit() arguments:
      Create subclass of 'kt.HyperModel' and define build() and fit() methods
         fit() method:
           - takes 'hyperparameter object (e.g. 'hp' in below code) and compiled model (e.g. 'model') plus
             all model.fit() arguments
           - fits the model, and then returns the history object
           - the fit() method may use hyperparameters to decide how to preprocess the data, tweak the batch and more
        build() method:
          - does the exact same thing as the previous 'build_model' function pass to the KT tuner 
          - that is,  builds, compiles, and returns a Kera model
          - as a result, the below example build() method justs returns 'build_model()' call 
      Pass instance of this subclass to the tuner of your choice

       Keras hyperparameter tuners include:
         kt.Hyperband tuner
           - starts by training many different models for a few epochs, then it elimiates the worst of the models
             and only keeps the top 1 / factor models (i.e. the top 1/3 in below example) repeating this process
             until a single model is left
           - the 'max_epoch' argument controls the number of epochs that the best model will be trained for
           - the whole process is repeated based on the 'hyperband_iteration' value (2 in below example)
           - the train number of training epochs across all models for each hyperband iteration is about:
             max_epochs * (np.log(max_epochs) / np.log(factor))**2

         kt.BayesianOptimization tuner
           - gradually learns which regions of the hyperparameter space are most promising by a fitting a probabilistic
             model called 'Guassian process'
           - this allows it gradually zoom in on the best hyperparameters
           - the downside is this algorithm has its own hyperparameters:
             alpha: represents the level of noise you expect in the performance measures across trial (default: 10**-4)
             beta: specifies how much you want the algorithm to explore, instead of simply exploiting the known
                  good regions of hyperparameter space (default: 2.6)


    Code: Create Keras Tuner Library Subclass for fine-tuning preprocessing hyperparameters or model.fit() arguments:

        >>> class MyClassificationHyperModel(kt.HyperModel):
        >>>     def build(self, hp):
        >>>         return build_model(hp)
        >>> 
        >>>     def fit(self, hp, model, X, y, **kwargs):
        >>>         if hp.Boolean("normalize"):
        >>>             norm_layer = tf.keras.layers.Normalization()
        >>>             X = norm_layer(X)
        >>>         return model.fit(X, y, **kwargs)
        >>> 
        >>> 

    Code: Pass instance of Keras Tuner Library Subclass to kt.Hyperband Tuner and call its search method
          Note: Adds EarlyStopping and TensorBoard callbacks 

        >>> hyperband_tuner = kt.Hyperband(
        >>>     MyClassificationHyperModel(), objective="val_accuracy", seed=42,
        >>>     max_epochs=10, factor=3, hyperband_iterations=2,
        >>>     overwrite=True, directory="my_fashion_mnist", project_name="hyperband")
        >>> 
        >>> root_logdir = Path(hyperband_tuner.project_dir) / "tensorboard"
        >>> tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)
        >>> 
        >>> early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)
        >>> 
        >>> hyperband_tuner.search(X_train, y_train, epochs=10,
        >>>                        validation_data=(X_valid, y_valid),
        >>>                        callbacks=[early_stopping_cb, tensorboard_cb])
            Trial 60 Complete [00h 00m 58s]
            val_accuracy: 0.8194000124931335
            
            Best val_accuracy So Far: 0.8784000277519226
            Total elapsed time: 00h 23m 58s

    Code: Pass instance of Keras Tuner Library Subclass to kt.BayesianOptimization Tuner and call its search method
          Note: Adds EarlyStopping callback 

        >>> bayesian_opt_tuner = kt.BayesianOptimization(
        >>>     MyClassificationHyperModel(), objective="val_accuracy", seed=42,
        >>>     max_trials=10, alpha=1e-4, beta=2.6,
        >>>     overwrite=True, directory="my_fashion_mnist", project_name="bayesian_opt")
        >>> bayesian_opt_tuner.search(X_train, y_train, epochs=10,
        >>>                           validation_data=(X_valid, y_valid),
        >>>                           callbacks=[early_stopping_cb])
            Trial 10 Complete [00h 00m 49s]
            val_accuracy: 0.8396000266075134
            
            Best val_accuracy So Far: 0.8628000020980835
            Total elapsed time: 00h 06m 34s

    
    Code: Start tensorboard to explore hyperparameter tuning result data

        >>> %tensorboard --logdir=./my_fashion_mnist/hyperband/tensorboard


  Number of Hidden Layers (pages 349 - 350)

    one or two hidden layers
      - for many problems (e.g. MNIST dataset), you can start with 1 or 2 hidden layers plus a few hundred neurons
    complex problems
      - for complex problems, you can ramp up the hidden layers until you start overfitting the training set data
    very complex problems
      - such as image classification or speech recognitioin, typically required networks of dozens of layers (or
        even hundreds, but not fully connected ones) and they need a huge amount of training data
    training [very complex networks] from scratch
      - rarely do you train these networks from scratch; it is much more common to reuse parts of the pretrained
        state-of-art that performs a similar task

  Number of Neurons per Hidden Layer (pages 350 - 351)

    neurons in input and output layers
      - determined gy the type of input and outputs your tasks require
    neurons in hidden layers
      - generally using the same number of neurons in all hidden layers performs just well or better than
        using fewer and fewer at each layer 
      - that said, for some datasets, it can sometimes help to make the hidden layer bigger [more neurons]
        that sequence layers
    sizing neurons in hidden layers
      - you can gradually increase the number of neurons in the hidden layers till the training data is overfitting
      - you can try build a model with slight more layers and/or neurons than you actually need and use early
        stopping and other regularization techniques to prevent overfitting


  Learning Rate, Batch Size, and Other Hyperparameters (pages 351 - 353)
     
    learning rate
      - most important hyperparameter
      - if other hyperparameters are update, then use other hyperparameters updates and then update the learning rate
      - find a good learning rate:
         - train a model for a few hundred iterations, starting with a very low learning rate (e.g. 10**-3), and gradually
           increasing it up to a very large learning rate (e.g. 10). This is done by multiplying the learning rate
           by a constant factor at each iteration (e.g. (10/10**-5)**1/500  -> ~2000)
         - plot the loss a function of the learning rate (using a log scale for the learning rate). You should see it
           drop at first, but after a while the learning rate will be too large, and the loss will shoot back up.
           The optimal learning will be a little before the loss starting shooting back up (typically 10x lower
           that the low point)

    optimizer
      - use a better optimizer than plain gradient descent

    batch size
      - batch size impact model performance and training time
      - some researchers recommend the largest batch size that can fit in your GPU RAM.
         - however, this size can lead to training instabilities
      - other researchers recommend small batch sizes (from 2 to 32)

    activation function
      - the ReLU activation function is a good default for all hidden layers
      - output layer activation function depends on your task

    number of iterations
     - in most cases, the number of training iterations does not need to be tweaked: just use early stopping


Chapter 10 Exercises sklearn class info:

Chapter 10 Exercises:

    -> see exercise_notebooks/10_exercises.ipynb

------------------------------------------------------
Chapter 11 Training Deep Neural Newtorks 
------------------------------------------------------
  Training a Deep Neural Network (DNN) problems:
    - gradients growing ever smaller or larger when flowing backward through the DNN during training
       - both make lower layers harder to train
    - not enough training data or too costly to label 
    - training may be extremely slow
    - model with millions of parameters would severely risk overfitting

The Vanishing / Exploding Gradients Problems (pages 357 - 373)

   Vanishing Gradients problem:
     - gradients often get smaller and smaller as the algorithm progresses down to the lower layers
     - as a result, the gradient descent update leaves the layer weights virtually unchanged, and
       training never converges to a good solution

   Exploding Gradients problem:
     - gradients often can bigger and bigger until layers get insanely large weights, and the algorithm
       diverges

   Unstable Gradients problem:
     - more generally, DNNs suffer from 'unstable gradients': different layers may learn at widely
       different speeds

   Unstable Gradients Suspected causes:
     - combination of the sigmoid (logistic) activation function and the weight initialization 
       technique at the time (i.e. normal distribution with a mean of 0 and stardard deviation of 1)
     - with this activation function and initialization scheme, the variance of the output of each 
       layer is much greater that the variance of its inputs
       - going forward in the network, the variance keeps getting increasing after each layer until the
         activation saturates at the top layers
       - this saturation is model worse by the fact that the sigmoid function has mean of 0.5 and not 0
         (the tanh function has a mean of 0 and behaves slightly better)

  Glorot and He Initialization (pages 359 - 361)

    Glorot proposed way to alleviate the unstable gradients problem
      - the variance of the outputs of each layer to be equal to the variance of its inputs, and we need
        the gradients to have equal variance before and after flowing through a layer in the reverse
        direction
        - it is not possible to guarantee both unless fan-in equal fan-out of a layer
        - proposed compromise, use: fan-avg = (fan-in + fan-out) / 2

   Equation 11-1  Glorot initialization (when using the sigmoid activation function) 

      Normal distribution with mean 0 and variance: sigma**2 = 1 / fan-avg

      Or a uniform distribution between -r and +r with r = (3 / fan-avg)**-1/2

    Glorot initialization (or Xavier initialization)  
      - Equation 11-1 where fan-avg = (fan-in + fan-out) / 2
      - can speed up training considerable, and one of the practices that led to the success of deep learning
      - by default, Keras using Glorot initialization with a uniform distribution 

    LeCun initialization  
      - equivalent to Glorot initializaiton when fan-in = fan-out
      - in Glorot initialization (equation 11-1), just replace fan-avg with fan-in 

    Table 11-1. Initialization parameters for each type of activation function

    Initialization      Activation functions                       sigma**2 (Normal)
    ---------------     ----------------------                     --------------------
    Glorot              None, tanh, sigmoid, softmax                1 / fan-avg
    He                  ReLU, Leaky ReLU, ELU, GLU, Swish, Mish     2 / fan-in
    LeCun               SELU                                        1 / fan-in

      where sigma**2 is the variance


    Code: Setting Kernel initializer to use 'He Normal distribution' 

        >>> import tensorflow as tf
        >>> dense = tf.keras.layers.Dense(50, activation="relu", kernel_initializer="he_normal")

        Note: To set He uniform distribution initializer:  kernel_initializer="he_uniform"

    Code: Using VarianceScaling initializer to set 'uniform' distribution based on 'fan-avg' (Glorot uniform)

        >>> he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode="fan_avg", distribution="uniform")
        >>> dense = tf.keras.layers.Dense(50, activation="sigmoid", kernel_initializer=he_avg_init)

  Better Activation Functions (pages 361 - 367)

    ReLU (Rectified Linear Unit) function:
      - performance much better than sigmoid in DNN mostly because it does not saturate for positive values,
        and because it is very fast
      Dying ReLU 
        - problem with ReLU activation function
        - during training some neurons effectively 'die' meaning the stop outputing anything other than 0
        - a neuron dies with its weights get tweaked in such a way that the input of the ReLU function 
          (i.e. the weighted sum of the neurons's inputs plus its bias term) is negative for all instances
          in the training set
        - when this happens, it just keeps outputing zeros, and the gradient descent does not affect it 
          anymore because the gradient of the ReLU function is zero when its input is negative

   Leaky ReLU (LeakyReLU)
      LeakyReLU-alpha(z) = max (alpha * z, z)
        - hyperparameter alpha defines how much the function 'leaks': it is the slope of the function for z < 0
      - Having a slope for z < 0 ensures the the leaky ReLU never die
      - paper found that the 'leaky ReLU variants' variants always outperformed the strick ReLU
      - paper also found setting alpha = 0.2 (huge leak) seemed to perform better than alpha = 0.01 (small leak)

   Randomized Leaky ReLU (RReLU)
     - alpha is picked randomly in a given range during training and is fixed to the average value during testing
     - paper reports it also performed fairly well, and seemed to act as a regularizer, reducing the
       risk of overfitting
     - currently, no official implementation in Keras, but fairly easy to implement on your own

   Parammetric Leaky ReLU (PReLU)
     - 'alpha' is authorized to be learned during training: instead of being an hyperparameter, it becomes
       a parameter that can be modified by backpropagation like any other parameter
     - paper reports it strongly outperformed ReLU on large image datasets, but on smaller datasets it runs
       the risk of overfitting the training data

    Code: Setting LeakyReLU activation function 

        >>> leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3
        >>> dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer="he_normal")

        Note: Can also set PReLU activation function using: tf.keras.layers.PReLU()

    Code: Using LeakyReLU activation function in a separate layer

        >>> model = tf.keras.models.Sequential([
        >>>     # [...]  # more layers
        >>>     tf.keras.layers.Dense(50, kernel_initializer="he_normal"),  # no activation
        >>>     tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer
        >>>     # [...]  # more layers
        >>> ])

    ReLU, leaky ReLU, and PReLU limiations:
      - they are not smooth functions: their derivatives abruptly change (at z = 0)
      - this sort of discontinuity can make the gradient descent bounce around the optimum, and slow down
        convergence

    ELU and SELU (pages 363 - 364)

      Exponentional Linear Unit (ELU) activation function:
        - see equation 11-2
        differences with ReLU function
          - it takes on negative values when z < 0, which allows the unit to have an average output closer
            to zero and helps alleviate the vanishing gradient
          - it has a non-zero gradient when z < 0, which avoids dead neurons problem
          - if 'alpha' is equal to '1', then the function is smooth everywhere which helps speed up gradient
            descent since it does not bounce as much to the left and right when z = 0
        Using with Keras: 
           - set activation='elu' 
           - should use 'He initialization'
        Drawbacks
          - slower to compute than ReLU function and its variants
          - Note: its faster convergence during training may compensate for the slow computation
          - at test time, ELU network is a bit slower that ReLU network
         

      Equation 11-2. ELU activation function

         ELU-alpha (z) = | alpha( exp(z) - 1     if z < 0
                         | z                     if z >= 0

      Scaled Exponentional Linear Unit (SELU) activation function:
        - scaled variant of the ELU activation function (about 1.05 times ELU, using alpha = 1.67)
        - if neural network composed exclusively of a stack of dense layers (i.e. an MLP), and if all hidden
          layers use SELU activation function, then the network will self-normalize
        self-normalize
          - the outputs of each layer will tend to preserve a mean of 0 and a standard deviation of 1 
            during training which solves the vanishing/exploding gradient problme
        Using with Keras: 
           - set activation='selu' 
        Self-normalization conditions:
          - input features must be standardized: mean 0 and standard deviation 1
          - hidden layer's weights must be initialized using LeCun normal inialization:
            in keras: kernel_initializer='lecun_normal'
          - only guaranteed with plan MLPs
          - cannot use regularization techniques like l1 or l2 regulariation, max-norm, batch-norm, or 
            regular dropout
        Traction:
          - due to these constraints, SELU has not gained a log of traction
          - GELU, Swish, and Mish activation functions seem to outperform SELU consistently

      Code: Implementing ELU and SELU (with alpha=1.67 and scale=1.05 suggested defaults):

        >>> alpha_0_1 = 1.6732632423543778
        >>> scale_0_1 = 1.0507009873554805
        >>> 
        >>> def elu(z, alpha=1):
        >>>     return np.where(z < 0, alpha * (np.exp(z) - 1), z)
        >>> 
        >>> def selu(z, scale=scale_0_1, alpha=alpha_0_1):
        >>>     return scale * elu(z, alpha)

      Code: Setting SELU activation function (with recommended LeCun Norm initiailizer) 

        >>> dense = tf.keras.layers.Dense(50, activation="selu", kernel_initializer="lecun_normal")

    GELU, Swish, and Mish (pages 364 - 367)


      Gaussian Error Linear Units (GELU) activation function:
        - smooth variant of ReLU activation function
        Equation info:
          - phi(z): the probability that a value sampled randomly from a normal distribution of mean 0 and variance 1 is lower than z
          - phi(z) ~= sigmoid( beta * z) where beta=1.702
        Approximate GELU:
          GELU(z) ~= z * signmoid ( 1.702 * z) 
          - GELU is computationally intensive, but Approximate GELU works very well and much faster to compute
          
      Equation 11-3. GELU activation function

          GELU(z) = z * phi(z)  = z * (1/2) [ 1 + erf(x/2**-1/2)] 

          Notes:
              1. GELU(z) ~= z * signmoid ( 1.702 * z) 
              2. erf (error function) 

      Swish or Sigmoid Linear Unit (SiLU) activation function
         Original Swish / SiLU:
           Swish(z) = z * sigmoid(z)
         Generalized Swish:
           Swish-beta(z) = z * sigmoid(beta * z)
           - Approximate GELU equal generalized Swish when beta = 1.702
         Beta:
           - can tune 'beta' like any hyperparameter
           - also possible to make 'beta' trainable and let gradient descent optimize it

      Mish activation function
        - smooth variant of ReLU activation function
        Equation:
           Mish(z) = z * tanh(softplus(z))   where softplus = log(1 + exp(z))
        comparison:
          - Mish generally output performed other activation functions - even Swish and GELU, but tiny margins
          - Mish overlaps Swish when z is negative and GELU with z is positive

      Code: Implementing Swish, Approximate GELU (Swish with beta=1.702), softplus, and Mish 

        >>> import numpy as np
        >>> 
        >>> def sigmoid(z):
        >>>     return 1 / (1 + np.exp(-z))
        >>> 
        >>> def swish(z, beta=1):
        >>>     return z * sigmoid(beta * z)
        >>> 
        >>> def approx_gelu(z):
        >>>     return swish(z, beta=1.702)
        >>> 
        >>> def softplus(z):
        >>>     return np.log(1 + np.exp(z))
        >>> 
        >>> def mish(z):
        >>>     return z * np.tanh(softplus(z))

      Activation Function recommendations
        ReLU
          - remains a good default for simple tasks
          - often as good as more sophisticated functions and fast to compute
        Swish
          - a better choice for more complex tasks
          - you can parameterized Swish with a learnable 'beta' for more complex tasks
        Mish
          - may give you slightly better results, but it requires a bit more compute
        Leaky ReLU or parameterized ReLU (PReLU)
          - if you care about run time latency for more complex tasks
        SELU
          - for deep MLP, you may want to give SELU a try
          - make sure to respect the constraints (standards inputs, LeCun normal initialization, regularization limitations)

  Batch Normalization (pages 367 - 372)

    Batch Normalization (BN):
      Concept:
        - add an operation just before or just after each hidden layer
        - this operation is to zero center and normalizes each input, then scales and shift the results using 
          two new parameters vectors per layer, one for scaling and one for shifting
        - essentially, the operation lets the model learn the scale and mean of each layer's inputs 
      training implementation:
        - in order to zero-center and normalize the inputs, the algorithm needs to estimate each input's mean and stardard deviation
        - it evaluates the mean and std deviation of the input over the current mini-batch (hence the name 'batch normalization)
      testing implementation:
        - most implementations (including keras) estimate the final statistics during training by using a moving average
          of the lyaer's input mean and std deviation
        learned parameters
          - four parameter vectors are learned in each batch-normalized layer: 
             - learned through regular backpropagation:
                 gamma: output scaled vector
                 beta:  output offset vector
             - estimated using an exponential moving average 
                 mu:    final input mean vector
                 sigma: final std deviation vectory
          - Note: 'mu' and 'sigma' are estimated during training, but they are only used after training (to replace
            batch input means and std deviation  ('mu' and 'sigma' in equation 11.4)

      Results:
        - vanishing gradient problem was strongly reduced to the point that they could use saturating activation functions
          such as tanh and sigmoid
        - the networks were less sensitive to weight initialization 
        - acts like a regularizer reducing the need for other regularization techniques (such as dropout)
        - has a runtime penality (neural networks make slower predictions)
          - note: after training, it often possible to fuse the BN layer with the previous layer, thereby avoiding 
            the runtime penalty
        - convergence is much faster with BN, so it will take fewer epochs to reach the same performance offseting
          the runtime penality
        - BN has become one of the most-used layers in deep neural networks
      Using with Keras:
        - just add 'BatchNormalization' layer before or after each hidden layer's activation function
        - you may add BN layer as the first layer, but a plain 'Normalization' layer generally performs just as well
          (except that you need to first call 'adapt()' method)
      Using BN before or afer activation function:
        - Sometimes applying BN before the activation function (after hidden layer) works better (there's a debate on this topic). 
           - the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization 
             layer some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:
      BN hyperparameters:
        - default hyperparameters will usually be fine, but you may occasionally need to tweak 'momentum'
        - momentum: used by BN when it updates the exponential moving average
           - a good value is typically close to 1; for example, 0.9, 0.99, or 0.999 (more 9s for larger datasets and smaller
             for min-batches
        - axis: determines which axis should be normalized

    Equation 11-4. Batch Normalization

      1. vector of input means: 
            mu-B = (1/m-B) SUM xi  where SUM is from i = 1 to  i = m-B 

      2. vector of input std deviation:  
           sigma-B ** 2 = (1/m-B) SUM (xi - mu-B)**2  where SUM is from i = 1  to i = m-B

      3. Vector of zero-centered and normalized inputs for instance i
           pred-x-i = (xi - mu-B) / (sigma-B**2 + epsilon)**-1/2

      4.  output of the BN (batch normalization) layer
           zi = gamma x pred-x-i  + beta


      Notes:
         m-B:     number of instances in the mini-batch
         epsilon: smoothing term: tiny number that avoids division by zero and ensures gradient doesn't grow too large
         gamma:   output scale vector for the layer (one scale parameter per input)
         beta:    output shift (offset) parameter vectory for the layer (one offset per input)
         x:       in 'zi' equation 'x' represents element-wise multiplication (each input is multiplied by its
                  corresponding output scale factor

      Code: Using Batch Normalization layers before each Dense hidden layer

        >>> # extra code - clear the name counters and set the random seed
        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>> 
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     tf.keras.layers.BatchNormalization(),
        >>>     tf.keras.layers.Dense(300, activation="relu", kernel_initializer="he_normal"),
        >>>     tf.keras.layers.BatchNormalization(), 
        >>>     tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>     tf.keras.layers.BatchNormalization(),
        >>>     tf.keras.layers.Dense(10, activation="softmax")
        >>> ])
        >>>
        >>> model.summary()  # report model info include layer types, output shape, no of paramters
        >>> 
        >>> [(var.name, var.trainable) for var in model.layers[1].variables]  # report layer 1 variables
            [('batch_normalization/gamma:0', True),
             ('batch_normalization/beta:0', True),
             ('batch_normalization/moving_mean:0', False),
             ('batch_normalization/moving_variance:0', False)]
        >>> 
        >>> # extra code - just show that the model works! 
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics="accuracy")
        >>> model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))


      Code: Using Batch Normalization layers after each Dense hidden layer (before activation function)
           Note: hidden layers followed by BN layers do not use activation function and do not need to use bias

        >>> # extra code - clear the name counters and set the random seed
        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>> 
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
        >>>     tf.keras.layers.BatchNormalization(),
        >>>     tf.keras.layers.Activation("relu"),
        >>>     tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
        >>>     tf.keras.layers.BatchNormalization(),
        >>>     tf.keras.layers.Activation("relu"),
        >>>     tf.keras.layers.Dense(10, activation="softmax")
        >>> ])
        >>> 
        >>> # extra code - just show that the model works! 
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
        >>>               metrics="accuracy")
        >>> model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))

  Gradient Clipping (pages 372 - 373)

     Gradient Clipping
       - technique to clip the gradient during backpropagation
       - generally used with recurrent neural networks where batch normalization is tricky
     Using with Keras
       - set optimizer 'clipvalue=<clipValue>' argument
          - will clip every component of the gradient vector to a value between -<clipValue> and +<clipValue>
       - set optimizer 'clipnorm=<clipNorm>' argument
          - will clip the whole gradient if the l2 normal is greater than than the threshold you pick
          - if so, it will normalize the gradient vector values between -<clipNormal> and +<clipNormal> 
              - example: if clipnorm=1, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595]

      Code: Using Gradient Clipping by setting 'clipvalue' argument

        >>> optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)

      Code: Using Gradient Clipping by setting 'clipnorm' argument

        >>> optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)

Reusing Pretrained Layers (pages 373 - 379)

  Transfer learning
    - it is generally not a good idea to train a large DNN from scratch without first trying to use an existing
      neural network that accomplishes a similar task
    - transfer learning is reusing layers from a neural network that performs a similar task 
    input size
      - if input pictures for new task don't have the same size as ones used in the original task, you will usually
        have to add a preprocessing step to the size expected by the original model
      - transfer learning works best when input have similar low-level features
    Which layers to reuse?
      - output layers of the original model should usually be replaced because it most likely not useful for the new task
        and probably will not have the right number of outputs
      - upper hidden layers (closest to output) less likely to be useful as lower layers since the high-level features
        that are most useful for the new task may differ significantly
      - the more similar the tasks, the more layers you will want to reuse
    Reusing layers:
      - try freezing all reused layers first (i.e. make their weights non-trainable so gradient descent will not modify 
        them) and train your model and see how it performs
      - then try unfreezing one or two of the top hidden layers to let back propagation tweak them and see if performance improves 
      - it is useful to reduce the learning rate when you unfreeze layers

  Transfer Learning and Keras (pages 375 - 377)

    Transfer Learning training example overview:
      - Let's split the fashion MNIST training set in two:
    
          X_train_A: all images of all items except for T-shirts/tops and pullovers (classes 0 and 2).
          X_train_B: a much smaller training set of just the first 200 images of T-shirts/tops and pullovers.
    
      - The validation set and the test set are also split this way, but without restricting the number of images.
    
      - We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B 
        (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in 
        set A (trousers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots) are somewhat similar to classes 
        in set B (T-shirts/tops and pullovers). However, since we are using Dense layers, only patterns that occur at the 
        same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can
        be detected anywhere on the image, as we will see in the chapter 14).

   Code: Split dataset for training Model A and Model B and train model A

        >>> # extra code - split Fashion MNIST into tasks A and B, then train and save
        >>> #              model A to "my_model_A".
        >>> 
        >>> pos_class_id = class_names.index("Pullover")
        >>> neg_class_id = class_names.index("T-shirt/top")
        >>> 
        >>> def split_dataset(X, y):
        >>>     y_for_B = (y == pos_class_id) | (y == neg_class_id)
        >>>     y_A = y[~y_for_B]
        >>>     y_B = (y[y_for_B] == pos_class_id).astype(np.float32)
        >>>     old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))
        >>>     for old_class_id, new_class_id in zip(old_class_ids, range(8)):
        >>>         y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A
        >>>     return ((X[~y_for_B], y_A), (X[y_for_B], y_B))
        >>> 
        >>> (X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
        >>> (X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
        >>> (X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
        >>> X_train_B = X_train_B[:200]
        >>> y_train_B = y_train_B[:200]
        >>> 
        >>> tf.random.set_seed(42)
        >>> 
        >>> model_A = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dense(8, activation="softmax")
        >>> ])
        >>> 
        >>> model_A.compile(loss="sparse_categorical_crossentropy",
        >>>                 optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), metrics=["accuracy"])
        >>> history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))
        >>> model_A.save("my_model_A")


   Keras Transfer learning steps (reusing model A layers for training model B):

     1. clone model (A) and set weights
        - you need to clone the models so not to impact Model A since model A and B will share layers
        - 'clone_model()' only sets the architecture and not the weights. Use "set_weights()' to copy the weights
     2. Reuse model A layers for model B and add new layer(s) 
        - copy model A layers to model B (except output layer in this case)
        - add new output layer
     3. Freeze the reused layers for training in the first few epochs 
        - Since new layers will be randomly initialized, there will be large error gradients that may wreck the
          reused layers' weights early during training, so reused layers weights should be frozen
        - To freeze, set trainable=False for reused layers during initial training
        
     4. Compile model after freezing reused layers
        - after freezing and unfreezing layers, model MUST be compiled

     5. train model for a few epochs

     6. Unfreeze layers, reduce learning rate, compile model, and continue training
        - not done in example
        - after unfreezing the reused layers, it is usually a good idea to reduce the learning rate to avoid
          damaging reused weights

     Note: This is contribute case , after experimenting with many cases, author fine one that worked


   Code: Clone model A and reuses its layers except output layer to retrain for model B

        >>> model_A = tf.keras.models.load_model("my_model_A")
        >>> 
        >>> tf.random.set_seed(42)  # extra code - ensure reproducibility
        >>> 
        >>> model_A_clone = tf.keras.models.clone_model(model_A)
        >>> model_A_clone.set_weights(model_A.get_weights())
        >>> 
        >>> # creating model_B_on_A from model_A_Clone
        >>> model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])
        >>> model_B_on_A.add(tf.keras.layers.Dense(1, activation="sigmoid"))
        >>> 
        >>> for layer in model_B_on_A.layers[:-1]:
        >>>     layer.trainable = False
        >>> 
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
        >>> model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>> 
        >>> history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))
        >>> 
        >>> for layer in model_B_on_A.layers[:-1]:
        >>>     layer.trainable = True
        >>> 
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
        >>> model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>> history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))
        >>> 
        >>> model_B_on_A.evaluate(X_test_B, y_test_B)
            
            63/63 [==============================] - 0s 667us/step - loss: 0.2546 - accuracy: 0.9385
            
            [0.2546142041683197, 0.9384999871253967]


     Where Transfer learning works:
       - does not work very well with small dense networks
          - presumably because small network learn few patterns, and dense networks learn very specific patterns
            which are likely not usefull in other tasks
       - does not work best with deep convolutional neural networks
         - these tend to learn fature detectors that are more general (especially in lower layers)

  Unsupervised Pretraining (pages 377 - 378)


    https://medium.com/lis-computer-vision-blogs/difference-between-different-neural-networks-c924050444a2
    https://ubiai.tools/comparing-gan-autoencoders-and-vaes/

    Autoencodoers (AEs)
      - Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. 
        E.g. in a network like this: output[i] has edge back to input[i] for every i. 
      - Typically, number of hidden units is much less then number of visible (input/output) ones. As a result, when 
        you pass data through such a network, it first compresses (encodes) input vector to "fit" in a smaller representation, 
        and then tries to reconstruct (decode) it back. 
      - The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation 
        (encoding) for input data.

              original input -----> Encoder -----> Compressed       ---------> Decoder ------> Reconstructed input
                                                   Representation
    Generative Adversarial Networks (GAN)
     - A GAN is a generative model that is trained using two neural network models by treating the unsupervised problem 
       as supervised and using both a generative and a discriminative model.
     - The generators role is to create synthetic outputs that closely resemble authentic data, often to the point of being 
       indistinguishable from real data.
     - The discriminators purpose is to determine which of the presented outputs are the result of artificial generation. 
       It is a binary classifier that assigns a probability score upon each data sample
          GAN Architecure:
              Real Image  ----> Sample   ------------------\
                                                            ------> Discrimator  --------> Real or Fake 
              Random Input ----> Generator ----> Sample ---/


      Unsupervised pretraining
        - used when you want to tackle a complex task for which you do not have much labeled training data
        - a model is trained on all data, including unlabeled data, using an unsupervised learning technique, then
          it is fine-tuned fo the final task on just the labeled data using a supervised learning technique
        - the unsupervised part may train one layer at at time or it may train the full model directly
        - can train an unsupervised model, sucha s autoencoder or Generative Adversarial Network (GAN), add the output
          layer for your task on the top, and fine-tune the final network using supervised learning



  Pretraining On Auxillary Tasks (pages 378 - 379)

    Pretraining On Auxillary Tasks 
      - used when do not have much labeled training data
      - train neural network on an auxiliary task
         - e.g. using pictures of random people on the web, train 1st neural network to detect whether two pictures
           feature the same person. This network would learn good feature detectors for faces
      - reuse lower layers from auxiliary task's network for your actual task
        - e.g. network to recognize faces

   Self-supervised learnin
     - when you automatically generate the labels from the data itself, then you train a model on the resulting 
       labeled dataset using supervised learning technique

Faster Optimizers (pages 379 - 388)
  Momentum (pages 379 - 381)

    Momumentum optimization
      - cares about previous gradient were at each interation
      - uses a moving average over the past gradients; where the gradient is high, weights updates will be large
      - uses Exponential Moving Average (EMA). It assigns greater weight on the more recent values
      - momumentum optimization will roll down the valley faster and faster till it reaches the bottom (optimum)
      - momumentum hyperparameters 'beta':
         - simulates friction mechanism and prevent the momemtum from growing too large
         - set between 0 (high friction) and 1 (no friction)
         - typical value is 0.9 usually works very well and almost always goes faster than regular gradient descent


    Equation 11-5. Momentum Algorithm

        1.  m    <--  Beta * m  - eta * grad-theta * J(theta)
        2. theta <-- theta + m

        where: 
            m:      momentum vector
            Beta:   momuntum hyperparameter
            eta:    Learning rate
            theta:  weight matrix
            grad-theta:  gradient with respect to theta (weights)


    Code: Functions to Build, Compile, and train model 

        >>> # extra code - a little function to test an optimizer on Fashion MNIST
        >>>
        >>> def build_model(seed=42):
        >>>     tf.random.set_seed(seed)
        >>>     return tf.keras.Sequential([
        >>>         tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>         tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>         tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>         tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
        >>>         tf.keras.layers.Dense(10, activation="softmax")
        >>>     ])
        >>> 
        >>> def build_and_train_model(optimizer):
        >>>     model = build_model()
        >>>     model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>>     return model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))


    keras.optimizer.SGD: 
      momentum: float hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations. 
                0 is vanilla gradient descent. Defaults to 0.0.

    Code: Build and train model using SGD using momentum optimizer hyperparameter

        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
        >>> history_momentum = build_and_train_model(optimizer)  # extra code



   
  Nesterov Accelerated Gradient (pages 381 - 382)

    Nesterov Accelerated Gradient (NAG) or Nesterov momentum:
      - measures thee gradient of the cost function not at the local position theta but slightly ahead in the
        direction of the momentum, at:  theta + beta * m
      - NAG is significantly faster than regular momemtum optimization

    Equation 11-6. Nesterov Accelerated Gradient Algorithm

        1.  m    <--  Beta * m  - eta * grad-theta * J(theta + Beta * m)
        2. theta <-- theta + m

        where: 
            m:      momentum vector
            Beta:   momuntum hyperparameter
            eta:    Learning rate
            theta:  weight matrix
            grad-theta:  gradient with respect to theta (weights)

    keras.optimizer.SGD: 
      nesterov: boolean. Whether to apply Nesterov momentum. Defaults to False

    Code: Build and train model using SGD using momentum optimizer hyperparameter plus NAG enabled

        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)
        >>> history_nesterov = build_and_train_model(optimizer)  # extra code

  AdaGrad (pages 382 - 383)

    AdaGrad;
      - the algorithm decays the learning rate, but it does int faster for steep dimensions that for dimensions
        with gentle slopes
      - first step (equation 11-6): square of the gradients into a vector 's'
      - 2nd step: the gradient vector scaled down by tha factor of (s + epsilon)**-1/2 


    Equation 11-7. AdaGrad Algorithm

        1.  s    <--  s  + grad-theta * J(theta)  _e-wise-mult_  grad-theta * J(theta)   
        2. theta <--  theta - eta * grad-theta  _ew-div_ (s + epsilon)**-1/2

        where: 
            s:      acculated square of the gradients into a vector 's'
            eta:    Learning rate
            theta:  weight matrix
            grad-theta:  gradient with respect to theta (weights)
            _ew-mult_:   represents element-wise multiplication 
            _ew-div_:   represents element-wise division 
            epsilon: smoothing term: tiny number that avoids division by zero and ensures gradient doesn't grow too large

    AdaGrad results:
      - performs well for simple quadratic problems, but often stops too early when training neural networks the
        learning rate gets scaled down so much that the algorithm ends up stopping before reaching the global optimum
      - Although Keras has Adagrad optimier, you should NOT use it to train deep neural networks

    Code: Build and train model using AdaGrad optimizer 

        >>> optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)
        >>> history_adagrad = build_and_train_model(optimizer)  # extra code

  RMSProp (pages 383 - 384)

    RMSProp:
      - fixes issue with AdaProp often stopping too early when the learning rate gets scaled down so much that the 
         algorithm ends up stopping before reaching the global optimum
      - fixes by accumulating only the gradients from the recent iterations as opposed to all the gradients by
        using an exponential decay, 'rho', in the first step
      - 'rho' decay rate is typicall set to 0.9 - this default works well so you do not need to tune 

    Equation 11-7. AdaGrad Algorithm

        1.  s    <--  rho * s + (1 - rho) *  grad-theta * J(theta)  _e-wise-mult_  grad-theta * J(theta)   
        2. theta <--  theta - eta * grad-theta  _ew-div_ (s + epsilon)**-1/2

        where: 
            rho:    decay rate
            s:      acculated square of the gradients into a vector 's'
            eta:    Learning rate
            theta:  weight matrix
            grad-theta:  gradient with respect to theta (weights)
            _ew-mult_:   represents element-wise multiplication 
            _ew-div_:   represents element-wise division 
            epsilon: smoothing term: tiny number that avoids division by zero and ensures gradient doesn't grow too large

    Code: Build and train model using AdaGrad optimizer 

        >>> optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
        >>> history_rmsprop = build_and_train_model(optimizer)  # extra code

    RMSProp results:
      - almost always performs better that AdaGrad
      - was the preferred optimization algorithm or many researchers until Adam optimization came around

  Adam (pages 384 - 385)

   Adaptive Momentum Estimation (AdaM) 
     - combines momentum optimization with RMSProp
        - just like momentum optimization, it keeps track of the exponentially decaying average of past gradients
        - just like RMSProp, it keeps track of exponentially decaying average of past squared gradients
        - these are estimationss of the mean and (uncentered) variance of the gradients
        hyperparameters:
          beta_1: 
             - momemtum decay; typically initialized to 0.9; 
             - corresponds to momentum optimization 'beta' hyperparameter
          beta_2: 
            - corresponds RMSProp 'rho' hyperparameter
            - typically initialized to 0.999
          eta (learning_rate): 
            - Adam is adaptive learning rate algorithm like AdaGrad and RMSProp, it requires less tuning of the learning
              rate hyperparameter, eta. 
              - you can often use the default value, eta = 0.001


    Code: Build and train model using AdaM optimizer 

        >>> optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        >>> history_adam = build_and_train_model(optimizer)  # extra code


  AdaMax (pages 385 - 386)

    AdaMax
      - AdaMax replaces the l2 norm in Adam with l-infinity nore (fancy way of saying the max)
      - this can make AdaMax more stable than Adam, but it really depends on the dataset
      - in General, AdaM performs better than AdaMax

    Code: Build and train model using AdaMax optimizer 

        >>> optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        >>> history_adamax = build_and_train_model(optimizer)  # extra code

  Nadam (pages 386 - 386)

    Nadam: 
      - is the AdaM optimization plus the Nesterov trick, so it will often converge slightly faster than Adam

    Code: Build and train model using Nadam optimizer 

        >>> optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        >>> history_nadam = build_and_train_model(optimizer)  # extra code

  AdamW (pages 386 - 386)

    AdamW
      - a AdaM variant that integrates 'weight decay' regularization technique
      weight decay:
        - reduces the size of the model's weights at each training iteration gy multiplying them bye a decay factor
          such as 0.99

    Code: Build and train model using AdamW optimizer 

        >>> optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-5, learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        >>> history_adamw = build_and_train_model(optimizer)  # extra code

    Adaptive optimization methods
      - including RMSProp, Adam, AdaMax, Nadaom, and AdamW
      - can lead to solutions to generalize poorly on some datasets
      - if so, try using NAG (Nesterov Accelerated Gradient or Nesterov momentum):
   
   first-order partial derivatives
     - all optimization techniques discussed so far are based on first-order partial derivatives
     - optimization literature also contains algorithms based on second-order partial derivatives. However, these are
       hard to apply to deep neural networks because the n**2 Hessians per output (where 'n' is then of parameters)

   Training Sparse Models
     - all optimization algorithms just discussed produce dense models, meaning that most parameters will be non-zero
     - if you need a blazingly fast model at runtime or you need it take up less memory, you may prefer to end up with 
       sparse model instead
     option to create sparse model:
       - train the model as usual, then get rid of the tiny weights (set them to zero). However, this will typically not lead
         to a very sparse model, and it may degrade the model's performance.
       - apply a strong l1 regularization during training, as it pushes the optimizer to zero out as many weights
         as it can (as discussed in the "Lasso Regression")
     another option to create sparse model:
       - check out the TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API capable of
         iteratively removing connections during training based on their magnitude


   Table 11-2 compares all the optimizers we've discussed so far (* is bad, ** is average, *** is good)

      Class                               Convergence speed        Convergence Quality
      --------------------------------    -----------------        --------------------
      SGD                                  bad                     good
      SGD(monentum=...)                    average                 good
      SGD(monentum=..., nesterov=True)     average                 good
      AdaGrad                              good                    bad (stops too early)
      RMSProp                              good                    average or good
      Adam                                 good                    average or good
      AdaMax                               good                    average or good
      Nadam                                good                    average or good
      AdamW                                good                    average or good

Learning Rate Schedules (pages 388 - 392)

  Learning Rate Overview
    - if set too high, training may diverge
    - if set too low, training will eventually converge, but it will take a long time
    - if set slightly too high, it may make progress very quickly at first, but it end up dancing around
      the optimum, and never really settling down

  Find a good learning rate
    - (from Chapter 10) by training the model for a few hundred iterations, expontentially increasing the
      learning rate from a very small value to a very large value, then looking at the learning curve and 
      picking a learning rate slightly lower than the one at which the learning curve starts shooting up
    Learning Curve
      - plots of the model's training error and validation error as a function of the training iteration

  Learning schedules:
    - strategies to reduce the learning rate during training
    Most common learning schedules:
      Power scheduling:
        - sets the learning rate to a function of the iteration number 't': eta(t) = eta0 / (1 + t/s)**c
          where: eta: learning rate
                 eta0: initial learning rate
                 s:    step
                 c:    power (typically 1)

        - After 's' steps, the learning rate is down to eta0/2, after 2 * s steps  is down to eta0/3, 
          after 3 * s step is down to eta0/4,  ....

      
      Code: Power scheduling:
        Note: The InverseTimeDecay scheduler uses learning_rate = initial_learning_rate / (1 + decay_rate * step / decay_steps). 
              If you set staircase=True, then it replaces step / decay_step with floor(step / decay_step).

        >>> lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
        >>>     initial_learning_rate=0.01,
        >>>     decay_steps=10_000,
        >>>     decay_rate=1.0,
        >>>     staircase=False
        >>> )
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
        >>> history_power_scheduling = build_and_train_model(optimizer)  # extra code


      Expontential scheduling:
        - set the learning rate to eta(t) = eta0 * 0.1**t/s
        - the learning rate will gradually drop by a factor of 10 for every 's' steps

      Code: Expontentially Scheduling:
        Note: learning_rate = initial_learning_rate * decay_rate ** (step / decay_steps)

        >>> lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        >>>     initial_learning_rate=0.01,
        >>>     decay_steps=20_000,
        >>>     decay_rate=0.1,
        >>>     staircase=False
        >>> )
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
        >>> history_exponential_scheduling = build_and_train_model(optimizer)  


      Piecewise constant scheduling
         - use a constant learning rate for a number of epochs (e.g. eta0 = 0.1 for 5 epochs), then smaller learning
           rate for another number of epochs (e.g. eta1 = 0.001), and so on
         - can work well, but requires fiddling with learning rates and number of epochs for each

      Code: Piecewise Constant Scheduling:

        >>> lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
        >>>     boundaries=[50_000, 80_000],
        >>>     values=[0.01, 0.005, 0.001]
        >>> )
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
        >>> history_piecewise_scheduling = build_and_train_model(optimizer) 

      Performance scheduling:
        - measure validation error every 'N' steps (just like early stopping), and reducing learning rate by a 
          factor of 'lambda' when error stops dropping

      Code: Performance Scheduling:

        >>> lr0 = 0.01
        >>> n_epochs = 25
        >>> model = build_model()
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>> 
        >>> lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
        >>> history = model.fit(X_train, y_train, epochs=n_epochs, validation_data=(X_valid, y_valid), callbacks=[lr_scheduler])

      1cycle scheduling:
        - starts by increasing the initial learning rate eta0 growing linearly up to eta1 halfway through training
        - then, it decreases the learning rate linearly down to eta0 again during 2nd half of training
        - finishing the last few epochs by dropping the rated down by several orders of magnitude (still linearly)
        - the maximum learning rate eta1 is chosen using the same approach we used to find the optimal learning rate
          and the initial learning rate is usually 10 times lower

    Best learning schedule:
      - according to a 2013 paper comparing popular learning schedules using momentum optimization to train DNN
        for speech recognition, 'performance scheduling' and 'expontial scheduling' performed well. They 
        perferred 'exponential scheduling' because it easier to tune and converged slightly faster

Avoiding Overfitting Through Regularization (pages 392 - 400)

  Deep Neural Network Need for regularization
    - DNN have tens of thousands of parameters, sometimes even millions
    - this gives DNN great flexibility, but also makes then prone to overfitting
    - Regularizaiton is often need to prevent overfitting
   
   Regularization techniques previously covered:
     - early stopping
     - batch normalization
        - designed to solve unstable gradient problems, but also acts like a good regularizer

  l1 and l2 Regularization (pages 393 - 394)

     l2 regularization
       - to constrain a neural networkss connection weights

      Code: dense layer with l2 regularization 

        >>> layer = tf.keras.layers.Dense(100, activation="relu",
        >>>                               kernel_initializer="he_normal",
        >>>                               kernel_regularizer=tf.keras.regularizers.l2(0.01))

     l2 regularization
       - to help create a sparse model (with many weights equal to 0)

      Code: dense layer with l1 regularization 

        >>> layer = tf.keras.layers.Dense(100, activation="relu",
        >>>                               kernel_initializer="he_normal",
        >>>                               kernel_regularizer=tf.keras.regularizers.l1(0.01))
 
     l1 and l2 regularization
        -> use 'kernel_regularizer=tf.keras.regularizers.l1_l2()'


   Regularizing all dense layers
      - typically you want to apply the same regularizer to all the layers in your network as well as the 
        using the same activation function and same initialization

      Code: Using l2 regularization, 'relu' activation, & 'he_normal' initialization on all dense layers

        >>> from functools import partial
        >>> 
        >>> RegularizedDense = partial(tf.keras.layers.Dense,
        >>>                            activation="relu",
        >>>                            kernel_initializer="he_normal",
        >>>                            kernel_regularizer=tf.keras.regularizers.l2(0.01))
        >>> 
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     RegularizedDense(100),
        >>>     RegularizedDense(100),
        >>>     RegularizedDense(10, activation="softmax")
        >>> ])
        >>> 
        >>> # extra code - compile and train the model
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>> history = model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))

  Dropout (pages 394 - 397)

    Dropout
      - one of the most popular regularization techniques for DNN
      - for state of the art neural networks, it gives 1% - 2% accuracy boost
      technique
        - at every training step, every neuron (including input but excluding output neurons) has a probability
          of 'rho' of being temporarily 'dropped out' meaning it will be entirely ignored during this training step,
          but it may be activated during the next step
        - after training, neurons don't get dropped
        - using dropout makes neurons less sensitive to slight changes in the inputs
      'rho' hyperparameter 
         - called 'dropout rate'
         - typically set between 10% and 50%; closer to 20% - 30% in recurrent neural networks; and closer to 40% - 50%
           for convolutional neural networks
       In practice:
         - usually apply dropout only to the neurons in the top one to three layers (excluding output layer)
         - need to divided the connection weights by the 'keep probability' (1 - rho) during training
       keras:
         - uses 'tf.keras.layers.Dropout()' layer
           - during training it randomly drops some inputs (setting them to 0) and divides the remaining inputs by
             the 'keep probability' (1 - rho)
       training loss vs validation loss
          - since dropout is only active during training, comparing the training loss and validation loss can be misleading
          - make sure to evaluate training loss without dropout (e.g. after training)
       overfitting
         - if model is overfitting, you can increase the dropout rate
       unfitting
         - if model is unfitting, you can decrease the dropout rate
       large layers vs small layers
         - increase dropout rate for large layers and reduce it for small layers
       performance / convergence
         - dropout does tend to significantly slow down convergence, but it often results in a better model when
           tuned properly. So, it is generally well worth the extra time and effort especially for large models
       SELU activation function
         - if you want to regularize a self-normalizing network based on the SELU activation function, 
           you should use 'alpha dropout'
           - 'alpha dropout' preserves the mean and std deviation for the inputs

      Code: Using Dropout regularization

        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=[28, 28]),
        >>>     tf.keras.layers.Dropout(rate=0.2),
        >>>     tf.keras.layers.Dense(100, activation="relu",
        >>>                           kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dropout(rate=0.2),
        >>>     tf.keras.layers.Dense(100, activation="relu",
        >>>                           kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dropout(rate=0.2),
        >>>     tf.keras.layers.Dense(10, activation="softmax")
        >>> ])
        >>> 
        >>> 
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
        >>> history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))

        >>> # with dropout, evaluate training loss without dropout (after training)
        >>> model.evaluate(X_train, y_train)
        >>> model.evaluate(X_test, y_test)

  Monte Carlo (MC) Dropout (pages 397 - 399)

    MC Dropout
      - can boost the performance of any trained 'dropout' model without having to retrain it
      - make multiple predictions with dropout enabled, then average the prediction
      - MC dropout tends to improve the reliability of the model's probability estimates
      MC samples hyperparameter
        - (100 samples in below example)
        - the higher the more accurate the predictions until some point where you will notice little improvement
        - doubling it, doubles the inference time
      MC dropout with other layers that behave in special way during training
        - should not force training mode on other layers that act in special way during training like BatchNormailation
        - instead create special class, e.g. 'MCDropout' to call tf.keras.layers.Dropout()' with training enabled

      Code: Improving Predictions using MC Dropout regularization to improve 'dropout' model
         Note: Code makes 100 predictions for each of 10K test instances, averages the predictions

        >>> y_probas = np.stack([model(X_test, training=True)
        >>>                      for sample in range(100)])
        >>> y_proba = y_probas.mean(axis=0)

      Code: Using MC Dropout regularization via 'MCDropout' class 

        >>> class MCDropout(tf.keras.layers.Dropout):
        >>>     def call(self, inputs, training=None):
        >>>         return super().call(inputs, training=True)
        >>> 
        >>> # extra code - shows how to convert Dropout to MCDropout in a Sequential model
        >>> Dropout = tf.keras.layers.Dropout
        >>> mc_model = tf.keras.Sequential([
        >>>     MCDropout(layer.rate) if isinstance(layer, Dropout) else layer
        >>>     for layer in model.layers
        >>> ])
        >>> mc_model.set_weights(model.get_weights())
        >>> mc_model.summary()

  Max-Norm Regularization (pages 399 - 400)

    Max-Norm Regularization
      - for each neuron, it contains the weights 'w' of the incoming connection such that ||w||2 =< r
           where: 
               r:        max-norm hyperparameter
               ||.||2:   is the l2 norm

       - does not a regularization loss term to the overall loss function
       - Instead, it typically implemented by computing ||w||2 after each training step, and rescaling 'w' if needed
          (w <-  w * r / ||w||2)
       - can also help alleviate the unstable gradient problems
      'r' hyperparameter
        - reducing 'r' increase the amount of regularization and helps reduce overfitting
       
      Code: Using Max-Norm regularization

        >>> dense = tf.keras.layers.Dense( 100, activation="relu", kernel_initializer="he_normal",
        >>>     kernel_constraint=tf.keras.constraints.max_norm(1.))

Summary and Practical Guide (pages 400 - 402)

    Table 11-3. Default DNN configuration

    Hyperparameter              Default value
    -----------------           --------------
    Kernel initializer          He initialization
    Activation Function         ReLU if shallow, Swish if deep
    Normalization               None if shallow, batch norm if deep
    Regularization              Early stopping, weight decay if needed
    Optimizater                 Nesterov accelerated gradients or AdamW
    Learning rate scheduler     Performance scheduling or 1cycle


    Self-normalize
      - if the network is a simple stack of dense layers, then it can self-normalize, and should use the configuration
        Table 11-4 instead.

    Table 11-4. DNN configuration for self-normalizing net

    Hyperparameter              Default value
    -----------------           --------------
    Kernel initializer          LeCun initialization
    Activation Function         SELU 
    Normalization               None (self-normalization)
    Regularization              Alpha Dropout if needed
    Optimizater                 Nesterov accelerated gradients 
    Learning rate scheduler     Performance scheduling or 1cycle



Chapter 11 Exercises:

    -> see exercise_notebooks/11_exercises.ipynb


------------------------------------------------------
Chapter 12 Custom Models and Training with Tensorflow
------------------------------------------------------

A Quick Tour of TensorFlow (pages 403 - 407)

  TensorFlow:
    - a powerful library for numerical computations, particularly well suited and fine-tuned for large-scale machine
      learning (but you can use it for anything that requires heavy computations).

  TensorFlow core feature summary:
    - core is very similar to Numpy, but with GPU support
    - compulation graph analysis: extracts the computational graph from a python function, optimizes it, and running
      it efficiently 
    - portable: computation graphs can be exported in a portable format, so you can train a TensorFlow model in one
      environment (e.g. python on Linux), and run it on another (e.g. using Java on Android)
    - autodiff: implements reverse-mode autodiff and provides excellent optimizers, such as RMSProp and Nadam

  TensorFlow additional features built on the core features:
    - keras, data loading and preprocessing ops (tf.data, tf.io, etc), image processing (tf.image), signal processing
      ops (tf.signal), and more

  Figure 12-1 TensorFlow's Python API (page 405)

  TensorFlow OS support 
    - runs on Windows, Linux, maxOS
    - runs on mobile devices (using TensorFlow lite) including iOS and Andriod
  TensorFlow programming language support
    - in addition to phthon API, there are C++, Java, Swift APIs
    - Javascript implementation caled 'TensorFlow.js' that makes it possible to run your models directly 
      in your Browser

  TensorFlow ecosystem includes
    - TensorBoard for visualization
    - TensorFlow Extended (TFX) (https://tensorflow.org/tfx) 
      - a set of libraries built by Google to productionize TensorFlow projects: it includes tools for data validation,
        preprocessing, model analysis, and serving (with TF Serving)
    - TensorFlow Hub provides a way to easily download and resuse pretrained neural networks
       - TensorFlow Model Garden (https://github.com/tensorflow/models): 
         - You can get may neural networks architectures some of the pretrained 
     - TensorFlow Resources (https://tensorflow.org/resources): 
     - TensorFlow Projects (https://github.com/jtoy/awesome-tensorflow)
     - technical questions support: 
         at: https://stackoverflow.com  tag your question with 'tensorflow' and 'python'
     - file bugs: https://github.com/tensorflow/tensorflow
     - general discussions: https://discuss.tensorflow.orig

Using TensorFlow like NumPy (pages 407 - 412)

  Tensors:
    - TensorFlow API revolve around tensors which flow from operation to operation
    - a tensor is similar to a Numpy ndarray; it is usually a multidimensional array, but it 
      can also hold a scalar (simple value such as 42)

  Tensors and Operations (pages 407 - 407)

     Tensor flow operations:
       - create a tensor with 'tensor.constant()'
       - 'shape' and 'dtype' work on tensors
       - indexing works much like Numpy: 
          e.g. t[:, 1:],   t[..., 1, tf.newaxis]
          Note: '...' is equivalent to ':', i.e.:  t[:, 1, tf.newaxis]
       - math operations include: tf.add(), tf.multiply(), tf.square(), tf.exp(), tf.sqrt(), 
       - Numpy similar operations include: tf.reshape(), tf.squeeze(), tf.tile()
       - some tensor operation functions have different names from Numpy including:
         tf.reduce_mean(), tf.reduce_sum(), tf_reduce_max, tf.math.log() are generally equivalent to
         np.mean(),        np.sum(),        np.max(),      np,log()
       - tf.transpose(t) functions similar to numpy t.T, but tensor creates a new tensor 

     Code: Create a tensor
        >>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix
        >>> t
            <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
            array([[1., 2., 3.],
                   [4., 5., 6.]], dtype=float32)>
        >>> t.shape
            TensorShape([2, 3])
        >>> t.dtype
            tf.float32

        >>> t[:, 1:]
            <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
            array([[2., 3.],
                   [5., 6.]], dtype=float32)>`
        >>> t[..., 1, tf.newaxis]
            <tf.Tensor: shape=(2, 1), dtype=float32, numpy=
            array([[2.],
                   [5.]], dtype=float32)>

     Code: Create a tensor scaler:
        >>> tf.constant(42)
            <tf.Tensor: shape=(), dtype=int32, numpy=42>

  Tensors and NumPy (pages 407 - 409)

    Tensors play nice with Numpy
      - you create a tensor for Numpy array, and visa versa
      - you apply TensorFlow operations to Numpy arrays and Numpy operations to tensors
      Note: Numpy uses 64-bit precisions by default, and TensorFlow using 32-bit precision
            by default (because 32-bit precision is generally more than enough for neural networks)

     Code: Convert between tensor and Numpy
        >>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix
        >>> import numpy as np
        >>> a = np.array([2., 4., 5.])
        >>> tf.constant(a)
            <tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>

        >>> t.numpy()
            array([[1., 2., 3.],
                   [4., 5., 6.]], dtype=float32)

        >>> np.array(t)
            array([[1., 2., 3.],
                   [4., 5., 6.]], dtype=float32)

        >>> tf.square(a)
            <tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>

        >>> np.square(t) 
            array([[ 1.,  4.,  9.],
                   [16., 25., 36.]], dtype=float32)

  Type Conversions (pages 409 - 410)

    Type Conversions
      - tensorFlow does NOT type conversions automatically; it just raises and exception 
        if you try to execute an tensor operation on incompatible types (i.e. 32-bit and 64-bit)
      - if you really need to convert types, uses tf.cast(<tensor>, <type>)
        
     Code: try to add 32-bit tensor to 64-bit tensor
        >>> try:
        >>>     tf.constant(2.0) + tf.constant(40., dtype=tf.float64)
        >>> except tf.errors.InvalidArgumentError as ex:
        >>>     print(ex)
            cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: 

     Code: using 'tf.cast' to add 32-bit tensor to 64-bit tensor
        >>> t2 = tf.constant(40., dtype=tf.float64)
        >>> tf.constant(2.0) + tf.cast(t2, tf.float32)
            <tf.Tensor: shape=(), dtype=float32, numpy=42.0>

  Variables (pages 410 - 410)
    Tensor variables:
      - tf.constant() create immutable values
      - for mutable values, create with 'tf.variable()'
      'tf.variable()' 
         - can perform the same operations with it, plays nice with Numpy, and just as picky on types
         - can be modified using 'assign()' method (or 'assign_add()' or 'assign_sub()')
         - can modify individual cels (or slices) using the cell's assign() method or by using the
           'scatter_update()' or 'scatter_nd_update()' methods
              scatter_nd_ update(): Scatter updates into an existing tensor according to indices.
              scatter_update(): applies sparse update to a variable reference 

     Code:  Create a tensor variable and used assign to update it

        >>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])
        >>> v
            <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
            array([[1., 2., 3.],
                   [4., 5., 6.]], dtype=float32)>
        >>> v.assign(2 * v)
            <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
            array([[ 2.,  4.,  6.],
                   [ 8., 10., 12.]], dtype=float32)>
        >>> v[:, 2].assign([0., 1.])
            <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
            array([[ 2., 42.,  0.],
                   [ 8., 10.,  1.]], dtype=float32)>

     Code:  Update a tensor variable and using scatter_nd_update() and scatter_update()

        >>> v.scatter_nd_update(
        >>>     indices=[[0, 0], [1, 2]], updates=[100., 200.])
            <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
            array([[100.,  42.,   0.],
                   [  8.,  10., 200.]], dtype=float32)>
        >>> sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],
        >>>                                 indices=[1, 0])
        >>> v.scatter_update(sparse_delta)
            <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
            array([[4., 5., 6.],
                   [1., 2., 3.]], dtype=float32)>

  Other Data Structures (pages 410 - 412)

    Other Tensor Datastructures includes:
      Spare tensors (tf.SparseTensors)
        - efficiently represented tensors containing mostly zeros
        - the tf.sparse package contains operations for sparse tensors
      Tensor Arrays (tf.TensorArrays)
        - Are a lists of tensors. They all have fixed lengths by default, but can optionally be made
          extensible
      Ragged tensors (tf.RaggedTensors)
        - represent lists of tensors, all of the same rank (dimensions) and data type, but varying sizes
        - the tf.ragged package contains operations for ragged tensors
      String tensors (tf.string)
        - are regular tensors of the type tf.string
        - the represent 'byte strings', not Unicode strings
        - the tf.string package contains operations for byte strings and Unicode strings (convert one into the other)
      Sets:
        - Are represented as regular tensors (or sparse arrays).
        - for example, tf.constant([1,2],[3,4]]) represents two sets [1,2] and [3,4]. More generally, each set 
          is represented by a vector in the tensor's last axis
        - the tf.sets package contains operations for maniplating sets
      Queues
        - offers various queues: FIFOQueue, PriortyQueue, RandomShuffleQueue, PaddingFIFOQueue
        - these classes are in the  tf.queue package 

Customing Models and Training Algorithms (pages 412 - 433)

  Custom Loss Functions (pages 412 - 413)
    Custom Loss Function:
      - if your dataset does not fit available loss functions, you can create a custom loss function
      - to implement, create a function that takes the labels and the model's predictionss as arguments, and 
        uses TensorFlow operations to compute a tensor containing all the losses (one per sample)

    Code: Using custom 'huber' loss function 
        Note: Keras does provide Huber loss function (tf.keras.losses.Huber class)

        >>> def huber_fn(y_true, y_pred):
        >>>     error = y_true - y_pred
        >>>     is_small_error = tf.abs(error) < 1
        >>>     squared_loss = tf.square(error) / 2
        >>>     linear_loss  = tf.abs(error) - 0.5
        >>>     # where returns 'squared_loss' if 'is_small_error' is True, else 'linear_loss' 
        >>>     return tf.where(is_small_error, squared_loss, linear_loss)
        >>> 
        >>> input_shape = X_train.shape[1:]
        >>> 
        >>> tf.keras.utils.set_random_seed(42)
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
        >>>                           input_shape=input_shape),
        >>>     tf.keras.layers.Dense(1),
        >>> ])
        >>> 
        >>> model.compile(loss=huber_fn, optimizer="nadam", metrics=["mae"])
        >>> 
        >>> model.fit(X_train_scaled, y_train, epochs=2,
        >>>           validation_data=(X_valid_scaled, y_valid))

  Saving and Loading Models That Contain Custom Components (pages 413 - 415)

    Saving and Loading Model with custom loss function
       - saving model with custom loss fuction (e.g. model.save("<saveModelName>" ) works fine 
       - when loading model, you will need to provide a dictionary that maps the custom (e.g. loss) 
         function name to the actual (loss) function via "custom_objects" argument

    Code:  Save and load model with custom loss function

        >>> # save model
        >>> model.save("my_model_with_a_custom_loss")  # extra code - saving works fine 
        >>>
        >>> # load model
        >>> model = tf.keras.models.load_model("my_model_with_a_custom_loss",
        >>>                        custom_objects={"huber_fn": huber_fn})
        >>>
        >>> # train load model with custom loss function
        >>> model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))


     Configured loss function:
       - if you want a custom threshold (instead -1 to 1), one solution is to create a configurable 
         loss function 
       - However, configurable threshold will not be saved, so you will need to specify the threshold
         when loading the model (see below code)

    Code:  Create a configured loss function, save model, and load model specifying the configured threhold

        >>> # create configured loss function with configurable threshold
        >>> def create_huber(threshold=1.0):
        >>>     def huber_fn(y_true, y_pred):
        >>>         error = y_true - y_pred
        >>>         is_small_error = tf.abs(error) < threshold
        >>>         squared_loss = tf.square(error) / 2
        >>>         linear_loss  = threshold * tf.abs(error) - threshold ** 2 / 2
        >>>         return tf.where(is_small_error, squared_loss, linear_loss)
        >>>     return huber_fn
        >>> 
        >>> # specify configured loss function with model.compile()
        >>> model.compile(loss=create_huber(2.0), optimizer="nadam", metrics=["mae"])
        >>> 
        >>> model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
        >>> 
        >>> # save model
        >>> model.save("my_model_with_a_custom_loss_threshold_2")
        >>> 
        >>> load model with configured loss function specifying loss function with custom _objects
        >>> model = tf.keras.models.load_model("my_model_with_a_custom_loss_threshold_2",
        >>>                                    custom_objects={"huber_fn": create_huber(2.0)})
        >>> 
        >>> model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))


     Configured loss function via creating custom subclass of 'tf.kearas.losses.Loss' class:
       - see example code on page 414
       - pass losss configurable parameter (e.g. 'threshold') to __init__() constructor 
       - need to implement 'get_config()' method to return a dictionary mapping each hyperparameter name
         to its value (in example, adding 'threshold' to parameters)
       - implement 'call()' method to take labels and predictions, and compute all the instance losses 

  Custom Activation functions, Initializers, Regularizers, and Contraints (pages 415 - 416)

    Custom Keras functionalities
      - Most of Keras functionalities, such as losses, regularizers, constraints, initializers, metrics,
        activation functions, layers, and even full models can be customized in same way as the
        previous section showed for loss functions
      - most of the time, you will just need to write a simple function with the appropriate inputs
        and outputs
      - in the below example: 
          - the activation function will be applied to the output of this Dense layer, and its results
            will be passed on the to the next layer
          - the layer's weights will be initialized by the value returned by the initializer
          - at each training step the weights will be passed the regularization function to compute
            the regularization loss, which will be added to the main loss to get the final loss for training
          - the constraint function will be called after each training step, and teh layer's weights
            will be replaced by the constrained weights


    Code:  Custom activation, initializer, l1 regularizer, and (weight) contraint function examples

        >>> def my_softplus(z):
        >>>     return tf.math.log(1.0 + tf.exp(z))
        >>> 
        >>> def my_glorot_initializer(shape, dtype=tf.float32):
        >>>     stddev = tf.sqrt(2. / (shape[0] + shape[1]))
        >>>     return tf.random.normal(shape, stddev=stddev, dtype=dtype)
        >>> 
        >>> def my_l1_regularizer(weights):
        >>>     return tf.reduce_sum(tf.abs(0.01 * weights))
        >>> 
        >>> def my_positive_weights(weights):  # return value is just tf.nn.relu(weights)
        >>>     return tf.where(weights < 0., tf.zeros_like(weights), weights)
        >>>
        >>> # using custom functions in dense layer:
        >>> layer = tf.keras.layers.Dense(1, activation=my_softplus,
        >>>                               kernel_initializer=my_glorot_initializer,
        >>>                               kernel_regularizer=my_l1_regularizer,
        >>>                               kernel_constraint=my_positive_weights)
 
  Custom Metrics (pages 416 - 419)

    Streaming Metrics (or statefull metrics)
      - metrics are gradually updated batch after batch
      - that is, it metric value computed over all the batches, not just the value for the last batch 


     Code: Precision Streaming metric 

        >>> # call keras Precision() metric function and directly passing in the labels and predictions
        >>> precision = tf.keras.metrics.Precision()
        >>> 
        >>> # pass in 4 TP & 1 FP - precision(<labels>, <predictions>)
        >>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
        >>> <tf.Tensor: shape=(), dtype=float32, numpy=0.8>
        >>> 
        >>> # pass in 0 TP & 3 FP - precision result is 50% (4 TP, and 4 FP over 2 batches)
        >>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
        >>> <tf.Tensor: shape=(), dtype=float32, numpy=0.5>
        >>> 
        >>> precision.result()
        >>> <tf.Tensor: shape=(), dtype=float32, numpy=0.5>
        >>> 
        >>> precision.variables
        >>> [<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,
        >>>  <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]
        >>> 
        >>> # reset precision state
        >>> precision.reset_states()


    Custom streaming metric 
      - create a subclass of the 'tf.keras.metrics.Metric' class
      - In subclass constructor use 'add_weight()' method to add variables needed to track of the metric's
        state over multiple batches
      - subclass'es update_state() method is called when you use an instance of the this class as a 
        function. It updates the variables, given the labels and predictions for one batch
      - subclass'es 'result()' method computes and returns the final results
        - when you use the metric as a function, the 'update_state()' method gets called first, then
          the 'result()' method is called and the output is returned.
      - subclass'es 'get_config()' method is used to save an variable that need to be saved with
        the model (e.g. threshold)
      - subclass'es 'reset_states()' method default implementation resets are variables to 0.0, but
        you can override it

     Code: Implement 'HuberMetric' class as a Custom Streaming metric 

        >>> class HuberMetric(tf.keras.metrics.Metric):
        >>>     def __init__(self, threshold=1.0, **kwargs):
        >>>         super().__init__(**kwargs)  # handles base args (e.g., dtype)
        >>>         self.threshold = threshold
        >>>         self.huber_fn = create_huber(threshold)
        >>>         self.total = self.add_weight("total", initializer="zeros")
        >>>         self.count = self.add_weight("count", initializer="zeros")
        >>> 
        >>>     def update_state(self, y_true, y_pred, sample_weight=None):
        >>>         sample_metrics = self.huber_fn(y_true, y_pred)
        >>>         self.total.assign_add(tf.reduce_sum(sample_metrics))
        >>>         self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
        >>> 
        >>>     def result(self):
        >>>         return self.total / self.count
        >>> 
        >>>     def get_config(self):
        >>>         base_config = super().get_config()
        >>>         return {**base_config, "threshold": self.threshold}

     Code: Simplier implementation of 'HuberMetric' class as a Custom Streaming metric 

        >>> class HuberMetric(tf.keras.metrics.Mean):
        >>>     def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):
        >>>         self.threshold = threshold
        >>>         self.huber_fn = create_huber(threshold)
        >>>         super().__init__(name=name, dtype=dtype)
        >>> 
        >>>     def update_state(self, y_true, y_pred, sample_weight=None):
        >>>         metric = self.huber_fn(y_true, y_pred)
        >>>         super(HuberMetric, self).update_state(metric, sample_weight)
        >>> 
        >>>     def get_config(self):
        >>>         base_config = super().get_config()
        >>>         return {**base_config, "threshold": self.threshold}

     Custom metric using a simple function vs Custom streaming metric (or stateful metric)
      - keras automatically calls simple metric function for each batch, and it keeps track of the mean 
        during each epoch
      - The only benefit of our HuberMetric class (subclass of tf.keras.metrics.Metric') is that
        the 'threshold' (custom) variable will be saved
      - some metrics like 'precision' can not be simply averaged over batches; in those cases, there is
        no option that to implement a stream metric

  Custom Layers (pages 419 - 422)

    Lambda custom layer 'tf.keras.layers.Lambda'
      - used to wrap arbitrary expressions as a layer
      - if you want create a custom layer without any weights, the simpliest option is to write
        a function and wrap it in a 'tf.keras.layers.Lambda' layer

     Code: Create an exponential layer by wrapping exp() in a  Lambda layer 

        Note: Adding an exponential layer at the output of a regression model can be useful if the values to predict 
              are positive and with very different scales (e.g., 0.001, 10., 10000).
        Note: Exponential function is one of the standard activation functions, so you can just use
              activation='exponential'

        >>> exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))
        >>> 
        >>> tf.keras.utils.set_random_seed(42)
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Dense(30, activation="relu", input_shape=input_shape),
        >>>     tf.keras.layers.Dense(1),
        >>>     exponential_layer
        >>> ])
        >>> model.compile(loss="mse", optimizer="sgd")
        >>> model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))
        >>> model.evaluate(X_test_scaled, y_test)


     Custom Stateful Layer (layer with weights)
       - create a subclass of the 'tf.keras.layers.Layer' 
       - its constructor takes all the hyperparameter arguments, and calls the parent constructor, 
         which takes care of the standard arguments such as input_shape, trainable, and name
           - converts activation argument the activation function (with ' tf.keras.activations.get')
       - its build() method is create the layer's variables byt calling the 'add_weight()' method 
          - at end of the 'build()' method, you must call the parent's 'build*) method which just sets
            self.built=True [not shown in example?]
       - its 'call()' method performs the desired operation (in example, it performs matrix multiplication)
       - its 'get_config()' method is used to save configuration (variable) dat


     Code: Create a simplified version of the 'Dense' layer as a custom layer

        >>> class MyDense(tf.keras.layers.Layer):
        >>>     def __init__(self, units, activation=None, **kwargs):
        >>>         super().__init__(**kwargs)
        >>>         self.units = units
        >>>         self.activation = tf.keras.activations.get(activation)
        >>> 
        >>>     def build(self, batch_input_shape):
        >>>         self.kernel = self.add_weight(
        >>>             name="kernel", shape=[batch_input_shape[-1], self.units],
        >>>             initializer="he_normal")
        >>>         self.bias = self.add_weight(
        >>>             name="bias", shape=[self.units], initializer="zeros")
        >>> 
        >>>     def call(self, X):
        >>>         return self.activation(X @ self.kernel + self.bias)
        >>> 
        >>>     def get_config(self):
        >>>         base_config = super().get_config()
        >>>         return {**base_config, "units": self.units,
        >>>                 "activation": tf.keras.activations.serialize(self.activation)}
        >>> # extra code - shows that a custom layer can be used normally
        >>> tf.keras.utils.set_random_seed(42)
        >>> model = tf.keras.Sequential([
        >>>     MyDense(30, activation="relu", input_shape=input_shape),
        >>>     MyDense(1)
        >>> ])
        >>> model.compile(loss="mse", optimizer="nadam")
        >>> model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
        >>> model.evaluate(X_test_scaled, y_test)


     Custom layer with different behavior during training and during testing
       - e.g. if it uses Dropout or BatchNormalization
       - must add a training argument to the 'call()' method, and use this method to decide what to do

     Code: Create custom layer that adds Gaussin noise (for regularization) during training, but not during testing
        Note: 'tf.keras.layers.Layer.GaussianNoise' layer already does this

        >>> class MyGaussianNoise(tf.keras.layers.Layer):
        >>>     def __init__(self, stddev, **kwargs):
        >>>         super().__init__(**kwargs)
        >>>         self.stddev = stddev
        >>> 
        >>>     def call(self, X, training=None):
        >>>         if training:
        >>>             noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
        >>>             return X + noise
        >>>         else:
        >>>             return X
        >>>
        >>> # extra code - tests MyGaussianNoise
        >>> tf.keras.utils.set_random_seed(42)
        >>> model = tf.keras.Sequential([
        >>>     MyGaussianNoise(stddev=1.0, input_shape=input_shape),
        >>>     tf.keras.layers.Dense(30, activation="relu",
        >>>                           kernel_initializer="he_normal"),
        >>>     tf.keras.layers.Dense(1)
        >>> ])
        >>>
        >>> model.compile(loss="mse", optimizer="nadam")
        >>> model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
        >>> model.evaluate(X_test_scaled, y_test)

  Custom Models (pages 422 - 424)


     Code: Create 'ResidualBlock' custom block with multiple layers
         Notes:
             - contains: n_dense layers
             - use 'call()' methods to connect all ResidualBlock inputs to the 'ResidualBlock' outputs
               as example of custom functionality that could be implemented

        >>> class ResidualBlock(tf.keras.layers.Layer):
        >>>     def __init__(self, n_layers, n_neurons, **kwargs):
        >>>         super().__init__(**kwargs)
        >>>         self.hidden = [tf.keras.layers.Dense(n_neurons, activation="relu", kernel_initializer="he_normal")
        >>>                        for _ in range(n_layers)]
        >>> 
        >>>     def call(self, inputs):
        >>>         Z = inputs
        >>>         for layer in self.hidden:
        >>>             Z = layer(Z)
        >>>         return inputs + Z
        >>> 

     Custom Models:
       - create a subclass of 'tf.keras.Model'
       - create the layers in the constructor
       - use layers in the 'call()' method
       - if you want to save the model with 'save()' and load the model 'tf.keras.models.load_model()', 
         you implement 'get_config()' method to save the configuration data (in both 'ResidualBlock'
         and 'ResidualRegressor'.

     Code: Create 'ResidualRegressor' custom model which instantiates the custom 'ResidualBlock' 

        >>> class ResidualRegressor(tf.keras.Model):
        >>>     def __init__(self, output_dim, **kwargs):
        >>>         super().__init__(**kwargs)
        >>>         self.hidden1 = tf.keras.layers.Dense(30, activation="relu",
        >>>                                              kernel_initializer="he_normal")
        >>>         self.block1 = ResidualBlock(2, 30)
        >>>         self.block2 = ResidualBlock(2, 30)
        >>>         self.out = tf.keras.layers.Dense(output_dim)
        >>> 
        >>>     def call(self, inputs):
        >>>         Z = self.hidden1(inputs)
        >>>         for _ in range(1 + 3):
        >>>             Z = self.block1(Z)
        >>>         Z = self.block2(Z)
        >>>         return self.out(Z)


  Losses and Metrics Based on Model Internals (pages 424 - 426)

    Defining custom losses based on model internals
      - the previous custom losses and metrics solutions were based on the labels and the predictions 
        (and optionally weights)
      - However, there are cases where you need to define custom loss based on model internals, compute
        it based on any part of the model you want, then pass the result to the 'add_loss()' method
      - example case: custom regression MLP composed of stack of 5 hidden layers, plus output layer. 
        It is also includes a auxilary output on top of the upper hidden layer. 
        The auxilary output will have a 'reconstruction loss' (mean squared difference between the 
        'reconstruction' and the inputs).
        Reconstruction loss will be added to the main loss.

     Code: Custom model with 'reconstruction loss' calculation using custom internals (e.g. 'inputs') 

        >>> class ReconstructingRegressor(tf.keras.Model):
        >>>     def __init__(self, output_dim, **kwargs):
        >>>         super().__init__(**kwargs)
        >>>         self.hidden = [tf.keras.layers.Dense(30, activation="relu",
        >>>                                              kernel_initializer="he_normal")
        >>>                        for _ in range(5)]
        >>>         self.out = tf.keras.layers.Dense(output_dim)
        >>>         self.reconstruction_mean = tf.keras.metrics.Mean(
        >>>             name="reconstruction_error")
        >>> 
        >>>     def build(self, batch_input_shape):
        >>>         n_inputs = batch_input_shape[-1]
        >>>         self.reconstruct = tf.keras.layers.Dense(n_inputs)
        >>> 
        >>>     def call(self, inputs, training=None):
        >>>         Z = inputs
        >>>         for layer in self.hidden:
        >>>             Z = layer(Z)
        >>>         reconstruction = self.reconstruct(Z)
        >>>         recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        >>>         self.add_loss(0.05 * recon_loss)
        >>>         if training:
        >>>             result = self.reconstruction_mean(recon_loss)
        >>>             self.add_metric(result)
        >>>         return self.out(Z)
        >>> 
        >>> tf.keras.utils.set_random_seed(42)
        >>> model = ReconstructingRegressor(1)
        >>> model.compile(loss="mse", optimizer="nadam")
        >>> history = model.fit(X_train_scaled, y_train, epochs=5)
        >>> y_pred = model.predict(X_test_scaled)


    Model Class
      - is a subclass of the 'Layer' class, so models can be used exactly like layers
      - Model have some additional functionalities (than layers), including of course compile(), fit(), evaluate(),
        and predict() methods, plus 'get_layer()' method and 'save()' method
      - layers should subclass the 'Layer' class and models should subclass the 'Model()' class

  Computing Gradients Using Autodiff (pages 426 - 430)

    tf.GradientTape example:
      - create a tf.GradientTable context that will automatically record every operation that involves
        a variable
      - then, have tape of the result of 'z' with regards to both variables [w1, w2] 


    Code: Reverse-model autodiff GradientTape calculation for f(w1,w2) = 3*w1**2  + 2*w1*w2:
          at w1 = 5.0, w2 = 3.0

        >>> def f(w1, w2):
        >>>     return 3 * w1 ** 2 + 2 * w1 * w2
        >>> 
        >>> w1, w2 = tf.Variable(5.), tf.Variable(3.)
        >>> with tf.GradientTape() as tape:
        >>>     z = f(w1, w2)
        >>> gradients = tape.gradient(z, [w1, w2])
        >>>
        >>> gradients
            [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,
             <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]
        

    GradientTape by default is not presistent
      - tape is automatically erased immediately after you call 'gradient()', so you will get
        an exception if you call 'gradient()' twice
      - if you need call 'gradient()' more than once, you must make the tape persistent and delete
        each time you are done with it

    Code: Using persistent 'gradient()' and cleaning up

        >>> with tf.GradientTape(persistent=True) as tape:
        >>>     z = f(w1, w2)
        >>> 
        >>> dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0
        >>> dz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!
        >>> # delete 'tape' with done with it
        >>> del tape
        >>> dz_dw1, dz_dw2
            (<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,
             <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)

     
    GradientTape/tape tacking operations
      - 'tape' will only track operations involving variables
      - if you need to compute teh gradient of 'z' with regards to anything other than variables, 
        the result will be 'None'
      - you can force tape to watch any tensors you like, to record every operations that involves
        them. You can then compute the gradient with regards to these tensors (e.g. inputs), as
        if they were variables

   
     Code:

        >>> # gradients returns 'None' for non-tf.variables() (e.g. tf.constants())
        >>> c1, c2 = tf.constant(5.), tf.constant(3.)
        >>> with tf.GradientTape() as tape:
        >>>     z = f(c1, c2)
        >>> 
        >>> gradients = tape.gradient(z, [c1, c2])
        >>> 
        >>> gradients
            [None, None]
        >>> 
        >>> # use 'tape.watch()' to force 'tape' to track c1 & c2 constants
        >>> with tf.GradientTape() as tape:
        >>>     tape.watch(c1)
        >>>     tape.watch(c2)
        >>>     z = f(c1, c2)
        >>> 
        >>> gradients = tape.gradient(z, [c1, c2])
        >>> gradients
            [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,
             <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]

     Gradient of a vector
       - if you try to compute the gradient of a vector, for example a vector containing multiple losses,
         then tensorFlow will compute the gradients of a vector's sum
       - if need to get the individual gradients (e.g. gradients of each loss with regard to the model parameters),
         you must call the tape's 'jacobian()' method: it will perform reverse-mode autodiff once for each 
         loss in the vector (all in parallel by default)

      Code: tape.gradient() computes the gradient of the vector's sum 

        >>> def f(w1, w2):
        >>>     return 3 * w1 ** 2 + 2 * w1 * w2
        >>> w1, w2 = 5, 3
        >>>
        >>> # extra code - if given a vector, tape.gradient() will compute the gradient of the vector's sum.
        >>> with tf.GradientTape() as tape:
        >>>     z1 = f(w1, w2 + 2.)
        >>>     z2 = f(w1, w2 + 5.)
        >>>     z3 = f(w1, w2 + 7.)
        >>> 
        >>> tape.gradient([z1, z2, z3], [w1, w2])
            [<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,
             <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]
        >>> 
        >>> # extra code - shows that we get the same result as the previous cell
        >>> with tf.GradientTape() as tape:
        >>>     z1 = f(w1, w2 + 2.)
        >>>     z2 = f(w1, w2 + 5.)
        >>>     z3 = f(w1, w2 + 7.)
        >>>     z = z1 + z2 + z3
        >>> 
        >>> tape.gradient(z, [w1, w2])
            [<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,
             <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]

  Custom Training Loops (pages 430 - 433)

     Custom loop training
       - used when the 'fit()' method may not be flexible enough for what you need to do
         - example: Wide & Deep uses two different optimizers: one for the wide path and the other for the deep path
       - Unless you really need the extra flexibility, you should use the 'fit()' method

     Custom loop training implementation:
        - two nested loops: one for the epochs, and one for the batches
        - in batch loop:
          -  call 'random_batch()' function to sample a random batch of instances
          - in 'tf.GradientTape()' block
            - call model() with current batch instances to make predictions
            - compute the loss which is equal to the main_loss and plus the other losses
               - in this model, the is one regularization loss per layer
               - since 'mean_square_error()' returns one loss per instance, tf.reduce_mean() is used to 
                 compute the loss over the batch
               - sum the main_loss and other losses using tf.add_n()
            - use 'tape' to compute gradient loss with regard to each trainable variable
            - apply the gradient losses to the optimizer (optimizer.apply_gradient())
           - update the loss metrics and print them in the status bar
         - reset metrics at end of each epoch

      Code: Custom loop for wide and deep example

        >>> tf.keras.utils.set_random_seed(42)  # extra code - to ensure reproducibility
        >>>  
        >>>  # define Sequential model - 2 dense layers (one output) 
        >>> l2_reg = tf.keras.regularizers.l2(0.05)
        >>> model = tf.keras.models.Sequential([
        >>>     tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal", kernel_regularizer=l2_reg),
        >>>     tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)
        >>> ])
        >>> 
        >>> 
        >>> # define random_batch function to randomly sample batch instances
        >>> def random_batch(X, y, batch_size=32):
        >>>     idx = np.random.randint(len(X), size=batch_size)
        >>>     return X[idx], y[idx]
        >>> 
        >>> # define print_status_bar to print status bar while running
        >>> def print_status_bar(step, total, loss, metrics=None):
        >>>     metrics = " - ".join([f"{m.name}: {m.result():.4f}"
        >>>                           for m in [loss] + (metrics or [])])
        >>>     end = "" if step < total else "\n"
        >>>     print(f"\r{step}/{total} - " + metrics, end=en
        >>> 
        >>> # define hyperparameters, optimizer, loss function (MSE), and metrics (MAE)
        >>> n_epochs = 5
        >>> batch_size = 32
        >>> n_steps = len(X_train) // batch_size
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        >>> loss_fn = tf.keras.losses.mean_squared_error
        >>> mean_loss = tf.keras.metrics.Mean()
        >>> metrics = [tf.keras.metrics.MeanAbsoluteError()]
        >>> 
        >>> # create custom loop (used instead of 'fit()')
        >>> # outer loop is epoch loop
        >>> for epoch in range(1, n_epochs + 1):
        >>>     print(f"Epoch {epoch}/{n_epochs}")
        >>>
        >>>     # inner loop is batch loop
        >>>     for step in range(1, n_steps + 1):
        >>>         X_batch, y_batch = random_batch(X_train_scaled, y_train)
        >>>         with tf.GradientTape() as tape:
        >>>             y_pred = model(X_batch, training=True)
        >>>             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
        >>>             loss = tf.add_n([main_loss] + model.losses)
        >>> 
        >>>         gradients = tape.gradient(loss, model.trainable_variables)
        >>>         optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        >>> 
        >>>         # extra code - if your model has variable constraints
        >>>         for variable in model.variables:
        >>>             if variable.constraint is not None:
        >>>                 variable.assign(variable.constraint(variable))
        >>> 
        >>>         mean_loss(loss)
        >>>         for metric in metrics:
        >>>             metric(y_batch, y_pred)
        >>> 
        >>>         print_status_bar(step, n_steps, mean_loss, metrics)
        >>> 
        >>>     for metric in [mean_loss] + metrics:
        >>>         metric.reset_states()

TensorFlow Functions and Graphs (pages 433 - 438)

  tf.function()
    - used to convert a python function to a TensorFlow function
    - can be used exactly like the original Python function, and will return the same results
      (but as a tensor)
    - under the hood, tf.function() analyzes the computations performed by the python function (e.g. cube())
      and generate and equivalent computation graph
    - the original python function is available by using <pyton_function>.python_function()
    - tensorFlow optimizes the computation graph, pruning unused nodes, simplifying expressions (e.g. 
       1 + 2 would get replaced with 3), and more
    - as a result, the TF function will usually run faster than the original python function

  Code: convert python function (cube) to TF function

        >>> # create a python function
        >>> def cube(x):
        >>>     return x ** 3
        >>> cube(2)
            8
        >>>
        >>> # convert python function to TF function
        >>> tf_cube = tf.function(cube)
        >>> tf_cube
            <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x24f21c2ac50>
        >>>
        >>> tf_cube(2)
            <tf.Tensor: shape=(), dtype=int32, numpy=8>
        >>>
        >>> tf_cube(tf.constant(2.0))
            <tf.Tensor: shape=(), dtype=float32, numpy=8.0>
  
  tf.function() decorator
    - alternatively, you decorator the python function with @tf.function to convert it to a TF function

  Code: convert python function (cube) to TF function using decorator 

        >>> # decorator equivalant:  tf_cube = tf.function (tf_cube)
        >>> @tf.function
        >>> def tf_cube(x):
        >>>     return x ** 3
        >>>
        >>> f_cube(2)
            <tf.Tensor: shape=(), dtype=int32, numpy=8>
        >>>
        >>> # original python function is still available:
        >>> tf_cube.python_function(2)
            8


  tf.function() with Accelerated Linear Algebra (XLA)
    - if you set jit_compile=True when calling tf.function(), then the TensorFlow will use Accelerated Linear
      Algrebra (XLA) to compile dedicated kernels for your graph, often fusing multiple operations
      - for example, XLA can compute tf.reduce_sum(a * b + c) in one step instead of 3 steps

   custom functions with kera models
     - when you write a custom loss function, a custom metric function, a custom layer, or any other custom
       function and use it with a Kera model, Keras automatically converts your function into a TF function
       (no need to use tf.function())
     - if you want Keras to use XLA, you need to just set jit_compile=True when call the compile() method
     - you can tell Keras not to convert your python functions to TF functions when creating a custom layer
       or custom model by setting 'dynamic=True' or set 'run_eagerly=True' when calling the model's compile() method

  AutoGraph and Tracing (pages 435 - 437)

    Tensorflow generate graphs
      - Autograph step: analyze the python function's source code to capture all the control statements (e.g. for loops, while loops,
        if statements, break, continue, & return)
      - after analyzing the function code, Autograph outputs and upgraded version of that function in which all
        control flow statements are replaced by the appropriate TensorFlow operations such as 'tf.while_loop()' for loops
        and 'tf.cond()' for if statements

    Tensorflow using graphs
      - tensorflow calls 'upgraded' function, but instead of passing the argument, it passes a 'symbolic tensor' - a tensor
        without any actual value, only a name, a data type, and shape
      graph mode
        - meaning that each tensorFlow operation will add a node in the graph to represent itself and its output tensor(s)
        - in graph mode, TF operations do not perform any computations
        - graph mode is the default mode
      eager execution, or eagar model 
        - regular mode (runs the original python code)
        
    Tensorflow viewing graphs
      - in order to view the generated function's soruce, you can call tf.autograph.to_code(<moduleName>.python_function)

  TF Function Rules (pages 437 - 438)

    tf.function:
      - converting a Python function that performs TensorFlow operations into a TF function usually just require
        decorating it with: '@tf.function'

    Converting to 'tf.function' rules:
      External libraries
        - external libraries including Numpy and std library will run only during tracing; they will not be part 
          of the graph
        - examples: use tf.reduce_sum() instead of 'np.sum()', tf.sort() instead of built-in 'sorted()' function, and so on

        random numbers
        - if you define a TF function f(x) that just returns 'np.random.rand()', a random number will be generated
          when the function is traced, so f(tf.constant(2.) and f(tf.constant(3.) will generate the same random number,
          but f(tf.constant([2., 3.])) will return a different number (because input shape is different)
          - to generate a random number each call, replace 'np.random.rand()' with tf.random.uniform([])
        code side effects
          - if your non-TensorFlow code has side-effects (e.g. logging, updating python counter), side effect will
            occur only when function is traced
        wrap python code in 'tf.py_function()'
          - you can wrap arbitrary Python code in a 'tf.py_function()' operation, but doing so will hinder performance

      Calling other Python Functions or TF functions
        - you can call other python functions or TF functions, but they should follow the same rules as TensorFlow will
          capture their operations in the computation graphs
        - other functions do NOT need to be decorated

      TensorFlow variables
        - if the function creates a TensorFlow variable (or dataset, queue, etc.), it must do so upoon the very first call 
        - it is preferable to variables outside of the TF function (e.g. in buld() method or custom layer
        - if you want to assign a new value to a variable, may sure you call its 'assign()' method instead of '=' operator

      Python Source code
        - source code of your python function should be available to tensorFlow
        - if source code is unavailable, then the graph generation process will fail or have limited functionality

      Vectorized implementations
        - for performance reasons, you should prefer a vectorized implementation whenever you can, rather than
          using loops

Chapter 12 Exercises (page 438):

    -> see exercise_notebooks/12_exercises.ipynb



------------------------------------------------------
Chapter 13 Loading and Preprocessing Data with TensorFlow
------------------------------------------------------

  tf.data
    - TensorFlow's own data loading and preprocessing API
    - reads from multiple files in parallel using multithreading and queuing, shuffling and batching samples, ...
    - let's you handle datasets that don't fit in memory
    - can read text files (e.g. CSV), binary files with fixed record sizes, and binary files that use TensorFlow's
      TFRecord format which supports varying record sizes
    - supports reading from SQL databases and open source extensions to from all sorts of data sources

   TFRecord
     - a flexible and efficient binary format usually containing protocol buffers (an open source binary format)
     https://www.kaggle.com/code/ryanholbrook/tfrecords-basics
       - A TFRecord is a binary file that contains sequences of byte-strings. Data needs to be serialized (encoded as a byte-string) 
         before being written into a TFRecord.

       tf.data:
       - you can use 'tf.data.Dataset.from_tensor_slices()' to create tensorFlow dataset from a python list, e.g.:
         >>> from tensorflow.data import Dataset, TFRecordDataset
         >>> from tensorflow.data.experimental import TFRecordWriter
         >>> # construct a small dataset
         >>> ds = Dataset.from_tensor_slices([b'abc', b'123'])
       - How do we write a dataset as a TFRecord? If your dataset is composed of byte-strings, you can use 'data.TFRecordWriter'. 
         To read it back again, use 'data.TFRecordsDataset'
         >>> # Write the dataset to a TFRecord
         >>> writer = TFRecordWriter(PATH)
         >>> writer.write(ds)
                 
         >>> # Read the dataset from the TFRecord
         >>> ds_2 = TFRecordDataset(PATH)
         >>> for x in ds_2:
         >>>     print(x)
             tf.Tensor(b'abc', shape=(), dtype=string)
             tf.Tensor(b'123', shape=(), dtype=string)

       Serialization:
       - You can use 'tf.io.serialize_tensor' to turn a tensor into a byte-string and 'tf.io.parse_tensor' to turn it back
         >>> # create simple tensor 
         >>> x = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)
         >>> print('x:', x, '\n')
             x: tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) 
             
         >>> # Serialize tensor
         >>> x_bytes = tf.io.serialize_tensor(x)
         >>> print('x_bytes:', x_bytes, '\n')
             x_bytes: tf.Tensor(b'\x08\x04\x12\x08\x12\x02\x08\x02\x12\x02\x08\x02"\x04\x01\x02\x03\x04', shape=(), dtype=string) 
             
         >>> # unserialize tensor
         >>> print('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))
             x: tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8)

       tf.Example:
       - The most convenient way of serializing data in TensorFlow is to wrap the data with 'tf.Example'. This is a record format 
         based on Google's protobufs but designed for TensorFlow. It's more or less like a dict with some type annotations.


   Preposing Layers
     - can be embedded in your model
     - can ingest raw data directly

The tf.data API (pages 442 - 453)

  tf.data.Dataset
    - 'tf.data' API revolves around the concept of a 'tf.data.Dataset'
    - represents a sequence of data items
    - datasets may contain tuples of tensors, or dictionary of name/tensor pairs, or even nested tuples and
      dictionary of tensors
    - When slicing a tuple, dictionary, or a nested structure, the dataset will only slice the tensors it
      contains while preserving the tuple/dictionarly structure
    Basic Mechanics (https://www.tensorflow.org/guide/data)
      - To create an input pipeline, you must start with a data source. 
      - For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or 
        tf.data.Dataset.from_tensor_slices(). 
      - Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use 
        tf.data.TFRecordDataset().

  from_tensors vs from_tensor_slices:
    from_tensors:
      -  combines the input and returns a dataset with a single element

      >>> t = tf.constant([[1, 2], [3, 4]])
      >>> ds = tf.data.Dataset.from_tensors(t)
      >>> [x for x in ds]
          [<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
           array([[1, 2],
                  [3, 4]], dtype=int32)>]

    from_tensor_slices:
      - Creates a Dataset whose elements are slices of the given tensors. Returns 'DatasetV2'
      - creates a dataset with a separate element for each row of the input tensor:

         >>> t = tf.constant([[1, 2], [3, 4]])
         >>> ds = tf.data.Dataset.from_tensor_slices(t)
         >>> [x for x in ds]
             [<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>,
              <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>]

  Code: create a dataset using 'tf.data.Dataset.from_tensor_slices()'
    
        >>> import tensorflow as tf
        >>> 
        >>> X = tf.range(10)  # any data tensor
        >>> dataset = tf.data.Dataset.from_tensor_slices(X)
        >>> dataset
            <_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor(0, shape=(), dtype=int32)
            tf.Tensor(1, shape=(), dtype=int32)
            . . .
            tf.Tensor(8, shape=(), dtype=int32)
            tf.Tensor(9, shape=(), dtype=int32)

  Code: create a nested dataset using 'tf.data.Dataset.from_tensor_slices()' 

        >>> X_nested = {"a": ([1, 2, 3], [4, 5, 6]), "b": [7, 8, 9]}
        >>> dataset = tf.data.Dataset.from_tensor_slices(X_nested)
        >>> for item in dataset:
        >>>     print(item)
            {'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}
            {'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}
            {'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}

  Chaining Transformations (pages 443 - 445)

    dataset transform methods
      - once you have a dataset, you can apply all sorts of transformations to it by calling transformation methods
      - each transformation method returns a new dataset
      Dataset.repeat ( count=None, name=None )
        - Repeats this dataset so each original value is seen count times
        - returns a new dataset that repeats the items in the original dataset
      Dataset.batch (batch_size, drop_remainder=False, num_parallel_calls=None, deterministic=None, name=None)
        - returns a new dataset that groups the items in the previous dataset into groups
        - Combines consecutive elements of this dataset into batches in the new dataset.
        - The components of the resulting element will have an additional outer dimension, which will be batch_size 
          (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly 
          and 'drop_remainder' argument is False)
      Dataset.map (map_func, num_parallel_calls=None, deterministic=None, name=None)
        - This transformation applies 'map_func' to each element of this dataset, and returns a new dataset containing 
          the transformed elements, in the same order as they appeared in the input
      Dataset.filter ( predicate, name=None )
        - Filters this dataset according to predicate.
      Dataset.take ( count=None, name=None )
        - Creates a Dataset with at most count elements from this dataset.


    Code: transform datasets using repeat() and batch()

        >>> dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))
        >>> dataset = dataset.repeat(3).batch(7)
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)
            tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)
            tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)
            tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)
            tf.Tensor([8 9], shape=(2,), dtype=int32)
            
    Code: transform datasets using map(), filter(), and take

        >>> dataset = dataset.map(lambda x: x * 2)  # x is a batch
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)
            tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)
            tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)
            tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)
            
        >>> dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)
        >>> for item in dataset:
        >>>     print(item)tf.Tensor([16 18], shape=(2,), dtype=int32)
            tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)
            tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)
            tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)
            
        >>> for item in dataset.take(2):
        >>>     print(item)
            tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)
            tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)


  Shuffling the Data (pages 445 - 446)

    Dataset.shuffle ( buffer_size, seed=None, reshuffle_each_iteration=None, name=None )
      - This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, 
        replacing the selected elements with new elements. 
      - For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.

     Dataset.shuffle() Note: 
       - This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, 
         replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or 
         equal to the full size of the dataset is required.

    Shuffle data by using multiple randomly filed files:
      - to handle the shuffle() large dataset limitation, split the source data into multiple files, and then randomly 
        and read them simultaneously, interleaving their records
      - then on top, you can add a shuffling buffer using the shuffle() method
       

    Code: create shuffling dataset buffer using  shuffle()

        >>> dataset = tf.data.Dataset.range(10).repeat(2)
        >>> dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)
            tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)
            tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)

  Interleaving Lines from Multiple Files (pages 446 - 448)

    Code: Load California dataset, then split it into a training set, a validation set and a test set:

        >>> # extra code - fetches, splits and normalizes the California housing dataset
        >>> 
        >>> from sklearn.datasets import fetch_california_housing
        >>> from sklearn.model_selection import train_test_split
        >>> 
        >>> housing = fetch_california_housing()
        >>> X_train_full, X_test, y_train_full, y_test = train_test_split(
        >>>     housing.data, housing.target.reshape(-1, 1), random_state=42)
        >>> X_train, X_valid, y_train, y_valid = train_test_split(
        >>>     X_train_full, y_train_full, random_state=42)

   Code: Split dataset into 20 training files, 10 validation files, and 10 test files

     For a very large dataset that does not fit in memory, you will typically want to split it into many files first, 
       then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing 
       dataset and saving it to 20 CSV files:
  
        >>> # extra code - split the dataset into 20 parts and save it to CSV files
        >>> 
        >>> import numpy as np
        >>> from pathlib import Path
        >>> 
        >>> def save_to_csv_files(data, name_prefix, header=None, n_parts=10):
        >>>     housing_dir = Path() / "datasets" / "housing"
        >>>     housing_dir.mkdir(parents=True, exist_ok=True)
        >>>     filename_format = "my_{}_{:02d}.csv"
        >>> 
        >>>     filepaths = []
        >>>     m = len(data)
        >>>     chunks = np.array_split(np.arange(m), n_parts)
        >>>     for file_idx, row_indices in enumerate(chunks):
        >>>         part_csv = housing_dir / filename_format.format(name_prefix, file_idx)
        >>>         filepaths.append(str(part_csv))
        >>>         with open(part_csv, "w") as f:
        >>>             if header is not None:
        >>>                 f.write(header)
        >>>                 f.write("\n")
        >>>             for row_idx in row_indices:
        >>>                 f.write(",".join([repr(col) for col in data[row_idx]]))
        >>>                 f.write("\n")
        >>>     return filepaths
        >>> 
        >>> train_data = np.c_[X_train, y_train]
        >>> valid_data = np.c_[X_valid, y_valid]
        >>> test_data = np.c_[X_test, y_test]
        >>> header_cols = housing.feature_names + ["MedianHouseValue"]
        >>> header = ",".join(header_cols)
        >>> 
        >>> train_filepaths = save_to_csv_files(train_data, "train", header, n_parts=20)
        >>> valid_filepaths = save_to_csv_files(valid_data, "valid", header, n_parts=10)
        >>> test_filepaths = save_to_csv_files(test_data, "test", header, n_parts=10)

    Dataset.list_files ( file_pattern, shuffle=None, seed=None, name=None)
      - A dataset of all files matching one or more glob patterns
      - by default, list_files() function returns a dataset that shuffles the filepaths
      - to disable filepaths shuffling, set argument 'shuffle=false'

    Dataset.interleave ( map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None, name=None)
      - Maps map_func across this dataset, and interleaves the results.
      - For example, you can use Dataset.interleave() to process many input files concurrently
      - for interleaving() to work best, it is preferable to have files of identical lengths, otherwise the end of 
        the longest file will not be interleaved
      - be default, interleave() does not use parallelism: it just reads one line at a time from each file, sequentially
      - to read files in parallel, set 'num_parallel_calls' argument to the number of threads you want to read, or set it
        to 'tf.data.AUTOLINE' to make TensorFlow choose the right number of threads based on the available CPUs

      - in the below example: 
        - the interleave() 'map_func' uses a 'lambda' function
        - interleave() method will create a dataset that will pull five filepaths from the filepath dataset, and
          for each one it will call the function you gave it (lambda function) to create a new dataset (in this case
          a 'TextLineDataset)


    Dataset.TextLineDataset ( filenames, compression_type=None, buffer_size=None, num_parallel_reads=None, name=None)
      - Creates a Dataset comprising lines from one or more text files
      - loads text from text files and creates a dataset where each line of the files becomes an element of the dataset.
      - it is possible to pass a list of filepaths to the TextLineDataset(), it will go through each file in order, line by line
        - if you set 'numb_parallel_reads' argument to a number greater than 1, the dataset will readd the number files
          in parallel and interleave their lines (without having to call interleave() method). However, it will NOT
          shuffle the files, not will it skip the header lines

    Code: create an input pipeline using list_files(), interleave(), and TestLineDataset()

        >>> filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)
        >>>
        >>> n_readers = 5
        >>> dataset = filepath_dataset.interleave(
        >>>     lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        >>>     cycle_length=n_readers)

  Preprocessing the Data (pages 448 - 449)

    tf.io.decode_csv (records, record_defaults, ...)
      - Convert CSV records to tensors. Each column maps to one tensor.
      - returns a list of scalar tensors (one per column)
      - takes two arguments: 
         records: line to parse; 
         record_defaults: an array containing the default values for each column in CSV (or default type)

    tf.stack ( values, axis=0, name='stack' )
      - Stacks a list of rank-R tensors into one rank-(R+1) tensor.
      - in example code, converts a list of scalars (from tf.io.decode_csv()) to a 1D tensor array

    Code: Preprocess data - calculate std deviation & mean, reads a CSV line, and scale it 

        >>> # extra code - use sklearn StandardScalar to compute the mean and standard deviation of each feature
        >>> 
        >>> from sklearn.preprocessing import StandardScaler
        >>> 
        >>> scaler = StandardScaler()
        >>> scaler.fit(X_train)
            
        >>> 
        >>> X_mean, X_std = scaler.mean_, scaler.scale_  # extra code
        >>> n_inputs = 8
        >>> 
        >>> def parse_csv_line(line):
        >>>     # create an array with default instance values plus target type (no value)
        >>>     defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
        >>>     # process CSV line and return a list of scalers
        >>>     fields = tf.io.decode_csv(line, record_defaults=defs)
        >>>     # 1D array for inputs (X) and 1D array for target  (one value)
        >>>     return tf.stack(fields[:-1]), tf.stack(fields[-1:])
        >>> 
        >>> def preprocess(line):
        >>>     # convert CSV line to x array and y array
        >>>     x, y = parse_csv_line(line)
        >>>     # return scaled x and return y
        >>>     return (x - X_mean) / X_std, y
            
        >>> # verify it works (note: b'....' used to convert string to byte string
        >>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')

  Putting Everything Together (pages 449 - 450)

    dataset.prefetch( buffer_size, name=None)
      - Creates a Dataset that prefetches elements from this dataset.
      - Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared 
        while the current element is being processed. This often improves latency and throughput, at the cost of using 
        additional memory to store prefetched elements.
      
    Code: Create function for loading preprocessing data from multiple CSV files (see figure 13-2 page 450) 

        >>> def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,
        >>>                        n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,
        >>>                        batch_size=32):
                # use list_files() to return dataset containing a shuffled filepath list
        >>>     dataset = tf.data.Dataset.list_files(filepaths, seed=seed)
        >>>
        >>>     # interleave() method will create a dataset that will pull 'n_readers' filepaths from the filepath dataset
                #   dataset using 'n_reader' parellel threads, and for each one it will call the function you gave it  
                #   (lambda function) to create a new dataset (in this case a 'TextLineDataset)
                # TestLineDataset() will creates a dataset comprise lines from 'n_reader' text files skipping the 1st line 
        >>>     dataset = dataset.interleave(
        >>>         lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        >>>         cycle_length=n_readers, num_parallel_calls=n_read_threads)
        >>>
        >>>     # map() uses preprocess() to transform (scale) the dataset
        >>>     # preproces fcn uses mean (X_mean) and std deviation (X_std) to scale a line of data 
        >>>     dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
        >>>    
        >>>     # use shuffle() to create a buffer of size 'shuffle_buffer_size' with randomly shuffled data 
        >>>     dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)
        >>>
        >>>     # uses batch() to return a new dataset that groups items by the batch size and use prefetch()
        >>>     #  to prefetch 1st batch
        >>>     return dataset.batch(batch_size).prefetch(1)

        >>> # extra code - show the first couple of batches produced by the dataset
        >>> 
        >>> example_set = csv_reader_dataset(train_filepaths, batch_size=3)
        >>> for X_batch, y_batch in example_set.take(2):
        >>>     print("X =", X_batch)
        >>>     print("y =", y_batch)
        >>>     print()

  Prefetching (pages 450 - 452)

    Using dataset.prefetch( buffer_size, name=None) 
      - it is best to always be one batch ahead
      - that is, while our training algorithm is working on one batch, the data will already be working in parallel
        on getting the next batch data ready
     
    dataset.cache()
      - if the dataset is small enough to fit in memory, you can significantly speed up training by using dataset.cache()
        to cache the contents to RAM
      - you should generally do this after loading and preprocessing the data, but before shuffling, repeating,
        batching, and prefetching. This way, each instance will only be read and preprocessed once (instead of once
        each epoch), but the data will still be shuffled differently each epoch, and the next batch will be prepared
        in advanced
  
    Code: Printout info on dataset class methods:

        >>> # extra code - list all methods of the tf.data.Dataset class
        >>> for m in dir(tf.data.Dataset):
        >>>     if not (m.startswith("_") or m.endswith("_")):
        >>>         func = getattr(tf.data.Dataset, m)
        >>>         if hasattr(func, "__doc__"):
        >>>             print(" {:21s}{}".format(m + "()", func.__doc__.split("\n")[0]))

      dataset class methods:
         apply()              Applies a transformation function to this dataset.
         as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.
         batch()              Combines consecutive elements of this dataset into batches.
         bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.
         cache()              Caches the elements in this dataset.
         cardinality()        Returns the cardinality of the dataset, if known.
         choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.
         concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.
         element_spec()       The type specification of an element of this dataset.
         enumerate()          Enumerates the elements of this dataset.
         filter()             Filters this dataset according to `predicate`.
         flat_map()           Maps `map_func` across this dataset and flattens the result.
         from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)
         from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.
         from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.
         get_single_element() Returns the single element of the `dataset`.
         group_by_window()    Groups windows of elements by key and reduces them.
         interleave()         Maps `map_func` across this dataset, and interleaves the results.
         list_files()         A dataset of all files matching one or more glob patterns.
         map()                Maps `map_func` across the elements of this dataset.
         options()            Returns the options for this dataset and its inputs.
         padded_batch()       Combines consecutive elements of this dataset into padded batches.
         prefetch()           Creates a `Dataset` that prefetches elements from this dataset.
         random()             Creates a `Dataset` of pseudorandom values.
         range()              Creates a `Dataset` of a step-separated range of values.
         reduce()             Reduces the input dataset to a single element.
         rejection_resample() A transformation that resamples a dataset to a target distribution.
         repeat()             Repeats this dataset so each original value is seen `count` times.
         sample_from_datasets()Samples elements at random from the datasets in `datasets`.
         scan()               A transformation that scans a function across an input dataset.
         shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.
         shuffle()            Randomly shuffles the elements of this dataset.
         skip()               Creates a `Dataset` that skips `count` elements from this dataset.
         snapshot()           API to persist the output of the input dataset.
         take()               Creates a `Dataset` with at most `count` elements from this dataset.
         take_while()         A transformation that stops dataset iteration based on a `predicate`.
         unbatch()            Splits elements of a dataset into multiple elements.
         unique()             A transformation that discards duplicate elements of a `Dataset`.
         window()             Returns a dataset of "windows".
         with_options()       Returns a new `tf.data.Dataset` with the given options set.
         zip()                Creates a `Dataset` by zipping together the given datasets.


  Using the Dataset with Keras (pages 452 - 453)

    Code: Use csv_reader_dataset() fcn (see above) to create prefetch model, then create model, train and
          evaluate model using train_set/valid_set/test_set prefetch objects 

        >>> # create prefetch objects to be passed to fit(), evaluate(), predict()
        >>> train_set = csv_reader_dataset(train_filepaths)
        >>> valid_set = csv_reader_dataset(valid_filepaths)
        >>> test_set = csv_reader_dataset(test_filepaths)
        >>> type(train_set)
            tensorflow.python.data.ops.prefetch_op._PrefetchDataset
        >>> 
        >>> # extra code - for reproducibility
        >>> tf.keras.backend.clear_session()
        >>> tf.random.set_seed(42)
        >>> 
        >>> # create sequential model to train
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
        >>>                           input_shape=X_train.shape[1:]),
        >>>     tf.keras.layers.Dense(1),
        >>> ])
        >>> model.compile(loss="mse", optimizer="sgd")
        >>> # train model - passing 'train_set' prefetch object as training data
        >>> model.fit(train_set, validation_data=valid_set, epochs=5)
        >>> 
        >>> # pass 'test_set' prefetch object to 'evaluate()' as test set input 
        >>> test_mse = model.evaluate(test_set)
        >>> new_set = test_set.take(3)  # pretend we have 3 new samples
        >>> y_pred = model.predict(new_set)  # or you could just pass a NumPy array

    Code: Using prefetch objects in custom training loop - get batch datasets from prefetch objects

        >>> # extra code - defines the optimizer and loss function for training
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        >>> loss_fn = tf.keras.losses.mean_squared_error
        >>> 
        >>> n_epochs = 5
        >>> for epoch in range(n_epochs):
        >>>
        >>>     # obtain batch datasets from 'train_set' prefetch object
        >>>     for X_batch, y_batch in train_set:
        >>>         # extra code - perform one Gradient Descent step
        >>>         #              as explained in Chapter 12
        >>>         print("\rEpoch {}/{}".format(epoch + 1, n_epochs), end="")
        >>>         with tf.GradientTape() as tape:
        >>>             y_pred = model(X_batch)
        >>>             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
        >>>             loss = tf.add_n([main_loss] + model.losses)
        >>>         gradients = tape.gradient(loss, model.trainable_variables)
        >>>         optimizer.apply_gradients(zip(gradients, model.trainable_variables))


    Code: Using prefetch objects in custom training loop implemented with TF function 

        >>> # create TF function
        >>> @tf.function
        >>> def train_one_epoch(model, optimizer, loss_fn, train_set):
        >>>     # obtain batch datasets from 'train_set' prefetch object
        >>>     for X_batch, y_batch in train_set:
        >>>         with tf.GradientTape() as tape:
        >>>             y_pred = model(X_batch)
        >>>             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
        >>>             loss = tf.add_n([main_loss] + model.losses)
        >>>         gradients = tape.gradient(loss, model.trainable_variables)
        >>>         optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        >>> 
        >>> # use TF function in custom training loop
        >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        >>> loss_fn = tf.keras.losses.mean_squared_error
        >>> for epoch in range(n_epochs):
        >>>     print("\rEpoch {}/{}".format(epoch + 1, n_epochs), end="")
        >>>     train_one_epoch(model, optimizer, loss_fn, train_set)


The TFRecord Format (pages 453 - 459)

  TFRecord Format:
    - TensorFlow's preferred format for storing large amount of data and reading it efficiently
    - simple binary format that contains a sequence of binary records of varying sizes
    - each record is comprised of a length, a length CRC checksum, then the actual data, and the CRC checksum for 
      the data
    - You can create a TFRecord using a tf.io.TFRecordWriter
    - you can read a TFRecord using a tf.data.TFRecordDataset

  tf.data.TFRecordDataset
    - by default, will read files one by one
    - you can read multiple files in parallel and interleave their records by passing the constructor a
      list of filepaths and setting 'num_parallel_reads' to a number greater than one
    - you can also read multiple files by using dataset.list_files() with dataset.interleave()  

    Code: Create a TFRecord using tf.io.TFRecordWriter

        >>> with tf.io.TFRecordWriter("my_data.tfrecord") as f:
        >>>     f.write(b"This is the first record")
        >>>     f.write(b"And this is the second record")

    Code: Read a TFRecord using tf.data.TFRecordDataset

        >>> filepaths = ["my_data.tfrecord"]
        >>> dataset = tf.data.TFRecordDataset(filepaths)
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor(b'This is the first record', shape=(), dtype=string)
            tf.Tensor(b'And this is the second record', shape=(), dtype=string)

    Code: Read multiple TFRecord files using tf.data.TFRecordDataset

        >>> # extra code - shows how to read multiple files in parallel and interleave them
        >>> 
        >>> # create 5 TFRecord files to read
        >>> filepaths = ["my_test_{}.tfrecord".format(i) for i in range(5)]
        >>> for i, filepath in enumerate(filepaths):
        >>>     with tf.io.TFRecordWriter(filepath) as f:
        >>>         for j in range(3):
        >>>             f.write("File {} record {}".format(i, j).encode("utf-8"))
        >>> 
        >>> # Read 3 TFRecord files in parallel 
        >>> dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)
        >>> for item in dataset:
        >>>     print(item)
            tf.Tensor(b'File 0 record 0', shape=(), dtype=string)
            tf.Tensor(b'File 1 record 0', shape=(), dtype=string)
            tf.Tensor(b'File 2 record 0', shape=(), dtype=string)
            . . .
            tf.Tensor(b'File 0 record 1', shape=(), dtype=string)
            tf.Tensor(b'File 4 record 2', shape=(), dtype=string)

  Compressed TFRecord Files (pages 454 - 454)

    Compressed TFRecords
      - it can sometimes be useful to compress your TFRecords, especially if they need to be loaded via network
        connection
      - create compressed TFRecord file by setting the tf.io.TFRecordOptions(compression_type="<compressionType>")
      - read compressed TFRecord file(s) by setting the 'tf.data.TFRecordDataset' "compression_type" argument 

    Code: Example code showing how create compressed TFRecord file and read compressed TFRecord

        >>> Create compressed TFRecord
        >>> options = tf.io.TFRecordOptions(compression_type="GZIP")
        >>> with tf.io.TFRecordWriter("my_compressed.tfrecord", options) as f:
        >>>     f.write(b"Compress, compress, compress!")
        >>> 
        >>> Read compressed TFRecord
        >>> dataset = tf.data.TFRecordDataset(["my_compressed.tfrecord"], compression_type="GZIP")


  A Brief Introduction to Protocol Buffers (pages 454 - 456)

    protocol buffers (protbufs)
      Overview
       - even though each record can use any binary format you want, TFRecord files usually containn serialized
        'protocol buffers' (also called 'protobufs).
       - protobufs is a portable, extensible, and efficient binary format developed by Google
       - protobufs are widely used, in particular in gRPC (https://grpc.io)
      Protobuf example definition:
         - use version 3 of the protobuf format ("proto3")
         - each "Person" object may optionally have a name of type string, and id of type int32, and 0 or more
           email fields, each of type string
         - the numbers 1, 2, and 3 are the field identifiers
      compiling protobuf definition
        - once you have a protobuf definition in a '.proto' file, you can compile it using 'protoc'
        - this requires 'protoc' the protobuf compiler to generate access classes in python (or other language)
      protobuf definitions
        - you will generally use in TensorFlow have already been compiled for you, and their classes are part
          of the TensorFlow library

    Code: Create an example protobuf definition .proto file

        >>> # create person protobuf .proto definition file for person
        >>> %%writefile person.proto
        >>> syntax = "proto3";
        >>> message Person {
        >>>     string name = 1;
        >>>     int32 id = 2;
        >>>     repeated string email = 3;
        >>> }
            
        >>> # compile person.proto protobuf definition
        >>> !protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports
        >>> 
        >>> # list created person protobuf files
        >>> %ls person*
            person.desc    person.proto   person_pb2.py
            
        >>> # import and use 'person_pb2' protobuff
        >>> from person_pb2 import Person  # import the generated access class
        >>> 
        >>> person = Person(name="Al", id=123, email=["a@b.com"])  # create a Person
        >>> print(person)  # display the Person
            name: "Al"
            id: 123
            email: "a@b.com"
            
        >>> person.name  # read a field
            'Al'
        >>> person.name = "Alice"  # modify a field
        >>> person.email[0]  # repeated fields can be accessed like arrays

        >>> # serialize person protobuff
        >>> serialized = person.SerializeToString()  # serialize person to a byte string
        >>> serialized
            b'\n\x05Alice\x10{\x1a\x07a@b.com\x1a\x07c@d.com'
            
        >>> person2 = Person()  # create a new Person
        >>> person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)
            27
        >>> person == person2  # now they are equal
            True

  TensorFlow Protobufs (pages 456 - 457)

    Example protobuf (tf.train.Example):
      - the main protobuf typically used in TFRecord file

    Definition of the tf.train.Example protobuf:

        syntax = "proto3";
        
        message BytesList { repeated bytes value = 1; }
        message FloatList { repeated float value = 1 [packed = true]; }
        message Int64List { repeated int64 value = 1 [packed = true]; }
        message Feature {
            oneof kind {
                BytesList bytes_list = 1;
                FloatList float_list = 2;
                Int64List int64_list = 3;
            }
        };
        message Features { map<string, Feature> feature = 1; };
        message Example { Features features = 1; };



    The tf.train.Feature message type 
       - Can accept one of the following three types (See the .proto file for reference). Most other generic types 
         can be coerced into one of these:
     
         tf.train.BytesList (the following types can be coerced): 
           string, byte
         tf.train.FloatList (the following types can be coerced):  
           float (float32), double (float64)
         tf.train.Int64List (the following types can be coerced): 
             bool, enum, int32, uint32, int64, uint64
           

    Conversion of current format (e.g CSV) to Example Protobuf to TFRecord
      - code below shows converiong person_example to tf.train.Example
      - next code shows serializing and writing out a TFRecord


    Code: create a 'tf.train.Example' representing the same person as earlier

        >>> from tensorflow.train import BytesList, FloatList, Int64List
        >>> from tensorflow.train import Feature, Features, Example
        >>> 
        >>> person_example = Example(
        >>>     features=Features(
        >>>         feature={
        >>>             "name": Feature(bytes_list=BytesList(value=[b"Alice"])),
        >>>             "id": Feature(int64_list=Int64List(value=[123])),
        >>>             "emails": Feature(bytes_list=BytesList(value=[b"a@b.com",
        >>>                                                           b"c@d.com"]))
        >>>         }))
  
    Code: Serialize and write 'my_contacts.tfrecord' TFRecord containing 5 'person_example' 

        >>> with tf.io.TFRecordWriter("my_contacts.tfrecord") as f:
        >>>     for _ in range(5):
        >>>         f.write(person_example.SerializeToString())



  Loading and Parsing Examples (pages 457 - 459)

    tf.data.TFRecordDataset
     - you can read a TFRecord using a tf.data.TFRecordDataset

    Dataset.map (map_func, num_parallel_calls=None, deterministic=None, name=None)
      - This transformation applies 'map_func' to each element of this dataset, and returns a new dataset containing 
        the transformed elements, in the same order as they appeared in the input

    tf.io.parse_single_example:
      - parses a single 'Example' protobuf
      - takes at least two arguments: a string scalar tensor containg the serialized data; and a description
        of each features
        Feature description (parse_single_example() arg 2):
          - the description is a dictionary that maps each feature name to either:
            tf.io.FixedLenFeature(<shape>, <dtype>, <default_value>=None)
              - indicates the features shape, type, and default value
              - fixed length features are parsed as regular tensors
            tf.io.VarLenFeature(<dtype>)
              - indicating only the type if the length of the feature's list may vary (e.g. email in example)
              - variable length features are parsed as sparse tensors


    Code: load 'my_contacts.tfrecord' to 'dataset' and print 'dataset' contents out 

        >>> feature_description = {
        >>>     "name": tf.io.FixedLenFeature([], tf.string, default_value=""),
        >>>     "id": tf.io.FixedLenFeature([], tf.int64, default_value=0),
        >>>     "emails": tf.io.VarLenFeature(tf.string),
        >>> }
        >>> 
        >>> def parse(serialized_example):
        >>>     return tf.io.parse_single_example(serialized_example, feature_description)
        >>> 
        >>> dataset = tf.data.TFRecordDataset(["my_contacts.tfrecord"]).map(parse)
        >>> for parsed_example in dataset:
        >>>     print(parsed_example)


    Code: Converting 'email' sparse tensor to dense tensor:

       >>> tf.sparse.to_dense(parsed_example["emails"], default_value=b"")
          <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>

    Code: directly accessing 'email' sparse tensor

      >>> parsed_example["emails"].values
          <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>

    tf.io.parse_example( <serialized>, <features>, ...)
       - Parses Example protos into a dict of tensors.
       - can be used to parse example protobuf batch by batch (with .batch(<numberOfExamples>)

    Code: using 'tf.io.parse_example()' to parse batch by batch  

        >>> def parse(serialized_examples):
        >>>     return tf.io.parse_example(serialized_examples, feature_description)
        >>> 
        >>> dataset = tf.data.TFRecordDataset(["my_contacts.tfrecord"]).batch(2).map(parse)
        >>> for parsed_examples in dataset:
        >>>     print(parsed_examples)  # two examples at a time

   Example protobuf 'ByteList'
     - can contain any binary data including serialized objects
     - can use 'tf.io.encode_jpeg()' to encode an image using JPEG and put this binary data in a ByteList
       and then you can use 'tf.io.decode_jpeg() to parse the data and get the original image
     - can call 'tf.io.decode_image()' to parse and decode an image using JPEG, BMP, GIF, or PNG images
     - can store any tensor you want in 'ByteList' by serializing the tensor using 'tf.io.serialize_tensor() then
       putting the resulting byte string in a Byte List feature

    Lists of Lists
      - Example protobuf is quite flexible and will probably be sufficient for most use cases
      - However, Example protobuf but it may be combersome to use when dealing with lists of list
      - use 'SequenceExample' for handling lists of lists

  Handling Lists of Lists Using the SequenceExample Protobuf (pages 459 - 459)

     SequenceExample:
       - contains a Features object for the contextual data and a FeatureLists object that contains one or more
         name FeatureList objects (e.g. a featureList name "content" and another name "comments")
       - each FeatureList contains a list of Feature objects each of which may be a list of byte strings, a list
         of 64-bit integers, or a list of floats
       - use tf.io.parse_single_sequence_example() to parse a single SequenceExample or tf.io.parse_sequence_example()
         to parse a batch

     SequenceExample Protobuf definition
        syntax = "proto3";
        
        message FeatureList { repeated Feature feature = 1; };
        message FeatureLists { map<string, FeatureList> feature_list = 1; };
        message SequenceExample {
            Features context = 1;
            FeatureLists feature_lists = 2;
        };



Keras Preprocessing Layers (pages 459 - 475)

    Preparing your data for Neural network :
      - typically requires normalizing the numerical features, encoding the categorical features and text, 
        cropping and resizing images, etc.

    Preprocessing options
      preprocessing ahead of time
        - using any tools you like, such as Numpy, Pandas, or Scikit-learn
      preprocessing on the fly using dataset's map method
        - preprocessing all input data on the fly using 'map()' method while loading it with 'tf.data()'
      preprocessing layers in your model
        - adding preprocessing mayers directly inside your model so it can preprocess all input data
          on the fly during training

  The Normalization Layers (pages 460 - 463)

    Normalization Layer
      - use to standardize inputs
      - can either specify the mean and variance of each feature when creating the layer OR
        pass the training set to the layer's 'adapt()' method so the layer can measure the features means
        and variance on its own
        Note: a few hundred randomly selected instances from training set will generally be sufficient
              to estimate the feature means and variances

     Code: Add Normalization to model and use it's 'adapt() to calculate mean & variance

          >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
          >>> norm_layer = tf.keras.layers.Normalization()
          >>> model = tf.keras.models.Sequential([
          >>>     norm_layer,
          >>>     tf.keras.layers.Dense(1)
          >>> ])
          >>> model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))
          >>> 
          >>> # adapt() method computes the mean and variance of every feature
          >>> norm_layer.adapt(X_train)  
          >>> model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)

     deploying Normalization layer options:
       Include Normalization layer in model:
         - straightforwaard, but it will slow down training (only slightly in the case of the Normailization layer)
         - preprocessing is performed on the fly during training only once per epoch
       Standalone Normalization layer:
         - speeds up training 
         - you can train a model on the scaled data, this time without the normalization layer
         - won't preprocess inputs when we deploy it in production
       Create a new model that wraps adapt (Standalone) Normalization layer with training model:
         - can deploy this final wrapped model to production 
         - it will take care of both preprocessing its imputs and making predictions
         

     Code: Use standalone Normalization layer to preprocess (scale) data, and then pass preprocessed data to the model

          >>> norm_layer = tf.keras.layers.Normalization()
          >>> # adapt() method computes the mean and variance of every feature
          >>> norm_layer.adapt(X_train)
          >>> X_train_scaled = norm_layer(X_train)
          >>> X_valid_scaled = norm_layer(X_valid)
          >>> 
          >>> # pass Normalization layer preprocessed data to the Sequential model
          >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
          >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
          >>> model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))
          >>> model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))

     Code:  Wrap standalone Normalization layer with model (without preprocessing)

          >>> final_model = tf.keras.Sequential([norm_layer, model])
          >>> X_new = X_test[:3]           # pretend we have a few new instances (unscaled)
          >>> y_pred = final_model(X_new)  # preprocesses the data and makes predictions
          >>> y_pred
              <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
              array([[0.97657156],
                     [1.6060407 ],
                     [2.3913486 ]], dtype=float32)>

     Code:  Preprocessing (scaling data) using 'map()' method with normalization layer

        >>> # extra code - creates a dataset to demo applying the norm_layer using map()
        >>> dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(5)
        >>> 
        >>> dataset = dataset.map(lambda X, y: (norm_layer(X), y))
        >>> 
        >>> list(dataset.take(1))  # extra code - shows the first batch

     Code: Implementing and using a Custom Normalization layer

        >>> # create a custom layer to normalize (scale) inputs
        >>> class MyNormalization(tf.keras.layers.Layer):
        >>>     def adapt(self, X):
        >>>         self.mean_ = np.mean(X, axis=0, keepdims=True)
        >>>         self.std_ = np.std(X, axis=0, keepdims=True)
        >>> 
        >>>     def call(self, inputs):
        >>>         eps = tf.keras.backend.epsilon()  # a small smoothing term
        >>>         return (inputs - self.mean_) / (self.std_ + eps)
        >>> 
        >>> # Use custom normalization layer
        >>> my_norm_layer = MyNormalization()
        >>> my_norm_layer.adapt(X_train)
        >>> X_train_scaled = my_norm_layer(X_train)

  The Discretization Layers (pages 463 - 463)

    Discretization Layer
      - transforms a numerical feature into a categorical feature by mapping value ranges (called bins)
        to categories
      - can specify specific ranges for each category
      - can specify the number of bins, call 'adapt()' to find approximate boundaries of the bins based on the
        value of the percentilies (e.g. 3 bins, then percentilies will be 33rd and 66th)

    Note: Categorical identifiers (e.g. from discretization layers) should be directly passed to neural networks,
          instead, they should be encoded (e.g using one-hot encoding)

    Code: Using Discretization layer to map age to 3 bins: less 18, 18 to less than 50, and 50 or over

        >>> age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])
        >>> discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])
        >>> age_categories = discretize_layer(age)
        >>> age_categories
            <tf.Tensor: shape=(6, 1), dtype=int64, numpy= array([[0], [2], [2], [1], [1], [0]], dtype=int64)>

    Code: Using Discretization layer to map age to 3 bins using 'adapt()' to calcuate the percentilies for bins

        >>> discretize_layer = tf.keras.layers.Discretization(num_bins=3)
        >>> discretize_layer.adapt(age)
        >>> age_categories = discretize_layer(age)
        >>> age_categories
            <tf.Tensor: shape=(6, 1), dtype=int64, numpy= array([[1], [2], [2], [1], [2], [0]], dtype=int64)>

  The CategoryEncoding Layers (pages 463 - 465)

    CategoryEncoding layer
      - when there are only a few options (e.g. less than a dozen or 2), then one-hot encoding is often a
        good option
      multi-hot encoding (default)
        - it will try to encode more than one categorical feature at a time where the output will contains a 1
          for each category present in any input feature
      output_mode="count"
        - set when it is useful to know how many times each category occurred
        - the output tensor will contain the number of occurences of each category
      one-hot encode each feature separately (num_tokens=<#ofFeatures> x <#ofNumOfTokensPerFeature>)
        - and concatenate the outputs


    Code: Using CategoryEncoding layer (on output from Discretization layer)

        >>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)
        >>> onehot_layer(age_categories)
            <tf.Tensor: shape=(6, 3), dtype=float32, numpy=
            array([[0., 1., 0.],
                   [0., 0., 1.],
                   [0., 0., 1.],
                   [0., 1., 0.],
                   [0., 0., 1.],
                   [1., 0., 0.]], dtype=float32)>

    Code: Using CategoryEncoding layer on 2 ages inputs with with  multi-hot encoding

        >>> two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])
        >>> onehot_layer(two_age_categories)
            <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
            array([[1., 1., 0.],
                   [0., 0., 1.],
                   [1., 0., 1.]], dtype=float32)>


    Code: Using CategoryEncoding layer on 2 ages inputs with output_mode="count"

        >>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3, output_mode="count")
        >>> onehot_layer(two_age_categories)
            <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
            array([[1., 1., 0.],
                   [0., 0., 2.],
                   [1., 0., 1.]], dtype=float32)>

  The StringLookup Layers (pages 465 - 466)

    StringLookup layer
      - used to convert categorical text features to encode as integers (default) or one-hot encodes
      - uses 'adapt()' method to find the distinct categories 
      - use 'output_mode="one_hot" to encode with one-hot vector for each category instead of an integers (default)
      - known categories are numbered starting at '1', from the most frequent category to the least frequent
      - Unknown categories by default (categories not present list passed to 'adapt()') are mapped to category '0'
      - Instead of mapping unknown categories to '0', you can specify the number Out-of-vocabulary (OOV) buckets to use, 
        with the num_oov_indices argument. Each unknown category will get mapped psuedorandomly to one of the OOV
        buckets allowing the model to distinguish at least some of the rare categories
        - OUV buckets numbers are from 0 to num_oov_indices (if num_oov_indices=5, then from 0 to 4)
      - hashing collisions: when multiple unknown categories get mapped to the same OOV due to insufficient OOV buckets
      StringLookup.adapt() method
        - Computes a vocabulary of integer terms from tokens in a dataset.
      StringLookup.vocabulary_size() method
      - returns The integer size of the vocabulary, including optional mask and oov indices. 
      - by default, vocabulary size includes 'category 0' for mapping unknown categories
      - total number of categories including known categories plus OOV buckets (one by default)


    Code: Using 'StringLookup' layer to map cities to integers
          Note: Montreal maps to category '0' since it was not in 'adapt()' input category list

        >>> cities = ["Auckland", "Paris", "Paris", "San Francisco"]
        >>> str_lookup_layer = tf.keras.layers.StringLookup()
        >>> # Call adapt() to map 'cities' list to integers
        >>> str_lookup_layer.adapt(cities)
        >>> str_lookup_layer([["Paris"], ["Auckland"], ["Auckland"], ["Montreal"]])
            <tf.Tensor: shape=(4, 1), dtype=int64, numpy= array([[1], [3], [3], [0]], dtype=int64)>


    Code: Using 'StringLookup' layer to map cities to integers and specifying Out-of-Vocabulary (OOV) buckets

        >>> cities = ["Auckland", "Paris", "Paris", "San Francisco"]
        >>> # specify 5 OOV buckets (the OOV buckets will be from 0 to 4 and known buckets will start at 5)
        >>> str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)
        >>> str_lookup_layer.adapt(cities)
        >>> # Note: in this example, both "Foo" and "Baz" were both mapped to OOV bucket 4
        >>> str_lookup_layer([["Paris"], ["Auckland"], ["Foo"], ["Bar"], ["Baz"]])
            <tf.Tensor: shape=(5, 1), dtype=int64, numpy= array([[5], [7], [4], [3], [4]], dtype=int64)>

    Code: Using 'StringLookup' layer to map cities to one-hots

        >>> cities = ["Auckland", "Paris", "Paris", "San Francisco"]
        >>> str_lookup_layer = tf.keras.layers.StringLookup(output_mode="one_hot")
        >>> str_lookup_layer.adapt(cities)
        >>> str_lookup_layer([["Paris"], ["Auckland"], ["Auckland"], ["Montreal"]])
            <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
            array([[0., 1., 0., 0.],
                   [0., 0., 0., 1.],
                   [0., 0., 0., 1.],
                   [1., 0., 0., 0.]], dtype=float32)>

  The Hashing Layers (pages 466 - 466)

    hashing layer
      - maps categories pseudorandomly to buckets, but stable across multipe runs as long as number of
        bin is unchanged
      - for each category, it computes a hash, modulo the number buckets (or 'bins')
      - benefit of the hashing layer is that it does not need to be adapted
      - however, it can have hashing collisions (as shown in example)
      - it usually preferable to stick with the 'StringLookup' layer

  Encoding Categorical Features Using Embedding (pages 466 - 471)

    Embedding
      - a dense representation of some higher-dimensional data, such as a category, or a word in a vocabulary
      - a 50k categories with one-hot encoding would produce 50k-dimensional sparse vectory. In contrast, an
        embedding would be comparably smale dense vector with ~100 dimensions
      - in deep learning, embedded are usually initialized randonly, and then they are trained with gradient 
        descent, along with other model parameters
      - since embedding are trainable, they will gradually improve during training.
      - training tends to make enbeddings useful representations of the categories - this is called representation learning
      representation learning
        - is a process in machine learning where algorithms extract meaningful patterns from raw data to create 
          representations that are easier to understand and process. 
      word embedding 
        - embeddings of individual words
        - word embeddings of similar words tend to be close, and some axes seem to encode meaningful concepts (Figure 13-7)
      embedding reuse
        - embeddings generally be useful representations fo the task at hand, but quite often these same embeddings
          can be reused successfully for other tasks
        - most common example of embedding reuse is word embedding
      - embeddings typically have 10 to 300 dimensions (not 2D as used in example), depending on the task,
        the vocabulary size, and the size of your training set

    Embedding Layers
      - wraps an embedding matrix
      - maps input information from a high-dimensional to a lower-dimensional space, allowing the network to learn more about 
        the relationship between inputs and to process the data more efficiently. 
      embedding matrix
        - this matrix has one row per category and one column per embedded dimension
      - by default, it is initialized randomly
      - since it is initialized randomly, it does not make sense to use it outside a model as a standalone 
        preprocesing layer unless you intialize it with pretrained weights
      - To convert a category ID to an embedding, the 'embedding layer' looks up and returns the row that
        corresponds to that category
      - if you want to embed a 'categorical text attribute', you can simple chain a StringLookup layer 
        with an Embedded layer (see example code)

     Emedding layer example description:
      https://www.baeldung.com/cs/neural-nets-embedding-layers
      - For example, in natural language processing (NLP), we often represent words and phrases as one-hot vectors, where each 
        dimension corresponds to a different word in the vocabulary. These vectors are high-dimensional and sparse, which makes 
        them difficult to work with.
      - Instead of using these high-dimensional vectors, the embedding layer could map each word to a low-dimensional vector, where 
        each dimension represents a particular feature of the word
      
     Types of Embedding Layers include:
       - Text embedding
          - example: ChatGPT which has a GPT-3 architecture consists of two main components: an encoder and a decoder. Of course, 
            the encoder part transforms an input text into an embedding vector, which the decoder part uses to generate an answer 
            and convert it back to a text output.
       - Image embedding
          - Image embedding is a technique for representing images as dense embedding vectors. These vectors capture some visual 
            features of the image, and we can use them for tasks such as image classification, object detection, and similar.
       - Graph embedding and others
          - Analogously as for images, we use graph embedding to represent graphs as dense embedding vectors in a lower-dimensional space.
       

    Code: Initialize an Embedded Layer with 5 rows and 2D embedding and use it to encode some categories

        >>> tf.random.set_seed(42)
        >>> embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)
        >>> embedding_layer(np.array([2, 4, 2]))
            <tf.Tensor: shape=(3, 2), dtype=float32, numpy=
            array([[-0.02855514, -0.02782702],
                   [ 0.00126302, -0.00207893],
                   [-0.02855514, -0.02782702]], dtype=float32)>

    Code: Embed Categorical text attribute by chaining a StringLookup layer with an Embedded layer

        >>> tf.random.set_seed(42)
        >>> ocean_prox = ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
        >>> # create and adapt() StringLookup layer
        >>> str_lookup_layer = tf.keras.layers.StringLookup()
        >>> str_lookup_layer.adapt(ocean_prox)
        >>>
        >>> lookup_and_embed = tf.keras.Sequential([
        >>>     tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),  # WORKAROUND to Keras bug
        >>>     str_lookup_layer,
        >>>     tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(), output_dim=2)
        >>> ])
        >>> lookup_and_embed(np.array(["<1H OCEAN", "ISLAND", "<1H OCEAN"]))
            <tf.Tensor: shape=(3, 2), dtype=float32, numpy=
            array([[ 0.02400447,  0.04597949],
                   [-0.04505685,  0.03960491],
                   [ 0.02400447,  0.04597949]], dtype=float32)>

        >>> str_lookup_layer.vocabulary_size()  # 5 categories in 'ocean_prox' dataset plus unknown category '0'
            6

    Code: Kera model that processes a categorical text feature along with regular (8) numerical features
          where 'categorical text feature' uses previous 'lookup_and_embedded' Sequential model to embed it

        >>> # extra code - set seeds and generates fake random data
        >>> # (feel free to load the real dataset if you prefer)
        >>> tf.random.set_seed(42)
        >>> np.random.seed(42)
        >>> X_train_num = np.random.rand(10_000, 8)
        >>> X_train_cat = np.random.choice(ocean_prox, size=10_000)
        >>> y_train = np.random.rand(10_000, 1)
        >>> X_valid_num = np.random.rand(2_000, 8)
        >>> X_valid_cat = np.random.choice(ocean_prox, size=2_000)
        >>> y_valid = np.random.rand(2_000, 1)
        >>> 
        >>> num_input = tf.keras.layers.Input(shape=[8], name="num")
        >>> cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name="cat")
        >>> cat_embeddings = lookup_and_embed(cat_input)
        >>> encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])
        >>> outputs = tf.keras.layers.Dense(1)(encoded_inputs)
        >>> model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])
        >>> model.compile(loss="mse", optimizer="sgd")
        >>> history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,
        >>>                     validation_data=((X_valid_num, X_valid_cat), y_valid))

  Text Preprocessing (pages 471 - 473)

    TextVectorization Layer
      - for basic text preprocessing
      - must either pass it a vocabulary upon creation, or let it learn the vocabuarly from some training 
        data using its 'adapt()' method
      - when encoding sentences, unknown words are encoded as '1's, and shorter sentences are padded with zeros
      TextVectorization.adapt() method
        - first, by default, converts training sentences to lowercase and removes punctuation
        - next, the sentences are split on whitespaces, and resulting words are sorted by frequency, producing
          the final vocabulary
      TextVectorization arguments include:
        standardize=
          None:  preserves case and punctuation (no standardization)
          <std_fcn>:  passes standardization function, <std_fcn>
        split=
          lower_and_strip_punctuation (default): Text will be lowercased and all punctuation removed.
          None: prevents splitting
        <splitting_fcn>: passes splitting function, <splitting_fcn>
        output_sequence_length=<output_sequence_length>
          - used to ensure all output sequences all get cropped or padded to the desired length
        ragged=
          True: get a ragged tensor rather than a regular tensor
        output_mode=
          int (default): Outputs integer indices, one integer index per split string token. When output_mode == "int", 
              0 is reserved for masked locations; this reduces the vocab size to max_tokens - 2 instead of max_tokens - 1.
          multi_hot: Outputs a single int array per batch, of either vocab_size or max_tokens size, containing 1s in 
              all elements where the token mapped to that index exists at least once in the batch item.
          count:  Like "multi_hot", but the int array contains a count of the number of times the token at that index 
              appeared in the batch item.  
          tf_idf: term-frequency x inverse-document-frequency (TF-IDF): This is similar to the count encoding, but words 
              that occur frequently in the training data are downweighted, and conversely, rare words are upweighted 

    TF-IDF variant used by TextVectorization
      - multiplies each word count by a weight equal to log(1 + d/(f + 1)) where d: total number of sentences in the
        training data; and f: counts how many of these training sentences contain the given word

    ragged tensor:
      - the shape of a ragged tensor isn't a fixed tuple of dimensions. Instead, it comprises a nested 
        structure indicating the lengths of each dimension for every element, allowing for variations in shape

    TextVectorization Layer limitations:
      - fairly good for basic natural language processsing tasks
      - it only works with languages that separate words and spaces
      - it doesn't distinguish between homonyms (e.g. bear vs teddy bear) 
      - gives no hint to your model that words like "evolution" and "evolutionary" are related
      - if multi-hot or TF-IDF encoding are used, then the order of the words is lost
      homonym:
         two or more words having the same spelling or pronunciation but different meanings
    
    Code: TextVectorization using output_mode="int" (default):

        >>> train_data = ["To be", "!(to be)", "That's the question", "Be, be, be."]
        >>> text_vec_layer = tf.keras.layers.TextVectorization()
        >>> text_vec_layer.adapt(train_data)
        >>> text_vec_layer(["Be good!", "Question: be or be?"])
        >>> <tf.Tensor: shape=(2, 4), dtype=int64, numpy=
            array([[2, 1, 0, 0],
                   [6, 2, 1, 2]], dtype=int64)>

    Code: TextVectorization using output_mode="tf_idf" (default) and ragged="True":

        >>> text_vec_layer = tf.keras.layers.TextVectorization(ragged=True)
        >>> text_vec_layer.adapt(train_data)
        >>> text_vec_layer(["Be good!", "Question: be or be?"])
            <tf.RaggedTensor [[2, 1], [6, 2, 1, 2]]>

    Code: TextVectorization using output_mode="tf_idf":

        >>> text_vec_layer = tf.keras.layers.TextVectorization(output_mode="tf_idf")
        >>> text_vec_layer.adapt(train_data)
        >>> text_vec_layer(["Be good!", "Question: be or be?"])
            <tf.Tensor: shape=(2, 6), dtype=float32, numpy=
            array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        , 0.        ],
                   [0.96725637, 1.3862944 , 0.        , 0.        , 0.        , 1.0986123 ]], dtype=float32)>
            

        >>> # log(1 + d/(f + 1)): for the 'be' word used twice in 2nd sentence "Question: be or be?"
        >>> # 4 training sentences (d = 4), 3 training sentences containing 'be' (f = 3)
        >>> 2 * np.log(1 + 4 / (1 + 3))
            1.3862943611198906

        >>> # log(1 + d/(f + 1)): for the 'question' word used twice in 2nd sentence "Question: be or be?"
        >>> # 4 training sentences (d = 4), 1 training sentences containing 'question' (f = 1)
        >>> 1 * np.log(1 + 4 / (1 + 1))
            1.0986122886681098


    Other Text processing options:
      TensorFlow Text Library (https://tensorflow.org/text)
        - provides more advanced features than TextVectorization
        - includes several subword tokenizers capable of splitting the text into tokens smaller than words
      Pretrained language model components
         
  Using Pretrained Language Model Components (pages 473 - 474)

    TensorFlow Hub Library (https://www.tensorflow.org/hub)
      - a repository of trained machine learning models for text, image, audio, and more
      - model components are called 'modules' - browse 'https://tfhub.dev' to find a 'module' for your need
      - to use, just need to include 'hub_layer' in your model (see example)

    Code: Install tensorflow_hub

        >>> # install tensorflow_hub
        >>> %pip install tensorflow_hub
        >>> # restart kernel

    Code: example using 'nnlm-en-dim50' example in tensorflow_hub - fails likely to deprecation issue

        >>> import tensorflow_hub as hub
        >>> 
        >>> hub_layer = hub.KerasLayer("https://tfhuba.dev/google/nnlm-en-dim50/2")
        >>> sentence_embeddings = hub_layer(tf.constant(["To be", "Not to be"]))
        >>> sentence_embeddings.numpy().round(2)

    Open source Transformers library by Hugging Face
      https://huggingface.co/docs/transformers
      - makes it easy to include powerful language model components inside your model
      - browse the Hugging Face Hub (https://huggingface.co/models)
         - choose the model you want, and use the provided example code examples to get started

  Image Preprocessing Layers (pages 474 - 475)

    Keras 3 image preprocessing layers:
      tf.keras.layers.Resizing
        - resizes the image to the desired size
        - example: Resizing(height=100, width=200) resizes each image to 100 x 200 
        - if you set 'crop_to_aspect_ratio=True', the the image will be cropped to the target image ratio
          to avoid distortion
      tf.keras.layers.Rescaling
        - rescales the pixel values
        - example: Rescaling(scale=2/255, offset=-1) scales the values from 0 -> 255 to -1 -> 1
      tf.keras.layers.CenterCrop
        - crops the image, keeping only a center patch of the desired size

   Code: Load temple image from Scikit-Learn 'load_sample_images()' function and plot it

        >>> import matplotlib.pyplot as plt
        >>> from sklearn.datasets import load_sample_images
        >>> images = load_sample_images()["images"]
        >>> 
        >>> plt.imshow(images[0])
        >>> plt.axis("off")
        >>> plt.show()

   Code: Crop temple image using ' tf.keras.layers.CenterCrop' and plot it

        >>> crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)
        >>> cropped_images = crop_image_layer(images)
        >>> 
        >>> plt.imshow(cropped_images[0] / 255)
        >>> plt.axis("off")
        >>> plt.show()

   Kera data augmenttation layers:
     - RandomCrop, RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomHeight, RandomWidth,
       and RandomContrast

The TensorFlow Datasets Project (pages 476 - 477)

   TensorFlow Datasets (TFDS) (https://tensorflow.org/datasets):
     - make it eash to load common datasets from small ones like MNIST or Fashion MNIST to huge datasets like Image
     - visit https://homl.info/tfds for a full list and description of the datasets
     - not bundled with tensorflow, so it needs to be installed (see below)
     - to use 'import tensorflow_datasets' usually as 'tfds', then call the tfds.load()' function which will 
       download the data you want, and return data as a dictionary of datasets
     https://www.tensorflow.org/datasets/add_dataset
     - TFDS process datasets into a standard format (external data -> serialized files), which can then be loaded as 
       machine learning pipeline (serialized files -> tf.data.Dataset). 

   Code: Install TensorFlow Datasets:

        >>> # install tensorflow_datasets (needed to use '--user' option to prevent privilege error):
        >>> %pip install --user tensorflow-datasets       
        >>> # restart kernel

   Code: Download TFDS MNIST dataset

        >>> import tensorflow_datasets as tfds
        >>> 
        >>> datasets = tfds.load(name="mnist")
        >>> mnist_train, mnist_test = datasets["train"], datasets["test"]

     tfds.load() arguments:
         name:
           - name of dataset to be download
         as_supervised 	bool: 
            - if True, the returned tf.data.Dataset will have a 2-tuple structure (input, label) according to 
              builder.info.supervised_keys. 
            - If False, the default, the returned tf.data.Dataset will have a dictionary with all the features. 
         split:
            - Which split of the data to load (e.g. 'train', 'test', ['train', 'test'], 'train[80%:]',...). 
            - If None, will return all splits in a Dict[Split, tf.data.Dataset]

   Code: Complete example using TFDS MNIST dataset:

        >>> # download dataset including splitting dataset to 90% for training and 10% for testing, & whole dataset for testing
        >>> train_set, valid_set, test_set = tfds.load(
        >>>     name="mnist",
        >>>     split=["train[:90%]", "train[90%:]", "test"],
        >>>     as_supervised=True
        >>> )
        >>>
        >>> train_set = train_set.shuffle(10_000, seed=42).batch(32).prefetch(1)
        >>> valid_set = valid_set.batch(32).cache()
        >>> test_set = test_set.batch(32).cache()
        >>>
        >>> tf.random.set_seed(42)
        >>> model = tf.keras.Sequential([
        >>>     tf.keras.layers.Flatten(input_shape=(28, 28)),
        >>>     tf.keras.layers.Dense(10, activation="softmax")
        >>> ])
        >>>
        >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
        >>>               metrics=["accuracy"])
        >>> history = model.fit(train_set, validation_data=valid_set, epochs=5)
        >>> test_loss, test_accuracy = model.evaluate(test_set)

Chapter 13 Exercises (page 477 - 478):

    -> see exercise_notebooks/13_exercises.ipynb



------------------------------------------------------
Chapter 14 Deep Computer Vision Using Convolutional Neural Networks
------------------------------------------------------

  Convolutional Neural Networks (CNN)
    - used in computer image recognition since the 1980's
    - they are also used for other tasks, such as voice recognition and natural language processing

The Architecture of the Visual Cortexx (pages 480 - 481)

   CNN:
     LeNet-5 Architecture
       - widely used by banks to recognize handwritten digits on checks
       - architecture building blocks includes fully connected layers with sigmoid activation functions,
         but it also introduces 'convolutional layers' and 'pooling layers'
  
    Deep Neural Networks (DNNs) vs Convolutional Neural Networks (CNMs) for image recognition
      - DNNs work fine for small images (e.g. MNIST), but it breaks down for large images because of the 
        huge number of parameters. Example: If 100 x 100 pixel image has 10K pixels, and if the 1st layer
        has just 1000 neurons (which already severely restricts the amount of information), this means 10M 
        connections.
      - CNN solves this problem using partially connected layers and weight sharing

Convolutional Layers (pages 481 - 491)

    Convolutional Neural Networks Introduction:
      https://www.jeremyjordan.me/convolutional-neural-networks/

    Stride: 
      - Stride is a parameter that dictates the movement of the kernel, or filter, across the input data, such as an image. 
      - When performing a convolution operation, the stride determines how many units the filter shifts at each step. 
      - This shift can be horizontal, vertical, or both, depending on the stride's configuration.
      - For example, a stride of 1 moves the filter one pixel at a time, while a stride of 2 moves it two pixels. 
      - A larger stride will produce a smaller output dimension, effectively downsampling the image.

     Convolutional Layer connectivity:
       - neurons in convolutional layer are only connected to pixels in the 'receptive field' of the previous layer
       receptive field
         - refers to the region of the input image that a particular neuron in a convolutional layer is "looking at" 
           or taking into account when making its predictions or feature extractions
         - field of a neuron in a particular layer can be thought of as the area of the input image that directly 
           influences the neurons output [in the next layer].
        - A neuron located in 'row i', 'column j' of a given layer is connected to the outputs of the neurons in the
          previous layer located in 'rows i to i + f_h - 1, columns j to j + f_w -1. where f_h and f_w are the height
          and width of the receptive field
        stride:
          - the horizontal or vertical step size from one receptive field to the next

  Filters (pages 484 - 485)

    Filters (or convolutional kernels or kernels or windows):
      - designed to detect specific patterns or features in the input data. For example, in image processing, 
        filters might be designed to detect edges, corners, or textures. In deep learning, the weights of these 
        filters are learned automatically through training on large datasets.
      - convolutional layers will automatically lear the most useful filters for its task
      - small weight matrices which are the size of the receptive field
      
    Feature map:
      - highlights the areas in an image that activate the filter the most
      - example: Applying a horizontal filter to an image will output a feature map that highlights horizontal lines 
                 and blurs (ignore) everything else

  Stacking Multiple Features Maps (pages 485 - 487)

    convolutional layer with multiple filters
      - a convolutional layer has multiple filters (you decide how many) and outputs one feature map per filter
      - it has one neuron per pixel in each feature map, and all neurons within a given feature map share the same 
        parameters
      - a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of
        detecting multiple features anywhere in its inputs
      - each convolutional layer outputs one feature map per filter

    Equation 14-1: Computing the output of neuron in a convolutional layer

    z_i,j,k = b_k +  SUM_u  SUM_v SUM_k  x_i',j',k' x  w_u,v,k',k   

      with:  
           i' = i x s_h + u
           j' = j x s_w + v

           SUM_u is from u = 0  to f_h - 1
           SUM_v is from v = 0  to f_w - 1
           SUM_k is from k' = 0 to f_n' - 1

       In this equation:
          z_i,j,i: the output of the neuron located in row i, column j in the feature map i of the convolutional layer (layer l)

          s_h and s_w: vertical and horizontal strides
          f_h and f_w: height and width of the receptive field
          f_n': number of feature maps in the previous layer (layer l - 1)

          x_i',j',k': the output neuron located in layer l - 1, row i', column j', feature map k' (or channel k' if
            the previous layer is an input layer)

          b_k: the bias term for feature map k (in layer l)

          w_u,v,k',k:  connection weight between any neuron in feature map k of the layer l and its inputs located 
            at row u, column v (relative to the neuron's receptive field) and feature map k'

                                                                        
  Implementing Convolutional Layers with Keras (pages 487 - 490)

    Keras Convolution2D layer (alias Conv2D):
      - creates a 2D convolution layer
      - relies on TensorFlows 'tf.nn.con2d() operation
      - 2D convolution layer refers to the number 'spatial dimensions (height and width), but layers takes a
        4D input which includes batch size (e.g. number of images) and channels (3 for RBG)
      tf.keras.layers.Conv2D(filters=<filterSize>, kernel_size=<kernelSize>, padding="valid", strides=1)
        filter_size: 
          - the dimension of the output space (the number of filters [feature maps] in the convolution).
        kernel_size: 
          - int or tuple/list of 2 integer, specifying the size of the convolution window [filter]. 
        padding
          - string, either "valid" or "same" (case-insensitive). 
          - "valid" means no padding. 
          - "same" results in padding evenly to the left/right or up/down of the input. When padding="same" 
        stride:
          - int or tuple/list of 2 integer, specifying the stride length of the convolution. 
          - if stride > 1 (in any direction), then the output size will not be equal to the input size, even if 
            padding='same' (see example below)

    Conv2D weights:
      - Conv2D holds all the layer's weights includeing the kernels and the biases
      - the kernels weights are randomly initialized, while the biases weights are initialized to zero
      - weights are accessible via the weights attribute or as Numpy arrays via get_weights() method
      - the kernels [weights] array is 4D, and its shape is [kernel_height, kernel_width, input_channels, output_channels]
      - the biases [weights] array is 1D, and its shape is [output_channels]
      - the number of output channels is equal to the number output feature maps which is also equal to the number of filters

    Conv2D activation function and kernel initializer:
      - defaults: activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros'
      - you generally want to specify and activation function (such as ReLU) when creating a Conv2D layer, also specify
        the corresponding kernel initializer (such as 'He' initialization) 
      - Like 'Dense' layers, convolutional layer perform linear operations, so if you stack multiple convolutional
        layers with any activation functions they would all be equivalent to a single convolutional layer

    Conv2D hyperparameters:
      - include: filters, kernel_size, padding, strides, activation, kernel_initializer, etc.
      - you can use cross-validation to the right hyperparameters values, but this is time consuming


    Code: Load 2 Sample Images for use with Kera Convolution2D layers 

         >>> # Let's load two sample images, rescale their pixel values to 0-1, and center crop them to small 70120 images:
         >>> from sklearn.datasets import load_sample_images
         >>> import tensorflow as tf
         >>> 
         >>> images = load_sample_images()["images"]
         >>> images = tf.keras.layers.CenterCrop(height=70, width=120)(images)
         >>> images = tf.keras.layers.Rescaling(scale=1 / 255)(images)
         >>> # image shape: 2 images, 70 x 120 image (after CenterCrop - originally 427 x 640) , 3 color (RBG) channels
         >>> images.shape
             TensorShape([2, 70, 120, 3])
             

    Code: Create Keras Convolutional2D layer (alias: Conv2D) with default padding (padding='valid')

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> # use kernel/filter size: 7 x 7, with 32 filters
         >>> conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)
         >>> fmaps = conv_layer(images)
         >>> 
         >>> # shape: 2 images, 64 x 114 output feature map size,  and 32 output feature maps
         >>> fmaps.shape
         >>> TensorShape([2, 64, 114, 32])

    Code: Create Keras Convolutional2D layer (alias: Conv2D) with zero padding (padding='same')

         >>> conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding="same")
         >>>
         >>> # shape: 2 images, 70 x 120 output feature map size,  and 32 output feature maps
         >>> fmaps = conv_layer(images)
             TensorShape([2, 70, 120, 32])

    Code: Create Keras Convolutional2D layer (alias: Conv2D) with zero padding (padding='same') and stride=2

         >>> # extra code - shows that the output shape when we set strides=2
         >>> conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding="same", strides=2)
         >>>
         >>> # shape: 2 images, 35 x 60 output feature map size,  and 32 output feature maps
         >>> fmaps = conv_layer(images)
         >>> fmaps.shape
             TensorShape([2, 35, 60, 32])

    Code: Create function to calculate Conv2D output feature map size and zero padding

         >>> # extra code - this utility function can be useful to compute the size of the
         >>> #              feature maps output by a convolutional layer. It also returns
         >>> #              the number of ignored rows or columns if padding="valid", or the
         >>> #              number of zero-padded rows or columns if padding="same"."""
         >>> 
         >>> import numpy as np
         >>> 
         >>> def conv_output_size(input_size, kernel_size, strides=1, padding="valid"):
         >>>     if padding=="valid":
         >>>         z = input_size - kernel_size + strides
         >>>         output_size = z // strides
         >>>         num_ignored = z % strides
         >>>         return output_size, num_ignored
         >>>     else:
         >>>         output_size = (input_size - 1) // strides + 1
         >>>         num_padded = (output_size - 1) * strides + kernel_size - input_size
         >>>         return output_size, num_padded
         >>> 
         >>> conv_output_size(np.array([70, 120]), kernel_size=7, strides=2, padding="same")
             (array([35, 60]), array([5, 5]))


    Code: Create Keras Convolutional2D layer (alias: Conv2D) with zero padding (padding='same') and stride=2

         >>> kernels, biases = conv_layer.get_weights()
         >>> kernels.shape
             (7, 7, 3, 32)
         >>> biases.shape
             (32,)


  Memory Requirements (pages 490 - 491)

    RAM requirements:
      - convolutional layers require a huge amount of RAM especially during training because the reverse pass
        of backpropagation requiresm all the intermediate values computed during the forward pass
      training example:
        - convolutional layer with 200 5 x 5 filters, and stride=1, and padding='same'
        - input:  150 x 100 RGB image (3 channels)
        then:
        - number of parameters:    (5 x 5 x 3 + 1) x 200 = 15200 (the +1 is for bias terms)
        - 200 feature maps comtain 150 x 100 neurons and each neuron needs to computed a weighted sum
          of its 5 x 5 x 3 = 75 inputs which is 225M float multiplications [200 x 75 x 150 x 100 = 225M]
        - if using 32-bit, convolutional layer output will occupy [200 x 150 x 100 x 32] 96M bits (12MB) 
          of RAM for just 1 instance. if training a batch of 100 instances, then this layer will use up 
          to 1.2GB of RAM
      inference:
        - during inference, the RAM ocuppied by one layer can be released as soon as the next layer has been
          computed, so you only need as RAM as required by by two consecutive layers
      Reducing training memory options:
        - reduce mini-batch size, reducing dimensionality using a stride, removing a few layers, using 16-bit floats
          instead of 32-bit, or distributing the CNN across multiple devices

Pooling Layers (pages 491 - 493)

    Pooling Layers:
      - pooling layer goal is to 'subsample' (i.e. shrink) the input image in order to reduce the computational
        load, memory usages, and the number of parameters (thereby limiting the risk of overfitting)
      - you define the pooling layers size, the stride, and padding type
      - it does NOT have weights; all it does is aggregate the inputs using an aggregation function such as max or mean
      max pooling layer:
        - most common pooling layer type
        - only the max input value in the receptive field makes it to the next layer, while other inputs are dropped
        - typically works on every input channel independently, so the output depth (i.e. number of channels) is the
          same as the input depth
        max pooling layer invariance:
          - introduces some level of 'invariance' to small translations as shown in Figure 14-10 (page 493)
          - inserting a max pooling layer every few layers in a CNN, it is possible to get some level of 'translation
            invariance at layre scale
          - offers a small amount of 'rotational invariance' and slight 'scale invariance'
          - the goal in many cases is 'equivariance' not 'invariance': a small change to the inputs should lead
            to a corresponding small change in the outputs

Implementing Pooling Layers with Keras (pages 493 - 495)

     tf.keras.layers.MaxPool2D( pool_size=(2, 2), strides=None, padding='valid', ...):
       - MaxPool2D is alias for MaxPooling2D
       - Max pooling operation for 2D spatial data.
       arguments:
         pool_size: 
           - int or tuple of 2 integers, factors by which to downscale (dim1, dim2). 
           - If only one integer is specified, the same window length will be used for all dimensions. 
         strides: 
           - int or tuple of 2 integers, or None. Strides values. 
           - If None, it will default to pool_size. 
           - If only one int is specified, the same stride size will be used for all dimensions. 

    Code: MaxPool2D example:

         >>> max_pool = tf.keras.layers.MaxPool2D(pool_size=2)
         >>> output = max_pool(images)

    Average pooling layer
      - use AveragePool2D (alias AvgPool2D), that is use: tf.keras.layers.AvgPool2D()
      - works like max pooling layer except it computes the mean instead of the average
      - used to be very popular, by now people mostly use max pooling layers as they generally perform better
      - max pooling offers stronger translation invariance that average pooling, an it requires slightly less compute

    Depthwise max pooling layer
      - max pooling performed along the depth dimension instead of the spatial dimension
      - allows CNN to learn the invariant to various features, for example, it could learn, multiple filters
        each detecting rotation of the same pattern
      - keras does not include depthwise max invariance - custom code example provided

    Global Average pooling layer
      - use GlobalAveragePooling2D (alias GlobalAvgPool2D), that is use: tf.keras.layers.GlobalAvgPool2D()
      - it computes the mean of each entire feature map
      - it justs outputs a single number perf feature map and per instance
      - it can be useful just before the output layer (shown later in chapter)

    Code: GlogalAvePool2D example along with equivalent Lambda layer:

         >>> global_avg_pool = tf.keras.layers.GlobalAvgPool2D()
         >>> 
         >>> The following layer is equivalent:
         >>> 
         >>> global_avg_pool = tf.keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))
         >>>
         >>> global_avg_pool(images)
         >>> <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
             array([[0.64338624, 0.5971759 , 0.5824972 ],
                    [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>

CNN Architectures (pages 495 - 515)

  Typical CNN Architectures:
    - stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, 
      then another few convolutional layers (+ReLU layer), then another pooling layer, and so on
    - at the top of the stack, a regular feedforward neural network is added, composed of a few fully connected
      layers (+ReLUs), and the final layer outputs the predictions (e.g. a softmax layer that outputs the
      estimated class probabilities

  Convolutional kernels too large mistake
    - a common mistake is to use a Convolutional kernels that are too large
    - for example, instead of using a convolutional layer with 5 x 5 kernel, stack 2 layers with 3 x 3 kernels:
      it will use fewer parameters and require fewer computations, and it will usually perform better
    - one exception is for the 1st convolution layer: it can typically have a large kernel (e.g. 5 x 5), usually
      with a stride  of 2 or more, usually. This will reduce the spatial dimensions of the image without losing
      too much information, and since the input image only has 3 channels in general, it will not be too costly

  python [functools] parital functions
    - Partial functions allow us to fix a certain number of arguments of a function and generate a new function
    - The partial function takes a callable (usually another function) and a series of arguments to be pre-filled 
      in the new partial function. 

  Dropout layer: tf.keras.layers.Dropout(rate)
    - Applies dropout to the input.
    - The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, 
      which helps prevent overfitting.
    - Note that the Dropout layer only applies when training is set to True in call(), such that no values are dropped 
      during inference
    dropout regularization / technique:
     - works by randomly dropping out selected neurons during the training phase. 
     - Neurons in later layers do not reap the contribution of dropped-out neurons during forward propagation, 
       nor will updates be made to the dropped-out neurons during backpropagation.

    Code: Download and prepare Fashion MNIST Dataset for CNN model


         >>> # extra code - loads the mnist dataset, add the channels axis to the inputs,
         >>> #              scales the values to the 0-1 range, and splits the dataset
         >>> 
         >>> mnist = tf.keras.datasets.fashion_mnist.load_data()
         >>> (X_train_full, y_train_full), (X_test, y_test) = mnist
         >>>
         >>> # add channels axis to input train and test datasets and standardize grayscale values
         >>> X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255
         >>> X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255
         >>>
         >>> X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
         >>> y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

    CNN example to tackle the Fashion MNIST Dataset
      - use functools.partial() function to define DefaultConv2D with specified defaults
      - create Sequential model
        - 1st layer is a DefaultConv2D with 64 fairly large filters (7 x 7). 
          - Use default stride=1, because input images are not large
          - set input_shape=[28, 28, 1], because images are 28 x 28 pixels with a single color channel (grayscale)
        - 2nd layer is a max pooling layer that uses the default pools size of 2, so it divides each spatial
          dimension by a factor of 2
        - repeat structure twice of :
           DefaultConv2D(filters=128), DefaultConv2D(filters=128), MaxPool2D()
           - for larger images, you could repeat this structure several more times
           - number of filters doubles (64 -> 128) as we climb the CNN towards the output layer
           - it is common to double the number of filters after each pooling layer
        - Flatten inputs to fully connected network
           - uses Flatten() layer
           - must flatten inputs to fully connected network since it expects a 1D array of features for each instance 
        - Next is the fully connected network:
           - composed of 2 hidden layers and a dense output layer
           - since its a classification task with 10 classes, the output layer has 10 units, and it uses a softmax
             activation function
           - also add 2 dropout layers with a dropout rage of 50% to reduce overfitting
       Compile:
         - if you compile this model using loss="sparse_categorical_crossentropy" and fit the model to the Fashion MNIST
           training set, it should reash over 92% accuracy

    Code: CNN example to tackle the Fashion MNIST Dataset

         >>> from functools import partial
         >>> 
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding="same",
         >>>                         activation="relu", kernel_initializer="he_normal")
         >>> model = tf.keras.Sequential([
         >>>     DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),
         >>>     tf.keras.layers.MaxPool2D(),
         >>>     DefaultConv2D(filters=128),
         >>>     DefaultConv2D(filters=128),
         >>>     tf.keras.layers.MaxPool2D(),
         >>>     DefaultConv2D(filters=256),
         >>>     DefaultConv2D(filters=256),
         >>>     tf.keras.layers.MaxPool2D(),
         >>>     tf.keras.layers.Flatten(),
         >>>     tf.keras.layers.Dense(units=128, activation="relu", kernel_initializer="he_normal"),
         >>>     tf.keras.layers.Dropout(0.5), 
         >>>     tf.keras.layers.Dense(units=64, activation="relu", kernel_initializer="he_normal"),
         >>>     tf.keras.layers.Dropout(0.5),
         >>>     tf.keras.layers.Dense(units=10, activation="softmax")
         >>> ])
         >>> 
         >>> # extra code - compiles, fits, evaluates, and uses the model to make predictions
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
         >>>               metrics=["accuracy"])
         >>> history = model.fit(X_train, y_train, epochs=10,
         >>>                     validation_data=(X_valid, y_valid))
         >>> score = model.evaluate(X_test, y_test)
         >>> X_new = X_test[:10]  # pretend we have new images
         >>> y_pred = model.predict(X_new)
         >>> 
             Epoch 10/10
             1719/1719  230s 134ms/step - accuracy: 0.9272 - loss: 0.2122 - val_accuracy: 0.9128 - val_loss: 0.2923
             313/313  13s 43ms/step - accuracy: 0.9089 - loss: 0.3507
             1/1  0s 211ms/step

  LeNet-5 (pages 498 - 499)

    LeNet-5 architecture:
      - see Table 14-1. LeNet-5 architecture (page 498)
      - most widely known CNN architecture
      - similar to our MNIST CNN example model: a stack of convolutional layers and [average] pooling layers, 
        followed by a dense network 
      - main difference with more modern classification CNNs is the activation functions: today, we use ReLU instead
        of tanh, and softmax instead of RBE

    The famous LeNet-5 architecture had the following layers:
	Layer 	Type 	                Maps 	Size 	        Kernel size 	Stride 	Activation
	Out 	Fully connected 	- 	10 	        - 	        - 	RBF
	F6 	Fully connected 	- 	84 	        - 	        - 	tanh
	C5 	Convolution 	        120 	1  1 	        5  5 	        1 	tanh
	S4 	Avg pooling 	        16 	5  5 	        2  2 	        2 	tanh
	C3 	Convolution 	        16 	10  10 	5  5 	        1 	tanh
	S2 	Avg pooling     	6 	14  14 	2  2 	        2 	tanh
	C1 	Convolution 	        6 	28  28 	5  5 	        1 	tanh
	In 	Input 	                1 	32  32 	- 	        - 	-

  AlexNet (pages 499 - 502)

    AlexNet-5 architecture:
      - see Table 14-2. AlexNet-5 architecture (page 498)
      - similar to LeNet-5, except only larger and deeper
      - first to stack convolutional layers directly on top of another one instead of stacking a pooling layer on top
        of each convolutional layer
      overfitting regularization uses 2 techniques:
        dropout
          - use dropout with 50% dropout rate during training
        data augmentation
          - performs data augmentation by randomly shifting the training images by various offsets, flipping them 
            horizontally, and changing the lighting conditions
      Local Response normalization (LRN):
        - uses competitive normalization step immediately after the ReLU step of Convolutional layers C1 & C2 (see table 14-2)
          called Local Response Normalization
        - this encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider
          range of features, ultimately improving generalization 
        - Equation 14.2. Local response normalization (LRN) shows how to apply

  GoogleNet (pages 502 - 505)

    GoogleNet
      - its performance came in large part from the fact that the network was much deeper that previous CNNs
      - this was made possible by 'subnetworks' called 'inception modules' which allow GoogleNet to use parameters
        more efficiently that previous architectures (has 10 times fewer parameters than AlexNet (~6M instead of ~60M)
      inception module:
       - figure 14-14. Inception Module (page 502)
       - think of an inception model as a convolutional layer on steriods able to output feature maps that capture
         complex patterns at various scales
       - notation "3 x 3 + 1(S)" means that the lyaer uses 3 x 3 kernel, stride 1, and "same" padding
       - input signal is first fed to 4 different layers in parallel
       - top convolution layers [in parallel] use different kernel sizes (1 x 1, 3 x 3, and 5 x 5)
       - all convolutional layers use ReLU activation function, 
       depth concatentation layer
         - all layers use stride of 1, and "same" padding including Max pooling layer, so their outputs all have the
           same height and width as their inputs
         - this makes it possible to concatenate all the outputs along the depth dimension in the depth concatentation
           layer
       convolutional layers with 1 x 1 kernels serve 3 purposes:
         - although the cannot capture spatial patterns, then can capture patterns along the depth dimension (across channels)
         - serve as 'bottleneck layers' meaning they reduce dimensionality 
         - each pair of convolutional layers ([1 x 1, 3 x 3]) and ([1 x 1, 5 x 5]) acts like a single convolutional
           layer, capable of capturing more complex patterns
          
    GoogleNet network:
      - Figure 14-15 GoogleNet architecture (page 504)
      - notation "64, 3x3+2(S)" means that the layer uses 64 filters, 3 x 3 kernel, stride 2, and "same" padding
      - first 2 layers (Convolutional 64, 7x7+2(S), Max Pool 64, 3x3+2(S)) divide the image's height and width by 4
        (so the area is divided by 16), to reduce the computational but uses a large kernel/filter so much info is preserved
      - 3rd layer: local response normalization layer ensures the preivousl layers learn a wide variety of featurs 
      - 4th & 5th layers: 2 convolutional layers, 1st acts as a bottleneck layer
      - 6th layer: local response normalization layer ensures the preivousl layers learn a wide variety of featurs 
      - 7th layer: max pooling layer reduces the image height and width by 2 to speed up computation
      CNN Backbone (11 layers - 8th to 19th) 
        - stack of nine inception modules, interleaved with a couple of max pooling layers
      Global average pooling layer (20th layer)
        - outputs the mean of each feature map
        - GoogleNet input images are typically 224 x 224 pixels, so after 5 max pooling layers, each dividing the 
          height and width by 2, the feature maps are down to 7 x 7
      Last layers (21th - ??)
        - dropout layer for regularization
        - fully connected with 1000 units (since there are 1000 classes)
        - Softmax activation function to output estimated class probabilities

  VGGNet (pages 505 - 505)

    VGGNet:
      - by Visual Geometry Group (VGG)
      - has 2 or 2 convolutional layers and a pooling layer, the again 2 or 3 convolutional layers and a pooling
        layer, and so on (reaching a total of 16 or 19 convolutional layers), plus a final dense network with
        2 hidden layers and the output layer
      - it uses small 3 x 3 filters

  ResNet (pages 505 - 509)

    Skip connections (also called shortcut connections):
      - the signal feeding into a layer is also added to the output of a layer located higher in the stack
      - when you initialize a regular neural network, it weights are close to zero, so the network just outputs
        values close to zero.  If you add skip connection, the reqult network outputs a copy of its inputs; in other
        words, it initiallly models the identity function. If the target is fairly close to the identity function 
        (which is often the case), this will speed up training
      - if you add many skip connections, the network ca n start making progress even if several layers have not
        started learning

    Residual Learning 
      - you add the input 'x' to the output of the network (i.e. you add a 'skip connection'), then the network will be
        forced to model 'f(x) = h(x) - x' rather than h(x). This is called residual learning
      - see figure 14-16. Residual Learning (page506)
      - with skip connections, the deep residual networks can be seen as a stack of residual units (RU), when each
        'residual unit' is small neural network with a skip connection


    ResNet's architecture:
      - winning  variant used an extremely deep CNN Composed of 152 layers (other variants had 34, 50, and 101 layers)
      - The key to being able to train such a deep network is to use 'skip connections': the signal feeding into a layers
        is also added to the output of a layer located higher in the stack
      - Figure 14-18 ResNet architecture (page 507)
      - Starts and end simular to GoogleNet with deep stack of residual units in between
      starts:
      - notation "64, 3x3+2(S)" means that the layer uses 64 filters, 3 x 3 kernel, stride 2, and "same" padding
      - first 2 layers (Convolutional 64, 7x7+2(S), Max Pool 64, 3x3+2(S)) divide the image's height and width by 4
        (so the area is divided by 16), to reduce the computational but uses a large kernel/filter so much info is preserved
      Deep Stack Residual Units 
        Residual Units:
          - composed of 2 convolutional layers using 3 x 3 kernels and preserving spatial dimensions (stride 1, "same" padding) 
            with batch normalization on 2nd convolutional layer plus skip connection and ReLU activation 
          - the number of feature maps is doubled (i.e. 64 to 128) every few residual units, at the same time as their
            height and width are halved (using 1st convolutional layer with stride 2)
            - when this happens, the skip connection inputs cannot be added directly to outputs of the residual unit
              because they do not have the same shape. To solve this, skip connection inputs are passed through a 1 x 1
              skip 2, "same" pad convolutional layer then added to the residual using outputs 
            - see figure 14-19. Skip connection when chanaging feature map size and depth (page 508)
      Ends:
       Global average pooling layer 1024 (?12th layer)
        - outputs the mean of each feature map
       Last layers (21th - ??)
        - fully connected with 1000 units (since there are 1000 classes)
        - Softmax activation function to output estimated class probabilities

    ResNet-34:
      - a ResNet with 34 layers (only counting convolutional layers and fully connected layers) containing 3 RUs,
         that output 64 feature maps, 4 RUs with 128 feature maps, 6 RUs with 256 feature maps, and 3 RUs with
         512 feature maps

    ResNet-152:
      - uses slightly different residual units. Instead of two 3 x 3 convolutional layers with 256 feature maps,
        it uses 3 convolutional layers: first a 1 x 1 convolutional layer with 64 feature maps (4 x less), which
        acts as a bottleneck layer, then a 3 x 3 layer with 64 feature maps, and finally anothe 1 x 1 convolutional
        layer with 256 feature maps (4 times 64) that restores the original depth
      - contain 3 such RUs that output 256 feature maps, the 8 RUs with 512 feature maps, a whopping 36 RUs with
        1024 maps and finally 3 RUs with 2048 feature maps

  Xception (pages 509 - 510)

    Xception (stands for Extreme Inception)
      - merges the ideas of GoogleNet and ResNet, but it replaces the 'inception modules' with special type of layer
        called 'depthwise separable convolutional layer' (or 'separable convolutional layer')

    depthwise separable convolutional layer (or 'separable convolutional layer')
      - makes the strong assumption that spatial patterns and cross channel paters can be modeled separately
      Composed or 2 parts:
        1. applies a single spatial filterr to each input feature map
        2. looks exclusively for cross-channel patterns - it is just a regular convolutional layer with 1 x 1 filter

      https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728
        - a depthwise separable convolution splits a kernel into 2 separate kernels that do two convolutions: 
           the depthwise convolution and the pointwise convolution
        depthwise convolution
          - we give the input image a convolution without changing the depth
          example: 12 x 2 x 3 input (12 x 12 input with 3 Channels) using 3 kernels with 5 x 5 x 1 filter
            - Each 5x5x1 kernel iterates 1 channel of the image giving out a 8x8x1 image for each channel. 
              Stacking these 3 images creates a 8x8x3 image
              Note: normal convolutional would use a 5 x 5 x 3 filter instead of a 5 x 5 x 1 filter
        pointwise convolution
          - The pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through 
            every single point. This kernel has a depth of however many channels the input image has
          example: input from depthwise convolution: 8 x 8 x 3
            - iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.
            - We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.
          example summary:
            In a more abstract way, if the original convolution function is 12x12x3 - (5x5x3x256) > 12x12x256, 
            we can illustrate this new convolution as 12x12x3 - (5x5x1x1) - (1x1x3x256) ->12x12x256.

    Xception Architecture
      starts with:
        - 2 regular convolutional layers 
      middle:
        - uses only separable convolutions (34 in all) plus a few max pooling layers 
      End
        - the usual final layers (a global average pooling layer, and a dense output layer)


    Separable convolutional layers:
      - use fewer parameters, less memory, and few computations that regular [normal] convolutional layers,
        and often perform better
        - consider using them by default, except after layers with few channels (such as input channel)
        Keras SeparableConv2D
          - use instead of Conv2D - it's a drop-in replacement
        Keras DepthwiseConv2D
          - implements the first part of the depthwise separable convolutional layer (ie. applying one spatial
            filter per input feature map)

  SENet (pages 510 - 512)

    Sequeeze-and-Excitation Network (SENet)
      - extends existing architectures such as inception networks (called SE-Inception) and ResNets (called SE-ResNet), 
        and boosts their performance
        - the boost comes from SENets adds a small neural network called 'SE Block' to every inception module
        or 'residual unit' in the original architecture

     SE-Block
       - analyzes the output of the unit is attached to, focusing exculsively on the depth dimension (does not look
         at any spatial patterns), and learns which features are most active together. It then uses this information to 
         recalibrate the feature maps
       - SE-block is composed of 3 layers:
          - a global pooling layer
          - a hidden dense layer using ReLU activation function
          - and a dense output layer using signmoid activation fuction
          - see figure 14-23. SE block architecture (page 512)

  Other Noteworthy Architectures (pages 512 - 514)

    ResNetXt
      - improves upon the residual units in ResNet
      - ResNetXt residual units are composed of many parallel stacks (e.g. 32 stacks) with 3 convolutional layers each

    DenseNet
      - composed of several dense blocks, enach made up of a few densely connected convolutional layers
      - 'densely connected' means the output of each layer is fed as input to every layer after it within the 
        same block

    MobileNet
      - streamlined modeuls designed to be lightweight and fast, making them popular for mobile and web applications
      - based on depthwise spearable convolutional layers, like Xception

    CSPNet (Cross Stage Partial Network)
      - similar to DenseNet, but part of each dense block's input is concatenated directly to that block's output,
        without going through the block
       
    EfficientNet
      - arguablly the most important model in this list - remains amoung the best models out there today
      - it uses scaled down version of ImageNet (with fewer and smaller images), and then 'compound scaling' to create
        a larger and larger versions of this architecture
      - it uses 'compound scaling' which is based on the logarithmic measure of the 'compute budget', noted 'theta':
        if your compute budget doubles, the 'theta' increases by 1. That is, the floating point operations available
        for training is proportional to 2**theta

  Choosing the Right CNN Architectures (pages 514 - 515)

     Choosing best CNN model for your project:
       - it depends on what matters most to you: Accuracy?, Model size? Inference speed on CPU, on GPU?
       - table 14-3. Pretrained models available in Keras (page 514) - lists the best pretrained models currently
         available in Keras

        Table 14-3. Pretrained models available in Keras

	Model 	                Size 	 Accuracy 	Para-   Depth 	ms / inference step 
	                        (MB)   Top-1   Top-5    meters          CPU     GPU
        ---------------         ------  -----   ----    ------  -----   ------  ----------- 
	MobileNetV2 	        14 	71.3% 	90.1% 	3.5M 	105 	25.9 	3.8
	MobileNet 	        16 	70.4% 	89.5% 	4.3M 	55 	22.6 	3.4
	NASNetMobile 	        23 	74.4% 	91.9% 	5.3M 	389 	27.0 	6.7
	EfficientNetB0 	        29 	77.1% 	93.3% 	5.3M 	132 	46.0 	4.9
	EfficientNetB1 	        31 	79.1% 	94.4% 	7.9M 	186 	60.2 	5.6
	EfficientNetB2 	        36 	80.1% 	94.9% 	9.2M 	186 	80.8 	6.5
	EfficientNetB3 	        48 	81.6% 	95.7% 	12.3M 	210 	140.0 	8.8
	EfficientNetB4 	        75 	82.9% 	96.4% 	19.5M 	258 	308.3 	15.1
	InceptionV3 	        92 	77.9% 	93.7% 	23.9M 	189 	42.2 	6.9
	ResNet50V2 	        98 	76.0% 	93.0% 	25.6M 	103 	45.6 	4.4
	EfficientNetB5 	        118 	83.6% 	96.7% 	30.6M 	312 	579.2 	25.3
	EfficientNetB6 	        166 	84.0% 	96.8% 	43.3M 	360 	958.1 	40.4
	ResNet101V2 	        171 	77.2% 	93.8% 	44.7M 	205 	72.7 	5.4
	InceptionResNetV2 	215 	80.3% 	95.3% 	55.9M 	449 	130.2 	10.0
	EfficientNetB7 	        256 	84.3% 	97.0% 	66.7M 	438 	1578.9 	61.6

        Table 14-3 Notes:
          - table sorted by model size
          - full list is at: https://keras.io/api/applications/
          - table shows Keras class names to (in the tf.keras.application package)
          - Model size is in MB
          - The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.
          - Depth refers to the topological depth of the network. This includes activation layers, batch normalization layers etc.
          - Depth counts the number of layers with parameters
          - Time per inference step is the average of 30 batches and 10 repetitions. using:
              CPU: AMD EPYC Processor (with IBPB) (92 core)
              RAM: 1.7T
              GPU: Tesla A100
              Batch size: 32

Implementing a ResNet-34 CNN with Keras (pages 515 - 516)

    ResNet-34:
      starts:
      - notation "64, 7x7+2(S)" means that the layer uses 64 filters, 7 x 7 kernel, stride 2, and "same" padding
      - first 2 layers (Convolutional 64, 7x7+2(S), Max Pool 64, 3x3+2(S)) divide the image's height and width by 4
        (so the area is divided by 16), to reduce the computational but uses a large kernel/filter so much info is preserved

      - a ResNet with 34 layers (only counting convolutional layers and fully connected layers) containing 3 RUs,
         that output 64 feature maps, 4 RUs with 128 feature maps, 6 RUs with 256 feature maps, and 3 RUs with
         512 feature maps
      - see figure 14-19. Skip connection when chanaging feature map size and depth (page 508)

      Ends:
       Global average pooling layer 1024 (?12th layer)
        - outputs the mean of each feature map
       Last layers (21th - ??)
        - fully connected with 1000 units (since there are 1000 classes)
        - Softmax activation function to output estimated class probabilities

      - see Figure 14-18 ResNet architecture (page 507) [to see 

    Code: ResNet-34 implementation from scratch with Keras

         >>> DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,
         >>>                         padding="same", kernel_initializer="he_normal", use_bias=False)
         >>> 
         >>> class ResidualUnit(tf.keras.layers.Layer):
         >>>     def __init__(self, filters, strides=1, activation="relu", **kwargs):
         >>>         super().__init__(**kwargs)
         >>>         self.activation = tf.keras.activations.get(activation)
         >>>         self.main_layers = [
         >>>             DefaultConv2D(filters, strides=strides),
         >>>             tf.keras.layers.BatchNormalization(),
         >>>             self.activation,
         >>>             DefaultConv2D(filters),
         >>>             tf.keras.layers.BatchNormalization()
         >>>         ]
         >>>         self.skip_layers = []
         >>>         if strides > 1:
         >>>             self.skip_layers = [
         >>>                 DefaultConv2D(filters, kernel_size=1, strides=strides),
         >>>                 tf.keras.layers.BatchNormalization()
         >>>             ]
         >>> 
         >>>     # call() methods is used to connect all 'ResidualUnit' inputs to the 'ResidualUnit' outputs
         >>>     def call(self, inputs):
         >>>         Z = inputs
         >>>         for layer in self.main_layers:
         >>>             Z = layer(Z)
         >>>         skip_Z = inputs
         >>>         for layer in self.skip_layers:
         >>>             skip_Z = layer(skip_Z)
         >>>         return self.activation(Z + skip_Z)
         >>> 
         >>> # ResNet start with Convolution 64, 7 x 7 + 2(S) followed by Max Pooling 64, 3x3+2(S)
         >>> model = tf.keras.Sequential([
         >>>     DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),
         >>>     tf.keras.layers.BatchNormalization(),
         >>>     tf.keras.layers.Activation("relu"),
         >>>     tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding="same"),
         >>> ])
         >>> 
         >>> # add 3 RU's with 64 feature maps, 3 RU's with 128 feature maps, 4 RU's with 256 feature maps, 
         >>> #    and 6 RU's with 512 feature maps, 
         >>> prev_filters = 64
         >>> for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
         >>>     strides = 1 if filters == prev_filters else 2
         >>>     model.add(ResidualUnit(filters, strides=strides))
         >>>     prev_filters = filters
         >>> # ends with GlobalAvePool 1024, Fully Connected unit layer to output layer
         >>> model.add(tf.keras.layers.GlobalAvgPool2D())
         >>> model.add(tf.keras.layers.Flatten())
         >>> model.add(tf.keras.layers.Dense(10, activation="softmax"))

Using Pretrained Models with Keras (pages 516 - 518)

    Keras pretrained models:
      - standard pretrained models like GoogleNet or ResNet are available with a single line of code in 
        'tf.keras.applications' packages
      - to use, you first need to ensure the images have the right size, e.g ResNet expects 224 x 224 images, other
        models may expect sizes, such as 299 x 299
      - pretrained models expect the images are preprocessed in a specific way. In some cases, they may expect 
        inputs to be scaled 0 to 1, or from -1 to 1, and so on.
      - Each pretrained model provides a 'preprocess_input()' function that you can use to preprocess your image.
        These fuctions assume the original pixel values range form 0 to 255

    ResNet-50 pretrained models:
      - load using ' tf.keras.applications.ResNet50()'
      - expects 224 x 224 pixel image
      - can use Keras's Resizing layer to resize images
      - resnet.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel 
        with respect to the ImageNet dataset
      - output from 'resnet50.predict()' is a matrix with one row per image and one column per class (in this ImageNet 
        pretrained dataset case, there are 1000 classes).
      - if you want to display the top 'K' predictions, including 'class name' and the 'estimated probability', use
        'decode_predictions(<prediction>, top=<top K>)' function

    Code: Use ResNet50 pretrained model (on ImageNet dataset) to predict image type for 2 images in 'load_sample_images() 

         >>> # load ResNet-50 model, pretrained with ImageNet dataset
         >>> model = tf.keras.applications.ResNet50(weights="imagenet")
             Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5
             102967424/102967424  10s 0us/step
             
         >>> # using Keras's Resizing layer to resize 2 sample images to 224 x 224 size expected by ResNet50
         >>> images = load_sample_images()["images"]
         >>> images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)
         >>>
         >>> # Use resnet50 'preprocess_input()' to preprocess images to expected format for ResNet50 
         >>> inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)
         >>>
         >>> # make model predictions for sample images
         >>> Y_proba = model.predict(inputs)
         >>> Y_proba.shape
             1/1  3s 3s/step
             (2, 1000)
             
         >>> # display top K = 3 predictions:
         >>> top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)
         >>> for image_index in range(len(images)):
         >>>     print(f"Image #{image_index}")
         >>>     for class_id, name, y_proba in top_K[image_index]:
         >>>         print(f"  {class_id} - {name:12s} {y_proba:.2%}")
         >>> 
             Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json
             35363/35363  0s 2us/step
             Image #0
               n03877845 - palace       54.69%
               n03781244 - monastery    24.72%
               n02825657 - bell_cote    18.55%
             Image #1
               n04522168 - vase         32.66%
               n11939491 - daisy        17.81%
               n03530642 - honeycomb    12.06%

Pretrained Models for Transfer Learning (pages 518 - 521)

    Code: Load the 'tf_flowers' dataset using TensorFlow Datasets

         >>> import tensorflow_datasets as tfds
         >>> 
         >>> dataset, info = tfds.load("tf_flowers", as_supervised=True, with_info=True)
         >>> dataset_size = info.splits["train"].num_examples
         >>> class_names = info.features["label"].names
         >>> n_classes = info.features["label"].num_classes
             . . .
             Dataset tf_flowers downloaded and prepared to C:\Users\pat\tensorflow_datasets\tf_flowers\3.0.1. 
               Subsequent calls will reuse this data.
         >>> dataset_size
             3670
         >>> class_names
             ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']
         >>> n_classes
             5

    Code: Re-Load the 'tf_flowers' dataset, this time split it into 10% test, 15% validation, & 75 testing

         >>> # reload flower dataset, this time split it 10% for testing, 15% for validation, and 75% for training
         >>> test_set_raw, valid_set_raw, train_set_raw = tfds.load( "tf_flowers",
         >>>     split=["train[:10%]", "train[10%:25%]", "train[25%:]"], as_supervised=True)



    Code: Re-size 'tf_flowers' dataset images to 224 x 224 and process inputs with 'Xception.preprocess_input()'
          Note: Xception expects image color values between -1 to 1

         >>> tf.keras.backend.clear_session()  # extra code - resets layer name counter
         >>> 
         >>> batch_size = 32
         >>> # create Sequential model for resizing and Xception preprocessing images
         >>> preprocess = tf.keras.Sequential([
         >>>     tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),
         >>>     tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)
         >>> ])
         >>>
         >>> # resize/preprocess/batch train/test/valid set images plus shuffle/prefetch train set
         >>> train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))
         >>> train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)
         >>> valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)
         >>> test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)

     data augmentation:
       - since dataset is not very large, a some data augmentation will help
       - below example creates data augmentation model that we will embedded in our final model
       - during training, it will randomly flip the images horizontally, rotate them a bit (+- 0.05 * 2PI), 
         and tweek the contrast (~ +- 20%)

     Code: Create Data Augmentation (flip, rotate, & contrast) Sequential Model 

         >>> data_augmentation = tf.keras.Sequential([
         >>>     tf.keras.layers.RandomFlip(mode="horizontal", seed=42),
         >>>     tf.keras.layers.RandomRotation(factor=0.05, seed=42),
         >>>     tf.keras.layers.RandomContrast(factor=0.2, seed=42)
         >>> ])
         >>> 
         >>> # example of using 'data_augmentation' model on 1 valid batch (note: training=true must be set)
         >>> for X_batch, y_batch in valid_set.take(1):
         >>>     X_batch_augmented = data_augmentation(X_batch, training=True)
         >>> 

     tf.keras.preprocessing.image.ImageDataGenerator class
       - Applies a transformation to an image according to given parameters.  Note: Deprecated
       - easy way to load images from disk and augment them in various ways: you can shift each image, rotate it,
         flip horizontally or vertically, shear it, or apply any transformation function you want

     Loading Xception model with pretrained data plus changing top-layver
       - load Xception model pretrained ImageNet dataset using tf.keras.applications.xception.Xception(weights="imagnet")
       - exclude the top of the Xception network by setting 'include_top=false'. This excludes the global average pooling
         layer and the dense output layer. 
       - add our own global average pooling layer (feeding it the output of the base model), followed by a
         dense output layer with one unit per class, using the softmax activation function
       - Wrap in a Keras model
       - see code below:

     Code: Load Xception model pretrained on ImageNet and replace top-of-network (globalAvePool & dense output layers) 

         >>> # load the pretrained model, without its top layers, and replace them with our own, 
         >>> #   for the flower classification task:
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> base_model = tf.keras.applications.xception.Xception(weights="imagenet", include_top=False)
         >>> avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
         >>> output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)
         >>> model = tf.keras.Model(inputs=base_model.input, outputs=output)


     Training Pre-trained Models steps:
       - freeze weights of the pretrained layers (base model layers) at beginning of training
       - compile the model and start training
       - validation accurary should reach a bit over 80% and stop improving. This means new top-layers are now well trained
       - unfreeze some base model's top layers (e.g. unfreeze layers 56+)
       - set lower learning rate (e.g. change for 0.1 to 0.01) to avoid damaging the pretrained weights
       - re-compile model (required after unfreezing), and continue training
       - model shoud reach ~92% accuracy on test set

     Code: Train pre-trained model (see details above)

         >>> #  train the model for a few epochs, while keeping the base model weights fixed:
         >>> for layer in base_model.layers:
         >>>     layer.trainable = False
         >>> 
         >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
         >>> history = model.fit(train_set, validation_data=valid_set, epochs=3)
             
             . . .
             Epoch 3/3
             86/86  758s 9s/step - accuracy: 0.9268 - loss: 0.2580 - val_accuracy: 0.8421 - val_loss: 0.7664
             
             
         >>> # Optionally, print base_model layer names for layer 0 (input_layer_2) to layer 131 (block14_sepconv2_act)
         >>> for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):
         >>>     for idx in indices:
         >>>         print(f"{idx:3}: {base_model.layers[idx].name:22}", end="")
         >>>     print()
             
              0: input_layer_2          33: block4_pool            66: block8_sepconv1_act    99: block11_sepconv2_act  
             . . .
             23: block3_pool            56: block7_sepconv1_act    89: block10_sepconv2_act  122: conv2d_3    
             . . .
             32: conv2d_2               65: add_5                  98: block11_sepconv1_bn   131: block14_sepconv2_act  
             
         >>> # Now that the weights of our new top layers are not too bad, we can make the top part of the base model trainable 
         >>> # again, and continue training, but with a lower learning rate:
         >>>
         >>> # Unfreeze layers 56 and above (that's the start of the residual unit 7 out of 14)
         >>> for layer in base_model.layers[56:]:
         >>>     layer.trainable = True
             
         >>> # Continue Training after unfreezing base_model layer 56+, must compile after unfreezing 
         >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
         >>> history = model.fit(train_set, validation_data=valid_set, epochs=10)
             . . .
             Epoch 10/10
             86/86  539s 6s/step - accuracy: 0.9988 - loss: 0.0034 - val_accuracy: 0.9201 - val_loss: 0.3874


Classification and Localization (pages 521 - 523)

    Localizing an object in a picture
      - can be expressed as a regression task (see chap 10)
      - common approach is to predict the horizontal and vertical coordinates of the objects centet, as well
        as its height and width - means having 4 numbers to predict
      - do not need much change to the model, just add a 2nd dense output layer with 4 units (typically on top
        of the global average pooling layer), and it can be trained using MSE loss

      Code:  Update Pretained Model to include 4 unit Dense Output Layer to predict object [flower] location 

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> base_model = tf.keras.applications.xception.Xception(weights="imagenet", include_top=False)
         >>> avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
         >>> class_output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)
         >>> loc_output = tf.keras.layers.Dense(4)(avg)
         >>> model = tf.keras.Model(inputs=base_model.input, outputs=[class_output, loc_output])
         >>> optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  # added this line
         >>> model.compile(loss=["sparse_categorical_crossentropy", "mse"],
         >>>               loss_weights=[0.8, 0.2],  # depends on what you care most about
         >>>               optimizer=optimizer, metrics=["accuracy"])

    Localizing object bounding box in dataset
      - flower dataset does not have bounding boxex around flowers, so we would need to add them ourselves
      - Open source image labeling tools: VGG Image Annotator, Labelling, OpenLabel, or Image or commerical
        tool like LabelBox, Supervisely, else crowdsourcing

    Localizing object using bounding box in dataset
      - Each dataset item should be a tuple of the form (images, (class_labels, bounding_boxes))
      - bounding boxes should be normalized with height and width, all range from 0 to 1
      - it is common to predict the square root of the height and width instead of the height and width directly
      - MSE often works as a good cost function to train the model, but not a great metric to evaluate the model
      - most common metric (cost function) for evaluting a bounding box  is 'IoU'
      - 'intersection over Union (IoU): the area of the overlap between the predicted bounding box and the target
        bounding box, divided by the area of their union

Object Detection (pages 523 - 530)

    Object Detection:
      - the task of classifying and localizing multiple object in image
      objectness score:
        - the estimated probability that the image does indeed contain an object centered near the middle
        - this is a binary classification output; it can be produced by a dense output layer with as single unut, using
          the sigmoid activation function and trained using the binary cross-entropy loss

    Sliding CNN approach
      - illustrated in Figure 14-25. Detecting multiple objects bmy sliding a CNN across the image
      - in this example, the image was chopped into 5 x 7 grid, and then sliding a 3 x 3 region cross the 
        image and making predictions at each step
      - this gives a total of 15 predicted bounding boxes organized in 3 x 5 (5 - 3 + 1, 7 - 3 + 1) grid
      - this technique is straightforward, but it will often detect the same object multiple times, at slighth
        different positions

    Non-max suppression
      - some preprocessing of the 'sliding CNN approach' is needed to get rid of all the unnecessary bounding boxes
      - non-max suppression is a common approach:
         1. get rid of all the bounding boxes for the objectness score is below some threshold
         2. Find the remaining bounding box with the highest objectness score, and get rid of all the remaining bounding
            boxes that overlap a lot with it (with IoU greater than 60%)
         3. repeat step 2 until there are no bounding boxes to get rid of
      - this approach to object detection works pretty well, but it requires running the CNN many times (15 times in 
        this example)

  Fully Convolutional Networks (pages 525 - 527)

    Fully Convolutional Networks (FCNs)
      - fully convolutional network (FCN) is faster way to slide a CNN across an image
      - introduced in 2015 for semantic segmentation (the task of classifying every pixel in an image according to
        to class of the object it belongs to
      - A fully convolutional neural network (FCN) is a special type of CNN that only contains convolutional layers. As a 
        result of using only convolutional layers, FCNs can work with input images of any size, while standard CNNs only accept 
        fixed-size images.
      - an FCN contains only convolutional layers (and pooling layers, which have the same property), it can be trained
        and executed on images of any size
      - you can replace the dense layers  at the top of a CNN with convolutional layers
      convert dense layer to convolutional layer
        - the number of filters in the convolutional layer must be equal to the number of units in the dense layer,
          the filter size must be equal to the size of the input feature maps, and you must use "valid" (no padding). The
          stride may be set to 1 or more.
        - dense layer expects a specific input size (since it has one weight per input feature, a convolutional layer will
          happily process images of any size (however, it does expect its input to have specific number of channels, since 
          each kernl contains a different set of weights for each input channel).
      - it's exactly like taking the original CNN and sliding it across the images using 8 steps per row and 8 steps per
        column

  You Only Look Once (pages 527 - 530)

    You Only Look Once (YOLO)
      - fast and accurate object detection architecture
      - similar to FCNs (Fully Convolutional Networks) but with the following differences:
         - For each Grid cell, YOLO only considers objects whose bounding boxes center lies within that cell
           - the bounding box coordinates are relative to that cell, where (0,0) means top-left corner of the cell and
             (1,1) menas bottom right
         -  It outputs 2 bounding boxes for each grid cell (instead of just one), which allows the model to hand cases
            where 2 objects are so close to each other that their bounding box centers lie with the same cell.
            - Each bounding box comes with its own objectness score
         - YOLO outputs a class probability  distribution for each grid cell, predicting 20 class probabilities per grid
           cell since YOLO was trained on PASCAL VOC dataset, which contains 20 classes
           - This produces a coarse 'class probability map' 
           - Note that the model predicts one class proability distribution per grid cell, not per bounding box

      Object detection Models 
        - many object detection models are available in 'TensorFlow Hub', often with pretrained weights such as YOLOv5,
          SSD (https://homl.info/ssd), Faster R-CNN (https://homl.info/fastercnn), and EfficientDet 
          (https://homl.info/efficientdet)
        - SSD and EfficientDet are 'look once' detection models, similar to YOLO
        - EfficientDet is based on the EfficientNet convolutional architecture
        - Faster R-CNN is is more complex: the image must first go through a CNN, then the output is passed to 'Region
          Proposal Network (RPN) that proposes bounding boxes that are most likely to contain an object classifier
        - The best place to start using these models is TensorFlow Hub's excellent tutorial (https://homl.info/objdet)

Object Tracking (pages 530 - 531)

    Object Tracking:
      - challenging task: objects move, tehy may grow or shrink as they closer or further away from the camera ...

    DeepSORT (https://homl.info/deepsort)
      - one of the most popular object tracking systems 
      - based on a combination of classical algorithms and deep learning
         - uses 'Kalman filters' to estimate the most likely current position of an object given prior detections
         - uses deep learning model to measure the resemblance between new detections and existing tracked objects
         - uses "Hungrarian algorithm' to map new detections to existing tracked objects
      Kalman filter:
        - For example, imagine a red ball that just bounced off a blue ball traveling in the opposite direction
        - Based on the previous position of the balls, it will predict that the balls will go through each other:
          indeed, it assumes that objects move at a constant speed, so it will not expect the bounce
      Hungrarian algorithm:
        - it only considers positions, then it would happily map the new detections to the wrong balls, as if they
          had just gone through each other and swapped color
          - However, due to the 'resemblance measure', this algorithm will notice the problem. Assuming the balls
          are not too similar, the algorithm will map the new detections to the correct balls
      - DeepSORT implementations available on GitHub include TensorFlow implementation of YOLOv4 + DeepSORT:
        https://github.com/theAIGuysCode/yolov4-deepsort
      

Semantic Segmentation (pages 531 - 535)

    semantic segmentation:
      - each pixel is classified according to the class of the objects it belongs to (e.g. road, car, pedestrian, building, etc)
      - the main difficulty in this task is that when images go through a regular CNN, they gradually lost their
        spatial resolution (due to layers with strides greater than 1); so a regular CNN may end up knowing that there's a person 
        somewhere in the bottom left of the image, but it will not be much more precise than that.

    upsampling
      - increasing the size of an image
      transposed convolutional layer
        - this is equivalent to first stretching the image by inserting rows and columns (full of zeros), then performing
          regular convolutional (see figure 14-28. Upsampling using a transposed convolutional layer)
        - you can think of it as a regular convolutional layer that uses 'fractional strides' (e.g. the stride is 1/2 
          in figure 14-2)
        - In Keras, you can use Conv2DTranspose layer
        - transposed convolutional layer, the 'stride' defines how the input will be stretched, not the size of the filter
          steps, so the larger the stride, the larger the output

     Other Keras Convolutional Layers:
       tf.keras.layers.Conv1D:
         - a convolutional layer for 1D inputs, such as a time series or text (sequences of letters or words)
       tf.keras.layers.Conv3D
         - a convolutional layer for 3D inputs, such as 3D PET scans
       dilation_rate
         - setting the dilation_rate hyperparameter of any convolutional layer to a value of 2 or more creates an 
           a-trous convolutional layer ('a-trous' is french for "with holes")
           - this is equivalent to using a regular convolutional layer with a filter dilated by inserting rowss and
           columns to zero (i.e. holes)

Chapter 14 Exercises (page 535 - 536):

    -> see exercise_notebooks/14_exercises.ipynb

------------------------------------------------------
Chapter 15 Processing Sequences using RNNs and CNNs
------------------------------------------------------
    Recurrent Neural Networks (RNNs):
      - a class of net that can predict the future (well, up to a point)
      - RNNs can analyze time series data , such as the number of daily active users on your website, the hourly temperature
        of your city, your home's power consumption, etc.
      - RNNs can work on sequences of arbitrary lengths, rather than on fixed-size inputs
      ARMA family of models
        - often used to forecast time series, and use them as baselines to compare with our RNNs
      RNNs Main Difficulties
        - Unstable Gradients
          - can be alleviated using various techniques include 'recurrent dropout' and 'recurrent layer normalization'
        - Limited short-term memory
          - can be extended using LSTM and GRU cells. 
       WaveNet
         - a CNN architecture capable of hanling sequences of tens of thousands of tiem steps

Recurrent Neurons and Layers (pages 538 - 542) 

    Recurrent Neuron Netowrk
      - at each step 't' every neuron receives both the input vector x_t and the output vector from the previous time step
        y_^_(t-1)
      - each recurrent neuron has two sets of wetights: on for the input x_t and the other for the outputs of the of the
        previous type step, y_^_(t-1). Let's call these weights w_x and w_y_^

      - output vector of the whole recurrent layer can be compute as shown in Equation 15-1

     Equation 15-1. Output of a recurrent layer for a single instance.

       y_^_t = phi ( W_x_T x_t + W_y_^_T y_^_(t-1)  + b)

       where:
         b: bias vectory
         phi(.): activation function (e.g. ReLU)
     

     Equation 15-2. Outputs of a recurrent layer neurons for all instances in a pass.

       Y_^_t = phi ( X_t W_x + Y_^_(t-1) W_y_^ + b)

             = phi ([ X_t Y_^_(t-1)] W + b)    

             where:  W = | W_x   |
                         | W_y_^ |

        in 15-2 equation:
              Y_^_t: an m x n_neurons matrix containing the layer's outputs at time step 't' for each instance in the
                     mini-batch ('m' is the number instances in the mini-batch and n_neurons is the number of 'neurons'

              X_t:   is an m x n_inputs matrix containing the inputs of all instances (n_inputs is the number of
                     input features

              W_x:   is an n_inputs x n_neurons matrix containing connection weights of the inputs of the current time step

              W_y_^: is an n_neurons x n_neurons matrix containing connection weights of the outputs of the previous time step

              b:     is a vector of size n_neurons containing each neurons's bias term

              The weights of matrices W_x and W_y_^ are often concatentated verifically intoa a single W of shape
                (n_inputx + n_neurons) x n_neurons (see 2nd line of Equation 15-2)

                The notation [ X_t Y_^_(t-1) ] represents the horizontal concatentation of the maxtrices X_t and Y_^_(t-1)

  Memory Cells (pages 540 - 541)

    Memory:
      - since the output of a recurrent neuron at time step 't' is a function of all the inputs from previous time step
        you could savy it has a form of 'memory'

    Memory Cell (or simple 'cell'):
      - a part of a neural network that preserves some state across time steps
      - a single recurrent neuron or a layer or recurrent neurons is a very basic cell, capable of learning only short
        patterns (typically about 10 steps long, but htis varies depending on the task)
      cell state:
        - a cell's state at time step 't', denoted h_f (the 'h' stands for 'hidden) is a functon of some inputs at that
          time step and its state at the previous time step: h_t = f(x_t, h_(t-1))
        - its output at time step 't', denoted y_^_t, is also a function of the previous state and the current inputs


  Input and Output Sequences (pages 541 - 542)

    Sequence-to-sequence
      - an RNN can simultaneously take a sequence of inputs and produce a sequence of outputs (top-left of figure 15-4)
      - useful to forecast time series (e.g. daily power consumption)

    Sequence-to-vector
      - feed the network a sequence of inputs and ignore all outputs except the last one (top-left figure 15-4)
      - example, feed the network a sequence of words corresponding to movie reviews, and output a sentiment score

    vector-to-sequence
      - feed the network the same input vector over and over again at each time step and let it output a sequence
        (bottom-left fig 15-4)
      - example, the input could be an image (or CNN output), and output could be a caption for the image

    encoder, decoder, encoder-decoder
      - could have sequence-to-vector network, called an 'encoder', followed by a vector-to-sequence network, called
        a 'decoder' (bottom-right network of fig 15-4)
      - example, could be used for translating a sentence from one language to another
      - use a two step process, feed the network a sentence from one language, the encoder would convert this sentence
        into a single vector representation, ant eh decoder would decode this vector into a sentence in another language

Training RNNs (pages 542 - 543) 
     
     Backpropogation through time (BPTT):
       - to train an RNN, the trick is to unroll it through time, then use regular backpropagation (see figure 15-5 
       page 543)
       - Just like in regular backpropagation, there is a forward pass through the unrolled network. Then the output
         sequence is evaluated using a loss function  Loss(Y_0, Y_1, Y_2, Y_T; Y_^_0 Y_^_1, ..., Y_^_T) (where Y_i, is
         the ith target, Y_^_i is the ith prediction, and T is the max time step)
           - Note: the 'Loss' function max ignore some outputs

Forecasting a Time Series (pages 543 - 565) 

    time series:
      - data with values at different time steps, usually ater regular intervals
    multivariate time series:
      - time series with multiple value per time step (e.g. bus and train ridership)
    univariate time series:
      - time series with single value per time step (e.g. only train ridership)
    forecasting
      - predicting future values is the most typical task when dealing with time series
    
    weekly seasonality
      - when a similar pattern is clearly repeated  every week
    naive forecasting
      - simply copying a past value to make our forecast (copy last weeks values)
      - copying the latest known vlaues (e.g. forecasting tha tomorrow will be the same as today). However, 
        in the ridership example, copying values from the previous week works better due to strong
        weekly seasonality
    differencing
      - the difference between two time series (e.g. value at time 't' mis the value time t - 7 [days])
    autocorrelated
      - when a time series is correlated with a lagged version of itself
    
    Common Metrics:

      Mean Absolute Percentage Error (MAPE)
        - the percentage equivalent of mean absolute error (MAE). That MAE / target
        - Mean absolute percentage error measures the average magnitude of error produced by a model

      MAE, MAPE, MSE 
        - are among the most common metrics you can use to evaluate your forecasts
        - choosing right metric depends on the taks

      MSE (mean squared error): 
         MSE(X,h) = [ (1/m) SUM (h(xi) - yi)**2 ] where SUM is from i=1 to i=m

         m: number of instances in thd dataset
         xi: a vector of all the features values of the ith instances of the dataset, and
             yi is the label (desired output)
         X: is a matrix containing all the features excluding labels of all instances in the dataset
         h: your systems prediction's function, also called the 'hypothesis'

      MAE (mean absolute error):
         MAE(X,h) =  (1/m) SUM |(h(xi) - yi)|  where SUM is from i=1 to i=m

      Code: Calculate Bus & Rail March - May 2019 Ridership MAE and MAEP with previous week:

         >>> # calcuate bus & rail ridership differences with previous week [diff 7 days]
         >>> diff_7 = df[["bus", "rail"]].diff(7)["2019-03":"2019-05"]

         >>> # calcuate the Ridership MAE with previous week
         >>> # Mean absolute error (MAE), also called mean absolute deviation (MAD):
         >>> diff_7.abs().mean()
             bus     43915.608696
             rail    42143.271739
             dtype: float64

         >>> # Mean absolute percentage error (MAPE):
         >>> targets = df[["bus", "rail"]]["2019-03":"2019-05"]
         >>> (diff_7 / targets).abs().mean()

     differencing
      - the difference between two time series (e.g. value at time 't' mis the value time t - 7 [days])
      - a common technique used to remove trend and seasonality from a time series
         - example, 12 month ridership differencing not only removed yearly seasonality, but it also removed
           the long term trends (ridership downward trend in the 12 month differencing became a roughly
           constant negative value)

      Stationary time series
        - one whose statistical properties remain constant over time, without seasonality or trends

   
  The ARMA Model Family (pages 549 - 552)

    Autoregressive moving average (ARMA) model:
      - computes its forecasts using a simple weighted sun of laggard values and corrects these forecasts by
        adding a moving average
      - the moving average component is computed using a weighted sum of the last few forecast errors 
        (see equation 15.3)
      -  assumes tha
      
      Equation 15-3. Forecasting using an ARMA model

      y_^(t)  = SUM_p [alpha_i  y(t - 1) ]  +  SUM_q [ theta_i  epsilon(t-1)

           with:  epsilon(t)  = y(t)  -  y_^(t)


           where: SUM_p is from i = 1  to 'p'
                  SUM_q is from i = 1  to 'q'

          In this equation:
              y_^(t):    model's forecast for time step 't'
              y(t):      the time series' value at time step 't'
              y_^(t):    model's forecast for time step 't'

              First sum (SUM_p):   
                - weighted sum of th past 'p' values of the time series, using learned weights 'alpha_i'
                - 'p' is a hyperparameter, and it determines how far back into the past the model should look
                - the sum is the 'autoregressive' component of the model: it performs regression based on past values

              Second sum (SUM_q):   
                - the weighted sum of the past 'q' forecast errors 'epsilon(t), using the learned weight 'theta_i'
                - 'q' is a hyperparameter
                - this sum is the moving average component of the model

    Autoregressive moving average (ARMA) model and differencing:
      -  ARMA assumes that time series is stationary, if it is not stationary, the differencing may help
      - differencing over a single step will eliminate any linear trend (will give the slope of the time series at each step
      - running 'd' consective rounds of differencing computes an appropriate of the 'd_th' order deraviative of the
        time series, so it iwll eliminate polynomial trends fo the degree 'd'. 
      - This hyperparameter 'd' is called the 'order of integration'

    Autoregressive integrated moving average (ARIMA) model
      - differencing is the central contribution of ARIMA model
      - this model runs 'd' rounds of differencing to make the time series more stationary, then applies regular ARMA model

    Seasonal Autoregressive integrated moving average (SARIMA) model
      - models the time series in the same way as the ARIMA, but it additionally models a seasonal component for a given
        component (e.g. weekly), using the exact same ARIMA approach 
      - it has a total of 7 hyperparameters, the same 'p', 'd', and 'q' hyperparameters as ARIMA, plus additional
        'P', 'D', and 'Q' to model the seasonal patterns, and lastly the period of the seasonal pattern, noted 's'
      - the hyperparameters 'P', 'D', and 'Q' are just like 'p', 'd', and 'q', but they are used to model the 
        time series as 't - s', 't - 2s', 't - 3s', etc.

    Code: Fit SARIMA model to rail time series to forecast tomorrow's (June 1, 2019) ridership

         >>> # install statsmodel library
         >>> if "google.colab" in sys.modules:
         >>>     %pip install -q -U statsmodels

         >>> # using ARIMA from statsmodel library
         >>> from statsmodels.tsa.arima.model import ARIMA
         >>> 
         >>> # get daily rail ridership from 01/01/19 to 5/31/19 where 'asfreq("D")' is daily frequency
         >>> origin, today = "2019-01-01", "2019-05-31"
         >>> rail_series = df.loc[origin:today]["rail"].asfreq("D")
         >>>
         >>> # create ARIMA instance with hyperparameters: order: p=1, d=0, q=0; seasonal order: P=0, D=1, Q=1, s=7
         >>> model = ARIMA(rail_series, order=(1, 0, 0), seasonal_order=(0, 1, 1, 7))
         >>>
         >>> # fit() model and make a forecast
         >>> model = model.fit()
         >>> y_pred = model.forecast()  # returns 427,758.6 - off by 12.9% because Holiday was not considered
             
         >>> y_pred[0]  # ARIMA forecast for June 1, 2019:
             427758.626285891
         >>> df["rail"].loc["2019-06-01"]  # target value (actual Jun 1, 2019 ridership)
             379044
         >>> df["rail"].loc["2019-05-25"]  # naive forecast (value from one week earlier)
             426932

    statsmodel info:
         install:
         >>>     %pip install -q -U statsmodels
         import:
         >>> from statsmodels.tsa.arima.model import ARIMA
         pass data:
           - pass the data to the model at construction time, instead of passing it to the fit() method 
             (as with Scikit-Learn API)

    Picking SARIMA hyperparameters:
      - simplest: run a grid search where you evaluate hyperparameter combinations where the lowest MAE wins
      - good p, q, P, Q values are usually fairly small (typically 0 to 2, sometimes up to 5 or 6) 
      - good D values are typically 0 or 1 sometimes 2
      - good s (seasonal pattern period) - main seasonal pattern's period - in example 7 since the ridership has 
        a strong weekly seasonality

    Autoregressive Model:
      - A statistical model is autoregressive if it predicts future values based on past values. 
      - Autoregressive models predict future values based on past values.

    https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/
    Autocorrelation function (ACF):
      - the correlation between two observations at different points in a time series. For example, values that are 
        separated by an interval might have a strong positive or negative correlation
      - The autocorrelation function (ACF) assesses the correlation between observations in a time series for a set of lags. 
      - The ACF for time series y is given by: Corr (y_t,y_tk), k=1,2,...,

    Partial Autocorrelation function (ACF):
      - The partial autocorrelation function is similar to the ACF except that it displays only the correlation 
        between two observations that the shorter lags between those observations do not explain. 
      - For example, the partial autocorrelation for lag 3 is only the correlation that lags 1 and 2 do not explain. In 
        other words, the partial correlation for each lag is the unique correlation between those two observations after 
        partialling out the intervening correlations.

  Preparing the Data for Machine Learning Models (pages 552 - 555)

    tf.keras.utils.timeseries_dataset_from_array()
      - takes a time series as input, and builds a tf.data.Dataset containing all the windows of the desired length,
        as well as the corresponding targets

    Code: time_series_dataset_from_array() example
          - takes a time series containing numbers 0 to 5 and create a dataset containing all the windows
            of length 3 with their corresponding targets, grouped in batches of size 2
          - the windows are [0,1,2], [1,2,3], [2,3.4], and their respective targets are 3, 4, 5
          - since therea 3 windows in total which is not a multiple of the batch size (2), the last batch
            contains 1 window instead of 2

         >>> import tensorflow as tf
         >>> 
         >>> my_series = [0, 1, 2, 3, 4, 5]
         >>> my_dataset = tf.keras.utils.timeseries_dataset_from_array(
         >>>     my_series,
         >>>     targets=my_series[3:],  # the targets are 3 steps into the future
         >>>     sequence_length=3,
         >>>     batch_size=2
         >>> )
         >>> list(my_dataset)
             [(<tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[0, 1, 2], [1, 2, 3]])>,
               <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4])>),
              (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]])>,
               <tf.Tensor: shape=(1,), dtype=int32, numpy=array([5])>)]
             
         >>> for my_X,my_y in my_dataset:
         >>>     print(f'X:{my_X}, y:{my_y}')
             X:[[0 1 2] [1 2 3]], y:[3 4]
             X:[[2 3 4]], y:[5]

     
     tf.data.Dataset.range(6).window()
       - returns a dataset of window datasets:
       - to get rid of smaller windows (less than window size), pass "drop_remainder=True" to the window() method
       - returns a 'nested dataset', analogous to a list of lists
       flat_map() method:
         - takes a function as an argument, which allows you to transform each dataset in the nested dataset before
           flattening

     Code: uses windows() to create flatten dataset each with tensor of size 4 

         >>> dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)
         >>> dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))
         >>> for window_tensor in dataset:
         >>>     print(f"{window_tensor}")
             
             [0 1 2 3]
             [1 2 3 4]
             [2 3 4 5]

    Code: Use 'windows()' to create a flatten dataset with tensor of size and then map() to split window 
          into inputs and target

         >>> def to_windows(dataset, length):
         >>>     dataset = dataset.window(length, shift=1, drop_remainder=True)
         >>>     return dataset.flat_map(lambda window_ds: window_ds.batch(length))
         >>> 
         >>> dataset = to_windows(tf.data.Dataset.range(6), 4)
         >>> dataset = dataset.map(lambda window: (window[:-1], window[-1]))
         >>> list(dataset.batch(2))
             
             [(<tf.Tensor: shape=(2, 3), dtype=int64, numpy= array([[0, 1, 2], [1, 2, 3]], dtype=int64)>,
               <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4], dtype=int64)>),
              (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]], dtype=int64)>,
               <tf.Tensor: shape=(1,), dtype=int64, numpy=array([5], dtype=int64)>)]

         >>> for ds_X,ds_y in dataset:
         >>>     print(f'X:{ds_X}, y:{ds_y}')
             X:[0 1 2], y:3
             X:[1 2 3], y:4
             X:[2 3 4], y:5

     Code: Split data into training period, validation period, and test period then scale by 1/1M factor

         >>> # split ridership data to train, validation, test data and scale down by 1M (values ~between 0 and 1)
         >>> rail_train = df["rail"]["2016-01":"2018-12"] / 1e6
         >>> rail_valid = df["rail"]["2019-01":"2019-05"] / 1e6
         >>> rail_test = df["rail"]["2019-06":] / 1e6
             
     Code: Use timeseries_dataset_from_array() to create datasets for training validation

         >>> seq_length = 56
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> train_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     rail_train.to_numpy(),
         >>>     targets=rail_train[seq_length:],
         >>>     sequence_length=seq_length,
         >>>     batch_size=32,
         >>>     shuffle=True,
         >>>     seed=42
         >>> )
         >>> valid_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     rail_valid.to_numpy(),
         >>>     targets=rail_valid[seq_length:],
         >>>     sequence_length=seq_length,
         >>>     batch_size=32
         >>> )
             
  Forecasting Using a Linear Model (pages 555 - 556)

    Note: Below basic linear model example had MAE of 37,517 which is better than naive forecasting, 
          but worse thatn SARIMA model

     Code: Create basic linear [Sequential] model using Huber loss with early stopping

         >>> tf.random.set_seed(42)
         >>> model = tf.keras.Sequential([ tf.keras.layers.Dense(1, input_shape=[seq_length]) ])
             
         >>> early_stopping_cb = tf.keras.callbacks.EarlyStopping(
         >>>     monitor="val_mae", patience=50, restore_best_weights=True)
             
         >>> opt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)
         >>> model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])
         >>> history = model.fit(train_ds, validation_data=valid_ds, epochs=500, callbacks=[early_stopping_cb])
             . . .
             Epoch 183/500
             33/33  0s 2ms/step - loss: 0.0031 - mae: 0.0448 - val_loss: 0.0022 - val_mae: 0.0383

         >>> # extra code  evaluates the model
         >>> valid_loss, valid_mae = model.evaluate(valid_ds)
         >>> valid_mae * 1e6
             3/3  0s 11ms/step - loss: 0.0019 - mae: 0.0386
             37517.29801297188


  Forecasting Using a Simple RNN (pages 556 - 557)

    Code: Fit and evaluate utility function with early stopping

         >>> # extra code - defines a utility function we'll reuse several time
         >>> 
         >>> def fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500):
         >>>     early_stopping_cb = tf.keras.callbacks.EarlyStopping( monitor="val_mae", patience=50, restore_best_weights=True)
         >>>
         >>>     opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
         >>>     model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])
         >>>
         >>>     history = model.fit(train_set, validation_data=valid_set, epochs=epochs, callbacks=[early_stopping_cb])
         >>>     valid_loss, valid_mae = model.evaluate(valid_set)
         >>>     return valid_mae * 1e6


   tf.keras.layers.SimpleRNN()
     input_shape argument
       - expects a 3D input shape [batch size, time steps, dimensionality] where dimensionality is 1 for univariate
         time series and more for multivariate time series
       - ignores the first dimension (i.e. batch size), 
       - since recurrent layers can accept input sequences of any length, we can set the 2nd dimension to 'None' 
         which means 'any size'
     returns:
       - by default, recurrent layers in Keras only return the final output 
       - to make them return one output per time step, you must set the 'return_sequences=True'
     
    Code: Most Basic RNM containing a single recurrent layer with just one recurrent neuron

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> #   RNN(1, input_shape=[None, 1]) where  '1' means 1 neuron;  and
         >>> #       input_shape=[None, 1]' means: univariate sequence of any length
         >>> model = tf.keras.Sequential([ tf.keras.layers.SimpleRNN(1, input_shape=[None, 1]) ])
         >>> 
         >>> fit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.02)
             . . .
             3/3 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.1028
             
             102786.95076704025

      Most Basic univariate RNN:
        - sequence-to-vectory model since there's a single output neuron, the output vector has a size of 1
        - model is no good - has a MAE > 100K which is expected due to:
           1. the model only has a single recurrent neuron, so the only data it can use to make a prediction at each
              time step  is theinput value at the current time step and the output value of the previous time step
              - model has just 3 parameters - just one recurrent neuron with two input values (2 weights plus a biast term)
           2. the time series contains values from 0 to 1.4, but sin the default activation is 'tanh', the recurrent
             layer can can only output values between -1 and +1. There is no way to predict values between 1.0 and 1.4
         


    RNN with 32 recurrent neurons plus dense output layer:
      - According to book, MAE reaches 27703, however, below only read MAE of 82883


    Code:  univariate RNN model with 32 recurrent neurons and dense output layer with single output neuron and no activation function

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> univar_model = tf.keras.Sequential([
         >>>     tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),
         >>>     tf.keras.layers.Dense(1)  # no activation function by default
         >>> ])
         >>> 
         >>> # extra code  compiles, fits, and evaluates the model, like earlier
         >>> fit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate=0.05)

             33/33  0s 8ms/step - loss: 0.0227 - mae: 0.1760 - val_loss: 0.0147 - val_mae: 0.1629
             3/3  0s 8ms/step - loss: 0.0058 - mae: 0.0837 

             82883.8124871254

  Forecasting Using a Deep RNN (pages 557 - 559)

    Deep RNN:
      - a deep RNN with Keras is straightforward, just stack recurrent layers 
      return_sequences=True argument
        - set  return_sequences=True for all recurrent layer (except last one, if you only care about the last output)
        - if you forget to set the parameter for one recurrent layer, it will output a 2D array containing only the only
          the output of the last time step, instead of a 3D array containing outputs for all time steps
          - the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format

    Code: univariate Deep RNN  - 3 SimpleRNN layers plus dense output layer
          Note: 1st 2 RNNs are sequence-to-sequence layers, & 3rd RNN is sequence-to-vector

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> deep_model = tf.keras.Sequential([
         >>>     tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),
         >>>     tf.keras.layers.SimpleRNN(32, return_sequences=True),
         >>>     tf.keras.layers.SimpleRNN(32),
         >>>     tf.keras.layers.Dense(1)
         >>> ])
         >>> 
         >>> # extra code - compiles, fits, and evaluates the model, like earlier
         >>> fit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate=0.01)
             . . .
             33/33  1s 25ms/step - loss: 0.0025 - mae: 0.0406 - val_loss: 0.0020 - val_mae: 0.0354
             3/3  0s 8ms/step - loss: 0.0013 - mae: 0.0272     

             28913.522139191628


    RNN with 32 recurrent neurons plus dense output layer:
      - According to book, MAE reaches 31211, however, it actual MAE is 28913 which is much better that 1 layer 
        RNN with 32 neurons

  Forecasting Multivariate Time Series (pages 559 - 560)


    Multivariate time series
      - neural networks can deal with multivariate time series with almost no change


    Code: Create multivariate ridership dataframe and then split to 3 periods for training, validation, & testing 
           - includes rail & bus daily ridership plus next day one hot type

         >>> df_mulvar = df[["bus", "rail"]] / 1e6  # use both bus & rail series as input
         >>> df_mulvar["next_day_type"] = df["day_type"].shift(-1)  # we know tomorrow's type
         >>> # converted get_dummies's one-hot data type (dtype) from bool default to 'int' to prevent below 'tf.constant()' Valueerror 
         >>> #   reported inside  tf.keras.utils.timeseries_dataset_from_array() call
         >>> #  ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
         >>> df_mulvar = pd.get_dummies(df_mulvar, dtype='int')  # one-hot encode the day type
             
         >>> mulvar_train = df_mulvar["2016-01":"2018-12"]
         >>> mulvar_valid = df_mulvar["2019-01":"2019-05"]
         >>> mulvar_test = df_mulvar["2019-06":]
         >>> 
         >>> mulvar_test
             
              	                bus 	        rail 	next_day_type_A 	next_day_type_U 	next_day_type_W
             date 					
             2019-06-01 	0.473204 	0.379044 	0 	1 	0
             2019-06-02 	0.370049 	0.319334 	0 	0 	1
             ... 	... 	... 	... 	... 	...
             2021-11-29 	0.421322 	0.276090 	0 	0 	1
             2021-11-30 	0.450230 	0.302349 	0 	0 	0
             
             914 rows  5 columns

    Code: Create multivariate (with bus & rail ridership) train and validation time series tensor datasets

         >>> tf.random.set_seed(42)
         >>> 
         >>> seq_length = 56
         >>> train_multask_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     mulvar_train.to_numpy(),
         >>>     targets=mulvar_train[["bus", "rail"]][seq_length:],  # 2 targets per day
         >>>     sequence_length=seq_length,
         >>>     batch_size=32,
         >>>     shuffle=True,
         >>>     seed=42
         >>> )
         >>> valid_multask_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     mulvar_valid.to_numpy(),
         >>>     targets=mulvar_valid[["bus", "rail"]][seq_length:],
         >>>     sequence_length=seq_length,
         >>>     batch_size=32
         >>> )


    Code:  Create multivariate RNN model, fit and evaluate it, and get MAE for naive forecasts and multitask forecast 

        - only difference with univariate RNN is input shape is '5' wide instead of '1' plus dense layer has 2 outputs
        - RNN with 32 recurrent neurons and dense output layer with single output neuron and no activation function

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> 
         >>> # input_shape=[None, 5]' means: 5 wide multivariate sequence of any length
         >>> mulvar_model = tf.keras.Sequential([
         >>>     tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]), 
         >>>     tf.keras.layers.Dense(2)
         >>> ])
         >>> 
         >>> fit_and_evaluate(multask_model, train_multask_ds, valid_multask_ds, learning_rate=0.02)
             . . .
             Epoch 205/500
             33/33  0s 9ms/step - loss: 0.0012 - mae: 0.0343 - val_loss: 7.1989e-04 - val_mae: 0.0281
             3/3  0s 3ms/step - loss: 6.5829e-04 - mae: 0.0251 
             
             25890.452787280083
             
         >>> # extra code - evaluates the naive forecasts for bus
         >>> bus_naive = mulvar_valid["bus"].shift(7)[seq_length:]
         >>> bus_target = mulvar_valid["bus"][seq_length:]
         >>> (bus_target - bus_naive).abs().mean() * 1e6
             
             43441.63157894738
             
         >>> # extra code - evaluates the multitask RNN's forecasts both bus and rail
         >>> Y_preds_valid = multask_model.predict(valid_multask_ds)
         >>> for idx, name in enumerate(["bus", "rail"]):
         >>>     mae = 1e6 * tf.keras.metrics.mean_absolute_error(mulvar_valid[name][seq_length:], Y_preds_valid[:, idx])
         >>>     print(name, int(mae))
             
             3/3  0s 93ms/step
             bus 25999
             rail 25781

  Forecasting Several Time Steps Ahead (pages 560 - 562)

    Option 1: predict 1 time step at at time 
      - use sequence-to-vector model and output 1 value at a time
      - serial predict future values, one step (day) at a time and concatentate results before next step prediction
      - if this model mask an error at one time step, then the forecasts for the following time steps are impacted as well

    Code: Serially Predict rail ridership for next 14 days

         >>> import numpy as np
         >>> 
         >>> # X : 56 day rail ridership from validation period reshaped to 1,56,1 
         >>> X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]
         >>> # predict ridership 14 times and concatentation prediction to end of 'X'
         >>> for step_ahead in range(14):
         >>>     y_pred_one = univar_model.predict(X)
         >>>     X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)
         >>> 
         >>> X.shape   # 14 day predictions concatentated
             (1, 70, 1)
         >>> # The forecasts start on 2019-02-26, as it is the 57th day of 2019, and they end
         >>> # on 2019-03-11. That's 14 days in total.
         >>> Y_pred = pd.Series(X[0, -14:, 0], index=pd.date_range("2019-02-26", "2019-03-11"))
         >>> Y_pred
             array([[0.6835963 , 0.71308017, 0.70995367, 0.67047936, 0.37688345,
                     0.2994482 , 0.65427315, 0.712819  , 0.7280281 , 0.69648486,
                     0.6890661 , 0.39059213, 0.27703097, 0.63587916]], dtype=float32)

         

    Preparing Time Series Data for RNN in Tensorflow:
      https://mobiarch.wordpress.com/2020/11/13/preparing-time-series-data-for-rnn-in-tensorflow/
       
    Option 2: predict all time steps at once
      - train an RNN to predict next 14 days in one shot
      - still use sequence-to-vector model, but it will output 14 values instead of 1
      - still use 'timeseries_dataset_from_array()' but this time:
        - create dataset without targets (targets=None)
        - for initial dataset, increase sequences length by number of days to predict, 14 (sequence_length=seq_length + 14)
        - use .map() to customize dataset so that the sequence length is returned to 56 has 14 labels per sequence
        - use '.map()' method to apply custom function to each batch sequence, splitting them into inputs 
          and targets
            - for inputs, return input sequence length (e.g. 56) (instead of 
            - for labels, return output sequence length (e.g. 14) ; for each sequence, the 14 labels after last input

       - this approach forecasts for the next are better than for 14 days in the future, but it doesn't accumulate the errors
         like option 1

    Code: Use option 2 to forecast rail ridership for next 14 days

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> 
         >>> def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):
         >>>     return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]
         >>> 
         >>> ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     mulvar_train.to_numpy(),
         >>>     targets=None,
         >>>     sequence_length=seq_length + 14,
         >>>     batch_size=32,
         >>>     shuffle=True,
         >>>     seed=42
         >>> ).map(split_inputs_and_targets)
         >>>
         >>> ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(
         >>>     mulvar_valid.to_numpy(),
         >>>     targets=None,
         >>>     sequence_length=seq_length + 14,
         >>>     batch_size=32
         >>> ).map(split_inputs_and_targets)
         >>> 
         >>> 
         >>> tf.random.set_seed(42)
         >>> 
         >>> # input_shape=[None, 5]' means: 5 wide multivariate sequence of any length
         >>> ahead_model = tf.keras.Sequential([
         >>>     tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
         >>>     tf.keras.layers.Dense(14)
         >>> ])
         >>> 
         >>> 
         >>> # extra code - compiles, fits, and evaluates the model, like earlier
         >>> fit_and_evaluate(ahead_model, ahead_train_ds, ahead_valid_ds, learning_rate=0.02)
         >>> 
         >>> 
             Epoch 282/500
             33/33  0s 8ms/step - loss: 0.0029 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0354
             3/3  0s 8ms/step - loss: 0.0013 - mae: 0.0354 
             
             35355.404019355774
             
         >>> X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]
         >>> Y_pred = ahead_model.predict(X)  # shape [1, 14]
             1/1  0s 185ms/step

             

  Forecasting Using a Sequence-to-Sequence Model (pages 562 - 565)

    Sequence-to-Sequence forecasting
      - train model to forecast the next future values (e.g. 14 days) at each and every time step
      - turn the sequence-to-vector RNN into a sequence-to-sequence RNN

         >>> def to_windows(dataset, length):
         >>>     dataset = dataset.window(length, shift=1, drop_remainder=True)
         >>>     return dataset.flat_map(lambda window_ds: window_ds.batch(length))
         >>> 

     - Now let's create an RNN that predicts the next 14 steps at each time step. That is, instead of just forecasting 
       time steps 56 to 69 based on time steps 0 to 55, it will forecast time steps 1 to 14 at time step 0, then time 
       steps 2 to 15 at time step 1, and so on, and finally it will forecast time steps 56 to 69 at the last time step. 
       Notice that the model is causal: when it makes predictions at any time step, it can only see past time steps.


    Code: Example: convert series of numbers 0 to 6 into dataset containing sequences of 4 consecutive windows,
          each of length 3

         >>> my_series = tf.data.Dataset.range(7)
         >>> dataset = to_windows(to_windows(my_series, 3), 4)
         >>> list(dataset)
         >>> 
             [<tf.Tensor: shape=(4, 3), dtype=int64, numpy=
              array([[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5]], dtype=int64)>,
              <tf.Tensor: shape=(4, 3), dtype=int64, numpy=
              array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]], dtype=int64)>]
             
         >>> # Then use '.map()' function to split these elements into the desired inputs and targets:
             
         >>> dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))
         >>> list(dataset)
             
             [(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3], dtype=int64)>,
               <tf.Tensor: shape=(4, 2), dtype=int64, numpy=
               array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=int64)>),
              (<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4], dtype=int64)>,
               <tf.Tensor: shape=(4, 2), dtype=int64, numpy=
               array([[2, 3], [3, 4], [4, 5], [5, 6]], dtype=int64)>)]
             
    Code: Create utility function convert ridership dataset into sequences of inputs and targets 

         >>> # Let's wrap this idea into a utility function. It will also take care of shuffling (optional) and batching:
             
         >>> def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,
         >>>                        batch_size=32, shuffle=False, seed=None):
         >>>     ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)
         >>>     ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))
         >>>     if shuffle:
         >>>         ds = ds.shuffle(8 * batch_size, seed=seed)
         >>>     return ds.batch(batch_size)
         >>> 

    Code: Convert ridership train & valid datasets to sequence-to-sequence datasets

         >>> seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)
         >>> seq2seq_valid = to_seq2seq_dataset(mulvar_valid)

    Code: Create and Use sequence-to-sequence Model
          Note: SimpleRNN 'return_sequences=True' argument must be set to 'True' so that it will output a sequence
                instead of outputing a single vector at the last time step

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> seq2seq_model = tf.keras.Sequential([
         >>>     tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),
         >>>     tf.keras.layers.Dense(14)
         >>>     # equivalent: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(14))
         >>>     # also equivalent: tf.keras.layers.Conv1D(14, kernel_size=1)
         >>> ])
         >>> 
         >>> fit_and_evaluate(seq2seq_model, seq2seq_train, seq2seq_valid, learning_rate=0.1)
             
             . . .
             Epoch 315/500
             33/33  1s 15ms/step - loss: 0.0029 - mae: 0.0456 - val_loss: 0.0035 - val_mae: 0.0461
             3/3  0s 31ms/step - loss: 0.0045 - mae: 0.0503
             
         >>> X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]
         >>> y_pred_14 = seq2seq_model.predict(X)[0, -1]  # only the last time step's output
         >>> 
         >>> 45869.141817092896
         >>> 
         >>> # get MAE for prediction - note: the MAE gradually (performance) drops as the model forecasts 
         >>> #  further in the future
         >>> 
         >>> Y_pred_valid = seq2seq_model.predict(seq2seq_valid)
         >>> for ahead in range(14):
         >>>     preds = pd.Series(Y_pred_valid[:-1, -1, ahead], index=mulvar_valid.index[56 + ahead : -14 + ahead])
         >>>     mae = (preds - mulvar_valid["rail"]).abs().mean() * 1e6
         >>>     print(f"MAE for +{ahead + 1}: {mae:,.0f}")
         >>> 3/3  0s 95ms/step
         >>> MAE for +1: 23,991
         >>> MAE for +2: 28,219
         >>> . . .
         >>> MAE for +13: 35,768
         >>> MAE for +14: 33,436


    SimpleRNN and time series forecasting
      - can be quite good at forecasting time series or handling other kinds of sequences
      - but, the do not perform as well on long time series or sequences

Handling Long Sequences (pages 565 - 576) 

    RNN on long sequences
      - must run it over many time steps, making the unrolled RNN a very deep network
      - like other deep neural networks, it may suffer from unstable gradients problem
      - it may take forever to train, or training may be unstable
      - When an RNN processes a long sequence, it will gradually forget the first inputs in the sequence

  Fighting the Unstable Gradients Problem (pages 565 - 568)

    RNN unstable gradient problem
      - some tricks used to in deep networks can be used for RNNs: good parameter initialization, faster optimizations,
        dropout, etc
      - nonsaturating activations functions (e.g. ReLU) may lead the RNN to be even more unstable during training
         - nonsaturating activation functions does not prevent gradient descent updating the weights in a way that
           increases the outputs slightly at the first time step. Since weights are used at each time step, the outputs
           at the 2nd time step may also slightly increase, ..., and so on till the outputs explode
      - to reduce risk of explode outputs, use a smaller learning rate and use a saturating activiation function like 
        hyperbolici tanget, tanh
      - batch normalization cannot be used efficiently with RNNs as with deep feedforward networks. In fact, you can not
        use BN between time steps, only between recurrent layers
   
   Layer Normalization:
     - similar to batch normalization, but instead of normalizing across the batch dimension, layer normalization 
       normalizes across the features dimension
     - LN can compute the required statistics on the fly at each time step, independently for each instance. This means
       it behaves the same way during training and testing (unlike BN)
     - in an RNN, the LN layer is typically used right after the linear combination of inputs and the hidden states

    Custom memory cell:
       - LNSimpleRNNCell class inherits from tf.keras.layers.Layer, just like any custom layer
       - the constructor, __init__(), 
           - takes the number of 'units' and desired activation function ans set the 'state_size' and 'output_size' attributues
           - creates a SimpleRNN cells with no activation (so it can perform layer normalization after the linear operations
             but before the activation function)
           - creates a LayerNormalization layer, and fetches the desired activation function
       - the call() method:
         - applies the SimpleRNNCell which computes a linear combination of the current inputs and the previous hidden states
           and returns the results twice
             - in a SimpleRNNCell, the outputs are just equal to the hidden states: in other words, new_states[0] is equal
               to outputs, so we can safely ignore 'new_states' in the rest of the call() method
         - applies layer normalization, followed by the activation function
         - it returns the outputs twice, once as the outputs and once as the new hidden states

    Code: Create a custom memory cell that behaves like a SimpleRNNCell, except it will also apply layer normalization at each time step

         >>> class LNSimpleRNNCell(tf.keras.layers.Layer):
         >>>     def __init__(self, units, activation="tanh", **kwargs):
         >>>         super().__init__(**kwargs)
         >>>         self.state_size = units
         >>>         self.output_size = units
         >>>         self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,
         >>>                                                              activation=None)
         >>>         self.layer_norm = tf.keras.layers.LayerNormalization()
         >>>         self.activation = tf.keras.activations.get(activation)
         >>> 
         >>>     def call(self, inputs, states):
         >>>         outputs, new_states = self.simple_rnn_cell(inputs, states)
         >>>         norm_outputs = self.activation(self.layer_norm(outputs))
         >>>         return norm_outputs, [norm_outputs]
             
    Code: Use the custom memory cell 
          - create a tf.keras.layers.RNN() layer, passing custom memory cell, LNSimpleRNNCell, as an instance

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> custom_ln_model = tf.keras.Sequential([
         >>>     tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True, input_shape=[None, 5]),
         >>>     tf.keras.layers.Dense(14)
         >>> ])
         >>> 
             
         >>> # Just training for 5 epochs to show that it works (you can increase this if you want):
         >>> fit_and_evaluate(custom_ln_model, seq2seq_train, seq2seq_valid, learning_rate=0.1, epochs=5)
             . . .
             Epoch 5/5
             33/33  1s 18ms/step - loss: 0.0089 - mae: 0.1044 - val_loss: 0.0090 - val_mae: 0.0996
             3/3  0s 32ms/step - loss: 0.0099 - mae: 0.1032
             
             99554.01718616486

    Keras recurrent layers and cells dropout:
       - dropout hyperparameters
          - defines the droput rate to apply to the inputs
       - recurrent_dropout hyperparameter
         - defines the dropout rate for the hidden states between time steps

  Tackling the Short-Term Memory Problem (pages 568 - 576)

     RNN Short-term memory problem
       - due to transformation that goes through when traversing an RNN, some information is lost each time step
       - after a while, the RNN's state contains virtually no trace of first inputs
       - to tackle this problem, cells with long term memory have been introduced

   LSTM cells (pages 568 - 571)

     Long Short-term memory cell (LSTM):
       - if you consider the LSTM cell as a black box, it can be used like a basic cell, except it performs better,
         training converge faster, and it will detect longer-term patterns in the data
       - in Keras, simply use 'LSTM' layer instead of the 'SimpleRNN' layer


     Code: Use LSTM layer:

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> lstm_model = tf.keras.models.Sequential([
         >>>     tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),
         >>>     tf.keras.layers.Dense(14)
         >>> ])
             
         >>> # training for 5 epochs to show that it works (you can increase this if you want):
         >>> fit_and_evaluate(lstm_model, seq2seq_train, seq2seq_valid, learning_rate=0.1, epochs=5)
         >>> 
         >>> Epoch 5/5
         >>> 33/33  1s 17ms/step - loss: 0.0146 - mae: 0.1460 - val_loss: 0.0159 - val_mae: 0.1389
         >>> 3/3  0s 19ms/step - loss: 0.0166 - mae: 0.1415
         >>> 
         >>> 138939.2465353012

   How does LSTM cell work:
     - architecture is shown in Figure 15-12. An LSTM cell (page 569)
     - like a regular cell, except its state is split into two vectors h_(t) and c_(t) where h_(t) is short-term state
       and c_(t) is the long term state
     - as c_(t) tranverses the network, it goes through the 'forget gate' dropping some memories and then its adds some memories
       via addition operations (which adds the memories that were selected by the 'input gate')
     - after the addition operation, the long-term state is copied and passed through the 'tanh' function, and then
       filtered by the 'output gate'

   GRU cells (pages 571 - 572)
     
   Gated Recurrent Unit (GRU)
     - architecture is shown in Figure 15-13. An GRU cell (page 571)
     - popular variant of the LSTM cell 
     - simplified version of the LSTM cell, and it seems to perform just as well
     main GRU simiplifications from LSTM cell:
       - both state vectors are merged into a single vector h_(t)
       - single gate controller z_(t) controls both the 'forget gate' and the 'input gate'
         - if the gated controller outputs a '1', the 'forget gate' is open (= 1) and the 'input gate' is closed (1 - 1 = 0)
         - if it outputs a '0', the opposite happens
         - that is, whenever a memory must be stored, the location where it will be stored is erased first
       - there is no 'output gate'; the full state vector is output at every time step

   Keras tf.keras.layers.GRU
     - using it is just a matter of replacing the SimpleRNN or LSTM with GRU
     - also provides tf.kearas.layers.GRUCell, in case you want to create a custom cell based on a GRU cell

   RNNs with LSTM and GRU cells
     - main reasons behind the success of RNNs
     - they can tackle longer sequences than simple RNNs, but still have a fairly limited short-term memory, and they
       have a hard time learning long-term patterns in sequences of 100 time steps, or more such as audio samples,
       long time series, or long sentences

     Code: Use GRU layer:

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> gru_model = tf.keras.Sequential([
         >>>     tf.keras.layers.GRU(32, return_sequences=True, input_shape=[None, 5]),
         >>>     tf.keras.layers.Dense(14)
         >>> ])
             
         >>> # Just training for 5 epochs to show that it works (you can increase this if you want):
         >>> fit_and_evaluate(gru_model, seq2seq_train, seq2seq_valid, learning_rate=0.1, epochs=5)
             . . .
             Epoch 5/5
             33/33  1s 21ms/step - loss: 0.0106 - mae: 0.1173 - val_loss: 0.0117 - val_mae: 0.1151
             3/3  0s 23ms/step - loss: 0.0126 - mae: 0.1187
             
             115129.0163397789

   Using 1D convolutional layers to process sequences (pages 573)

   1D Convolutional layer
     - slides several kernels across a sequence, producing a 1D feature map per kernel
     - each kernel will learn to detect a single very short sequential pattern (no longer than the size of the kernel)
     - if you use 10 kernels, then the layer's output will be composed of 10 1D sequences (all of the same length), or
       equivalently you can view this ouytput as a single 10D sequence
     - you can build a neural network composed of a mix of recurrent layers and 1D convolutional layers (or even 1D pooling layers) 
     - it is actually possible to use only 1D convolutional layers and drop the recurrent layers entirely

   1D Convolutional Example:
     - starts with a 1D convolutional layer that downsamples thein input sequence by a factor of 2, using a stride of 2
     - the kernel size is larger than the stride, so all inputs will be used to compute the layer's output, and the model
       can learn to preserve the useful information, dropping only unimportant details 
     - by shortening the sequences the convolutional layer may help the GRU layers detect longer patterns, so we can afford
       to double the input sequence length to 112 days. 
     - Note: we must also crop off the first three time steps in the targets: indeed, the kernel size is 4, so the first 
       output of the convolutional layer will be based on the input time steps 0 to 3, and the first forecast will for time
       steps 4 to 17 (insteald of 1 to 14)


   Code: 1D Convolutional layer to process sequence

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>> conv_rnn_model = tf.keras.Sequential([
         >>>     tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2, activation="relu", input_shape=[None, 5]),
         >>>     tf.keras.layers.GRU(32, return_sequences=True),
         >>>     tf.keras.layers.Dense(14)
         >>> ])
             
         >>> # convert train and valid datasets to sequences
         >>> longer_train = to_seq2seq_dataset(mulvar_train, seq_length=112, shuffle=True, seed=42)
         >>> longer_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)
             
         >>> downsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))
         >>> downsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))
         >>> 
         >>> # training for 5 epochs to show that it works (you can increase this if you want):
         >>> fit_and_evaluate(conv_rnn_model, downsampled_train, downsampled_valid, learning_rate=0.1, epochs=5)
             . . .
             Epoch 5/5
             31/31 [==============================] - 1s 17ms/step - loss: 0.0115 - mae: 0.1256 - val_loss: 0.0124 - val_mae: 0.1159
             1/1 [==============================] - 0s 88ms/step - loss: 0.0124 - mae: 0.1159
             
             115850.42625665665

   WaveNet (pages 574 - 575)

     WaveNet:
       - they stack 1D convolutional layers, doubling the dilation rate (how spread apart each neuron's inputs are)
         at each layer
       - the first convolutional layer gets a glimpse of just the two time steps at a time, while the next one sees
         four time steps (its receptive field is 4 time steps long), the next one sees 8 time steps, and so on
       - see Figure 15-14. WaveNet architecture (page 574)
       - a single stack of 10 convolutional layers with these dilation rates (1, 2, 4, 8, ..., 256, 512) will act like a
         super-efficient convolutional layer with a kernel size of 1024 (except way faster, more powerful, and signficantly
         fewer parameters)

   WaveNet architecture example:
     - starts with explicit input layer - simplier than trying to setinp input_shape only on the first layer
     - continues with 1D convolutional layers using 'casual' padding 
       - casual padding: like 'same' padding, except that the zeros are appended only to the start of the input sequence, 
         instead of on both sides. This ensures that the convolutional layer does not peek into the future when making predictions
     - add similar pairs of of convolutional layers using growing dilation rates: 1, 2, 4, 8 and again 1, 2, 4, 8; and without
       activation functions
         - such convolutional layer is equivalent to a Dense layer with 14 units
     - thanks to 'casual padding' every convolutional  layer outputs a sequence of the same length as it input sequence, so the
       targets we use during training can be full 112-day sequences; no need to crop or downsize them


   Code: Using WaveNet architecture 

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility
         >>>
         >>> wavenet_model = tf.keras.Sequential()
         >>> wavenet_model.add(tf.keras.layers.InputLayer(input_shape=[None, 5]))
         >>>
         >>> for rate in (1, 2, 4, 8) * 2:
         >>>     wavenet_model.add(tf.keras.layers.Conv1D( filters=32, kernel_size=2, padding="causal", activation="relu", dilation_rate=rate))
         >>> wavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))
         >>> 
         >>> # Just training for 5 epochs to show that it works (you can increase this if you want):
         >>> 
         >>> fit_and_evaluate(wavenet_model, longer_train, longer_valid, learning_rate=0.1, epochs=5)
             . . .
             Epoch 5/5
             31/31  1s 15ms/step - loss: 0.0131 - mae: 0.1369 - val_loss: 0.0146 - val_mae: 0.1277
             1/1  0s 78ms/step - loss: 0.0146 - mae: 0.1277
             
             127744.94290351868

Chapter 15 Exercises (page 576):

    -> see exercise_notebooks/15_exercises.ipynb

------------------------------------------------------
Chapter 16 Natural Language Processing with RNNs and Attentions. pages 577 - 634
------------------------------------------------------


  RNN for NLP 
    - a common approach for Natural Language Processing (NLP) is to use Recurrent Neural Networks (RNNs)
    Character RNN (char-RNN)
      - trained to predict the next character in a sentence
      - allows one to generate original text
    Stateless RNN
      - learns on random portions of text at each iteration, without any information on the rest of the text
    Statefull RNN
      - preserves the hidden state between training iterations and continues reading where it left off, allowing
        it to learn longer patterns
    RNN for Sentiment Analysis
      - e.g. reading movie reviews and extracting the rater's feeling about the movie
    RNN encoder-decoder architectures
      - RNNs can be used to build encoder-decoder architectures capable of performing Neural Machine Translation (NMT)
        e.g. translating from English to Spanish

  Attention Mechanisms
    - neural networks components that learn to select the part of the inputs that the rest of the model should focus
      on at each time step
    - attention can be used to boost the performance of RNN-based encoder-decoder architectures 
    transformer
      - attention only networks to build a translation model
    transformers library
      - by Hugging Face

Generating Shakespearean Text Using a Character RNN (pages 578 - 587)

    Char-RNN
      - train an RNN to predict the next character in a sentence
      - can then be used generate novel text, one character at a time

  Creating the Training Dataset (pages 579 - 581)

    Code: Download shakespeare's works, create 'to_dataset()' function, and create dataset

          steps:
          1. download shakespeare 40k line works (shakespearse.txt)
          2. Create TextVectorization layer (lowercase and split by character encoding) and adapt shakespearse works
             - TextVectorization layer adapt() maps each char to an integer (token), starting at 2 (reserved: 0 for padding tokens, 
               1 for unknown chars)
          3. subtract 2 encoded TextVectorization to eliminated padding and unknown tokens
          4. create "to_dataset()" function to convert TextVectorization() sequence output to 32 batch x 100 elements 
             train/valid/test datasets
             "to_dataset() steps:
             a. ds = tf.data.Dataset.from_tensor_slices(sequence)
                Convert TextVectorization char sequence to dataset (1 element per slice)
             b. ds = ds.window(length + 1, shift=1, drop_remainder=True)
                Uses dataset.Windows() method to create shifting by 1 windows (of size length + 1) _WindowDatasets
                Note: window() method returns a dataset containing windows, where each window is itself represented as a dataset. 
                      Something like {{1,2,3,4,5},{6,7,8,9,10},...}, where {...} represents a dataset
             c. ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))
                Use dataset.flat_map() to flatten dataset (from dataset per window, to 1 dataset) compbined with dataset.batch() 
                to created a nested dataset batched by length + 1 -> creates _FlatMapDataset
                Note: The flat_map() method returns all the tensors in a nested dataset, after transforming each nested dataset. 
             d. ds = ds.shuffle(100_000, seed=seed)
                Optionally, shuffle dataset
             e. ds = ds.batch(batch_size)
                Batch up dataset by batchsize (32) -> batches of 32 arrays each containing length+1 (101) elements
             f. return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)
                return dataset and target where dataset size is  34853 x 32 x 100  
                   (note: 115394 / 32 = 34856 difference due to drop_remainder=True)
          4. use "to_dataset()" function to convert TextVectorization() sequence output to 32 batch x 100 elements 
             train (90%)/valid (5%)/test (5%) datasets

         >>> import tensorflow as tf
         >>> 
         >>> shakespeare_url = "https://homl.info/shakespeare"  # shortcut URL
         >>> filepath = tf.keras.utils.get_file("shakespeare.txt", shakespeare_url)
         >>> with open(filepath) as f:
         >>>     shakespeare_text = f.read()
         >>> 
         >>> # extra code - shows all 39 distinct characters (after converting to lower case)
         >>> "".join(sorted(set(shakespeare_text.lower())))
             
             \n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz"
               
         >>> text_vec_layer = tf.keras.layers.TextVectorization(split="character", standardize="lower")
         >>> text_vec_layer.adapt([shakespeare_text])
         >>> encoded = text_vec_layer([shakespeare_text])[0]   
             #         note: [0] just removes extra dimension - no data is removed! array[[21, ...,12]]> to array[21, ..., 12]>
             
         >>> encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use
         >>> n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39
         >>> dataset_size = len(encoded)  # total number of chars = 1,115,394
             
         >>> n_tokens
             39
         >>> 
         >>> dataset_size
             1115394
             
         >>> def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):
         >>>     # slice sequence to individual elements
         >>>     ds = tf.data.Dataset.from_tensor_slices(sequence)
         >>>     # create sliding windows of <length> size (100) elements, shifting 1 element per window 
         >>>     ds = ds.window(length + 1, shift=1, drop_remainder=True)
         >>>     # flatten windows so they can be increased by 1 element (e.g. from 100 to 101)
         >>>     ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))
         >>>     if shuffle:
         >>>         ds = ds.shuffle(100_000, seed=seed)
         >>>     # Create batches of windows
         >>>     ds = ds.batch(batch_size)
         >>>     # 
         >>>     return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)
             
         >>> # extra code - a simple example using to_dataset()
         >>> # There's just one sample in this dataset: the input represents "to b" and the
         >>> # output represents "o be"
         >>> list(to_dataset(text_vec_layer(["To be"])[0], length=4))
             
             [(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]], dtype=int64)>,
               <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]], dtype=int64)>)]
             
         >>> length = 100
         >>> tf.random.set_seed(42)
         >>> train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,
         >>>                        seed=42)
         >>> valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)
         >>> test_set = to_dataset(encoded[1_060_000:], length=length)

    How to use windows created by the Dataset.window() method in TensorFlow 2.0?
      https://stackoverflow.com/questions/55429307/how-to-use-windows-created-by-the-dataset-window-method-in-tensorflow-2-0

      question: WHY we need this flat_map step? 
      anwer: The window() method returns a dataset containing windows, where each window is itself represented as a dataset. Something like 
          {{1,2,3,4,5},{6,7,8,9,10},...}, where {...} represents a dataset. But we just want a regular dataset containing tensors: 
          {[1,2,3,4,5],[6,7,8,9,10],...}, where [...] represents a tensor. The flat_map() method returns all the tensors in a nested dataset, 
          after transforming each nested dataset. If we didn't batch, we would get: {1,2,3,4,5,6,7,8,9,10,...}. By batching each window to its 
          full size, we get {[1,2,3,4,5],[6,7,8,9,10],...} as we desired.

    Dataset Window length:
      - it faster to train RNN on shorter input sequences, but the RNN will not be able to learn any pattern longer than
        the input sequence length
        - you could choose small length than 100, but don't make it too small


  Building and Training the Char-RNN Model (pages 581 - 582)

    Code:  Build and train Shakespearn Stateless RNN Model with Embedding and GRU layers

           Model information:
             Embedding layer
               - use to encode the character IDs (reduce the dimensionality)
               - number of inputs dimensions is the number of distinct character IDs (n_tokens=39)
               - output dimensions is a hyperparameter that you can tune - setting to 16 for now
               - inputs will be a 2D tensor of shape [batch size (32), window length (100)]
               - outputs will be a 2D tensor of shape [batch size (32), window length (100), embedding size]
             GRU Layer
               - composed of 128 units which could be tweaked
             Dense output layer
               - must have n_token=39 units for the 39 distinct characters in the text, and we want the output
                 probability for each one at each time step
               - since 39 output probabilities should sum to 1 at each time step, softmax activation function is 
                 applied to the dense layer outputs
           compile information:
             - using "sparse_categorical_crossentropy" loss and Nadam optimizer
           shakespeare_model model wrapper information:
             - base model does not handle text processing, so this model wrapper has the textvectorization layer
             - lambda layer added to drop tokens 0 (pad) and 1 (unknown), which we will not use

        Warning: the following code may one or two hours to run, depending on your GPU. Without a GPU, it may take over 24 hours. 
        If you don't want to wait, just skip the next two code cells and run the code below to download a pretrained model.

        Note: the GRU class will only use cuDNN acceleration (assuming you have a GPU) when using the default values for the following 
        arguments: activation, recurrent_activation, recurrent_dropout, unroll, use_bias and reset_after.

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> model = tf.keras.Sequential([
         >>>     tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),
         >>>     tf.keras.layers.GRU(128, return_sequences=True),
         >>>     tf.keras.layers.Dense(n_tokens, activation="softmax")
         >>> ])
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> # due to error: ValueError: The filepath provided must end in `.keras` (Keras model format). Received: filepath=my_shakespeare_model
         >>> # changed "tf.keras.callbacks.ModelCheckpoint("my_shakespeare_model" ...)" 
         >>> #   to:  "tf.keras.callbacks.ModelCheckpoint("my_shakespeare_model.keras" ...)"
         >>> 
         >>> model_ckpt = tf.keras.callbacks.ModelCheckpoint( "my_shakespeare_model.keras", monitor="val_accuracy", save_best_only=True)
         >>> history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])
         >>> 
         >>> shakespeare_model = tf.keras.Sequential([
         >>>     text_vec_layer,
         >>>     tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens
         >>>     model
         >>> ])

    Code:  download pretrained model (since training takes 1 - 24 hours) then predict next char

         >>> # extra code - downloads a pretrained model
         >>> url = "https://github.com/ageron/data/raw/main/shakespeare_model.tgz"
         >>> path = tf.keras.utils.get_file("shakespeare_model.tgz", url, extract=True)
         >>> model_path = Path(path).with_name("shakespeare_model")
         >>> shakespeare_model = tf.keras.models.load_model(model_path)
         >>> 
         >>> y_proba = shakespeare_model.predict(["To be or not to b"])[0, -1]     # [0, -1] only last time step output
         >>> y_pred = tf.argmax(y_proba)  # choose the most probable character ID
         >>> text_vec_layer.get_vocabulary()[y_pred + 2]                            # add back 2 removed char
             1/1 [==============================] - 1s 566ms/step
             'e'

  Generating Fake Shakespearean Text (pages 582 - 584)

    temperature scaling
      - Temperature scaling divides the logits (inputs to the softmax function) by a learned scalar parameter. 
        I.e.  softmax = e^(z/T) / sum_i e^(z_i/T) where z is the logit, and T is the learned parameter. We learn 
        this parameter on a validation set, where T is chosen to minimize NLL.
      - to have more control over the diversity of the generated text, we can divide the logits by a number called
        the 'temperature'
      - a temperature close to 0 favors high-probability characters
      - a high temperature gives all chars equal probability
      - lower temperatures are preferred when generating fairly rigid and precise text, such as mathematical equations
      - higher temperatures are preferred when generating more diverse and creative texts

    Code:  Use 'temperature' setting to generating fake shakespearan text
        next_char() function info:
           a. use 'predict()' to obtain probability of next char
           b. rescale probability with 'temperature
           c. return one sample from a categorical distribition for the rescaled logits value
        extend_text() function info:
           - call next_char() function for 'n_chars'
        Generate fake shakespearan text for 0.01, 1, and 100 temperatures

         >>> def next_char(text, temperature=1):
         >>>     y_proba = shakespeare_model.predict([text])[0, -1:]
         >>>     rescaled_logits = tf.math.log(y_proba) / temperature
         >>>     char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]
         >>>     return text_vec_layer.get_vocabulary()[char_id + 2]
             
         >>> def extend_text(text, n_chars=50, temperature=1):
         >>>     for _ in range(n_chars):
         >>>         text += next_char(text, temperature)
         >>>     return text
             
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
             
         >>> 
         >>> print(extend_text("To be or not to be", temperature=0.01))
             . . .
             1/1 [==============================] - 0s 38ms/step
             To be or not to be the duke
             as it is a proper strange death,
             and the
             
         >>> print(extend_text("To be or not to be", temperature=1))
             . . .
             1/1 [==============================] - 0s 37ms/step
             To be or not to behold?
             
             second push:
             gremio, lord all, a sistermen,
             
         >>> print(extend_text("To be or not to be", temperature=100))
             1/1 [==============================] - 0s 54ms/step
             To be or not to bef ,mt'&o3fpadm!$
             wh!nse?bws3est--vgerdjw?c-y-ewznq

  Stateful RNN (pages 584 - 586)

    Stateless RNN
      - at each training iteration the model starts with a hidden state full of zeros, then it updates the state
        each time step, and after last time step, it throws it away as it is not needed anymore

    Stateful RNN
      - preserve the final state after processing a training batch and use it as the initial state for then next training batch
      - this way model can learn long-term patterns despite only backpropagating through short sequences


    Difference between Stateful and Stateless RNNs: 
      https://medium.com/@iqra.bismi/difference-between-stateful-and-stateless-rnns-2b397184e759

      Key Differences [between stateful and stateless RNNs]:
        - Memory Retention: Stateful RNNs retain memory across sequences, capturing long-term dependencies, while stateless RNNs 
          treat each sequence independently without retaining memory.
        - Sequence Order: Stateful RNNs preserve the order and continuity of sequences, whereas stateless RNNs treat sequences 
          as isolated entities.
        - Training and Inference: Stateful RNNs require careful management of sequence boundaries during training and inference, 
          while stateless RNNs do not have this requirement.

    Building Stateful RNN
      - each sequence in a batch starts exactly where the corresponding sequence in the previous batch left off
      - use sequential and non-overlapping input sequences (rather than shuffled and overlapping sequences previous used
        to train stateless RNNs)
      - when creating the tf.data.Dataset, when must use 'shift=length' (instead of shift=1) when calling 'window()' method
        and must not shuffle
      - batching is much harder - simpliest solution is use a batch size of 1

    Code: Create 'to_dataset_for_statefull_rnn()' function, and create statefull datasets

         Changes made 'to_dataset_for_statefull_rnn()' from 'to_dataset() function
           a. no batch size option, use batch size of 1  
           b. ds = ds.window(length + 1, shift=length, drop_remainder=True)
              - shift=length instead of 1
           c. remove shuffle option, and combined flat_map() and batch() using batch size of 1

         >>> def to_dataset_for_stateful_rnn(sequence, length):
         >>>     ds = tf.data.Dataset.from_tensor_slices(sequence)
         >>>     ds = ds.window(length + 1, shift=length, drop_remainder=True)
         >>>     ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)
         >>>     return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)
         >>> 
         >>> stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)
         >>> stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000], length)
         >>> stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)


    Code: Examine results of 'to_dataset_for_stateful_rnn() steps

         >>> length_tst=100
         >>> ds_tst1 = tf.data.Dataset.from_tensor_slices(encoded[:1000])
         >>> ds_tst2 = ds_tst1.window(length_tst + 1, shift=length_tst, drop_remainder=True)
         >>> ds_tst3 = ds_tst2.flat_map(lambda window: window.batch(length_tst + 1)).batch(1)
         >>> tst_ex  = ds_tst3.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)
         >>> 
         >>> cnt = 0
         >>> for window in ds_tst2:
         >>>     print([elem.numpy() for elem in window])
         >>>     cnt += 1
         >>>     if cnt > 4:
         >>>         break


    Code:  Build and train Shakespearn Stateful RNN Model with Embedding and GRU layers

        Changes to from stateless model to stateful model:
          model:
            Embedding layer: added: batch_input_shape=[1, None]
            GRU layer:       added: stateful=True
          ResetStatesCallback:
            - added to reset states after each epoch
          model_chkpt: 
             changed checkpoing save directory name
          model.fit(): 
              - used stateful train and validation datasets
              - added 'ResetStatesCallback' checkpoint

         >>> tf.random.set_seed(42)  # extra code  ensures reproducibility on CPU
         >>> model = tf.keras.Sequential([
         >>>     tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]),
         >>>     tf.keras.layers.GRU(128, return_sequences=True, stateful=True),
         >>>     tf.keras.layers.Dense(n_tokens, activation="softmax")
         >>> ])
         >>> 
         >>> class ResetStatesCallback(tf.keras.callbacks.Callback):
         >>>     def on_epoch_begin(self, epoch, logs):
         >>>         self.model.reset_states()
         >>> 
         >>> # extra code  use a different directory to save the checkpoints
         >>> model_ckpt = tf.keras.callbacks.ModelCheckpoint("my_stateful_shakespeare_model",  
         >>>     monitor="val_accuracy", save_best_only=True)
         >>> 
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> history = model.fit(stateful_train_set, validation_data=stateful_valid_set,
         >>>                     epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])



Sentiment Analysis (pages 587 - 595)

    Code: Download IMDb movie review dataset and split into train (90%), valid (10%), & test datasets
        IMDb: contains 50K reviews with target of 0 for negative and 1 for positive review

         >>> import tensorflow_datasets as tfds
         >>> 
         >>> raw_train_set, raw_valid_set, raw_test_set = tfds.load(
         >>>     name="imdb_reviews",
         >>>     split=["train[:90%]", "train[90%:]", "test"],
         >>>     as_supervised=True
         >>> )
         >>> tf.random.set_seed(42)
         >>> train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)
         >>> valid_set = raw_valid_set.batch(32).prefetch(1)
         >>> test_set = raw_test_set.batch(32).prefetch(1)

    Code: print 200 chars of train set 1st 4 reviews and label/target

         >>> for review, label in raw_train_set.take(4):
         >>>     print(review.numpy().decode("utf-8")[:200], "...")
         >>>     print("Label:", label.numpy())

     tokenize and detokenize text at the subwork level
       - can be used handle handle rare word it nwa never seen before and guess what it means 
       Byte Pairing Encoding (BPE):
         - works by splitting the whole training set into individual chars (including spaces) and
           merging the  most frequent adjacent pairs until the vocabulary reaches the desired size
       Subword Regularization
         - improves accuracy and robustness by introducing some randomness in tokenization during training
       TensorFlow Text (https://homl.info/tftext) Library
         - implements various tokenization strategies, including WordPlace (a variant of BPE)
       Tokenize Library by Hugging Face (https://homl.info/tokenizers)
          - implements a wide range of fast tokenizers



    Code: Create TextVectorization layer, create RNN review model using it plus Embedding, GRU, 
          & dense output layers, and then compile/fit movie review dataset

        TextVectorization layer: 
           - limits vocabulary to 1000 tokens (998 most frequent words plus padding and unknown word tokens)
           - map() to pass only reviews from train_set to adapt()
        Model:
           text_vec_layer
           Embedding layer
             - use to encode the character IDs (reduce the dimensionality) - that is, convert word IDs to embeddings
             - the embedding matrix needs to have one row per token in the vocabulary (vocab_size=1000), and
               one column per embedding dimension - using 128 dimensions, but this is a hyperparameter
               that you can tune
             - number of inputs dimensions is the number of distinct words IDs (vocab_size=1000)
             - output dimensions is a hyperparameter that you can tune - setting to embedded_size=128 for now
             GRU Layer
               - composed of 128 units which could be tweaked
             Dense output layer
               - 1 neuron and sigmoid activation function since this is a binary classification task: the model's 
                 output will be the estimated probability that review expresses a positive sentiment
        Compile information:
             - using "binary_crossentropy" loss and Nadam optimizer

         >>> vocab_size = 1000
         >>> text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)
         >>> text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))
         >>> 
         >>> embed_size = 128
         >>> tf.random.set_seed(42)
         >>> model = tf.keras.Sequential([
         >>>     text_vec_layer,
         >>>     tf.keras.layers.Embedding(vocab_size, embed_size),
         >>>     tf.keras.layers.GRU(128),
         >>>     tf.keras.layers.Dense(1, activation="sigmoid")
         >>> ])
         >>> model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> history = model.fit(train_set, validation_data=valid_set, epochs=2)
             Epoch 1/2
             704/704 [==============================] - 667s 942ms/step - loss: 0.6935 - accuracy: 0.5017 - val_loss: 0.6931 - val_accuracy: 0.5028
             Epoch 2/2
             704/704 [==============================] - 669s 951ms/step - loss: 0.6930 - accuracy: 0.5039 - val_loss: 0.6940 - val_accuracy: 0.5000

    Model fails to learn
      - accuracy remains close to 50%, not better than random chance
      - due to most sequences from TextVectorization end with many padding tokens to make them as long 
        as the longest sequence
      - even with GRU layer, it short-term memory is not great, so when it goes through many padding
        tokens, it ends up forgetting what the review was about
      1st Solution:
        - feed the model with batches of equal length sentences
      2nd Solution:
        - make RNN ignore padding tokens - can be done using 'masking'

  Masking (pages 590 - 593)

    Masking the model:
      - ignore padding tokens
      - to use, simply add "masking_zero=True" argument to Embedding layer

    Embedding model masking:
      - creates a 'mask tensor' equal to 'tf.math.not_equal(inputs, 0): a boolean tensor with the same shape as the inputs,
        and it is equal to False anywhere the token IDs are 0, or True otherwise
      - 'mask tensor' is automatically  propagated by the model to the next layer. If that layer's call() method
        has a mask argument, then it automatically receives the mask
      - each layer may handle the mask differently, but in general, they simply igmore masked time steps. For example,
        when are recurrent layer encoutersa masked time step, it simply copies the output from the previous time step
      
     Recurrent layers support for masking attribute
       - a recurrent layer's supports_masking attribute is True when 'return_sequences=True', but if false when 
         'return_sequences=False' since there is no need for a mask in this case
       - in below code example, GRU does not have 'return_sequences=True', so it will receive and use the mask
         automatically, but it will not propagate it any further

      Keras layers supporting masking:
        - include GRU, LSTM, Bidirectional, Dense, TimeDistributed, Add
        - convolutional layers including Conv1D do not support masking - it's not obvious how they would do so anyways
     
      Code: Create RNN review model with TextVectorization, Embedding, GRU, 
          & dense output layers, plus Masking enabled, and then compile/fit movie review dataset

         >>> embed_size = 128
         >>> tf.random.set_seed(42)
         >>> model = tf.keras.Sequential([
         >>>     text_vec_layer,
         >>>     tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),
         >>>     tf.keras.layers.GRU(128),
         >>>     tf.keras.layers.Dense(1, activation="sigmoid")
         >>> ])
         >>> model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> history = model.fit(train_set, validation_data=valid_set, epochs=5)
             Epoch 5/5
             704/704 [==============================] - 293s 417ms/step - loss: 0.2597 - accuracy: 0.8961 - val_loss: 0.3209 - val_accuracy: 0.8548

      Custom layer with masking support
        - add a mask argument to the 'call()' method
        - if mask must be propagated to next layers, then you should set 'self,supports_masking=True in the constructor
        - if mask must be update before it is propagated, then you must implement the 'compute_mask()' method

      Masking Layer
        -  if you model does not start with an Embedding layer, you may use the tf.keras.Masking layer instead
        - by default, it sets the mask to 'tf.math.reduce_any(tf.math.not_equal(X, 0), axis=1)' meaning the time steps
          where th last dimension is full of zeros will be masked out in subsequent layers
     

       Masking using Ragged Tensors
         - one approach to masking is to feed the model with ragged tensors
         - in practice, all you need to do is to set 'ragged=True' when creating the TextVectorization layer, so
           all the input sequences are represented as ragged tensors
         - Kera's recurrent layers have built-in support ragged tensor, so there's nothing else you need to do,
           just use this TextVectorization layer with ragged=True in your model

       Code: Masking using Ragged Tensors:

         >>> text_vec_layer_ragged = tf.keras.layers.TextVectorization( max_tokens=vocab_size, ragged=True)
         >>> text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))
         >>> text_vec_layer_ragged(["Great movie!", "This is DiCaprio's best role."])
         >>> 
         >>> text_vec_layer(["Great movie!", "This is DiCaprio's best role."])
             
             <tf.Tensor: shape=(2, 5), dtype=int64, numpy=
             array([[ 86,  18,   0,   0,   0],
                    [ 11,   7,   1, 116, 217]])>

  Reusing Pretrained Embeddings and Language Models (pages 593 - 595)

    Pretrained models
      - today, reusing pretrained language models is the norm (post ULMFit paper)

    Code: Build a classifier based on the Universal Sentence Encoder
         Notes:
           TFHUB_CACHE_DIR: 
             - by default, TensorFlow Humb models are save to a temporary directory, and they get downloaded again & again
             - this env variable used to specify directory of your choice, and then only download once

         >>> import os
         >>> import tensorflow_hub as hub
         >>> 
         >>> os.environ["TFHUB_CACHE_DIR"] = "my_tfhub_cache"
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> model = tf.keras.Sequential([
         >>>     hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4",
         >>>                    trainable=True, dtype=tf.string, input_shape=[]),
         >>>     tf.keras.layers.Dense(64, activation="relu"),
         >>>     tf.keras.layers.Dense(1, activation="sigmoid")
         >>> ])
         >>> model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> model.fit(train_set, validation_data=valid_set, epochs=10)

An Encoder-Decoder Network for Neural Machine Translation (pages 595 - 604)

    Neural Machine Translation (NMT) (https://homl.info/103)
      - will translate English sentences to Spanish
      - architecture (see Figure 16-3. A simple Machine translation model. page 596): 
          - [via textvectorization] initially each word is represented by its ID
          - Next, and embedding layer returns the word embedding
          - the embeddings are then fed to the encoder and the decoder
          - english sentences fed as inputs to the encoder
          - decoder outputs the Spanish translation
            - Note: Spanish translations are also used as inputs to the decoder during training (teacher forcing), but shifted 
              back by one step
          - at each step, the decoder output a score for each word in the output vocabulary (i.e. Spanish), then the softmax
            activation function turns these scores into probabilities. The word with the highest probability is output
          - at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, you need
            to feed it the work that it has just output at the previous step. This ill require an embedding lookup (not shown
            in figure 16-4)
            - see figure 16-4. At inference time, the decoder is fed as input the work it just output at the previous time step. page 597 

    Teacher forcing:
      - during training the decoder is given as input the word that it should have output at the previous step,
        regardless of what it actually output
      - for the very 1st word, the decoder is give the 'start-of-sequence' (SoS) token, and the decoder is expected to
        end the setence with an 'end-of-sequence' (EoS) token
      https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c
        - teacher Forcing remedies this as follows: After we obtain an answer for part (a), a teacher will compare our 
          answer with the correct one, record the score for part (a), and tell us the correct answer so that we can use 
          it for part (b).

    Code: Download dataset of English/Spanish sentence pairs and convert to English & spanish sentence lists
          steps:
          - download English/Spanish dataset
          - remove 2 spanish chars that TextVectorization layer does not handle
          - convert downloaded dataset to 'pairs' list (split on 'tab' separator)
          - create english dataset (sentences_en) and spanish targets (sentences_es)
          - print 3 sentence pairs

         >>> url = "https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
         >>> path = tf.keras.utils.get_file("spa-eng.zip", origin=url, cache_dir="datasets",
         >>>                                extract=True)
         >>> text = (Path(path).with_name("spa-eng") / "spa.txt").read_text()
             
         >>> import numpy as np
         >>> 
         >>> text = text.replace("", "").replace("", "")
         >>> pairs = [line.split("\t") for line in text.splitlines()]
         >>> np.random.seed(42)  # extra code - ensures reproducibility on CPU
         >>> np.random.shuffle(pairs)
         >>> sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists
             
         >>> for i in range(3):
         >>>     print(sentences_en[i], "=>", sentences_es[i])
             
             How boring! => Qu aburrimiento!
             I love sports. => Adoro el deporte.
             Would you like to swap jobs? => Te gustara que intercambiemos los trabajos?

    Code:  Create and Adapt English & Spanish TextVectorization layers 
         Steps: 
            - create 2 TextVectorization layers - one for each language and adapt to the text
              - limit vocab size to 1000 since dataset is small (118964 sentence pairs)
              - all sentences in dataset have a max of 50 words, set output_sequence_length to 50
              - added startofseq to begin and endofseq to end of spanish sentences inputs 
            - inspect first 10 tokens of both textvectorization vocabularies

         >>> vocab_size = 1000
         >>> max_length = 50
         >>> text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)
         >>> text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)
         >>> text_vec_layer_en.adapt(sentences_en)
         >>> text_vec_layer_es.adapt([f"startofseq {s} endofseq" for s in sentences_es])
             
         >>> text_vec_layer_en.get_vocabulary()[:10]
         >>> ['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']
             
         >>> text_vec_layer_es.get_vocabulary()[:10]
         >>> ['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']
             
    Code:  Create English train & valid datasets with Spanish targets plus Spanish Decoder train & valid datasets  
           - add "startofseq to beginning of sentences in Spanish decoder datasets
           - add "endofseq to end of sentences in Spanish targets

         >>> X_train = tf.constant(sentences_en[:100_000])
         >>> X_valid = tf.constant(sentences_en[100_000:])
         >>> X_train_dec = tf.constant([f"startofseq {s}" for s in sentences_es[:100_000]])
         >>> X_valid_dec = tf.constant([f"startofseq {s}" for s in sentences_es[100_000:]])
         >>> Y_train = text_vec_layer_es([f"{s} endofseq" for s in sentences_es[:100_000]])
         >>> Y_valid = text_vec_layer_es([f"{s} endofseq" for s in sentences_es[100_000:]])
             
    Code:  Build Translation Model with Kera Functional API (not sequential)
         Steps:
            - create encoder and decoder input layers
            - connect encoder/decorder outputs to previously created En/Es TextVectorization layers inputs
            - create encoder/decoder embedding layers with mask_zero=True is set to ensure masking is handled automaticatlly
            - connect En/Es TextVectorization outputs to embedding layers inputs
            - create Encoder/Decoder LSTM layer and connect encoder/decoder embedding outputs to Encoder/Decoder LSTM layers
              - For encoder LSTM: 
                 - set return_state=True so it will also return a reference to the layer's final state
                 - Note: LSTM layer has 2 states: short-term state & long-term state
                 - used '*encoder_state' to group both final states in a list
              - For decoder LSTM: 
                - set initial_state to encoder final states output (initial_state=encoder_state) 
            - create Decoder Dense output layer with the softmax activiation function to get the word probabilities 
              for each step
            - Create Kera Model to wrap this layers setting model inputs to encoder_inputs & decoder_inputs, and
              Y_prob to its outputs
            - compile and fit model

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)
         >>> decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)
             
         >>> embed_size = 128
         >>> encoder_input_ids = text_vec_layer_en(encoder_inputs)
         >>> decoder_input_ids = text_vec_layer_es(decoder_inputs)
         >>> encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)
         >>> decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)
         >>> encoder_embeddings = encoder_embedding_layer(encoder_input_ids)
         >>> decoder_embeddings = decoder_embedding_layer(decoder_input_ids)
             
         >>> encoder = tf.keras.layers.LSTM(512, return_state=True)
         >>> encoder_outputs, *encoder_state = encoder(encoder_embeddings)
             
         >>> decoder = tf.keras.layers.LSTM(512, return_sequences=True)
         >>> decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)
             
         >>> output_layer = tf.keras.layers.Dense(vocab_size, activation="softmax")
         >>> Y_proba = output_layer(decoder_outputs)
             
         >>> model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))
         >>> 
         >>> Epoch 10/10
         >>> 3125/3125 [==============================] - 673s 215ms/step - loss: 0.0782 - accuracy: 0.8402 - val_loss: 0.2027 - val_accuracy: 0.6763
         >>> 
         >>> <keras.callbacks.History at 0x7f897878ac10>

    Code:  Use Translation Model via 'translate()' function 
           - Since the decoder expects as input the word that was predicted at the previous time step
           - to make it simple, 'translate()' translate one word at a time 

         >>> 
         >>> def translate(sentence_en):
         >>>     translation = ""
         >>>     for word_idx in range(max_length):
         >>>         X = np.array([sentence_en])                       # encoder input 
         >>>         X_dec = np.array(["startofseq " + translation])   # decoder input
         >>>         y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas
         >>>         predicted_word_id = np.argmax(y_proba)
         >>>         predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]
         >>>         if predicted_word == "endofseq":
         >>>             break
         >>>         translation += " " + predicted_word
         >>>     return translation.strip()
             
         >>> 
         >>> translate("I like soccer")
             'me gusta el ftbol'
             
         >>> # Nice! However, the model struggles with longer sentences:
             
         >>> translate("I like soccer and also going to the beach")
             'me gusta el ftbol y a veces mismo al bus'  # a veces: sometimes, mismo: even


    How to improve this translation model:
      - increase training set size and add more LSTM layers in both the encoder and decoder


    Optimizing the Output Layer
      - when the ouput vocabulary is large, outputting a probability for each and every possible word can be slow
      solution 1:
        - look only at the logits output by the model for the correct word and for a random sample of incorrect words,
          then compute the approximation of the loss based only on these logits (sampled softmax technique)
        - in TensorFlow, you can uses the tf.nn.sampled_softmax_loss() function for this during trainining, and use
          the normal softmax function at inference time (sampled softmax cannot be used at inference time because
          it requires knowing the target)
      solution 2:
        - tie the weights of the output layer to the transpose of the decoder's embedding matrix (see chap 17)
       
  Bidirectional RNNs (pages 601 - 603)

    Casual recurrent layer:
      - at each time step, a regular recurrent layer only looks at past and present inputs before generating its outputs
      - casual meaning - it cannot look into the future
      - casual RNN makes sense for forecasting times series or in the decoder of a sequence-to-sequence (seq2seq) model

    bidirectional recurrent layer:
      - run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them 
        from right to left, then combine their outputs at each time step, typically by concatenating them
      - see figure 16-5. A bidirectional recurrent layer. page 602
      tf.keras.layers.Bidirectional layer
        - use to implement bildirectional layers


    Code: Add bidirectional layer to encoder of the previous translation model
        Steps:
           - replace encoder LSTM layer with encoder LSTM bidirectional layer
             - it returns 4 states (1 short-term and 1 long-term state from each LSTM) instead of 2, 
               so we need to concatentate both short-terms states and concatenate both long-term states 
           - re-create rest of translation model 

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> encoder = tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(256, return_state=True))
         >>> 
         >>> encoder_outputs, *encoder_state = encoder(encoder_embeddings)
         >>> encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)
         >>>                  tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)
             
         >>> # extra code - completes the model and trains it
         >>> decoder = tf.keras.layers.LSTM(512, return_sequences=True)
         >>> decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)
         >>> output_layer = tf.keras.layers.Dense(vocab_size, activation="softmax")
         >>> Y_proba = output_layer(decoder_outputs)
         >>> model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))
             
             Epoch 10/10
             3125/3125 [=========================] - 565s 181ms/step - loss: 0.0682 - accuracy: 0.8577 - val_loss: 0.1951 - val_accuracy: 0.6906
             
             <keras.callbacks.History at 0x7f892d2d5fa0>
             
         >>> translate("I like soccer")
             'me gusta el ftbol'


  Beam Search (pages 603 - 604)

    Beam Search:
      - for a trained encoder-decoder model, it is a technique that gives a model a chance to go back and fix mistakes
      - keeps track of a short list of the 'k' most promising sentences (e.g top 3), and at each decoder step it tries
        to extend them by one word, keeping on the 'k' most likely sentences
      - the parameter 'k' is call the beam width
      - good translations for fairly short sentences
      - due to limited short-term memory of RNNs, beam models are bad at translating long sentences
      TensorFlow Addons Library
        - includes a full seq2seq API that lets you build encoder-decoder models with attention including 'beam search'

Attention Mechanisms (pages 604 - 620)

    Attention Mechanisms
      - revolutionalized Neural Machine Translation (NMT) especially for long sentences
      - a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step
      - figure 16-7. Neural machine translation using an encoder-decoder network with an attention model. page 606
         - encoder-decoder: instead of just sending the encoder's final hidden state to the decoder, as well as the 
           previous target word at each step, it now sends all of the encoder's outputs to the decoder as well
          - since the decoder cannot deal with all those encoder's outputs at once, they are aggregated: at each time step,
            the decoder's memory cell computes a weighted sum of all the encoder outputs. This determines which words
            it will focus on at this step
          - the weight alpha_(t,i) is the weight of the i_th encoder output at the t_th decoder time step
          - For example, if the weight alpha_(3,2) is larger than alpha_(3,0) and alpha_(3,1), then the decoder will
            pay more att the encoder's output for the #2 word ('soccer' in 'I like soccer') than to the other 2 outputs,
            at least at this time step

    Alignment Model (or attention layer):
      - generates the alpha_(t,i) weights
      - neural network trained jointly with the encoder-decoder model used in an attention mechanism
      - starts with a Dense layer composed of a single neuron that processes each of the encoder's outputs along with
        the decoder's previous hidden state (e.g. h_(2) )
         - this layer outputs a score (or energy) forech encoder outut (e.g. alpha_(3,2) ): this score measures
           how well each output is alighed with the decoder's previous hidden state
      - finally, all scores go through a softmax layer to get a final weight for each encoder output (e.g. alpha_(3,2) )
      Bahdanau attention (or concatentative attention or additive attention)
        - above attention technique
      Luong attention (or multiplicative attention)
        - the goal of alignment model is to measure the similarity between one of the encoder's outputs and the
          decoder's previous hidden layer
        - simply compute the dot product of the these two vectors, as this is often a fairly good similarity measure
        - the dot product gives a score, and all the scores (at a given decoder time step) go through a softmax
          layer to give the final weights (just as in the Bahdanau attention)
        - another simplificaiton was to used the decoder's hidden state at the current time step rather than at
          the previous time step (e.g. h_(t) rather than h_(t-1), then use the output of the attention mechanism
          directly (noted: h~_(t) directly to compute the decoder's predictions, rather than using it to compute
          decoder's current hidden state
       "general" dot product approach
         - a variant of the dot product mechanism  where then encoder output first go through a fully connected 
           layer (without a bias term) before the dot products are computed
       dot product variants vs concatenative attention 
            - dot product variants performed better than concatentative attention. 

    Equation 16-1. Attention Mechanisms. page 607

        h~_(t)  = SUM_i [alpha_(t,i) y(i)]

             with alpha_(t,i)  =   exp (epsilon_(t,i)  /  SUM_i [ exp( epsilon_(t,i') ]


                                   |  h_(t)_T  y_(i)             dot
                                   | 
             and  epsilon_(t,i) =  |  h_(t)_T  W  y_(i)          general
                                   | 
                                   |  v_T tanh(W[h_(t); y_(i)])   concat

    BiLingual Evaluation Understudy (BLEU) Score
      - compares each translation produced by a model with several good translations produced by humans: it counts the
        number of n-grams (sequences of 'n' words) that appear in any target translation and adjusts the score to  take
        into account the frequency of the produced n-grams in the target translations

    Keras tf.keras.layers.Attention
      - for Luong attention
    Keras tf.keras.layers.AdditiveAttention
      - for Bahdanau attention
    Keras tf.keras.layers.Attention and Keras tf.keras.layers.AdditiveAttention
      - both expect a list as input containing two or 3 items: the 'queries', the 'keys', and optionally the 'values'
      - if no 'values' are passed, then they are automatically equal to the 'keys'
      - for the below Attention code example:
          - the decoder outputs are the 'queries'
          - the encoder outputs are both the 'keys' and the 'values'
          - for each decoder output (i.e. 'query'), the attention layer returns a weighted sum of the encoder
            outputs (i.e. keys/values) that are most similar to the decoder output


     
    Code: Add Luong attention to translator encoder-decoder model
        steps:
          - set LSTM 'return_sequence=True' when creating encoder layer
             - We need to feed all the encoder's outputs to the Attention layer, so we must add 
               return_sequences=True to the encoder
          - re-use encoder/decoder parts of the model
          - create the attention layer and pass the encoder outputs and decoder outputs to it
             - for simplicity, passing the decoder's outputs instead of its states; in practice, this works well
               and it is much easier to code
          - create model and train it as before

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> encoder = tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))
             
         >>> # extra code - this part of the model is exactly the same as earlier
         >>> encoder_outputs, *encoder_state = encoder(encoder_embeddings)
         >>> encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)
         >>>                  tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)
         >>> decoder = tf.keras.layers.LSTM(512, return_sequences=True)
         >>> decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)
             
         >>> attention_layer = tf.keras.layers.Attention()
         >>> attention_outputs = attention_layer([decoder_outputs, encoder_outputs])
         >>> output_layer = tf.keras.layers.Dense(vocab_size, activation="softmax")
         >>> Y_proba = output_layer(attention_outputs)
             
         >>> model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))
             
             Epoch 10/10
             3125/3125 [========================] - 581s 186ms/step - loss: 0.0929 - accuracy: 0.8205 - val_loss: 0.1903 - val_accuracy: 0.7077
             <keras.callbacks.History at 0x7f87e5c8ad90>
             
         >>> translate("I like soccer and also going to the beach")
             'me gusta el ftbol y tambin ir a la playa'

    
  Attention Is All You Need: The Original Tansformer Architecture (pages 609 - 620)

    transformer:
       - by google researchers - significantly trans
       - improved the state-of-the-art NMT without using any recurrent or convolutional layers, just attention mechanisms
         (plus embedded layers, dense layers, normalization layers, etc.)
       - see figure 16-8. The original 2017 transformer architecture. page 610
         - left part is the encoder
         - right part is the decoder
         - each embedding layer (at beginning of encoder and decoder) outputs a 3D tensor of shape [batch size, sequence length, enbedding size]
         - after embedding layers, the tensors are gradually transformed as they flow through the transformer, but their shape remains the same
         - after going through the decoder, each word representation goes through a final dense layer with a softmax activation
       encoder / decoder
         - contain N stacked modulues (N=6 for paper)
         - components enclude 2 embedding layers, several skip connections, each of them followed by a layer normalization; several 
           feedforward that are composed of 2 dense layers each (1st with ReLU activation & 2nd using no activation)
         - final outputs of whole encoder stack are fed to the decoder at each of the N levels
       output layer:
         - a dense layer using the softmax activation function
       encoder's multi-head attention layer
         - updates each word representation by attending (i.e. paying attention to) all other words in the same sentence
         - this is where vague words (e.g. 'like') become richer more accurate representations (e.g. "to be fond of")
       decoder's masked multi-head attention layer
         - its a 'casual' (only looks at past and present inputs) - when it processes a word, it doesn't attend to words after it 
       decoder's upper multi-head attention layer
         - cross-attention: where it pays attention to words in the English sentence (pays attention to 'soccer' when it processes 
           the 'el' [in 'el futbol']
       positional encoding
         - after input (English) / output (Spanish) embedding layers, and before N-stacked attention modules 
         - dense vectors (much like word embeddings) that represent the position of each word in a sentence. The nth positional
           encoding is added to the word embedded on the nth word in the sentence
           - needed because all layers in the transform architecture ignore word positions, and word positions do matter

  Positional Encoding (pages 612 - 614)

    Code: Previous code from building translation model that is used by Positional Encoding

         Steps:
            - create encoder and decoder input layers
            - connect encoder/decorder outputs to previously created En/Es TextVectorization layers inputs
            - create encoder/decoder embedding layers with mask_zero=True is set to ensure masking is handled automaticatlly
            - connect En/Es TextVectorization outputs to embedding layers inputs

         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)
         >>> decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)
             
         >>> vocab_size = 1000
         >>> max_length = 50
         >>> embed_size = 128
         >>> encoder_input_ids = text_vec_layer_en(encoder_inputs)
         >>> decoder_input_ids = text_vec_layer_es(decoder_inputs)
         >>> encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)
         >>> decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)
         >>> encoder_embeddings = encoder_embedding_layer(encoder_input_ids)
         >>> decoder_embeddings = decoder_embedding_layer(decoder_input_ids)

    Code: Add Positional Encoding to encoder and decoder inputs
         steps:
           - create positional embedded layer
           - get shape of encoder embedded layer 1 dim (50)
           - add encoder embedded layer outputs to positional embedded outputs (generated from range of encoder
             embedded 1 dim)
           - repeat for decocder

         >>> max_length = 50  # max length in the whole training set
         >>> embed_size = 128
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)
         >>> batch_max_len_enc = tf.shape(encoder_embeddings)[1]
         >>> encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))
         >>> batch_max_len_dec = tf.shape(decoder_embeddings)[1]
         >>> decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))

   Positional Encoding matrix
     - see Equation 16-2. Sine/Cosine positional encodings
     - after positional encoding are to the word embedding, the rest of the model has access to the absolute
       position of each word in a sentence, because there is a unique positional encoding for each position
     - No PositionalEncoding layer in TensorFlow - example code for sine/cosine positional encoding provided on page 614

  Multi-head attention (pages 615 - 620)

    Scaled dot-product attention layer
      - multi-head attention layer is based on scaled dot-product attention layer


      Equation 16-3. Scaled dot-product attention. page 615

        Attention(Q, K, V) = softmax ( (Q K_T ) / ( d_keys)**1/2 ) V

          where:
             Q: a matrix containing one row per query. Its shape is [n_queries, d_keys] where n_queries is the 
                number of queries and d_keys is the number of dimensions of each query and each key
             K: a matrix containing one row per key. Its shape is [n_keys, d_keys] where n_keys is the 
                number of keys and values
             V: a matrix containing one row per value. Its shape is [n_keys, d_values] where d_values is the 
                number of dimensions of each value


            Q K_T shape is [n_queries, n_keys]. It contains one similarity score for each query/key pair

            Scaling factor 1 / (d_keys)**1/2  scales down the similarity scores to avoid saturating the softmax function
               which would lead to tiny gradients

           Possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity
             scores, just before computing the softmax function

    tf.keras.layers.Attention layer
      - if you set 'use_scale=True', thne it will create an additional parameter that lest the layer learn how to properly
        downscale the similarity scores
      - the scaled dot-product attention used in the transformer model is almost the same, except it always scales similarity
        scores by the same (1 / (d_keys)**1/2) 
      - the Attention layer's inputa are just like Q, K, and V, except with extra batch dimension (the 1st dimension)

    Multi-head attention layer
      - see figure 16-10. multi-head attention layer architecture. page 616
      - contains a stack of scaled dot-product attention layers, each preceeded by a linear transformation of values, keys, 
        and queries (i.e. time-distributed dense layer with no activation function)
      - all outputs [of the stacked scaled dot-product] are concatentated and they go through a final linear time-distributed
        transformation
      - applies multiple different linear transformation ov the values, keys, and queries: this allows the model to apply
        different projections of the word representation into different subspaces, each focusing on a subset of the
        word characteristics (e.g. different meaning/representations of a word such as the word 'like')
      - then the scaled dot-product attention layer implement the lookup phase, and finally we concatentate all the
        results and project them back to the original space
        
    tf.keras.layers.MultHeadAttention layer
      - this is keras multi-head attention layer implementation
      - previously did not automatically [causal??] support masking, but now has "use_causal_masking=<bool>" option

    Code: Previous - Add Positional Encoding to encoder and decoder inputs
         steps:
           - create positional embedded layer
           - get shape of encoder embedded layer 1 dim (50)
           - add encoder embedded layer outputs to positional embedded outputs (generated from range of encoder
             embedded 1 dim)
           - repeat for decocder

         >>> max_length = 50  # max length in the whole training set
         >>> embed_size = 128
         >>> tf.random.set_seed(42)  # extra code - ensures reproducibility on CPU
         >>> pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)
         >>> batch_max_len_enc = tf.shape(encoder_embeddings)[1]
         >>> encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))
         >>> batch_max_len_dec = tf.shape(decoder_embeddings)[1]
         >>> decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))

    Code: Build transformer architecture (figure 16-8. page 610) which includes N-stacks modules with Multi-head attention layers
       Steps:
         - Create encoder pad mask for encoder inputs not equal to 0 - pay attention to all words in sentence
             Note: mask used to ignore encoder padding inputs
         - Create encoder N-stacked (N=2 instead of 6 transformer) modules which include: 
           - skip1 connections 
           - MultiHeadAttention layers with attention_mask=encoder_pad_mask using Encoder Embedding layer outputs
           - Normalization layer with Add layer (adds attention layer + skip1)
           - skip2 connections 
           - FeedForward block containing 2 dense layers (1st with ReLU activation, 2nd with no activation)
           - dropout layer
           - Normalization layer with Add layer (adds dropout layer + skip2)
            
         - Create decoder pad mask for decoder inputs not equal to 0 - pay attention to all words in sentence
             Note: mask used to ignore decoder padding inputs
         - Create causal mask (in future versions, should be able to use "use_causal_mask=True" option)
             Note: Causal mask is used to ignore all future tokens
         - Create decoder N-stacked (N=2 instead of 6 transformer) modules which include: 
           - skip1 connections 
           - MultiHeadAttention layers with attention_mask=causal_mask & decoder_pad_mask using Decoder Embedding layer outputs
           - Normalization layer with Add layer (adds attention layer + skip1)
           - MultiHeadAttention layers with attention_mask=decoder_pad_mask using Normalization layer outputs & encoder stack outputs
           - skip2 connections 
           - FeedForward block containing 2 dense layers (1st with ReLU activation, 2nd with no activation)
           - dropout layer
           - Normalization layer with Add layer (adds dropout layer + skip2)

         - Create Output Dense Layer with Softmax activation
         - Create model, compile, and fit (train) it

         >>> N = 2  # instead of 6
         >>> num_heads = 8
         >>> dropout_rate = 0.1
         >>> n_units = 128  # for the first Dense layer in each Feed Forward block
         >>> encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]
         >>> Z = encoder_in
         >>> for _ in range(N):
         >>>     skip = Z
         >>>     attn_layer = tf.keras.layers.MultiHeadAttention(
         >>>         num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)
         >>>     Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)
         >>>     Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
         >>>     skip = Z
         >>>     Z = tf.keras.layers.Dense(n_units, activation="relu")(Z)
         >>>     Z = tf.keras.layers.Dense(embed_size)(Z)
         >>>     Z = tf.keras.layers.Dropout(dropout_rate)(Z)
         >>>     Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
             
         >>> decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]
         >>> causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix
         >>>     tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)
             
         >>> encoder_outputs = Z  # let's save the encoder's final outputs
         >>> Z = decoder_in  # the decoder starts with its own inputs
         >>> for _ in range(N):
         >>>     skip = Z
         >>>     attn_layer = tf.keras.layers.MultiHeadAttention(
         >>>         num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)
         >>>     Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)
         >>>     Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
         >>>     skip = Z
         >>>     attn_layer = tf.keras.layers.MultiHeadAttention(
         >>>         num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)
         >>>     Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)
         >>>     Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
         >>>     skip = Z
         >>>     Z = tf.keras.layers.Dense(n_units, activation="relu")(Z)
         >>>     Z = tf.keras.layers.Dense(embed_size)(Z)
         >>>     Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
         >>> 
         >>> Y_proba = tf.keras.layers.Dense(vocab_size, activation="softmax")(Z)
         >>> model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])
         >>> model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
         >>> model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))
             . . . 
             Epoch 10/10
             3125/3125 [=================] - 821s 263ms/step - loss: 0.1492 - accuracy: 0.7279 - val_loss: 0.1538 - val_accuracy: 0.7281
             <keras.callbacks.History at 0x7f8946cdf9a0>
             
         >>> translate("I like soccer and also going to the beach")
             'me gusta el ftbol y yo tambin voy a la playa'

An Avalanche of Transformer Models (pages 620 - 624)

Vision Transformers (pages 624 - 629)

Hugging Face's Transformers Library (pages 629 - 633)

    Hugging Face
      - AI company that has built a whole ecosystem of easy-to-use open source tools for NLP 

    Hugging Face Transformer Library
      - allows you to  easily download a pretrained model, including its corresponding tokenizaer, and then 
        fine-tune on your own dataset, if needed
      - library supports TensorFlow, PyTorch, and JAX (with Flax library)
      - you just specify which task you wnat, such as sentiment analysis, and it downloads a default 
        pretrained model, ready to be used
      - Hugging Face transfomers pipeline doc:
        https://huggingface.co/docs/transformers/en/main_classes/pipelines
      - Hugging Face transfomers available models:
        https://huggingface.co/tasks

    Hugging Face Datasets:
      https://huggingface.co/datasets
      - includes standard datasets (such as IMBb) which can be used to fine-tune your model
      - similar to TensorFlow datasets, but it also provides tools to perform common preprocessing tasks on the fly
        such as masking

    Code: Install the Transformers and Datasets libraries if we're running on Colab:

         >>> %pip install --user -q -U transformers
         >>> %pip install --user -q -U datasets


    Code: Using Hugging Face Transformer pipeline() function for sentiment analysis task
          Note: huggingface/distilbert-base-uncased-finetuned-sst-2-english" is the default model for text classification
                tasks such as sentiment analysis

         >>> from transformers import pipeline
         >>> 
         >>> classifier = pipeline("sentiment-analysis")  # many other tasks are available
         >>> result = classifier("The actors were very convincing.")
         >>> result
            [{'label': 'POSITIVE', 'score': 0.9998071789741516}]

    Code: Using Hugging Face Transformer pipeline() function for text analysis with specific model

         >>> classifier(["I am from India.", "I am from Iraq."])
             [{'label': 'POSITIVE', 'score': 0.9896161556243896},
              {'label': 'NEGATIVE', 'score': 0.9811071157455444}]


         >>> model_name = "huggingface/distilbert-base-uncased-finetuned-mnli"
         >>> classifier_mnli = pipeline("text-classification", model=model_name)
         >>> classifier_mnli("She loves me. [SEP] She loves me not.")
             [{'label': 'contradiction', 'score': 0.9790192246437073}]
             
    Hugging Face tokenizers
      - the transformer libraries provide many classes including all sorts of tokenizers, models, configurations, 
        callbacks, and more
          

    Code: Use Hugging Face Transformer DistilBERT tokenizer to return tokens and then use tokens to predict classes
          steps:
            - load the same DistilBERT model, along with its corresponding tokenizer using
               TFAutoModelForSequenceClassification and AutomTockenizer classes
            - tokenize a couple pairs of sentences (Note: you can separate sentences with "," or [SEP]
              - the tokenizer output is a dictionary-lke instance of the BatchEncoding class which contains the 
                sequences of token IDs, as well as a mask containing 0s for padding tokens
            - pass the BatchEncoding object to the model; it returns a TFSequenceClassifierOutput object containing
              it predicted class logits
            - apply softmax activation function to convert these logits to class probabilities
            - use argmax function predict the class with the high probability for each input sentence pair
             
         >>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
         >>> 
         >>> tokenizer = AutoTokenizer.from_pretrained(model_name)
         >>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
         >>> 
         >>> token_ids = tokenizer(["I like soccer.", "We all love soccer!",
         >>>                        "Joe lived for a very long time. [SEP] Joe is old."],
         >>>                       padding=True, return_tensors="tf")
         >>> token_ids
             {'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=
             array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,
                      102,    0,    0,    0],
                    [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,
                     2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=
             array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}

         >>> outputs = model(token_ids)
         >>> outputs
             TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
             array([[-2.1123822 ,  1.1786768 ,  1.4101032 ],
                    [-0.01478079,  1.0962443 , -0.99199575]], dtype=float32)>, hidden_states=None, attentions=None)
                          
         >>> Y_probas = tf.keras.activations.softmax(outputs.logits)
         >>> Y_probas
             <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
             array([[0.01619701, 0.43523473, 0.5485683 ],
                    [0.22656073, 0.68817145, 0.08526789]], dtype=float32)>
             
         >>> Y_pred = tf.argmax(Y_probas, axis=1)
         >>> Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral
             <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1], dtype=int64)>
             
    Code: Fine-tune Hugging Face Transformer DistilBERT on your dataset

         >>> sentences = [("Sky is blue", "Sky is red"), ("I love her", "She loves me")]
         >>> X_train = tokenizer(sentences, padding=True, return_tensors="tf").data
         >>> y_train = tf.constant([0, 2])  # contradiction, neutral
         >>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
         >>> model.compile(loss=loss, optimizer="nadam", metrics=["accuracy"])
         >>> history = model.fit(X_train, y_train, epochs=2)
             
             1/1 [==============================] - 54s 54s/step - loss: 0.8417 - accuracy: 0.5000
             Epoch 2/2
             1/1 [==============================] - 1s 709ms/step - loss: 0.2526 - accuracy: 1.0000



Chapter 16 Exercises (page 633):

    -> see exercise_notebooks/16_exercises.ipynb

