{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62d9072-8db4-4b32-9426-c54800ab6de7",
   "metadata": {},
   "source": [
    "Chapter 11: Training Deep Neural Networks\n",
    "\n",
    "Chapter 11 exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71179ff-2a6a-4e1b-89d5-2dae3e705338",
   "metadata": {},
   "source": [
    "1. What is the problem with Glorot initialization and He initialization aim to fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7670d05-2006-419c-ba6e-3ff7f3c33215",
   "metadata": {},
   "source": [
    "-> Glorot proposed a way initialization strategy to alleviate the the unstable gradient problem. Similarly, the 'he iniatialization' strategy was proposed to address the unstable gradient problem when using the ReLU activation function, and the 'LeCun initialization' strategy for the SELU activation function.\n",
    "\n",
    "\n",
    "page 360:    Equation 11-1  Glorot initialization (when using the sigmoid activation function) \n",
    "\n",
    "      Normal distribution with mean 0 and variance: sigma**2 = 1 / fan-avg\n",
    "\n",
    "      Or a uniform distribution between -r and +r with r = (3 / fan-avg)**-1/2\n",
    "\n",
    "page 360: From Table 11-1:\n",
    "Note: for the normal (sigma**2) in 'He initialization' replace '1 / fan-avg' with '2 / fan-in';  \n",
    "for the normal (sigma**2) in 'LeCun initialization' replace '1 / fan-avg' with '1 / fan-in'\n",
    "\n",
    "page 359:     Glorot proposed way to alleviate the unstable gradients problem\n",
    "- the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction\n",
    "\n",
    "page 360: Some papers provided similar strategies for different activation functions ... The initialization strategy propposed for the ReLU activation function and its variants is called the 'He iniitialization'... For SELU, use Yann LeCun's initialization method, perferably with a normal distribution.\n",
    "\n",
    "book answer: Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac731f8c-d3ce-41eb-a5fb-2b58c065603e",
   "metadata": {},
   "source": [
    "2. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b37ff-8353-4c50-b249-a72e4c1323a0",
   "metadata": {},
   "source": [
    "-> No, all weights must be independently sampled, and therefore, they should not have the same value. If you initialize all weight to the same value (even if randomly selected), you model will act as if it had only one neuron per layer, and training will fail.\n",
    "\n",
    "page 311 (chapter 10) It is important to initialize all the hidden layers' connection weights randomly or else training will fail. For example, if you initialize all the weights and biases to zero, then all neuron in a give layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they remain identical. In other words, despite have hundreds of neurons per layer, you model will act as if it had only one neuron per layer: it won't be too smart. If instead, you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "page 358: \n",
    "Unstable Gradients problem:\n",
    "- more generally, DNNs suffer from 'unstable gradients': different layers may learn at widely different speeds\n",
    "\n",
    "Unstable Gradients Suspected causes:\n",
    "- combination of the sigmoid (logistic) activation function and the weight initialization technique at the time (i.e. normal distribution with a mean of 0 and stardard deviation of 1)\n",
    "- with this activation function and initialization scheme, the variance of the output of each layer is much greater that the variance of its inputs\n",
    "- going forward in the network, the variance keeps getting increasing after each layer until the activation saturates at the top layers\n",
    "- this saturation is model worse by the fact that the sigmoid function has mean of 0.5 and not 0 (the tanh function has a mean of 0 and behaves slightly better)\n",
    "\n",
    "book answer: No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d74432-3023-490c-8c24-7d991dab0717",
   "metadata": {},
   "source": [
    "3. Is it OK to initialize bias terms to zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc7550-2353-4c22-9e91-caae9cef2603",
   "metadata": {},
   "source": [
    "-> yes, it prefectly fine to initialize bias terms to zero. By default, Kera initializes biases to zero.\n",
    "\n",
    "page 322 (chapter 10): Noticed that the Dense layer initialized the connection weights randomly (which is needed to break symmetry; as discussed earlier), and biases were initialized to zero; which is fine.\n",
    "\n",
    "book answer: It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that's OK too; it does not make much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74cfbd-8fe7-435e-8330-351d79f6320d",
   "metadata": {},
   "source": [
    "4. In which cases would you want to to use each of the activation functions we discussed in this chapter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947c751-65a1-41c6-9dcd-8dc4d753e4e8",
   "metadata": {},
   "source": [
    "-> Activation functions For hidden layer:  RelU: a good default; Swish: better choice for complex task; Mish: may give slightly better results for complex tasks, but more compute; Leaky ReLU or PReLU: if you care about runtime latency for complex task, or need slightly better results than ReLU with minimal speed impact;\n",
    "-> Output layer activation functions for MLP regression: ReLU/softmax: if positive outputs; signoid/tanh: for bounded outputs \n",
    "-> Output layer activation function for MLP Classification: Sigmoid: for binary and multilabel classification (where you need to estimate the probability); softmax: for multiclass classifiation (to estimate probabilities for mutually exclusive classes)\n",
    "\n",
    "page 315: From table 10-1 Typical Regression MLP architecture\n",
    "Hyperparameter                  Typical value\n",
    "---------------                 -------------------------------\n",
    "output activation               None, or ReLU/softmax (if positive outputs) or sigmoid/tanh (if bounded outputs)\n",
    "\n",
    "page 316: From table 10-2 Typical Classification MLP architecture\n",
    "\n",
    "page 366:  hidden layers Activation Function recommendations\n",
    "Hyperparameter         Binary Classification    Multilabel Binary Classification   Multiclass Classification\n",
    "---------------        ----------------------   ---------------------------------  -------------------------\n",
    "output activation      Sigmoid                   Sigmoid                            Softmax\n",
    "\n",
    "ReLU\n",
    "- remains a good default for simple tasks\n",
    "- often as good as more sophisticated functions and fast to compute\n",
    "\n",
    "Swish\n",
    "- a better choice for more complex tasks\n",
    "- you can parameterized Swish with a learnable 'beta' for more complex tasks\n",
    "\n",
    "Mish\n",
    "- may give you slightly better results, but it requires a bit more compute\n",
    "\n",
    "Leaky ReLU or parameterized ReLU (PReLU)\n",
    "- if you care about run time latency for more complex tasks\n",
    "\n",
    "SELU\n",
    "- for deep MLP, you may want to give SELU a try\n",
    "- make sure to respect the constraints (standards inputs, LeCun normal initialization, regularization limitations)\n",
    "\n",
    "\n",
    "book answer: ReLU is usually a good default for the hidden layers, as it is fast and yields good results. Its ability to output precisely zero can also be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementations as well as from hardware acceleration. \n",
    "\n",
    "The leaky ReLU variants of ReLU can improve the model's quality without hindering its speed too much compared to ReLU. \n",
    "\n",
    "For large neural nets and more complex problems, GLU, Swish and Mish can give you a slightly higher quality model, but they have a computational cost. The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number in a fixed range (by default between –1 and 1), but nowadays it is not used much in hidden layers, except in recurrent nets. \n",
    "\n",
    "The sigmoid activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders; see Chapter 17). \n",
    "\n",
    "The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive. The softmax activation function is useful in the output layer to estimate probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bf522-5c53-441c-b506-96e892c80fc8",
   "metadata": {},
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g. 0.99999) when using SGD optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a36b4-bdf9-4760-b7fb-cdef1bcb7ca4",
   "metadata": {},
   "source": [
    "-> if the momentum hyperparameter is to close to 1, the momemtum vector will be more of average of past gradients than moving averaging resulting in the algoritm picking up too much speed carrying it right past the minimum.\n",
    "\n",
    "page 379: \n",
    "Momumentum optimization\n",
    "- cares about previous gradient were at each interation\n",
    "- uses a moving average over the past gradients; where the gradient is high, weights updates will be large\n",
    "- uses Exponential Moving Average (EMA). It assigns greater weight on the more recent values\n",
    "- momumentum optimization will roll down the valley faster and faster till it reaches the bottom (optimum)\n",
    "\n",
    "momumentum hyperparameters 'beta':\n",
    "- simulates friction mechanism and prevent the momemtum from growing too large\n",
    "- set between 0 (high friction) and 1 (no friction)\n",
    "- typical value is 0.9 usually works very well and almost always goes faster than regular gradient descent\n",
    "\n",
    "book answer: If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239939e-0423-4332-9790-f34902cb55d4",
   "metadata": {},
   "source": [
    "6. Name three ways to produce a sparse models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e864c8-b4cb-4dc4-a144-403e5566e707",
   "metadata": {},
   "source": [
    "-> 1. train the model as usual, then get rid of the tiny weights (set them to zero) - not recommended; 2. apply a strong l1 regularization during training, as it pushes the optimizer to zero out as many weights as it can; 3. Use TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API capable of iteratively removing connections during training based on their magnitude\n",
    "\n",
    "page 387:\n",
    "Training Sparse Models\n",
    "- all optimization algorithms just discussed produce dense models, meaning that most parameters will be non-zero\n",
    "- if you need a blazingly fast model at runtime or you need it take up less memory, you may prefer to end up with sparse model instead\n",
    "\n",
    "Options to create sparse model:\n",
    "- train the model as usual, then get rid of the tiny weights (set them to zero). However, this will typically not lead to a very sparse model, and it may degrade the model's performance.\n",
    "- apply a strong l1 regularization during training, as it pushes the optimizer to zero out as many weights as it can (as discussed in the \"Lasso Regression\")\n",
    "another option to create sparse model:\n",
    "- check out the TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API capable of iteratively removing connections during training based on their magnitude\n",
    "\n",
    "book answer: One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44103f6-303c-49c8-968d-501490c102e5",
   "metadata": {},
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (e.g. making predictions on new instances)? What about MC dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9282d-a354-4ec9-8356-6f8540536681",
   "metadata": {},
   "source": [
    "-> dropout does slow down convergence during training, but results in a better model\n",
    "-> dropout does not slow down inference. Dropout is enabled during training, but it is not enabled during inference (model.evaluate()) except when using MC dropout\n",
    "-> MC dropout just uses 'dropout' during the training, so MC Droput training impact is the same 'dropout' impact.\n",
    "-> MC dropout preforms multiple predictions with 'dropout' enable during the inference (evaluate()), then calculates the mean of these predictions. Enabling 'dropout' during the MC Dropout inference does slightly slow it down, but the main slow down is due to making multiple predictions. If you make 10 predictions per instance, it would have 10x+ inference slow down.  \n",
    "\n",
    "\n",
    "page 394:\n",
    "dropout technique\n",
    "- at every training step, every neuron (including input but excluding output neurons) has a probability of 'rho' of being temporarily 'dropped out' meaning it will be entirely ignored during this training step, but it may be activated during the next step\n",
    "- after training, neurons don't get dropped\n",
    "- using dropout makes neurons less sensitive to slight changes in the inputs\n",
    "\n",
    "page 397: \n",
    "dropout performance / convergence\n",
    "- dropout does tend to significantly slow down convergence, but it often results in a better model when tuned properly. So, it is generally well worth the extra time and effort especially for large models\n",
    "\n",
    "book answer: Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6baa93db-2f16-4bcd-add8-1bd355b339d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea227312-cbb1-403c-9485-63be7c5c679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8cd7b0-82da-414f-b391-460f2142af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"deep\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ced0c-7be2-476e-9fb4-f8dc21f5077c",
   "metadata": {},
   "source": [
    "#### 8. Practice training a deep neural network on CIFAR10 image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888110de-30a9-4d1f-b052-6e6b0b6532b0",
   "metadata": {},
   "source": [
    "##### 8a. Build a DNN with 20 hidden layers of 100 neurons each (that's too may, but it's the point of this exercise). Use 'He initialization' and 'Swish activation' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f3b40-422b-460a-b6e1-590cdfb11d20",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset images are of color images with a (32 x 32) resolution. All the images in the dataset are of shape (32,32,3) where 3 represents the number of channels i.e R-G-B (Red,Green & Blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ec3bb2-c4c2-4db3-a98f-cdc99804521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"swish\",\n",
    "                                    kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8efac3-5120-49c0-a5e2-c550c38575f7",
   "metadata": {},
   "source": [
    "##### 8b. Using 'Nadam' optimizaztion and early stopping, train the network on the CIFAR10 dataset. You can load it with 'tf.keras.datasets.cifar10.load_data()'. The dataset is composed of 60,000 3x x 32-pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a 'softmax'  output layer with 10 neurons. Remember to search for the right learning rate  each time you change the model's architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd0eb7-6208-43a5-b1d8-a6d66b50b898",
   "metadata": {},
   "source": [
    "Let's add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df9f2f9-2f45-4b60-ace3-04ac7358ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f406dc7-e738-4209-9488-08d62a0f2c49",
   "metadata": {},
   "source": [
    "Book skipped step to search for the right learning rate - adding right learning rate search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe503e-1e03-45cf-9e21-7d38878c44f5",
   "metadata": {},
   "source": [
    "Create a custom callback for 'on_batch_end' (see paget 339 for custom callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bbd19-6b1d-4275-81d4-cffda6336dc4",
   "metadata": {},
   "source": [
    "Load cifar10 dataset and split fulll training data to train and valid data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977426b2-2c17-41b7-902b-ff5f02c27697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # workaround for ssl certificate errors\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb90550-b59d-42fd-bd7c-cffc9e979c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1aa816-b59d-447d-8de5-89792e0de656",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db594055-5b92-4a7c-abb1-317eccd8bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd60d-3635-495a-9222-f8a9f48f20d6",
   "metadata": {},
   "source": [
    "Since training dataset has 50k instances with (default) 32 instance per batch, each epoch will include 1562 batch. If you start at learning rate of 10**-5 and increates 1.05 (5 percent) per batch, your ending learning rate will be ~ 1.6 x 10**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd5e7bf-0710-47c8-a600-c8adeb1dd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969866f0-79a2-462d-8fe9-3a2cd9e14eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1407/1407 [==============================] - 22s 11ms/step - loss: 516.5344 - accuracy: 0.1425 - val_loss: 2.3410 - val_accuracy: 0.0976\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 10711872.0000 - accuracy: 0.1013 - val_loss: 13.5268 - val_accuracy: 0.0976\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00308b6-edd1-4433-8fff-a6f6fc7834ce",
   "metadata": {},
   "source": [
    "plot the loss as a functionof the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d283f89-e036-4e54-8237-f22e57121dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[22]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6d7e3-b55b-40ee-a4c7-c212f07d49c0",
   "metadata": {},
   "source": [
    "Find the min loss value and its correponding learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "473c2024-a558-4c4f-bc4d-f9c7697b36cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2631704807281494"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(expon_lr.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40386ff-a505-4dfc-ae82-c72bc0045c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expon_lr.losses.index(min(expon_lr.losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2360a57-e91a-4a88-b5aa-33aba5cbb2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.010007e-02'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:e}\".format(expon_lr.rates[1387])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116bb1da-b211-45fc-a6f3-4fe6c719f270",
   "metadata": {},
   "source": [
    "from page 351: The optimal learning will be a little before the loss starting shooting back up (typically 10x lower than the low point).\n",
    "\n",
    "estimated best learning rate: 1e-1 * 1.01e-2 = 1.01e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3edfbe1-d159-43f2-be91-87c99291edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19131606-b524-4ab1-a055-25491ebdc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"swish\",\n",
    "                                    kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4494ad2-8c6b-43d2-8133-23d2f8ffb536",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19121acf-c920-42ec-b6fa-5f45c72c3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2083208f-7ddb-44a4-ba22-d7e6a72f1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253269c8-3ec2-4428-9f4f-e5dfb071fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_model\", save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0cce9d9-c1cb-464d-9f92-ec5b18c760e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14420), started 2:44:02 ago. (Use '!kill 14420' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-faf038cff32f691f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-faf038cff32f691f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_cifar10_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6602a1d4-1a96-4bdd-99e2-7da1efb351f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 3.1158 - accuracy: 0.1986INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 26s 14ms/step - loss: 3.1158 - accuracy: 0.1986 - val_loss: 2.0224 - val_accuracy: 0.2450\n",
      "Epoch 2/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.9575 - accuracy: 0.2783INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.9573 - accuracy: 0.2784 - val_loss: 1.8650 - val_accuracy: 0.3148\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.8867 - accuracy: 0.3125 - val_loss: 1.9124 - val_accuracy: 0.2990\n",
      "Epoch 4/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.8512 - accuracy: 0.3297INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.8512 - accuracy: 0.3298 - val_loss: 1.8267 - val_accuracy: 0.3496\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.8078 - accuracy: 0.3445 - val_loss: 1.8291 - val_accuracy: 0.3306\n",
      "Epoch 6/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.7757 - accuracy: 0.3569INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.7759 - accuracy: 0.3569 - val_loss: 1.7587 - val_accuracy: 0.3626\n",
      "Epoch 7/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.7399 - accuracy: 0.3702INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.7399 - accuracy: 0.3703 - val_loss: 1.7081 - val_accuracy: 0.3906\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.7094 - accuracy: 0.3822 - val_loss: 1.7163 - val_accuracy: 0.3738\n",
      "Epoch 9/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.6842 - accuracy: 0.3953INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6842 - accuracy: 0.3953 - val_loss: 1.7061 - val_accuracy: 0.3964\n",
      "Epoch 10/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.6632 - accuracy: 0.4013INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6632 - accuracy: 0.4013 - val_loss: 1.6821 - val_accuracy: 0.3844\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6478 - accuracy: 0.4063 - val_loss: 1.7809 - val_accuracy: 0.3584\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.6320 - accuracy: 0.4136INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6320 - accuracy: 0.4136 - val_loss: 1.6597 - val_accuracy: 0.4038\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.6171 - accuracy: 0.4202 - val_loss: 1.6946 - val_accuracy: 0.4056\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6059 - accuracy: 0.4241 - val_loss: 1.6635 - val_accuracy: 0.4056\n",
      "Epoch 15/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.5958 - accuracy: 0.4274INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5957 - accuracy: 0.4273 - val_loss: 1.6134 - val_accuracy: 0.4214\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5797 - accuracy: 0.4328 - val_loss: 1.6767 - val_accuracy: 0.4062\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5762 - accuracy: 0.4328 - val_loss: 1.6319 - val_accuracy: 0.4162\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5655 - accuracy: 0.4389 - val_loss: 1.6834 - val_accuracy: 0.4034\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.4445INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.5530 - accuracy: 0.4445 - val_loss: 1.6112 - val_accuracy: 0.4248\n",
      "Epoch 20/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.5473 - accuracy: 0.4456INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5476 - accuracy: 0.4454 - val_loss: 1.5913 - val_accuracy: 0.4320\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5403 - accuracy: 0.4490 - val_loss: 1.6342 - val_accuracy: 0.4238\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5345 - accuracy: 0.4528 - val_loss: 1.6032 - val_accuracy: 0.4344\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5289 - accuracy: 0.4553 - val_loss: 1.6056 - val_accuracy: 0.4200\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5188 - accuracy: 0.4589 - val_loss: 1.6164 - val_accuracy: 0.4272\n",
      "Epoch 25/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.5120 - accuracy: 0.4582INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.5120 - accuracy: 0.4582 - val_loss: 1.5907 - val_accuracy: 0.4404\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5030 - accuracy: 0.4631 - val_loss: 1.5955 - val_accuracy: 0.4334\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4986 - accuracy: 0.4631 - val_loss: 1.6362 - val_accuracy: 0.4124\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4929 - accuracy: 0.4672 - val_loss: 1.6105 - val_accuracy: 0.4314\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4880 - accuracy: 0.4694 - val_loss: 1.6120 - val_accuracy: 0.4386\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4789 - accuracy: 0.4692 - val_loss: 1.6438 - val_accuracy: 0.4192\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5995 - accuracy: 0.4247 - val_loss: 1.6050 - val_accuracy: 0.4290\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 2.0797 - accuracy: 0.3511 - val_loss: 1.7991 - val_accuracy: 0.3402\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.7547 - accuracy: 0.3535 - val_loss: 1.7916 - val_accuracy: 0.3424\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.7081 - accuracy: 0.3767 - val_loss: 1.7444 - val_accuracy: 0.3758\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6790 - accuracy: 0.3876 - val_loss: 1.6779 - val_accuracy: 0.3906\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6592 - accuracy: 0.3975 - val_loss: 1.6975 - val_accuracy: 0.3868\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6422 - accuracy: 0.4036 - val_loss: 1.6730 - val_accuracy: 0.3946\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6242 - accuracy: 0.4136 - val_loss: 1.6478 - val_accuracy: 0.4004\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6074 - accuracy: 0.4185 - val_loss: 1.6813 - val_accuracy: 0.3934\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5971 - accuracy: 0.4211 - val_loss: 1.6740 - val_accuracy: 0.3896\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5840 - accuracy: 0.4310 - val_loss: 1.6395 - val_accuracy: 0.4172\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5697 - accuracy: 0.4348 - val_loss: 1.6701 - val_accuracy: 0.4066\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5624 - accuracy: 0.4373 - val_loss: 1.6530 - val_accuracy: 0.4072\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5490 - accuracy: 0.4431 - val_loss: 1.6783 - val_accuracy: 0.4092\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5381 - accuracy: 0.4474 - val_loss: 1.6193 - val_accuracy: 0.4226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x194f0ba6dd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3100d06c-f7fc-4b00-a7f2-d508c991dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 1.5907 - accuracy: 0.4404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5907032489776611, 0.44040000438690186]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "653e4782-0025-47a2-a07b-86cf3bb50ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"tensorboard.exe\" not found.\n",
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "# close tensorboard\n",
    "!taskkill /IM \"tensorboard.exe\" /F\n",
    "!rmdir /S /Q %temp%\\.tensorboard-info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3ec92-103c-4ee0-9c25-82e8634167e6",
   "metadata": {},
   "source": [
    "##### 8c. Now try adding batch normalization and compare the learning curves: is it converging faster than before? Does it produce better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f4fc1-e3be-4f29-ae81-30ced1c8ad05",
   "metadata": {},
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer.\n",
    "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c464a191-9163-49bc-8da0-7c39ae3e86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 2.0460 - accuracy: 0.2438INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 49s 24ms/step - loss: 2.0461 - accuracy: 0.2440 - val_loss: 1.9150 - val_accuracy: 0.3206\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.7947 - accuracy: 0.3513INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.7947 - accuracy: 0.3513 - val_loss: 1.8945 - val_accuracy: 0.3126\n",
      "Epoch 3/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.6812 - accuracy: 0.3977INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.6812 - accuracy: 0.3977 - val_loss: 1.7223 - val_accuracy: 0.3818\n",
      "Epoch 4/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.6136 - accuracy: 0.4236INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.6137 - accuracy: 0.4235 - val_loss: 1.6322 - val_accuracy: 0.4046\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5514 - accuracy: 0.4507 - val_loss: 1.6718 - val_accuracy: 0.3954\n",
      "Epoch 6/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.5066 - accuracy: 0.4645INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.5066 - accuracy: 0.4645 - val_loss: 1.5332 - val_accuracy: 0.4380\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4588 - accuracy: 0.4834 - val_loss: 1.7345 - val_accuracy: 0.3792\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4207 - accuracy: 0.4949 - val_loss: 1.5695 - val_accuracy: 0.4436\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.3915 - accuracy: 0.5079INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.3915 - accuracy: 0.5079 - val_loss: 1.4631 - val_accuracy: 0.4790\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3570 - accuracy: 0.5173 - val_loss: 1.4843 - val_accuracy: 0.4714\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3273 - accuracy: 0.5265 - val_loss: 1.5242 - val_accuracy: 0.4646\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3004 - accuracy: 0.5381 - val_loss: 1.5548 - val_accuracy: 0.4380\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2751 - accuracy: 0.5470 - val_loss: 1.5850 - val_accuracy: 0.4376\n",
      "Epoch 14/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.2497 - accuracy: 0.5559INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2496 - accuracy: 0.5559 - val_loss: 1.4058 - val_accuracy: 0.5064\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.2277 - accuracy: 0.5652 - val_loss: 1.5766 - val_accuracy: 0.4712\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.2152 - accuracy: 0.5712 - val_loss: 1.4833 - val_accuracy: 0.4776\n",
      "Epoch 17/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.1915 - accuracy: 0.5807INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_bn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1916 - accuracy: 0.5807 - val_loss: 1.4019 - val_accuracy: 0.5184\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1672 - accuracy: 0.5897 - val_loss: 1.5522 - val_accuracy: 0.4734\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.1529 - accuracy: 0.5908 - val_loss: 1.4988 - val_accuracy: 0.4868\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1317 - accuracy: 0.6012 - val_loss: 1.7593 - val_accuracy: 0.4168\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1220 - accuracy: 0.6054 - val_loss: 1.4599 - val_accuracy: 0.5032\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0990 - accuracy: 0.6113 - val_loss: 1.4808 - val_accuracy: 0.4904\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0863 - accuracy: 0.6175 - val_loss: 1.5569 - val_accuracy: 0.4874\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.0739 - accuracy: 0.6230 - val_loss: 1.5538 - val_accuracy: 0.4730\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0603 - accuracy: 0.6254 - val_loss: 1.5046 - val_accuracy: 0.4896\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.0421 - accuracy: 0.6343 - val_loss: 1.5035 - val_accuracy: 0.4872\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.0310 - accuracy: 0.6393 - val_loss: 1.5658 - val_accuracy: 0.4768\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.0201 - accuracy: 0.6397 - val_loss: 1.5552 - val_accuracy: 0.4892\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0063 - accuracy: 0.6446 - val_loss: 1.5354 - val_accuracy: 0.4786\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9916 - accuracy: 0.6501 - val_loss: 1.5217 - val_accuracy: 0.4924\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9849 - accuracy: 0.6538 - val_loss: 1.6236 - val_accuracy: 0.4756\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9735 - accuracy: 0.6585 - val_loss: 1.6784 - val_accuracy: 0.4582\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9551 - accuracy: 0.6625 - val_loss: 1.5546 - val_accuracy: 0.4896\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9503 - accuracy: 0.6649 - val_loss: 1.5608 - val_accuracy: 0.4894\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9428 - accuracy: 0.6675 - val_loss: 1.5435 - val_accuracy: 0.4890\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9271 - accuracy: 0.6722 - val_loss: 1.5315 - val_accuracy: 0.5066\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9131 - accuracy: 0.6787 - val_loss: 1.8187 - val_accuracy: 0.4638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1948186b590>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(\"swish\"))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model\", save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_bn_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e296fb9f-beb1-4c74-8a5b-ee93d1e03815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4019 - accuracy: 0.5184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4018566608428955, 0.5184000134468079]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81b5e3-0c22-403f-9157-28b80b8fff5d",
   "metadata": {},
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took 29 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 12 epochs and continued to make progress until the 17th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with 50.7% validation accuracy instead of 46.7%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 15s instead of 10s, because of the extra computations required by the BN layers. But overall the training time (wall time) to reach the best model was shortened by about 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592aee5-ca1d-47fc-ab8c-7cfd99ae05f4",
   "metadata": {},
   "source": [
    "8d. Try replacing batch normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e. standardize the inputs features, use 'LeCun normal initialization', make suer the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df0d4ad9-65d5-4232-8537-91a69a8f7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.9179 - accuracy: 0.3147INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 26s 14ms/step - loss: 1.9177 - accuracy: 0.3148 - val_loss: 1.7697 - val_accuracy: 0.3786\n",
      "Epoch 2/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.7099 - accuracy: 0.3928INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.7099 - accuracy: 0.3929 - val_loss: 1.6858 - val_accuracy: 0.3990\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.6184 - accuracy: 0.4282INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6184 - accuracy: 0.4282 - val_loss: 1.5964 - val_accuracy: 0.4310\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5525 - accuracy: 0.4531 - val_loss: 1.6098 - val_accuracy: 0.4434\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4995 - accuracy: 0.4745 - val_loss: 1.6161 - val_accuracy: 0.4324\n",
      "Epoch 6/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.4526 - accuracy: 0.4924INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.4524 - accuracy: 0.4925 - val_loss: 1.5238 - val_accuracy: 0.4698\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.4070 - accuracy: 0.5099 - val_loss: 1.5505 - val_accuracy: 0.4564\n",
      "Epoch 8/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 1.3612 - accuracy: 0.5256INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.3612 - accuracy: 0.5256 - val_loss: 1.4996 - val_accuracy: 0.4738\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3324 - accuracy: 0.5359 - val_loss: 1.5010 - val_accuracy: 0.4802\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3049 - accuracy: 0.5471 - val_loss: 1.5302 - val_accuracy: 0.4756\n",
      "Epoch 11/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.2732 - accuracy: 0.5610INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2731 - accuracy: 0.5610 - val_loss: 1.4967 - val_accuracy: 0.4886\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2434 - accuracy: 0.5755 - val_loss: 1.5408 - val_accuracy: 0.4848\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2242 - accuracy: 0.5773 - val_loss: 1.4989 - val_accuracy: 0.5056\n",
      "Epoch 14/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 1.1932 - accuracy: 0.5877INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_selu_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1933 - accuracy: 0.5876 - val_loss: 1.4545 - val_accuracy: 0.5124\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1735 - accuracy: 0.5964 - val_loss: 1.5383 - val_accuracy: 0.4982\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1505 - accuracy: 0.6050 - val_loss: 1.5224 - val_accuracy: 0.5004\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1328 - accuracy: 0.6127 - val_loss: 1.5098 - val_accuracy: 0.5140\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1051 - accuracy: 0.6205 - val_loss: 1.5153 - val_accuracy: 0.4916\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0895 - accuracy: 0.6258 - val_loss: 1.5500 - val_accuracy: 0.5140\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0744 - accuracy: 0.6303 - val_loss: 1.5302 - val_accuracy: 0.5020\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0485 - accuracy: 0.6414 - val_loss: 1.4972 - val_accuracy: 0.5004\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0394 - accuracy: 0.6440 - val_loss: 1.5580 - val_accuracy: 0.5046\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0184 - accuracy: 0.6540 - val_loss: 1.5841 - val_accuracy: 0.4928\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1264 - accuracy: 0.6224 - val_loss: 1.5343 - val_accuracy: 0.5114\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0131 - accuracy: 0.6536 - val_loss: 1.5330 - val_accuracy: 0.5066\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9709 - accuracy: 0.6676 - val_loss: 1.5531 - val_accuracy: 0.5088\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9516 - accuracy: 0.6756 - val_loss: 1.5542 - val_accuracy: 0.5098\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9399 - accuracy: 0.6830 - val_loss: 1.5965 - val_accuracy: 0.4990\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9335 - accuracy: 0.6834 - val_loss: 177.6218 - val_accuracy: 0.5148\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7440 - accuracy: 0.4764 - val_loss: 1.6232 - val_accuracy: 0.4518\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2949 - accuracy: 0.5490 - val_loss: 1.5855 - val_accuracy: 0.4690\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2118 - accuracy: 0.5812 - val_loss: 1.5466 - val_accuracy: 0.4792\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1642 - accuracy: 0.5967 - val_loss: 1.5669 - val_accuracy: 0.4826\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1364 - accuracy: 0.6088 - val_loss: 1.5403 - val_accuracy: 0.4956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1949beab090>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model\", save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_selu_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd30272b-ab66-40ae-a004-f61b9ccfb6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4545 - accuracy: 0.5124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.454453468322754, 0.5123999714851379]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2affc-1b89-4525-8c9e-89c6ff3098ee",
   "metadata": {},
   "source": [
    "This model reached the first model's validation loss in just 8 epochs. After 14 epochs, it reached its lowest validation loss, with about 50.3% accuracy, which is better than the original model (46.7%), but not quite as good as the model using batch normalization (50.7%). Each epoch took only 9 seconds. So it's the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f12b4-02cf-467c-9d37-839978adbcad",
   "metadata": {},
   "source": [
    "8e. Try regularizing the model with 'alpha dropout'. Then, without retraining your model, see if you can achieve better accuracy using 'MC dropout'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb20d01-e4c7-4c76-80d1-f58d3f79f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.9212 - accuracy: 0.3120INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 27s 14ms/step - loss: 1.9207 - accuracy: 0.3121 - val_loss: 1.7742 - val_accuracy: 0.3638\n",
      "Epoch 2/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.7107 - accuracy: 0.3939INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7109 - accuracy: 0.3939 - val_loss: 1.6690 - val_accuracy: 0.4178\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.6285 - accuracy: 0.4303INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6285 - accuracy: 0.4303 - val_loss: 1.6416 - val_accuracy: 0.4092\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 1.5749 - accuracy: 0.4439INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5749 - accuracy: 0.4439 - val_loss: 1.5907 - val_accuracy: 0.4418\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5237 - accuracy: 0.4670 - val_loss: 1.6269 - val_accuracy: 0.4442\n",
      "Epoch 6/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 1.4768 - accuracy: 0.4831INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4768 - accuracy: 0.4831 - val_loss: 1.5647 - val_accuracy: 0.4612\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4459 - accuracy: 0.4921 - val_loss: 1.5801 - val_accuracy: 0.4640\n",
      "Epoch 8/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 1.4035 - accuracy: 0.5113INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_cifar10_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4035 - accuracy: 0.5113 - val_loss: 1.4976 - val_accuracy: 0.4896\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3728 - accuracy: 0.5248 - val_loss: 1.5761 - val_accuracy: 0.4542\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3452 - accuracy: 0.5344 - val_loss: 1.5337 - val_accuracy: 0.4836\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3148 - accuracy: 0.5443 - val_loss: 1.5254 - val_accuracy: 0.4800\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2845 - accuracy: 0.5553 - val_loss: 1.5470 - val_accuracy: 0.4822\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2632 - accuracy: 0.5633 - val_loss: 1.5788 - val_accuracy: 0.4874\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3219 - accuracy: 0.5626 - val_loss: 1.7355 - val_accuracy: 0.4084\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2859 - accuracy: 0.5557 - val_loss: 1.5647 - val_accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1823 - accuracy: 0.5912 - val_loss: 1.6143 - val_accuracy: 0.5072\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1508 - accuracy: 0.6034 - val_loss: 1.6068 - val_accuracy: 0.5042\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1422 - accuracy: 0.6069 - val_loss: 1.5392 - val_accuracy: 0.5084\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1327 - accuracy: 0.6089 - val_loss: 1.5782 - val_accuracy: 0.5026\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1277 - accuracy: 0.6108 - val_loss: 1.7076 - val_accuracy: 0.4890\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1219 - accuracy: 0.6119 - val_loss: 1.6177 - val_accuracy: 0.5012\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1189 - accuracy: 0.6144 - val_loss: 1.6590 - val_accuracy: 0.5026\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1024 - accuracy: 0.6227 - val_loss: 1.5992 - val_accuracy: 0.5002\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0905 - accuracy: 0.6275 - val_loss: 1.6435 - val_accuracy: 0.4928\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2811 - accuracy: 0.5724 - val_loss: 1.6116 - val_accuracy: 0.4914\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1488 - accuracy: 0.6028 - val_loss: 1.6127 - val_accuracy: 0.5026\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0661 - accuracy: 0.6339 - val_loss: 1.5649 - val_accuracy: 0.5056\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0415 - accuracy: 0.6428 - val_loss: 1.7011 - val_accuracy: 0.5060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x194a11a0bd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(tf.keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_dropout_model\", save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_dropout_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d511dee0-f119-430e-83dd-022ce87de608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 54.3101 - accuracy: 0.2116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[54.31010437011719, 0.21160000562667847]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f65670-b001-4610-9ac1-15a46c9e91ed",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the MCAlphaDropout class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad9a22fd-19ff-477b-9f91-84001f82e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(tf.keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15f070-d150-4ea7-8c3a-14c5bf2d35c7",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9e77fb4-d0ee-49f3-a794-82a9ee50b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = tf.keras.Sequential([\n",
    "    (\n",
    "        MCAlphaDropout(layer.rate)\n",
    "        if isinstance(layer, tf.keras.layers.AlphaDropout)\n",
    "        else layer\n",
    "    )\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5095e-f4d4-404f-85a0-d9f5a77ef609",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fa13911-0e2d-48a6-89c3-1ddb52281f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return Y_probas.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80865b1b-6e5f-403f-b344-9ea2986499af",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d10f3e8-c630-4790-bf87-0376c64ac566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4888"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = (y_pred == y_valid[:, 0]).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2ae45-7685-4562-9af9-d1957abcb8d9",
   "metadata": {},
   "source": [
    "8f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "097457fe-d5c0-4277-ae3f-d6247ef4db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "\n",
    "model.add(tf.keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a14c9a-b66e-4588-9b02-5f674e5e43cb",
   "metadata": {},
   "source": [
    "The `find_learning_rate()` function trains the model using the `ExponentialLearningRate` callback, and it returns the learning rates and corresponding batch losses. At the end, it restores the model and its optimizer to their initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fe6db9d-d9fc-45d1-988f-cf1aacc6034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
    "                       max_rate=1):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90352972-21a5-489d-80cf-506d11b631b1",
   "metadata": {},
   "source": [
    "The `plot_lr_vs_loss()` function plots the learning rates vs the losses. The optimal learning rate to use as the maximum learning rate in 1cycle is near the bottom of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0df42abb-6fcb-4cfa-a75f-d6cc006de800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses, \"b\")\n",
    "    plt.gca().set_xscale('log')\n",
    "    max_loss = losses[0] + min(losses)\n",
    "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
    "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01b42d7b-7ef2-418b-b755-1ed521117e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.1768\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,\n",
    "                                   batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9645839f-2701-4343-ba33-a15deea672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(tf.keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=2e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fb3ec-8f90-4551-a20c-eb130c692b1a",
   "metadata": {},
   "source": [
    "The `OneCycleScheduler` custom callback updates the learning rate at the beginning of each batch. It applies the logic described in the book: increase the learning rate linearly during about half of training, then reduce it linearly back to the initial learning rate, and lastly reduce it down to close to zero linearly for the very last part of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "872d2211-99ef-44f1-b57e-b55b29b79f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
    "                 last_iterations=None, last_lr=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
    "                                   self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                   self.max_lr, self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                   self.start_lr, self.last_lr)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d55ec8fd-ff9f-476f-9992-4d7241396df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 6s 12ms/step - loss: 2.0452 - accuracy: 0.2900 - val_loss: 1.7732 - val_accuracy: 0.3848\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.7498 - accuracy: 0.3803 - val_loss: 1.6772 - val_accuracy: 0.4064\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.6259 - accuracy: 0.4216 - val_loss: 1.6788 - val_accuracy: 0.4154\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5520 - accuracy: 0.4486 - val_loss: 1.6572 - val_accuracy: 0.4238\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5034 - accuracy: 0.4690 - val_loss: 1.6730 - val_accuracy: 0.4282\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4624 - accuracy: 0.4825 - val_loss: 1.5626 - val_accuracy: 0.4554\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.4210 - accuracy: 0.4986 - val_loss: 1.6760 - val_accuracy: 0.4370\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.3520 - accuracy: 0.5189 - val_loss: 1.5076 - val_accuracy: 0.4882\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.2764 - accuracy: 0.5475 - val_loss: 1.5726 - val_accuracy: 0.4706\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.2044 - accuracy: 0.5728 - val_loss: 1.5060 - val_accuracy: 0.4990\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 1.1334 - accuracy: 0.5959 - val_loss: 1.5298 - val_accuracy: 0.5054\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.0623 - accuracy: 0.6207 - val_loss: 1.5165 - val_accuracy: 0.5066\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.9922 - accuracy: 0.6461 - val_loss: 1.5389 - val_accuracy: 0.5170\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 0.9260 - accuracy: 0.6704 - val_loss: 1.5692 - val_accuracy: 0.5184\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.8856 - accuracy: 0.6868 - val_loss: 1.5989 - val_accuracy: 0.5226\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "n_iterations = math.ceil(len(X_train_scaled) / batch_size) * n_epochs\n",
    "onecycle = OneCycleScheduler(n_iterations, max_lr=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d47a9f-dd59-4cfa-943e-2476ccf2d132",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 50.7% to 52.0%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d6532-780d-4c72-ac2e-ccd9fef57441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
