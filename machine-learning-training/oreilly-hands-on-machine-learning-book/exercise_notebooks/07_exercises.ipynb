{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536916a1-b3bc-4856-9c2e-457587f4435f",
   "metadata": {},
   "source": [
    "Chapter 7: Ensemble Learning and Random Forests\n",
    "\n",
    "Chapter 7 exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a507d79-29a7-43a6-aa80-70d679ccf141",
   "metadata": {},
   "source": [
    "1. If you trained five different models on the exact same training data, and they all achieve 95% precision, is there any change that you can combine these models together to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67805179-3af7-4b69-b105-557bda472ee1",
   "metadata": {},
   "source": [
    "-> By aggregating the predictions of each of the 5 classifer: the class that gets the most votes is the ensemble's prediction. Voting classification often achieves a higher accuracy that the best classifier in the ensemble especially when the predictors are as independent from each other as possible.\n",
    "\n",
    "page 212: A very simple way to create an even better classifer is to aggregate the predictions of each classifer: the class that gets the most votes is the ensemble's prediction. This majority-vote classifier is called 'hard voting classifier'.\n",
    "\n",
    "page 213: Somewhat surprisingly, this voting classification often achieves a higher accuracy that the best classifier in the ensemble. In fact, enve if each classifier is a 'weak learner' (meaning it does only slightly better than random guessing), the ensemble can still be a 'strong learner' (achieving high accuracy), provided there are a sufficient number weak learners in the ensemble and they are sufficiently diverse.\n",
    "\n",
    "page 214: Ensemble method works best when the predictors are as independent from each other as possible. One way to get diverse classifers is to train them using different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy.\n",
    "\n",
    "page 214: Scikit-Learn provides a VotingClassifier clas that's quite easy to use: just give it a list of name/predictor pairs and using it like a normal classifier. ...\n",
    "\n",
    "book answer: If you have trained five different models and they all achieve 95% precision, you can try combining them into a voting ensemble, which will often give you even better results. It works better if the models are very different (e.g., an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that's the whole point of bagging and pasting ensembles), but if not this will still be effective as long as the models are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598472fd-6cfe-41eb-b2cd-417458d69a70",
   "metadata": {},
   "source": [
    "2. What is the difference between hard and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd985eea-7b04-4ba6-be61-f2f22855ad88",
   "metadata": {},
   "source": [
    "-> 'hard voting' classifier give equal weight each predictor. That is, predicts the class with the highest probability based on the majoring voting of the classifiers in the ensemble. Soft voting' predicts the class with the highest class probability by average estimated class probability for each class and picks the class with the highest probability\n",
    "\n",
    "page 212: A very simple way to create an even better classifer is to aggregate the predictions of each classifer: the class that gets the most votes is the ensemble's prediction. This majority-vote classifier is called 'hard voting classifier'.\n",
    "\n",
    "page 215: If all classifiers are able to estimate class probabilities (i.e. if they all have 'predict_proba()'' method), then you can tell Scikit-Leasrn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called 'soft voting'. It often achieves higher performance than 'hard voting' because it gives more weight to highly confident votes. All you need to do is set voting classifier's voting hyperparameter to 'soft', and ensure that all classifiers can estimate class probabilities. This is not the case  for the the SVC class by default, so you need to set it probability hyperparameter to 'True' (this will make the 'SVC class' uses cross-valication to estimate class probabilities, slow down training, and it will add a predict_proba method).\n",
    "\n",
    "book answer: A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the average estimated class probability for each class and picks the class with the highest probability. This gives high-confidence votes more weight and often performs better, but it works only if every classifier is able to estimate class probabilities (e.g., for the SVM classifiers in Scikit-Learn you must set probability=True)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc6b90-9c9d-4314-b983-306c5b6d1a08",
   "metadata": {},
   "source": [
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, and random forests, or stacking ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbfac9-c626-43e8-a5fe-1eb7bb435656",
   "metadata": {},
   "source": [
    "-> Both bagging and pasting ensembles predictors can be trained in parallel so they can be distributed across multiple servers.\n",
    "-> Boosting trains classifiers sequentially, so training cannot be speed up by distributing it across multiple servers.\n",
    "-> Random Forest is decison tree ensemble using bagging (or pasting), so it can be distributed across multiple servers.\n",
    "-> Stacking ensembles train predictors and run cross-validation in parallel, so they can be distributed across multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.\n",
    "\n",
    "page 215: ... Another approach is to use the same training algorithm fore eavy predictor but train them on different random subset of the training set. When sampling is performed 'with replacement', this method is called 'bagging' (...) (short for 'bootstrap aggregating'). When sample is performed 'without replacement', it is called 'pasting' (...).\n",
    "note 1: [bagging] Imagine picking a card randomly from a deck of cards, writing it dlow, the placing it back in the deck before picking the next card: the same card could be sample multiple times.\n",
    "\n",
    "page 216: As you can see in Figure 7-4 [Bagging and Pasting involve training several predictors on different random samples of the training set], predictors can be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one reason bagging and pasting are popular methods: they scale well. \n",
    "\n",
    "page 220: As we have discussed, a 'random forest' (...) is an ensemble of decision trees, generally training via bagging method (or sometimes pasting), typicall with 'max_samples' set to the size of the training set. Instead of building a 'BaggingClassifier' class and passing it a 'DecisionTreeClassifiers', you can use the 'RandomForestClassifier' class, which is more convenient and optimized for decision trees (similarly, there is a 'RandomForestRegressor' class  for regression tasks). \n",
    "\n",
    "page 222: Boosting (originally called hypothesis boosting) refers to an ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct it predecessor. There are many boosting methods available, byt by far the most popular are 'AdaBoost' (...) (short for adaptive boosting) and 'gradient boosting'.\n",
    "\n",
    "page 222 - 223: One way for a new predictor to correct it predecessor is to pay a bit more attention to the training instances that the predecessor 'underfit'. This results in new predictors focusing more and more on the hard cases. This is the technique used by 'AdaBoost'. \n",
    "\n",
    "For example, when training an 'AdaBoost classifer', the algorithm first trains a base classifier (such as 'decision tree') and uses it to make predictions on the training set. The algorithm then increases the relative weight of the misclassified training instances. Then it trains a second classifier, using the updated weights, and againg makes predictions on the traning set, update the instance weights, and so on (see Figure 7-7 [AdaBoost sequential training with instanc weight updates])\n",
    "\n",
    "page 226: Another popular boosting algorithm is 'gradient boosting' (...). Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting it predecessor. However, instead of tweaking the instance weights at every interation like AdaBoost does, this method tring to fit the new predictor to the 'residual errors' make by the predictor.\n",
    "\n",
    "page 232: The last ensemble method we will discuss in this chapger is calledd 'stacking' (short for 'stacked generalization' (...). It is based on a simle idea: instead of using trivial functions (such as hard voting) to aggregate the preictions ofall in an ensemble, why don't we traing a model to perform this aggregation? Figure 7-11 [Aggregating preidtions using a blending predictor] shows such an ensemble performing a regression task on a new instance. Each of the bottom three prdictors predict a different value (3.1, 2.7, & 2.9), and then the final predictor (called a 'blender' or a 'meta learner') takes these predictions as inputs and makes the final preiction (3.0).\n",
    "\n",
    "page 232 - 233: To train the blender, you first need to build the blender training set. You can use 'cross_val_predict()' in the ensemble to get out-of-sample predictions fore ach instance in the original training set (Figure 7.12 [Training the blender in a stacking ensemble]), and use these can be used as input features to traing the blender; and targets can simply be copied from the original training set. Note that regardless of the number of features in the original training set (just one in this example), the blending training set will contain one input feature per predictor (three in this example). Once the blener is trained, teh base predictores are retrained one last time on the full original training set.\n",
    "\n",
    "book answer: It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential, and you will not gain anything by distributing training across multiple servers. Regarding stacking ensembles, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d264b9-8526-4b52-897e-d3e0f67de603",
   "metadata": {},
   "source": [
    "4. What the benefit of out-of-bag evaluations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae626ef6-f9e8-465a-acba-e7b67e4a6504",
   "metadata": {},
   "source": [
    "-> the out-of-bag evaluation, the OOB (out-of-bag - held out from the predictor training) can be used for validation instead of creating validation set. \n",
    "\n",
    "page 218: With bagging, some training instances may be sample serveral times for any given predictor, while others may not be sampled at all. By default a 'BaggingClassifier' samples 'm'  training instances 'with replacement' (bootstrap=True), where 'm' is the size of the training set. With this process, it can be show mathematically that only 63% of the training instances are sampled on average fore each predictor. The remaining 37% of the training instances are not sampled are called 'out-of-bag' (OOB) instances. Note that are not the same 37% for all predictors.\n",
    "\n",
    "page 218: A bagging ensemble can be evaluated using OOB instances, without the need for a seperate validation set: indeed, if there are enough estimators, then each instance in the training set will likely be an OOB instance of several estimators, so these estimators can be used to make a fair ensemble prediciton for that instance. Once you have prediction fore each instance, you can compute the ensemble's prediction accuracy (or any other metric)\n",
    "\n",
    "book answer: With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated using instances that it was not trained on (they were held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for an additional validation set. Thus, you have more instances available for training, and your ensemble can perform slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7b5c8-50a0-4de2-b2a1-1217d1d58299",
   "metadata": {},
   "source": [
    "5. What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30215372-0558-4536-9624-94b6601e7af1",
   "metadata": {},
   "source": [
    "-> extra-trees ensemble use random thresholds for each feature rather than searching for the best possible thresholds.\n",
    "-> extra-trees trades more bias for a lower variance. It also makes 'extra-trees classifier' much faster to train than regular 'random forests', because finding the best possible threshold for each feature at every node is one of the most time-consuming task of growing a tree.\n",
    "\n",
    "page 208 (chapter 6): Luckily, by averaging predictions over many trees, it's possible to reduce variance significantly. Such an 'ensemble of trees' is called a 'random forest', and it;s one of the most powerful types of models available today, as you will see in the next chapter.\n",
    "\n",
    "page 220: The random forest algorithm introduces extra randomness when growing trees instead of search for the very best features when splitting a node (Chapter 6), it searchs for the best feature amount a random subset of featurs. By default, it samples n**1/2 features ('n' is the total number of features). The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance generally yielding a novera better model. So, the following BaggingClassifier is equivalent to the previous RandomForestClassifier:\n",
    "    bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "        n_estimators=500, n_jobs=-1, random_state=42)\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    y_pred_bag = bag_clf.predict(X_test)\n",
    "\n",
    "page 220: When you are growing a tree in a random foest, at each node only a random subset of features is considered for splitting (as discussed earlier). It is posssible to make trees even more random by also using random thresholds for each feature rather than search for the best possible thresholds (like regular decision trees do). For this, simples set 'splitter=\"random\"' when creatng a DecisionTreeClassifier.\n",
    "\n",
    "page 221: A forest of such extremely random trees is called an 'extremely randomized trees' (...) (or 'extra-trees' for short) ensemble. Once again, this technique trades more bias for a lower variance. It also makes 'extra-trees classifier' much faster to train than regular 'random forests', because finding the best possible threshold for each feature at every node is one of the most time-consuming task of growing a tree.\n",
    "\n",
    "book answer: When you are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for Extra-Trees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. Moreover, since Extra-Trees don't search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c041cfc6-9f7c-4d33-a5ea-ecbd885df6d7",
   "metadata": {},
   "source": [
    "6. If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak, and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0d49a-3c74-4541-a8da-6d9e5167db9d",
   "metadata": {},
   "source": [
    "-> AdaBoost ensemble increases the weight of the 'unfitting instances' (misclassified) after each estimator stage, so increasing the number of stages/estimators ('n_estimators' hyperparater) can be used to reduced underfitting.  Reducing the regularization of the estimators used, can also be used to reduce underfitting.\n",
    "\n",
    "page 222 - 223: One way for a new predictor to correct it predecessor is to pay a bit more attention to the training instances that the predecessor 'underfit'. This results in new predictors focusing more and more on the hard cases. This is the technique used by 'AdaBoost'. \n",
    "\n",
    "For example, when training an 'AdaBoost classifer', the algorithm first trains a base classifier (such as 'decision tree') and uses it to make predictions on the training set. The algorithm then increases the relative weight of the misclassified training instances. Then it trains a second classifier, using the updated weights, and againg makes predictions on the traning set, update the instance weights, and so on (see Figure 7-7 [AdaBoost sequential training with instanc weight updates])\n",
    "\n",
    "page 226: Another popular boosting algorithm is 'gradient boosting' (...). Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting it predecessor. However, instead of tweaking the instance weights at every interation like AdaBoost does, this method tring to fit the new predictor to the 'residual errors' make by the predictor.\n",
    "\n",
    "page 226: If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator\n",
    "\n",
    "book answer: If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4ff55-0cba-4daf-9802-a4dc1a437de7",
   "metadata": {},
   "source": [
    "7. If your gradient boosting ensemble overfits the training set should you increase or decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110256c5-18e0-4216-86f0-52d85acf173d",
   "metadata": {},
   "source": [
    "-> If gradient boosting is overfitting, you can try to decrease the learning rate. Instead you may try to the reduce the number of estimators and add early stopping.\n",
    "\n",
    "page 228: The 'learning_rate' hyperparameter scales the contributions of each tree. If you set to a low value, such as 0.05, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called 'shrinkage'. Figure 7-10 [GBRT ensembles with not enough predictors (left) and just enough (right)] shows two GBRT [Gradient Boosted Regression Trees] ensembles trained with different hyperparameters: the one on the eft does not have enough trees to fit the training set, while the one on the righ has about the right amount. If we added more trees, the GBRT would start to overfit the training set.\n",
    "\n",
    "page 229: To find the optimal number of trees, you could perform cross-validation using 'GridSearchCV' or 'RandomizedSearchCV', as usual, buter there's a simpler way: if you set the 'n_iter_no_changer' hyperparameter to an integer value, say 10, the the 'GradientBoostingRegressor' will automatically stop adding more trees during training if it sees the last 10 trees did not help. This is simply 'early stopping' ... \n",
    "\n",
    "page 229: If you set 'n_iter_no_change' too low, training may stop too earcy and the model will underfit. But if yest it too high, it will overfit instead. We also set a fairly small learning rate and a high number of estimators, bu the actual number of estimators in the trained ensemble is much lower, thanks to early stopping:\n",
    "\n",
    "book answer: If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5f572-3c69-4d1a-8edc-bbe5baca948e",
   "metadata": {},
   "source": [
    "8. Voting Classifier\n",
    "\n",
    "Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g. 50,000 instances for training, 10,000 for validation, and 10,000 for testing), Then train various classifiers, such as a random forest classifier, and extra-trees classifier, and SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it non the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4e5280-cf33-474e-96d8-0e79539edd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293e53d-e509-41eb-9391-b93df98f0f75",
   "metadata": {},
   "source": [
    "8a.  Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g. 50,000 instances for training, 10,000 for validation, and 10,000 for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0c52a8-7e45-4832-a80f-dee4d498e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False, parser='auto')\n",
    "\n",
    "X_train, y_train = X_mnist[:50_000], y_mnist[:50_000]\n",
    "X_valid, y_valid = X_mnist[50_000:60_000], y_mnist[50_000:60_000]\n",
    "X_test, y_test = X_mnist[60_000:], y_mnist[60_000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc4cca-1e18-409e-9bb7-01e6f5820826",
   "metadata": {},
   "source": [
    "8b. Then train various classifiers, such as a random forest classifier, and extra-trees classifier, and SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca778634-fdb5-416c-bc2e-7e8b02d036fd",
   "metadata": {},
   "source": [
    "Note: The `LinearSVC` has a `dual` hyperparameter whose default value will change from `True` to `\"auto\"` in Scikit-Learn 1.5. To ensure this notebook continues to produce the same outputs, I'm setting it explicitly to `True`. Please see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759650c9-5154-40b8-9b03-90dddecbfe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(max_iter=100, tol=20, dual=True, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb095b4e-0c84-4c3e-88e0-b00bbe527e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(random_state=42)\n",
      "Training the ExtraTreesClassifier(random_state=42)\n",
      "Training the LinearSVC(max_iter=100, random_state=42, tol=20)\n",
      "Training the MLPClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991eef6c-d7ba-48d4-9eb2-98d2d43444bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9736, 0.9743, 0.8662, 0.9632]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_valid, y_valid) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdaa721-ab9e-49aa-8aef-30ba34dc512f",
   "metadata": {},
   "source": [
    "The linear SVM is far outperformed by the other classifiers. However, let's keep it for now since it may improve the voting classifier's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17db10-9b80-4e5e-b4c4-fe021eca925e",
   "metadata": {},
   "source": [
    "8c. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2825a07b-2ee7-4692-ad86-bbdf64b4adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;random_forest_clf&#x27;,\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;extra_trees_clf&#x27;,\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              LinearSVC(max_iter=100, random_state=42, tol=20)),\n",
       "                             (&#x27;mlp_clf&#x27;, MLPClassifier(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;random_forest_clf&#x27;,\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;extra_trees_clf&#x27;,\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              LinearSVC(max_iter=100, random_state=42, tol=20)),\n",
       "                             (&#x27;mlp_clf&#x27;, MLPClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>random_forest_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>extra_trees_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100, random_state=42, tol=20)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf',\n",
       "                              LinearSVC(max_iter=100, random_state=42, tol=20)),\n",
       "                             ('mlp_clf', MLPClassifier(random_state=42))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "named_estimators = [\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbc6228-b850-4313-9b1b-f4184d3c0fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f7bc6-3ccd-4723-b501-31c9d8500a89",
   "metadata": {},
   "source": [
    "The VotingClassifier made a clone of each classifier, and it trained the clones using class indices as the labels, not the original class names. Therefore, to evaluate these clones we need to provide class indices as well. To convert the classes to class indices, we can use a LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a859c7ab-e810-41f0-8f62-df5c07acde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_valid_encoded = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c205d80-7dec-40d3-ad21-c6a3c6824c20",
   "metadata": {},
   "source": [
    "However, in the case of MNIST, it's simpler to just convert the class names to integers, since the digits match the class ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b21136e-cb6d-4ce5-9dc6-32fdb5218c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_encoded = y_valid.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e056dd73-8344-47ae-8ec4-7a190b3170fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9736, 0.9743, 0.8662, 0.9632]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_valid, y_valid_encoded)\n",
    " for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd32130-e833-4c74-9f98-62448556c1d2",
   "metadata": {},
   "source": [
    "Let's remove the SVM to see if performance improves. It is possible to remove an estimator by setting it to \"drop\" using set_params() like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4bfa0e3-f33b-4c95-8d75-c6a50cac7a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;random_forest_clf&#x27;,\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;extra_trees_clf&#x27;,\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             (&#x27;svm_clf&#x27;, &#x27;drop&#x27;),\n",
       "                             (&#x27;mlp_clf&#x27;, MLPClassifier(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;random_forest_clf&#x27;,\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;extra_trees_clf&#x27;,\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             (&#x27;svm_clf&#x27;, &#x27;drop&#x27;),\n",
       "                             (&#x27;mlp_clf&#x27;, MLPClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>random_forest_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>extra_trees_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">drop</label><div class=\"sk-toggleable__content\"><pre>drop</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf', 'drop'),\n",
       "                             ('mlp_clf', MLPClassifier(random_state=42))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.set_params(svm_clf=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b1cd0-4d1e-47fb-a046-325e2ae29388",
   "metadata": {},
   "source": [
    "This updated the list of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74d8e381-f773-4b35-8623-b51f62282c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('random_forest_clf', RandomForestClassifier(random_state=42)),\n",
       " ('extra_trees_clf', ExtraTreesClassifier(random_state=42)),\n",
       " ('svm_clf', 'drop'),\n",
       " ('mlp_clf', MLPClassifier(random_state=42))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7355d3-ff62-44c8-97dc-75eaf48e6c62",
   "metadata": {},
   "source": [
    "However, it did not update the list of trained estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d94044b7-f16f-428c-9ed6-dc623561f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(random_state=42),\n",
       " ExtraTreesClassifier(random_state=42),\n",
       " LinearSVC(max_iter=100, random_state=42, tol=20),\n",
       " MLPClassifier(random_state=42)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83e0f6e4-feb8-4ddd-9ca3-9bb060f99c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_forest_clf': RandomForestClassifier(random_state=42),\n",
       " 'extra_trees_clf': ExtraTreesClassifier(random_state=42),\n",
       " 'svm_clf': LinearSVC(max_iter=100, random_state=42, tol=20),\n",
       " 'mlp_clf': MLPClassifier(random_state=42)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.named_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc41b81-f585-4d4a-b05b-84176d8b60a8",
   "metadata": {},
   "source": [
    "So we can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators, both in estimators_ and named_estimators_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a993c73-5cae-40a3-8c30-3215cfb6b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_trained = voting_clf.named_estimators_.pop(\"svm_clf\")\n",
    "voting_clf.estimators_.remove(svm_clf_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd09d4c-fc30-4028-baf4-913c6aec5b79",
   "metadata": {},
   "source": [
    "Now let's evaluate the VotingClassifier again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d95fc2-5b4c-402d-a1ba-dcd8c44d4081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde7c1c-7ef9-4672-8927-dc4e4e0f5f59",
   "metadata": {},
   "source": [
    "A bit better! The SVM was hurting performance. Now let's try using a soft voting classifier. We do not actually need to retrain the classifier, we can just set `voting` to `\"soft\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a7b1416-d38d-47d1-a0a4-d081bbc386f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2b0cb4d-d45b-4bb0-8026-43e375b2be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f024788-6515-45a1-8e9c-e2a938d8e44c",
   "metadata": {},
   "source": [
    "Nope, hard voting wins in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f34140-ce04-45fb-bfa0-2f026affdcb2",
   "metadata": {},
   "source": [
    "8d. Once you have found [an ensemble that performs better than the individual predictors], try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf664c3b-0c19-4d76-bde0-288a028e581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.968, 0.9703, 0.9614]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_test, y_test.astype(np.int64))\n",
    " for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c8e85-258e-4824-bf73-ab5adfe3595a",
   "metadata": {},
   "source": [
    "The voting classifier reduced the error rate of the best model from about 3% to 2.7%, which means 10% less errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37156b1c-e7dc-4b01-94e4-d9fc6d67459b",
   "metadata": {},
   "source": [
    "9. Stacking Ensemble\n",
    "\n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vecotr containing the set of predictions from all your classifiers for an image, and target is the image's class. Train a classifier on this new training set. Congratulations - you have trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trainined earlier? Now try again using a 'StackingClassifier' instead. Do you get better performance? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45490a-4f4b-44ce-b8dd-699252dfee5f",
   "metadata": {},
   "source": [
    "9a. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vecotr containing the set of predictions from all your classifiers for an image, and target is the image's class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95d99d22-0740-4288-8448-f6a59bd57216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3', '3', '3', '3'],\n",
       "       ['8', '8', '8', '8'],\n",
       "       ['6', '6', '6', '6'],\n",
       "       ...,\n",
       "       ['5', '5', '5', '5'],\n",
       "       ['6', '6', '6', '6'],\n",
       "       ['8', '8', '8', '8']], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_predictions = np.empty((len(X_valid), len(estimators)), dtype=object)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_valid_predictions[:, index] = estimator.predict(X_valid)\n",
    "\n",
    "X_valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad8ab911-21b4-4cb8-b1bc-3f5fd4e9fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True,\n",
    "                                            random_state=42)\n",
    "rnd_forest_blender.fit(X_valid_predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd82dcda-1225-4786-938b-d5d6ba372502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9743"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fda5c5-ca6c-464d-8434-c7b16e4f45a7",
   "metadata": {},
   "source": [
    "You could fine-tune this blender or try other types of blenders (e.g., an `MLPClassifier`), then select the best one using cross-validation, as always."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75f66d-61f9-4ad3-99ce-05f0393e1a04",
   "metadata": {},
   "source": [
    "9b. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c20320c-2df7-4732-bb32-274d4a0e2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=object)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3db721b3-9600-418e-8066-1a146afba66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "901c07dd-097b-4192-8065-7ccc51822561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9703"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989f589-ae62-4891-b02e-4a34446b5985",
   "metadata": {},
   "source": [
    "This stacking ensemble does not perform as well as the voting classifier we trained earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a43f47-30f1-479c-9bd5-7e98d3cbefa5",
   "metadata": {},
   "source": [
    "9c. \"Now try again using a 'StackingClassifier' instead. Do you get better performance? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c18455-12c5-4377-a468-1ac483315dba",
   "metadata": {},
   "source": [
    "Since `StackingClassifier` uses K-Fold cross-validation, we don't need a separate validation set, so let's join the training set and the validation set into a bigger training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfa53fb8-d2b2-4f96-9478-b8710ac12216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, y_train_full = X_mnist[:60_000], y_mnist[:60_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f5738-91ca-47bb-9bd5-d1197d3ddd7c",
   "metadata": {},
   "source": [
    "**Warning**: the following cell will take quite a while to run (15-30 minutes depending on your hardware), as it uses K-Fold validation with 5 folds by default. It will train the 4 classifiers 5 times each on 80% of the full training set to make the predictions, plus one last time each on the full training set, and lastly it will train the final model on the predictions. That's a total of 25 models to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "992ffbac-095b-48df-a5fc-b7081005bcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacking_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m stack_clf \u001b[38;5;241m=\u001b[39m StackingClassifier(named_estimators,\n\u001b[0;32m      4\u001b[0m                                final_estimator\u001b[38;5;241m=\u001b[39mrnd_forest_blender)\n\u001b[0;32m      5\u001b[0m stack_clf\u001b[38;5;241m.\u001b[39mfit(X_train_full, y_train_full)\n\u001b[1;32m----> 6\u001b[0m stacking_clf\u001b[38;5;241m.\u001b[39mfit(X_train_full, y_train_full)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stacking_clf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stack_clf = StackingClassifier(named_estimators,\n",
    "                               final_estimator=rnd_forest_blender)\n",
    "stack_clf.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f33d4c7a-70d5-4c9e-9a0f-89bb7b3a972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f8bee-c163-4e15-bd5b-2aabb93f613d",
   "metadata": {},
   "source": [
    "The `StackingClassifier` significantly outperforms the custom stacking implementation we tried earlier! This is for mainly two reasons:\n",
    "\n",
    "* Since we could reclaim the validation set, the `StackingClassifier` was trained on a larger dataset.\n",
    "* It used `predict_proba()` if available, or else `decision_function()` if available, or else `predict()`. This gave the blender much more nuanced inputs to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3134f8-e09b-449b-ab54-460f78a4685a",
   "metadata": {},
   "source": [
    "And that's all for today, congratulations on finishing the chapter and the exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c41f1-3446-4600-a9f6-6ff946b9b11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
