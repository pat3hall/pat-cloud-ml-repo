{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3fc31c-b1ed-4a5d-a3b2-69f8ff026ae6",
   "metadata": {},
   "source": [
    "Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "Chapter 16 exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34122bc1-4cfc-45dd-8137-e409e194602b",
   "metadata": {},
   "source": [
    "1. What are the pros and cons of using stateful RNN versus stateless RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43847e-2406-474f-834b-1cd77769c6b4",
   "metadata": {},
   "source": [
    "-> Stateless RNNs do not preserve the hidden state across sequences or batches. Therefore, they can only capture patterns whose length is less than the sequence size or window size, it was trained out. \n",
    "-> Stateful RNNs maintain their internal state across multiple sequences or batches of data. Thus, they can capture longer dependencies in the data. \n",
    "-> Since the stateful RNNs require training data to be fully sequential and non-overlapping, they are harder to set and train. \n",
    "-> Stateless RNNs used when the context of the previous sequence is not relevant or when data is shuffled randomly (example sentiment analysis of a sentence). Stateful RNNs when the order and continuity of sequences are essential, such as in generating music or predicting stock prices.\n",
    "\n",
    "page 584:\n",
    "Stateless RNN\n",
    "- at each training iteration the model starts with a hidden state full of zeros, then it updates the state each time step, and after last time step, it throws it away as it is not needed anymore\n",
    "\n",
    "Stateful RNN\n",
    "- preserve the final state after processing a training batch and use it as the initial state for then next training batch\n",
    "- this way model can learn long-term patterns despite only backpropagating through short sequences\n",
    "\n",
    "Building Stateful RNN\n",
    "- each sequence in a batch starts exactly where the corresponding sequence in the previous batch left off\n",
    "- use sequential and non-overlapping input sequences (rather than shuffled and overlapping sequences previous used to train stateless RNNs)\n",
    "- when creating the tf.data.Dataset, when must use 'shift=length' (instead of shift=1) when calling 'window()' method and must not shuffle\n",
    "- batching is much harder - simpliest solution is use a batch size of 1\n",
    "\n",
    "Difference between Stateful and Stateless RNNs: https://medium.com/@iqra.bismi/difference-between-stateful-and-stateless-rnns-2b397184e759\n",
    "\n",
    "Key Differences [between stateful and stateless RNNs]:\n",
    "\n",
    "a. Memory Retention: Stateful RNNs retain memory across sequences, capturing long-term dependencies, while stateless RNNs treat each sequence independently without retaining memory.\n",
    "\n",
    "b. Sequence Order: Stateful RNNs preserve the order and continuity of sequences, whereas stateless RNNs treat sequences as isolated entities.\n",
    "\n",
    "c. Training and Inference: Stateful RNNs require careful management of sequence boundaries during training and inference, while stateless RNNs do not have this requirement.\n",
    "\n",
    "__book answer:__\n",
    "\n",
    "Stateless RNNs can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on. Conversely, stateful RNNs can capture longer-term patterns. However, implementing a stateful RNN is much harder⁠—especially preparing the dataset properly. Moreover, stateful RNNs do not always work better, in part because consecutive batches are not independent and identically distributed (IID). Gradient Descent is not fond of non-IID datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca0522-4503-43c8-ad29-cd47b0a512a3",
   "metadata": {},
   "source": [
    "2. Why do people use encoder-decoder RNNs rather than plan sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a4fea-8ec9-4666-833f-18716e1b60ae",
   "metadata": {},
   "source": [
    "-> Sequence-to-sequence RNNs essentially translate a sentence one word at a time starting with the first word. Since words can have multiple means and the specific meaning is often depend on how it used a in sentence, translating one word at time will result in poor translations. On the other encoder-decoder will 1st process the whole sentence, and then translate it \n",
    "\n",
    "page 578:\n",
    "RNN encoder-decoder architectures\n",
    "- RNNs can be used to build encoder-decoder architectures capable of performing Neural Machine Translation (NMT) e.g. translating from English to Spanish\n",
    "\n",
    "pages 595 - 596\n",
    "Neural Machine Translation (NMT) (https://homl.info/103)\n",
    "- will translate English sentences to Spanish\n",
    "- architecture (see Figure 16-3. A simple Machine translation model. page 596): \n",
    "    - [via textvectorization] initially each word is represented by its ID\n",
    "    - Next, and embedding layer returns the word embedding\n",
    "    - the embeddings are then fed to the encoder and the decoder\n",
    "    - english sentences fed as inputs to the encoder\n",
    "    - decoder outputs the Spanish translation\n",
    "      - Note: Spanish translations are also used as inputs to the decoder during training (teacher forcing), but shifted back by one step\n",
    "    - at each step, the decoder output a score for each word in the output vocabulary (i.e. Spanish), then the softmax activation function turns these scores into probabilities. The word with the highest probability is output\n",
    "    - at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, you need to feed it the work that it has just output at the previous step. This ill require an embedding lookup (not shown in figure 16-4)\n",
    "      - see figure 16-4. At inference time, the decoder is fed as input the work it just output at the previous time step. page 597 \n",
    "\n",
    "\n",
    "__book answer:__\n",
    "\n",
    "In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence \"Je vous en prie\" means \"You are welcome,\" but if you translate it one word at a time, you get \"I you in pray.\" Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an Encoder–Decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bc71e-3103-42d5-9bd4-42ebe56d2b14",
   "metadata": {},
   "source": [
    "3. How can you deal with variable-length input sequences? What tool can you use to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2757a55-f424-4d5a-b975-13c38ee328e2",
   "metadata": {},
   "source": [
    "One approaching is padding shorter length sequences so all the seqences are the same length, and then using masking to tell the RNN model to ignore the padding tokens. To enable masking, add \"masking_zero=True\" argument to Embedding layer. If your model does not start with an Embedding layer, you may use the tf.keras.Masking layer instead. A TextVectorization layer will add padding to sequences less than the specified max_tokens size. Another approach is to feed the model with ragged tensors and set 'ragged=True' when creating the TextVectorization layer.\n",
    "\n",
    "page 590 - 591:\n",
    "Masking the model:\n",
    "- ignore padding tokens\n",
    "- to use, simply add \"masking_zero=True\" argument to Embedding layer\n",
    "\n",
    "Embedding model masking:\n",
    "- creates a 'mask tensor' equal to 'tf.math.not_equal(inputs, 0): a boolean tensor with the same shape as the inputs, and it is equal to False anywhere the token IDs are 0, or True otherwise\n",
    "- 'mask tensor' is automatically  propagated by the model to the next layer. If that layer's call() method has a mask argument, then it automatically receives the mask\n",
    "- each layer may handle the mask differently, but in general, they simply igmore masked time steps. For example, when are recurrent layer encoutersa masked time step, it simply copies the output from the previous time step\n",
    "\n",
    "Recurrent layers support for masking attribute\n",
    "- a recurrent layer's supports_masking attribute is True when 'return_sequences=True', but if false when 'return_sequences=False' since there is no need for a mask in this case\n",
    "- in below code example, GRU does not have 'return_sequences=True', so it will receive and use the mask automatically, but it will not propagate it any further\n",
    "\n",
    "Keras layers supporting masking:\n",
    "- include GRU, LSTM, Bidirectional, Dense, TimeDistributed, Add\n",
    "- convolutional layers including Conv1D do not support masking - it's not obvious how they would do so anyways\n",
    "\n",
    "\n",
    "page 592:\n",
    "Masking using Ragged Tensors\n",
    "- one approach to masking is to feed the model with ragged tensors\n",
    "- in practice, all you need to do is to set 'ragged=True' when creating the TextVectorization layer, so all the input sequences are represented as ragged tensors\n",
    "- Kera's recurrent layers have built-in support ragged tensor, so there's nothing else you need to do, just use this TextVectorization layer with ragged=True in your model\n",
    "\n",
    "__book answer:__\n",
    "Variable-length input sequences can be handled by padding the shorter sequences so that all sequences in a batch have the same length, and using masking to ensure the RNN ignores the padding token. For better performance, you may also want to create batches containing sequences of similar sizes. Ragged tensors can hold sequences of variable lengths, and Keras now supports them, which simplifies handling variable-length input sequences (at the time of this writing, it still does not handle ragged tensors as targets on the GPU, though). Regarding variable-length output sequences, if the length of the output sequence is known in advance (e.g., if you know that it is the same as the input sequence), then you just need to configure the loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence. But generally the length of the output sequence is not known ahead of time, so the solution is to train the model so that it outputs an end-of-sequence token at the end of each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90cd597-c7d6-43c1-ab55-4cd5e7e08825",
   "metadata": {},
   "source": [
    "4. What is a beam search, and why would you use it? What tool can you use to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd95d3-eb0c-4ed5-a87f-dc5cd3c8bac0",
   "metadata": {},
   "source": [
    "-> For a trained encoder-decoder model, it is a technique that gives a model a chance to go back and fix mistakes by keeping track of a short list of the 'k' most promising sentences (e.g top 3), and at each decoder step it tries to extend them by one word, keeping on the 'k' most likely sentences. The parameter 'k' is call the beam width. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. \n",
    "-> You can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons's seq2seq API provides an implementation.\n",
    "\n",
    "Beam Search (pages 603 - 604)\n",
    "- for a trained encoder-decoder model, it is a technique that gives a model a chance to go back and fix mistakes\n",
    "- keeps track of a short list of the 'k' most promising sentences (e.g top 3), and at each decoder step it tries to extend them by one word, keeping on the 'k' most likely sentences\n",
    "- the parameter 'k' is call the beam width\n",
    "- good translations for fairly short sentences\n",
    "- due to limited short-term memory of RNNs, beam models are bad at translating long sentences TensorFlow Addons Library\n",
    "- includes a full seq2seq API that lets you build encoder-decoder models with attention including 'beam search'\n",
    "\n",
    "__book answer:__\n",
    "Beam search is a technique used to improve the performance of a trained Encoder–Decoder model, for example in a neural machine translation system. The algorithm keeps track of a short list of the k most promising output sentences (say, the top three), and at each decoder step it tries to extend them by one word; then it keeps only the k most likely sentences. The parameter k is called the beam width: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. You can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons's seq2seq API provides an implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a04c1-abbb-4524-b98a-6a600b2572c9",
   "metadata": {},
   "source": [
    "5. What is an attention mechanism? How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5aae51-b225-40ab-87f6-46547dc8a9ea",
   "metadata": {},
   "source": [
    "-> An attention mechanism is a technique initially used in Encoder–Decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder's state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. \n",
    "-> The main benefit of using an attention mechanism is the fact that the Encoder–Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers.\n",
    "\n",
    "page 605:\n",
    "Attention Mechanisms\n",
    "- revolutionalized Neural Machine Translation (NMT) especially for long sentences\n",
    "- a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step\n",
    "- figure 16-7. Neural machine translation using an encoder-decoder network with an attention model. page 606\n",
    "   - encoder-decoder: instead of just sending the encoder's final hidden state to the decoder, as well as the previous target word at each step, it now sends all of the encoder's outputs to the decoder as well\n",
    "    - since the decoder cannot deal with all those encoder's outputs at once, they are aggregated: at each time step, the decoder's memory cell computes a weighted sum of all the encoder outputs. This determines which words it will focus on at this step\n",
    "    - the weight alpha_(t,i) is the weight of the i_th encoder output at the t_th decoder time step\n",
    "    - For example, if the weight alpha_(3,2) is larger than alpha_(3,0) and alpha_(3,1), then the decoder will pay more att the encoder's output for the #2 word ('soccer' in 'I like soccer') than to the other 2 outputs, at least at this time step\n",
    "\n",
    "\n",
    "pages 606 - 607\n",
    "Alignment Model (or attention layer):\n",
    "- generates the alpha_(t,i) weights\n",
    "- neural network trained jointly with the encoder-decoder model used in an attention mechanism\n",
    "- starts with a Dense layer composed of a single neuron that processes each of the encoder's outputs along with the decoder's previous hidden state (e.g. h_(2) )\n",
    "   - this layer outputs a score (or energy) forech encoder outut (e.g. alpha_(3,2) ): this score measures how well each output is alighed with the decoder's previous hidden state\n",
    "- finally, all scores go through a softmax layer to get a final weight for each encoder output (e.g. alpha_(3,2) )\n",
    "Bahdanau attention (or concatentative attention or additive attention)\n",
    "- above attention technique\n",
    "Luong attention (or multiplicative attention)\n",
    "- the goal of alignment model is to measure the similarity between one of the encoder's outputs and the decoder's previous hidden layer\n",
    "- simply compute the dot product of the these two vectors, as this is often a fairly good similarity measure\n",
    "- the dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights (just as in the Bahdanau attention)\n",
    "- another simplificaiton was to used the decoder's hidden state at the current time step rather than at the previous time step (e.g. h_(t) rather than h_(t-1), then use the output of the attention mechanism directly (noted: h~_(t) directly to compute the decoder's predictions, rather than using it to compute decoder's current hidden state\n",
    "\"general\" dot product approach\n",
    "- a variant of the dot product mechanism  where then encoder output first go through a fully connected layer (without a bias term) before the dot products are computed dot product variants vs concatenative attention \n",
    "- dot product variants performed better than concatentative attention.\n",
    "\n",
    "__book answer:__\n",
    "An attention mechanism is a technique initially used in Encoder–Decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder's state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. The main benefit of using an attention mechanism is the fact that the Encoder–Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers. See the next answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897efaa-d3bc-4a18-917e-49fdf38f0d3e",
   "metadata": {},
   "source": [
    "6. What is the most important layer in the transformer architecture? What is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7da40-d2d1-4a56-ac61-837608f49545",
   "metadata": {},
   "source": [
    "-> The most important layer in the  transformer architecuture is the multi-head attention layer. In the original transformer architecture there were 6 multi-head layers in the encoder, 6 masked multi-head layers plus 6 [regular] multi-head layers in the decoder. The multi-head attention layer updates each word representation by attending (i.e. paying attention to) all other words in the same sentence. That is, it improves words representation by using contextual clues.\n",
    "\n",
    "page 609 - 612\n",
    "transformer:\n",
    "  - by google researchers - significantly trans\n",
    "  - improved the state-of-the-art NMT without using any recurrent or convolutional layers, just attention mechanisms (plus embedded layers, dense layers, normalization layers, etc.)\n",
    "  - see figure 16-8. The original 2017 transformer architecture. page 610\n",
    "    - left part is the encoder\n",
    "    - right part is the decoder\n",
    "    - each embedding layer (at beginning of encoder and decoder) outputs a 3D tensor of shape [batch size, sequence length, enbedding size]\n",
    "    - after embedding layers, the tensors are gradually transformed as they flow through the transformer, but their shape remains the same\n",
    "    - after going through the decoder, each word representation goes through a final dense layer with a softmax activation\n",
    "  encoder / decoder\n",
    "    - contain N stacked modulues (N=6 for paper)\n",
    "    - components enclude 2 embedding layers, several skip connections, each of them followed by a layer normalization; several feedforward that are composed of 2 dense layers each (1st with ReLU activation & 2nd using no activation)\n",
    "    - final outputs of whole encoder stack are fed to the decoder at each of the N levels\n",
    "  output layer:\n",
    "    - a dense layer using the softmax activation function\n",
    "  encoder's multi-head attention layer\n",
    "    - updates each word representation by attending (i.e. paying attention to) all other words in the same sentence\n",
    "    - this is where vague words (e.g. 'like') become richer more accurate representations (e.g. \"to be fond of\")\n",
    "  decoder's masked multi-head attention layer\n",
    "    - its a 'casual' (only looks at past and present inputs) - when it processes a word, it doesn't attend to words after it \n",
    "  decoder's upper multi-head attention layer\n",
    "    - cross-attention: where it pays attention to words in the English sentence (pays attention to 'soccer' when it processes the 'el' [in 'el futbol']\n",
    "  positional encoding\n",
    "    - after input (English) / output (Spanish) embedding layers, and before N-stacked attention modules \n",
    "    - dense vectors (much like word embeddings) that represent the position of each word in a sentence. The nth positional encoding is added to the word embedded on the nth word in the sentence\n",
    "      - needed because all layers in the transform architecture ignore word positions, and word positions do matter\n",
    "\n",
    "__book answer:__\n",
    "\n",
    "The most important layer in the Transformer architecture is the Multi-Head Attention layer (the original Transformer architecture contains 18 of them, including 6 Masked Multi-Head Attention layers). It is at the core of language models such as BERT and GPT-2. Its purpose is to allow the model to identify which words are most aligned with each other, and then improve each word's representation using these contextual clues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dad4d-df6e-4bde-9b2f-91e39a87aef1",
   "metadata": {},
   "source": [
    "7. When would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc57acf-752b-414a-86bc-e33cc1f88063",
   "metadata": {},
   "source": [
    "-> Sampled softmax would be used when the output/target vocabulary is large (e.g. 5000 target vocabulary), so outputting a probability for each and every possible word can be slow.\n",
    "-> Sampled Softmax look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute the approximation of the cross-entropy loss based only on these logits (sampled softmax technique). This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits.\n",
    "\n",
    "page 600:\n",
    "Optimizing the Output Layer\n",
    " - when the ouput vocabulary is large, outputting a probability for each and every possible word can be slow\n",
    " solution 1:\n",
    "   - look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute the approximation of the loss based only on these logits (sampled softmax technique)\n",
    "    - in TensorFlow, you can uses the tf.nn.sampled_softmax_loss() function for this during trainining, and use the normal softmax function at inference time (sampled softmax cannot be used at inference time because it requires knowing the target)\n",
    "  solution 2:\n",
    "    - tie the weights of the output layer to the transpose of the decoder's embedding matrix (see chap 17)\n",
    "\n",
    "__book answer:__\n",
    "Sampled softmax is used when training a classification model when there are many classes (e.g., thousands). It computes an approximation of the cross-entropy loss based on the logit predicted by the model for the correct class, and the predicted logits for a sample of incorrect words. This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03777c5-ce60-4112-ae49-eecd06f65512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c6eb42-ab4a-4b0e-9baa-caeebf8bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"deep\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215b82b-6257-4bc5-ad6f-b0ceff93f1b2",
   "metadata": {},
   "source": [
    "#### 8. \n",
    "_Exercise:_ Embedded Reber grammars _were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's [nice introduction](https://homl.info/108) to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cf349-a4a5-4b5d-9c04-d6dee12df46f",
   "metadata": {},
   "source": [
    "First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90319c7-9c6b-43a5-ac65-20f8dda69381",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c529e74-1891-49e2-80d8-2de236bea68e",
   "metadata": {},
   "source": [
    "Let's generate a few strings based on the default Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9bc278-102c-4baa-b0d9-14ad56cf7b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b9c75-bbc8-4c4d-b2c7-ce4a248f908b",
   "metadata": {},
   "source": [
    "Looks good. Now let's generate a few strings based on the embedded Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4626b2f-6a03-479d-95d2-b1d57489e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c6706-d9dc-4c35-a250-b35071722f86",
   "metadata": {},
   "source": [
    "Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb810f9f-0e10-4baf-8aa0-d2c08879d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c35b3e77-ad9b-470e-86de-1441a3ed142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc46f6-4037-4cf7-a1e1-55afe361140e",
   "metadata": {},
   "source": [
    "We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48c2e802-0c13-4ff1-a761-4f2e71d124c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d96bc463-d4b7-414c-bfc2-219b88593138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66097b10-f488-4088-8036-b5e7632f8e39",
   "metadata": {},
   "source": [
    "We can now generate the dataset, with 50% good strings, and 50% bad strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9394312-1681-46a8-80fd-b8f5e367a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [\n",
    "        string_to_ids(generate_string(embedded_reber_grammar))\n",
    "        for _ in range(size // 2)\n",
    "    ]\n",
    "    bad_strings = [\n",
    "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "        for _ in range(size - size // 2)\n",
    "    ]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e5163b5-bb3b-4f8b-918e-1906ee9081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def014f-bbf8-4964-a251-b8052e53a111",
   "metadata": {},
   "source": [
    "Let's take a look at the first training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74f79268-b424-41ae-83eb-089c332f11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(22,), dtype=int32, numpy=array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fff250cb-bafa-4693-95f3-c7bd9fb3371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d0966-3544-4955-b5ec-a3266f34cb50",
   "metadata": {},
   "source": [
    "Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08810cbd-2d2b-4a07-9527-a28984c1bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Embedding.call().\n\n\u001b[1mFailed to convert elements of tf.RaggedTensor(values=Tensor(\"data:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_1:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by Embedding.call():\n  • inputs=tf.Tensor(shape=(None, None), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m, momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     14\u001b[0m                                     nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     16\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     18\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling Embedding.call().\n\n\u001b[1mFailed to convert elements of tf.RaggedTensor(values=Tensor(\"data:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_1:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by Embedding.call():\n  • inputs=tf.Tensor(shape=(None, None), dtype=int32)"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_size = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    tf.keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS),\n",
    "                              output_dim=embedding_size),\n",
    "    tf.keras.layers.GRU(30),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95,\n",
    "                                    nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dbafa-ccf3-460f-a76e-1fbfedee3d7a",
   "metadata": {},
   "source": [
    "Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb169bf7-9d1d-48c9-95f9-5e4ce9a15c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2003f7ba-3e2e-4884-bb03-ffdf70a17cd7",
   "metadata": {},
   "source": [
    "Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f576342-04d1-4416-a783-bbedc5166c20",
   "metadata": {},
   "source": [
    "#### 9.   \n",
    "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ea528-13cd-4c14-90a2-83cc96084e26",
   "metadata": {},
   "source": [
    "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84921ea9-f5fc-4300-be7c-783b193c14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a69e69-de8c-44ae-b1e4-d1fe5770099b",
   "metadata": {},
   "source": [
    "Here are a few random dates, displayed in both the input format and the target format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79346503-e45d-4a3c-aefc-49bc754efd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88857d-3c00-4423-b25e-99412caa65d3",
   "metadata": {},
   "source": [
    "Let's get the list of all possible characters in the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b72721e-af59-473f-a40d-fc60cd0db922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6945600-5d05-4657-9b5c-ef3bc6533ee2",
   "metadata": {},
   "source": [
    "And here's the list of possible characters in the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36ea7d05-eb6d-40ff-a25a-8001bcfa55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466d4bf-d73c-42e1-bcfb-9ae016d856b5",
   "metadata": {},
   "source": [
    "Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8560f5a-ef40-4ba6-864f-6a0ee8264226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5d92f5b-b061-4b09-bb49-561517ed7773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ec618bc-07b0-4479-bbcf-0ecdec701474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e5f1a28-dce9-4d10-895d-55e0e3709927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb94d754-7778-476e-8cf8-f00fac1b6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66beaaef-73f9-4e9b-95a8-5f010be93588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831258a-d726-4d27-80cb-466c57b01948",
   "metadata": {},
   "source": [
    "First version: a very basic seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24b705-d96e-42b2-b5e6-539cd3d1eaad",
   "metadata": {},
   "source": [
    "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
    "\n",
    "Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31d7e19b-8e55-4121-878b-c6187717612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:89: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.2996 - loss: 1.9909 - val_accuracy: 0.4997 - val_loss: 1.4815\n",
      "Epoch 2/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.5575 - loss: 1.2604 - val_accuracy: 0.6684 - val_loss: 0.9019\n",
      "Epoch 3/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.6887 - loss: 0.8251 - val_accuracy: 0.7338 - val_loss: 0.6843\n",
      "Epoch 4/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.7569 - loss: 0.6109 - val_accuracy: 0.8299 - val_loss: 0.4428\n",
      "Epoch 5/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8531 - loss: 0.3901 - val_accuracy: 0.9063 - val_loss: 0.2748\n",
      "Epoch 6/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.7826 - loss: 0.6842 - val_accuracy: 0.8959 - val_loss: 0.3424\n",
      "Epoch 7/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9213 - loss: 0.2813 - val_accuracy: 0.9642 - val_loss: 0.1663\n",
      "Epoch 8/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9727 - loss: 0.1376 - val_accuracy: 0.9870 - val_loss: 0.0856\n",
      "Epoch 9/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0704 - val_accuracy: 0.9945 - val_loss: 0.0475\n",
      "Epoch 10/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9964 - loss: 0.0392 - val_accuracy: 0.9970 - val_loss: 0.0294\n",
      "Epoch 11/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0239 - val_accuracy: 0.9982 - val_loss: 0.0197\n",
      "Epoch 12/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9995 - loss: 0.0156 - val_accuracy: 0.9990 - val_loss: 0.0136\n",
      "Epoch 13/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9998 - loss: 0.0106 - val_accuracy: 0.9996 - val_loss: 0.0098\n",
      "Epoch 14/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9997 - val_loss: 0.0073\n",
      "Epoch 15/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.9998 - val_loss: 0.0056\n",
      "Epoch 16/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9999 - val_loss: 0.0044\n",
      "Epoch 17/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9999 - val_loss: 0.0035\n",
      "Epoch 18/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9999 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9999 - val_loss: 0.0023\n",
      "Epoch 20/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9999 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
    "                           output_dim=embedding_size,\n",
    "                           input_shape=[None]),\n",
    "    tf.keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4b81a-4fb1-4268-ab8c-08c19a403e74",
   "metadata": {},
   "source": [
    "Looks great, we reach 100% validation accuracy (actaully 99.99)! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2169050f-4da1-490f-a9f1-7cd11b40f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df98cf-46b0-4cef-ba63-2f494cb1e202",
   "metadata": {},
   "source": [
    "Now we can use the model to convert some dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cf25286-5302-40d8-8a1c-edac7bf78736",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5feff0d2-86ad-49f4-a4cb-5b603af3a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step\n",
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7421fff-1873-4e4f-8ee4-9008b7a8f310",
   "metadata": {},
   "source": [
    "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3080aa05-1756-4ab2-aacc-8107f0e89e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2880a87-9bd1-4d63-93d4-e68bed04ecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step\n",
      "2020-02-02\n",
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4238b681-2812-451a-81c8-4930d03fb12a",
   "metadata": {},
   "source": [
    "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb037305-813c-49de-bc30-23c6c4bc7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict(X).argmax(axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63352ed7-965b-4b23-9f55-438cb0e4572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83837b8a-4ea5-4f8a-b99d-654ca4d03327",
   "metadata": {},
   "source": [
    "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37fdc6-ac26-4fa8-91ab-5774a6690241",
   "metadata": {},
   "source": [
    "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ee601-d2ff-4044-82f3-a7a943a8b661",
   "metadata": {},
   "source": [
    "Second version: feeding the shifted targets to the decoder (teacher forcing)¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c0576-3592-497f-bbff-9ce5e2176094",
   "metadata": {},
   "source": [
    "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
    "\n",
    "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
    "\n",
    "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
    "\n",
    "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
    "\n",
    "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89c63aff-01f2-4621-9bbe-3a0930a338a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e0c0b-91e7-4488-9c94-89efdebe6765",
   "metadata": {},
   "source": [
    "Let's take a look at the decoder's training inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bbd1b123-34a4-4169-a6a8-e2e6d1c29386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]])>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eef7b0-9481-41f9-b471-7e009b52e507",
   "metadata": {},
   "source": [
    "Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d97ce44d-25cf-475c-9324-53c9d641ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.3378 - loss: 1.8357 - val_accuracy: 0.6078 - val_loss: 1.0792\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.6881 - loss: 0.8767 - val_accuracy: 0.8545 - val_loss: 0.4369\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8977 - loss: 0.3308 - val_accuracy: 0.9287 - val_loss: 0.2425\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9809 - loss: 0.1116 - val_accuracy: 0.9956 - val_loss: 0.0517\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0402 - val_accuracy: 0.9989 - val_loss: 0.0249\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9958 - loss: 0.0351 - val_accuracy: 0.9980 - val_loss: 0.0286\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9996 - loss: 0.0198 - val_accuracy: 0.9997 - val_loss: 0.0125\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9999 - val_loss: 0.0081\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0057\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,\n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16617253-9d97-4d4d-984a-d99db23b277d",
   "metadata": {},
   "source": [
    "This model also reaches 100% validation accuracy, but it does so even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8a09b-17c4-4cae-b37a-9dec82280a96",
   "metadata": {},
   "source": [
    "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08e98f73-c1cd-466f-a964-88d09687caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4578fd43-3218-46f7-822f-72be3cbb1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0248696-b379-4511-b84d-0d03d06bac32",
   "metadata": {},
   "source": [
    "Works fine! Next, feel free to write a Transformer version. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755eeb7-17ea-4918-aa98-1148bfe51a9b",
   "metadata": {},
   "source": [
    "#### 10.\n",
    "_Exercise: Go through Keras's tutorial for [Natural language image search with a Dual Encoder](https://homl.info/dualtuto). You will learn how to build a model capable of representing both images and text within the same embedding space. This makes it possible to search for images using a text prompt, like in the [CLIP model](https://openai.com/blog/clip/) by OpenAI._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc3879-1e32-4574-9597-bb2d6fb7d3a3",
   "metadata": {},
   "source": [
    "This example requires TensorFlow 2.4 or higher. In addition, TensorFlow Hub and TensorFlow Text are required for the BERT model, and TensorFlow Addons is required for the AdamW optimizer. These libraries can be installed using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2589e842-6821-42ef-8865-87d255106449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-text (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-text\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90c2677c-565f-44fb-968d-22869c28fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cf5e55f-b773-4bd2-b1a0-a66e50774a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-text (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-text\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a00df775-b56d-4fe4-92b3-8e4e3f7b2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ad288-e14f-41d2-a7da-fd95d8fd6a16",
   "metadata": {},
   "source": [
    "likes there is no option for installing tensorflow-text on windows 10\n",
    "https://discuss.tensorflow.org/t/cant-install-tensorflow-text-2-11-0/13288/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804e0c3-8d54-4fee-a487-45bb963562fc",
   "metadata": {},
   "source": [
    "#### 11. \n",
    "Use the Hugging Face Transformers library to download a pretrained language model capable of generating text (e.g. GPT), and try generating more convincing Shakespearean text. You will need to use the model's generate() method - see Huggings Face's documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105399b-bb36-4274-9ab4-47c0c45cb562",
   "metadata": {},
   "source": [
    "First, let's load a pretrained model. In this example, we will use OpenAI's GPT model, with an additional Language Model on top (just a linear layer with weights tied to the input embeddings). Let's import it and load the pretrained weights (this will download about 445MB of data to `~/.cache/torch/transformers`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa591e1c-e20a-4329-83ba-c390500bbe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6294a0ba619340fea3b56d76e7f446e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFOpenAIGPTLMHeadModel: ['h.10.attn.bias', 'h.11.attn.bias', 'h.0.attn.bias', 'h.3.attn.bias', 'h.2.attn.bias', 'h.5.attn.bias', 'h.8.attn.bias', 'h.6.attn.bias', 'h.7.attn.bias', 'h.1.attn.bias', 'h.9.attn.bias', 'h.4.attn.bias']\n",
      "- This IS expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a189a-2a37-438a-853c-15e0c66b4d57",
   "metadata": {},
   "source": [
    "Next we will need a specialized tokenizer for this model. This one will try to use the spaCy and ftfy libraries if they are installed, or else it will fall back to BERT's BasicTokenizer followed by Byte-Pair Encoding (which should be fine for most use cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5720413-5d2e-4d45-8f42-475c3b7ac69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1457c9a-39c2-42cb-84ef-a54382f79962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3570, 1473], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello everyone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2464c198-623a-4c3d-b1a6-685c2948124b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]])>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f2296-9fd7-4e6e-b367-402268367834",
   "metadata": {},
   "source": [
    "Easy! Next, let's use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens. For an explanation of what all the hyperparameters do, make sure to check out this great [blog post](https://huggingface.co/blog/how-to-generate) by Patrick von Platen (from Hugging Face). You can play around with the hyperparameters to try to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f102a0a3-cdb5-4ee6-86f5-a4f1f789f9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239, 40477,   556,   524,  1724,  4338,   504,   481,\n",
       "          831,  1520,   546,   535, 12003,  4374,   240, 10341,   535,\n",
       "         2236,  1404, 21760,   239,   998,   524,  2170,  1063,  1098,\n",
       "          833,   604, 21617,   575,   240,   524,  9093,   626,   595,\n",
       "         4203,  1129,   239,   487,  1787],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   636,  1300,   666,   246, 12065,  1276,   240,   488,\n",
       "          557,   622, 14404,  1546,  1260,   481, 19995,   240, 14404,\n",
       "        30599,   636,  3226,   239,   500,   616,   638,   507,   509,\n",
       "         1816,   525,  1007,  1594,   636,  1443,   580,  1632,   240,\n",
       "          481,  6391,  7876,   498,   481],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   645,   512,   640,   246,  1250,   498,   547,\n",
       "         3766,   488,   595,   547,  1662,   240,   674,  4055,   704,\n",
       "         1254,   239,   256, 40477,   256,   249,   604,   664,  3367,\n",
       "          500, 32883,   246,  7339,   525,  1222,   964,  1038,   568,\n",
       "          485,  5018,   240,   256,   487],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   544,   481,  8018,   618,   498, 37736,   240,\n",
       "          616,  1546,   544,   481,  1432,  2636,   500,   481,  9687,\n",
       "          240,   488,   616,  1546,   544,   563, 15198,  2303,   702,\n",
       "          246,   762,   260,   498,   260,  2557,   240,   246, 26090,\n",
       "          531, 17378,   240,   246,  3128],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   812,   580,  3232,   239,   512,   804,   580,\n",
       "          481,  2093,   498,   246,   618,   239,   512,   804,  2985,\n",
       "          547,  7047,   488,   580,   547,  2093,   239,   256, 40477,\n",
       "          481,   618,   816,   714,   491,   524,  1712,   239, 40477,\n",
       "          256,   512,   761,   246, 19456]])>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbea32d-eb48-437b-b115-a08aff468b6f",
   "metadata": {},
   "source": [
    "Now let's decode the generated sequences and print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2b8aa43-416b-407e-952f-3e475807cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle. \n",
      " with his gaze fixed on the marquise's fearful features, domen's plan finally yielded. though his foot might once again have steeled him, his resolve did not dare work. he continued\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle would turn into a hostile world, and as our wretched ship walked the shores, wretched alliances would begin. in this way it was known that another land would soon be lost, the twin cities of the\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. if you are a part of my court and not my friends, then consider your words.'\n",
      "'i have no interest in governing a kingdom that craves nothing but to rule,'he\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, is the fifth king of mallorea, this ship is the best captain in the fleet, and this ship is undefended except by a man - of - war, a mallorean sorcerer, a powerful\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, will be yours. you 'll be the wife of a king. you 'll wear my crown and be my wife.'\n",
      " the king looked down at his brother. \n",
      "'you're a heartless\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc226a2-fa97-441e-affc-53aa26f985e4",
   "metadata": {},
   "source": [
    "You can try more recent (and larger) models, such as GPT-2, CTRL, Transformer-XL or XLNet, which are all available as pretrained models in the transformers library, including variants with Language Models on top. The preprocessing steps vary slightly between models, so make sure to check out this [generation example](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py) from the transformers documentation (this example uses PyTorch, but it will work with very little tweaks, such as adding `TF` at the beginning of the model class name, removing the `.to()` method calls, and using `return_tensors=\"tf\"` instead of `\"pt\"`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
