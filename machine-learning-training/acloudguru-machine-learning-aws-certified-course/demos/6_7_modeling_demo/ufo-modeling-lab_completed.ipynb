{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFO Sightings K-Means Clustering\n",
    "### Modeling Lab\n",
    "\n",
    "The goal of this notebook is to analyze where Mr. K should build his extraterrestrial life facilities using the K-Means algorithm. \n",
    "\n",
    "What we plan on accomplishing is the following:\n",
    "1. [Load dataset onto Notebook instance from S3](#Step-1:-Loading-the-data-from-Amazon-S3)\n",
    "2. [Cleaning, transforming, and preparing the data](#Step-2:-Cleaning,-transforming,-and-preparing-the-data)\n",
    "3. [Create and train our model](#Step-3:-Create-and-train-our-model)\n",
    "4. [Viewing the results](#Step-4:-Viewing-the-results)\n",
    "5. [Visualize using QuickSight](https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go ahead and import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from Amazon S3\n",
    "Next, lets get the UFO sightings data that is stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:275: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "bucket = 'modeling-ufo-lab1'\n",
    "prefix = 'ufo_dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, prefix, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reportedTimestamp</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventTime</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>weather</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sighting</th>\n",
       "      <th>physicalEvidence</th>\n",
       "      <th>contact</th>\n",
       "      <th>researchOutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-04-04T04:02:23.340Z</td>\n",
       "      <td>1977-03-31</td>\n",
       "      <td>23:46</td>\n",
       "      <td>circle</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>rain</td>\n",
       "      <td>Ila</td>\n",
       "      <td>Bashirian</td>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982-11-22T02:06:32.019Z</td>\n",
       "      <td>1982-11-15</td>\n",
       "      <td>22:04</td>\n",
       "      <td>disk</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Eriberto</td>\n",
       "      <td>Runolfsson</td>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-12-07T19:06:52.482Z</td>\n",
       "      <td>1992-12-07</td>\n",
       "      <td>19:01</td>\n",
       "      <td>circle</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Watsica</td>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-24T21:06:34.898Z</td>\n",
       "      <td>2011-02-21</td>\n",
       "      <td>20:56</td>\n",
       "      <td>disk</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Clifton</td>\n",
       "      <td>Bechtelar</td>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-03-09T16:18:45.501Z</td>\n",
       "      <td>1991-03-09</td>\n",
       "      <td>11:42</td>\n",
       "      <td>circle</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>mostly cloudy</td>\n",
       "      <td>Jayda</td>\n",
       "      <td>Ebert</td>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reportedTimestamp   eventDate eventTime   shape  duration  \\\n",
       "0  1977-04-04T04:02:23.340Z  1977-03-31     23:46  circle         4   \n",
       "1  1982-11-22T02:06:32.019Z  1982-11-15     22:04    disk         4   \n",
       "2  1992-12-07T19:06:52.482Z  1992-12-07     19:01  circle        49   \n",
       "3  2011-02-24T21:06:34.898Z  2011-02-21     20:56    disk        13   \n",
       "4  1991-03-09T16:18:45.501Z  1991-03-09     11:42  circle        17   \n",
       "\n",
       "   witnesses        weather firstName    lastName   latitude   longitude  \\\n",
       "0          1           rain       Ila   Bashirian  47.329444 -122.578889   \n",
       "1          1  partly cloudy  Eriberto  Runolfsson  52.664913   -1.034894   \n",
       "2          1          clear    Miller     Watsica  38.951667  -92.333889   \n",
       "3          1  partly cloudy   Clifton   Bechtelar  41.496944  -71.367778   \n",
       "4          1  mostly cloudy     Jayda       Ebert  47.606389 -122.330833   \n",
       "\n",
       "  sighting physicalEvidence contact researchOutcome  \n",
       "0        Y                N       N       explained  \n",
       "1        Y                Y       N       explained  \n",
       "2        Y                N       N       explained  \n",
       "3        Y                N       N       explained  \n",
       "4        Y                N       N       explained  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming, and preparing the data\n",
    "Create another DataFrame with just the latitude and longitude attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = df[['latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude\n",
       "0  47.329444 -122.578889\n",
       "1  52.664913   -1.034894\n",
       "2  38.951667  -92.333889\n",
       "3  41.496944  -71.367778\n",
       "4  47.606389 -122.330833"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   latitude   18000 non-null  float64\n",
      " 1   longitude  18000 non-null  float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 281.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any missing values? False\n"
     ]
    }
   ],
   "source": [
    "missing_values = df_geo.isnull().values.any()\n",
    "print('Are there any missing values? {}'.format(missing_values))\n",
    "if(missing_values):\n",
    "    df_geo[df_geo.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's go ahead and transform the pandas DataFrame (our dataset) into a numpy.ndarray. When we do this each row is converted to a Record object. According to the documentation, this is what the K-Means algorithm expects as training data. This is what we will use as training data for our model.\n",
    "\n",
    "[See the documentation for input training](https://sagemaker.readthedocs.io/en/stable/kmeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  47.329445, -122.57889 ],\n",
       "       [  52.664913,   -1.034894],\n",
       "       [  38.951668,  -92.333885],\n",
       "       ...,\n",
       "       [  36.86639 ,  -83.888885],\n",
       "       [  35.385834,  -94.39833 ],\n",
       "       [  29.883055,  -97.94111 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = df_geo.values.astype('float32')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and train our model\n",
    "In this step we will import and use the built-in SageMaker K-Means algorithm. We will set the number of cluster to 10 (for our 10 sensors), specify the instance type we want to train on, and the location of where we want our model artifact to live. \n",
    "\n",
    "[See the documentation of hyperparameters here](https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "output_location = 's3://' + bucket + '/model-artifacts'\n",
    "\n",
    "kmeans = KMeans(role=role,\n",
    "               instance_count=1,\n",
    "               instance_type='ml.c4.xlarge',\n",
    "               output_path=output_location,\n",
    "               k=num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the job name kmeans-geo-job-20240624172337\n"
     ]
    }
   ],
   "source": [
    "job_name = 'kmeans-geo-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: kmeans-geo-job-20240624172337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-24 17:23:39 Starting - Starting the training job...\n",
      "2024-06-24 17:23:54 Starting - Preparing the instances for training...\n",
      "2024-06-24 17:24:25 Downloading - Downloading input data...\n",
      "2024-06-24 17:24:55 Downloading - Downloading the training image.........\n",
      "2024-06-24 17:26:37 Training - Training image download completed. Training in progress.\n",
      "2024-06-24 17:26:37 Uploading - Uploading generated training model\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:29 INFO 139987154560832] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'true', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': ''}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:29 INFO 139987154560832] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2', 'force_dense': 'True', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:29 INFO 139987154560832] Final configuration: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '2', 'k': '10'}\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 WARNING 139987154560832] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Final configuration: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '2', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 WARNING 139987154560832] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 8 is a worker.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Create Store: local\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Setting up with params: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '2', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] number of center slices 1\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1719249991.1186738, \"EndTime\": 1719249991.1186972, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[2024-06-24 17:26:31.118] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 160000}\u001b[0m\n",
      "\u001b[34m[2024-06-24 17:26:31.166] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 46, \"num_examples\": 4, \"num_bytes\": 576000}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] processed a total of 18000 examples\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1719249991.1190233, \"EndTime\": 1719249991.1669362, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 23000.0, \"count\": 1, \"min\": 23000, \"max\": 23000}, \"Total Batches Seen\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Max Records Seen Between Resets\": {\"sum\": 18000.0, \"count\": 1, \"min\": 18000, \"max\": 18000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 18000.0, \"count\": 1, \"min\": 18000, \"max\": 18000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] #throughput_metric: host=algo-1, train throughput=374798.1830367116 records/second\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 WARNING 139987154560832] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] shrinking 100 centers into 10\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #0. Current mean square distance 25.211119\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #1. Current mean square distance 25.257729\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #2. Current mean square distance 24.108889\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #3. Current mean square distance 25.883657\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #4. Current mean square distance 22.865036\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #5. Current mean square distance 24.441793\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #6. Current mean square distance 26.839468\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #7. Current mean square distance 22.679510\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #8. Current mean square distance 22.621248\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] local kmeans attempt #9. Current mean square distance 24.011770\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] finished shrinking process. Mean Square Distance = 23\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] #quality_metric: host=algo-1, train msd <loss>=22.621248245239258\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] compute all data-center distances: inner product took: 23.0756%, (0.012079 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] predict compute msd took: 20.5764%, (0.010771 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] gradient: cluster size  took: 12.5519%, (0.006570 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] collect from kv store took: 11.9124%, (0.006236 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] splitting centers key-value pair took: 11.3709%, (0.005952 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] batch data loading with context took: 9.7430%, (0.005100 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] gradient: cluster center took: 4.3675%, (0.002286 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] update state and report convergance took: 2.5119%, (0.001315 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] gradient: one_hot took: 1.4990%, (0.000785 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] compute all data-center distances: point norm took: 1.3459%, (0.000705 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] compute all data-center distances: center norm took: 0.5953%, (0.000312 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] update set-up time took: 0.3188%, (0.000167 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] predict minus dist took: 0.1312%, (0.000069 secs)\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] TOTAL took: 0.05234527587890625\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1719249991.0960886, \"EndTime\": 1719249991.3913882, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 18.102169036865234, \"count\": 1, \"min\": 18.102169036865234, \"max\": 18.102169036865234}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 47.742366790771484, \"count\": 1, \"min\": 47.742366790771484, \"max\": 47.742366790771484}, \"_shrink.time\": {\"sum\": 222.6393222808838, \"count\": 1, \"min\": 222.6393222808838, \"max\": 222.6393222808838}, \"finalize.time\": {\"sum\": 223.88768196105957, \"count\": 1, \"min\": 223.88768196105957, \"max\": 223.88768196105957}, \"model.serialize.time\": {\"sum\": 0.18787384033203125, \"count\": 1, \"min\": 0.18787384033203125, \"max\": 0.18787384033203125}}}\u001b[0m\n",
      "\u001b[34m[06/24/2024 17:26:31 INFO 139987154560832] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1719249991.3914995, \"EndTime\": 1719249991.391718, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 1.6973018646240234, \"count\": 1, \"min\": 1.6973018646240234, \"max\": 1.6973018646240234}, \"totaltime\": {\"sum\": 339.5426273345947, \"count\": 1, \"min\": 339.5426273345947, \"max\": 339.5426273345947}}}\u001b[0m\n",
      "\n",
      "2024-06-24 17:26:49 Completed - Training job completed\n",
      "Training seconds: 144\n",
      "Billable seconds: 144\n",
      "CPU times: user 624 ms, sys: 46.4 ms, total: 671 ms\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans.fit(kmeans.record_set(data_train), job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Viewing the results\n",
    "In this step we will take a look at the model artifact SageMaker created for us and stored onto S3. We have to do a few special things to see the latitude and longitude for our 10 clusters (and the center points of those clusters)\n",
    "\n",
    "[See the documentation of deserilization here](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization)\n",
    "\n",
    "At this point we need to \"deserilize\" the model artifact. Here we are going to open and review them in our notebook instance. We can unzip the model artifact which will contain model_algo-1. This is just a serialized Apache MXNet object. From here we can load that serialized object into a numpy.ndarray and then extract the clustered centroids from the numpy.ndarray.\n",
    "\n",
    "After we extract the results into a DataFrame of latitudes and longitudes, we can create a CSV with that data, load it onto S3 and then visualize it with QuickSight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_algo-1\n",
      "Archive:  model_algo-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of model_algo-1 or\n",
      "        model_algo-1.zip, and cannot find model_algo-1.ZIP, period.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "model_key = 'model-artifacts/' + job_name + '/output/model.tar.gz'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).download_file(model_key, 'model.tar.gz')\n",
    "os.system('tar -zxvf model.tar.gz')\n",
    "os.system('unzip model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mxnet) (1.22.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mxnet) (2.31.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (2024.2.2)\n",
      "Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "Kmeans_model_params = mx.ndarray.load('model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [[  35.336853   -98.741165 ]\n",
       "  [  49.87383     -3.7976685]\n",
       "  [  -4.999058   112.205666 ]\n",
       "  [  34.935562  -118.7064   ]\n",
       "  [  31.74403    -82.60426  ]\n",
       "  [  41.340214   -75.29815  ]\n",
       "  [  46.147224  -119.76389  ]\n",
       "  [  62.17071   -148.79977  ]\n",
       "  [  40.93326    -87.650185 ]\n",
       "  [   1.0341873  -67.69947  ]]\n",
       " <NDArray 10x2 @cpu(0)>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kmeans_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.336853</td>\n",
       "      <td>-98.741165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.873829</td>\n",
       "      <td>-3.797668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.999058</td>\n",
       "      <td>112.205666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.935562</td>\n",
       "      <td>-118.706398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.744030</td>\n",
       "      <td>-82.604263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.340214</td>\n",
       "      <td>-75.298149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>46.147224</td>\n",
       "      <td>-119.763893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62.170712</td>\n",
       "      <td>-148.799774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.933262</td>\n",
       "      <td>-87.650185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.034187</td>\n",
       "      <td>-67.699471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude\n",
       "0  35.336853  -98.741165\n",
       "1  49.873829   -3.797668\n",
       "2  -4.999058  112.205666\n",
       "3  34.935562 -118.706398\n",
       "4  31.744030  -82.604263\n",
       "5  41.340214  -75.298149\n",
       "6  46.147224 -119.763893\n",
       "7  62.170712 -148.799774\n",
       "8  40.933262  -87.650185\n",
       "9   1.034187  -67.699471"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_centroids_kmeans = pd.DataFrame(Kmeans_model_params[0].asnumpy())\n",
    "cluster_centroids_kmeans.columns=df_geo.columns\n",
    "cluster_centroids_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and upload this dataset onto S3 and view within QuickSight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5XBKAJEHDKMH74DN',\n",
       "  'HostId': '9MAxQv+sh+EsK0mOnzJ7JK5Im8MUKW7QfsCUt+R6JcfKSFQih75kqdiRwVnuVnBDKoSe6dmOGw4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '9MAxQv+sh+EsK0mOnzJ7JK5Im8MUKW7QfsCUt+R6JcfKSFQih75kqdiRwVnuVnBDKoSe6dmOGw4=',\n",
       "   'x-amz-request-id': '5XBKAJEHDKMH74DN',\n",
       "   'date': 'Mon, 24 Jun 2024 19:01:45 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"fa689843dacbd5531082ec596ec96765\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"fa689843dacbd5531082ec596ec96765\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "cluster_centroids_kmeans.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'results/ten_locations_kmeans.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-1.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (1.22.4)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (2.2.1)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Downloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyogrio>=0.7.2->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->geopandas) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Downloading geopandas-1.0.0-py3-none-any.whl (323 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.4/323.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyogrio-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely, pyproj, pyogrio, geopandas\n",
      "Successfully installed geopandas-1.0.0 pyogrio-0.9.0 pyproj-3.6.1 shapely-2.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import geopandas and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The geopandas.dataset has been deprecated and was removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting world map data from geo pandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m worldmap \u001b[38;5;241m=\u001b[39m geopandas\u001b[38;5;241m.\u001b[39mread_file(\u001b[43mgeopandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnaturalearth_lowres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Creating axes and plotting world map\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/geopandas/datasets/__init__.py:18\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     12\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe geopandas.dataset has been deprecated and was removed in GeoPandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0. You can get the original \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m data from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mne_message\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnatural\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdataset\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mnybb_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m _prev_available:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(error_msg)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe geopandas.dataset has been deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas removed in GeoPandas 1.0. New sample datasets are now available \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the geodatasets package (https://geodatasets.readthedocs.io/en/latest/)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The geopandas.dataset has been deprecated and was removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/."
     ]
    }
   ],
   "source": [
    "# Getting world map data from geo pandas\n",
    "worldmap = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n",
    "\n",
    "# Creating axes and plotting world map\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "worldmap.plot(color=\"lightgrey\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
