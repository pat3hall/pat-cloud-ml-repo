------------------------------------------------------

AWS Certified Machine Learning - Specialty (MLS-C01)

------------------------------------------------------
Chapter 1 Introduction
------------------------------------------------------
1.1 Course Introduction

  Course Assumes you have:
    - 2+ years hands-on experience
    - prior experience with ML concepts and algorithm
    - At least one AWS associate certification

------------------------------------------------------
1.2 About the Exam

  Exam Prerequisties - AWS Recommended
    - At least one Associate certification or equivalent experience
    - 2+ years hands-on experience
    - Some experience with general machine learning topics and algorithms

  ML Speciality Exam
    - cost $300
    - 180 min, ~65 questions total (potentially plus beta test questions)
    - multiple choice and multiple-reponse (pick 3 or 5)
    - No partial credit for questions
    - score between 100 & 1000 with a min passing score of 750
    - scaled scoring models are used (some questions have more weight than others)
    - overall exam domains are at fixed percentages

  Exam Blueprint
    Domain 1: Data Engineering                                  20%
    Domain 2: Exploratory Data Analysis                         24%
    Domain 3: Modeling                                          36%
    Domain 4: Machine Learning Implementation and Operations    20%
    TOTAL                                                       100%

------------------------------------------------------
1.3 About this Course

   Specialty focus key topics
     - ML course focus is on SageMaker, Kinesis, & Glue

   Course Prerequisities
      - AWS experience
      - Need to Know Jupyter Notebook
         - Know how to start up a Notebook and run cells within a notebook
         - python knowledge
      - Introductory level understanding of ML concepts

------------------------------------------------------
1.4 A Note About Demo Lessons
------------------------------------------------------

Chapter 2 Data Collection

------------------------------------------------------
2.1 Introduction

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      Train Model --->  Evaluate Model --->
    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

  Goal
    - Understand the problem at hand
    - Understand the parts of input data
    - map our data into AWS

------------------------------------------------------
2.2 [Data Collection] Concepts

  Before you Begin:
    - What type of generalization are we seeking? [helps determine type of data needed]
    - Do we really need machine learning?
    - How will my ML generation be consumed?
      - can results be returned in real time or can a batch process be used? via an API?
    - what do we have to work with?
      - what kind of data is available?
    - how can I tell if the generalization is working?
      - is the ML processing working?


  Good Data
    Traits of good data           Traits of bad data               Why
    --------------------------    -----------------------------    ---------------------------------
    Large datasets                Small Datasets (less than 100    Generally, more data means better
                                  rows)                            model training

    Precise attribute types,      Useless attributes, not needed   Models need to train on important
      feature rich                  for solving problem at hand     features

    Complete fields, no missing   Missing values, null fields      Models can skew results when
      values                        for solving problem at hand     data points are missing

    Values are consistent         Inconsistent values              Models like clean and consistent
                                                                    data

    Solid distribution of         Lots of positive outcomes,       Models cannot learn with skewed
      outcomes                      few negative outcomes           distributions of outcomes

    Fair sampling                 Biased sampling                  Models will skew results with
                                                                    biased data

  How much data do you need?
    - you should have at least 10 times as many data points as the total number of features

  Where does data come from?
    - many differenc places and can include static data and streaming data

  Build a data repository
    - no matter how you get your data, you are building a data repository
    - find a way to congregate the data into a single data repository

------------------------------------------------------
2.3 General Data Terminology

  Datasets
   - collection of data
    dataset = input data = training/testing data

  Datasets parts
     column = attribute = feature
     row = obseration = sample = data point

  Datasets formats include:
     - JSON, CSV, tables
     - images, video, or audio

  Structured Data
    - has a defined schema
    - a schema is the information needed to interpret the data, including attribute names,
      and their assigned data types
    - examples include SQL databases, rational databases data stored in tables

  Unstructured Data
    - no defined schema or structured properties
    - makes up majority of data collected
    - examples: PDF, images, video, audio, tweets, social media streams

  Semi-Structured Data
    - unstructured for relational data, but has some organizational structure
    - examples: CSV, JSON, or XML
    - DB examples: no-SQL data, non-relational data, most public datasets

   Where is relational data stored:
     - in relational databases: Auora, PostgreSQL, MySQL, MariaDB, Oracle, SQL Server

   Data Warehouse
     - used to store petabytes and exabytes of data
     - collect data from many difference sources and congregate them together
     - normally, the input data is preprocessed before storing in the data warehouse

   Data Lakes
    - store massive amount of unstructured data in a repository
    - traditionally, no preprocess of data before storing
    - sources may include ML, Analytics, On-premises Data movement, Real-time data movememt
    - may be used to store historical data or data we are not yet sure what to do with

  Data Repository Summary:

    Databases
      - traditional relational databases
      - transactional
      - strict defined schema
    Data Warehouse
      - prepocessing performed on import (schema-on-write)
      - data is classified/stored with user in mind
      - ready to uses with BI tools (query and analysis)
    Data Lakes
      - preprocessing done on export (schema-on-read)
      - many different sources and formats
      - raw data may not be ready for use

------------------------------------------------------
2.4 Machine Learning Data Terminology

Resources:

  MNIST Database
    http://yann.lecun.com/exdb/mnist/

  ImageNet
    https://image-net.org/
     - ImageNet is an image database organized according to the WordNet hierarchy (currently only
       the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images
     - effort to provide researchers around the world with image data for training large-scale object
       recognition models.

  Labeled and Unlabeled Data
    Label Data
      - data where we already know what the target attribute is
      - data with the correctly predicted value (target) assigned to it
      - example: email dataset with spam / not spam labels
      - email feature example: from domain, body content,
      - example: planet image data with associate tag/labels: planet and sum labels
      - supervised learning is done with labeled data
    UnLabel Data
      - data that has been collected with no target/label attribute
      - examples: audio streams, log files, twitter streams
      - unsupervised learning is done with unlabeled data


  Data Features Types
    Categorical features
      - values that are associated with a group [fits into a finite group of values] or category
      - Qualitative (memory trick - has an 'l' like in Categorical)
      - Discrete
      - examples: spam/not spam; dog breed
    Continuous features
      - values that are expressed as measurable number
      - Quantitative (memory trick - has an 'n' like in Continuous)
      - if you can place the attribute value on a number line, the attribute is a continuous feature
      - examples: home price
      - can be infinite (potentially be as large or small depending on how it is defined)

  Data Types

    Text Data (Corpus Data)
      - datasets collected from text
      - Uses in Natural Language Processing (NLP), speech recognition, text to speech, and more
      - examples: Text Paper, text dialogue, newspaper clipping

    Ground Truth Data
      - refers to factual data that has been observed or measured
      - this data has successfully been labeled and can be trusted as 'truth' data

    Image Data
      - refers to dataset sets of tagged images
      - free image datasets include MNIST (labeld handwritten chars/numbers)
           and ImageNet (hierarchy of image classifications)

    Time Series Data
      - refers to datasets that capture changes over time
      - example: stock market prices

  Amazon Sagemaker Ground Truth Service
    - tool that helps build ground truth datasets by allowing different type os tagging/labeling processes
    - easily create labeled data


  ML Dataset Examples:

    Dataset Type            Example Cases                                      Format
    --------------------    ----------------------------------------------     ------------
    Image Data              Facial Recognition, action recognition, object     images, videos
                            detection, handwriting and char recognition

    Text Data               Reviews, new articles, messages, twitter and       text, csv
                              tweets dialogs

    Sound Data              Speech, music, other sounds                        mp3, text*
                                                                               * represented as text

    Signal Data             Electrical signals, motion-tracking, chemical      text
                               compounds

    Physical Data           high-energy physics, systems, astronomy, earth     text
                               science

    Biological Data         Human, animal, plants, microbes                    text


    Multi-variable Data     financial, weather, census, transit,               csv, text
                               internet, games

------------------------------------------------------
2.5 AWS Data Stores


  Simple Storage Service (s3)
    - unlimited data storage that provides object based storage fro any type of data
    - go to place for storing ML data
    - core ML services integrate with S3 including outputing the ML results to S3

  What is S3:
    - files can be from 0 bytes to 5 TB
    - unlimited storage
    - files/objects aret stored in buckets
    - S3 is a universal namespace (names must be unique globally)
    - example S3 endpoint: https://s3-us-east-1.amazonaws.com/machinelearningdata
                           https://<AWS region>.amazonaws.com/<bucketName>

         bucket endpoints formats:
           path style (supports ends in Sept 2020)
              https://<AWS region>.amazonaws.com/<bucketName>
           virtual hosted style:
              https://<bucketName>.s3.amazonaws.com

  Get Data into S3
    Upload via Console
      - bucket 'Upload' button
    Upload various SDKs
      - many programming language supported SDKs
    Use Command line interface

  Data Stores
     - S3, RDS, DynamoDB, RedShift, Timestream, Document DB

  Relational Database Service (RDS)
    - RDS Engines: Aurora, PostgreSQL, MySQL, MariaDB, Oracle, SQL Server
    - RDS is a fully managed relational  database
    - for application datastores that need transactional style databases

  Dynamo DB
    - noSQL datastore [non-relational database] used to store key value pairs
    - Now this service is best for schema-less data and unstructured or semi-structured data.
       table ->     {
       table name ->  "Characters": [
       item ------->    {"ID": 1,
            |             "Name": "Luke",            <---- key & value pairs - attribute
            |             "Evil": 0,
            |             "Affilication": "Rebel",
            |             "Weapon": "lightsaber"
            ------->    },
                        {"ID": 2,
             key --->     "Name": "Jabba the Hutt",   <--- value
                          "Evil": 1,
                          "Affilication": null,
                        }
                      ]
                    }

  RedShift
    - check out: Scott's CSA Pro Redshift lecture
    - is a fully managed, clustered petabyte data warehousing solution
    - congregates data from other data sources like S3, Dynamo DB, and more, and it allows you to store mass amounts
      of relational or non-relational semi-structured or structured data to create a data warehousing solution
    - And once your data is in Redshift, you can use SQL client tools, or business intelligence tools, or other
      analytics tools to query that data and find out important information about your data warehouse.

  RedShift spectrum:
    - allows you to query your Redshift cluster that has sources of S3 data.
    - it allows you to query your S3 data.
    - You can then use tools like QuickSight (BI tool) to create charts and graphs to actually visualize that data.

  Amazon Timestream
    - likely not on exam
    - is a fully-managed time series database service
    - allows you to plug in business intelligence tools and run SQL Lite queries on your time series data.
    time series data
      - includes data that's from things like IoT devices, IT systems, smart, industrial machines, things like server logs,
        or any other time series data. So think about stock market prices.

  Document DB
    - likely not on exam
    - place to migrate your MongoDB data
    - provides better performance and scalability than your traditional Mongo DB instances that are running on
      EC2, on premises, etc.


  NOTE: Need to review Solution Architect or Developer sessions for these DB services, etc

------------------------------------------------------
2.6 AWS Migration Tools
   -> missing from current Material but included in the slides


  Migrate workloads from AWS Data Pipeline
  https://aws.amazon.com/blogs/big-data/migrate-workloads-from-aws-data-pipeline/
    After careful consideration, we have made the decision to close new customer access to AWS Data Pipeline, effective July 25, 2024.
    AWS Data Pipeline existing customers can continue to use the service as normal.
    AWS continues to invest in security, availability, and performance improvements for AWS Data Pipeline, but we do not plan
    to introduce new features

   Migration tools

     Data Pipeline
       DynamoDB --->
       RDS      ---> Data Pipeline   ---> S3
       Redshift --->

     Data Pipeline Activity objects:
       CopyActivity
       EmrActivity
       HadoopActivity
       HiveActivity
       HiveCopyActivity
       PigActivity
       RedshiftCopyActivity
       ShellCommandActivity
       SqlActivity


     AWS Data Pipeline supports the following types of activities:
       https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-activities.html
       CopyActivity
         - Copies data from one location to another.
       EmrActivity
         - Runs an Amazon EMR cluster.
       HiveActivity
         - Runs a Hive query on an Amazon EMR cluster.
       HiveCopyActivity
         - Runs a Hive query on an Amazon EMR cluster with support for advanced data filtering and support for S3DataNode and DynamoDBDataNode.
       PigActivity
         - Runs a Pig script on an Amazon EMR cluster.
       RedshiftCopyActivity
         - Copies data to and from Amazon Redshift tables.
       ShellCommandActivity
         - Runs a custom UNIX/Linux shell command as an activity.
       SqlActivity
         - Runs a SQL query on a database.


     Data Pipeline Creation Steps:
        AWS console  -> Data Pipeline -> Create Pipeline

     Database Migration Services (DMS):
         - DMS support heterogeneous and homogenous migrations
       For:
         On-premise Database
         Database on EC2      ----> Data Migration Service   ---> S3
         Database on RDS
       To Create from Console:
         AWS Console ---> Database Migration Service ---> Source Endpoint
                                                     ---> Target Endpoint

     AWS Glue
       - Sources:
         S3, DynamoDB, RDS, RDS, Redshift, Database on EC2
       - Destinations:
         Athena, EMR, S3, Redshift
       - AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate
         data from multiple sources for analytics, machine learning (ML), and application development.
       - You can discover and connect to over 70 diverse data sources, manage your data in a centralized data catalog,
         and visually create, run, and monitor ETL pipelines to load data into your data lakes
       - With AWS Glue, you store metadata in the AWS Glue Data Catalog. You use this metadata to orchestrate ETL jobs that
         transform data sources and load your data warehouse or data lake.
        steps to setup:
          1. Populate the AWS Glue Data Catalog with table definitions.
          2. Define a job that describes the transformation of data from source to target.
            - Choose a table from the AWS Glue Data Catalog to be the source of the job.
              - Your job uses this table definition to access your data source and interpret the format of your data.
            - Choose a table or location from the AWS Glue Data Catalog to be the target of the job.
               - Your job uses this information to access your data store.
          3. Run your job to transform your data.
             - can run your job on demand, or start it based on a one of these trigger types:
                  cron schedule, event-based, or on demand
          4. Monitor your scheduled crawlers and triggered jobs.
        Classifiers:
          - Classifiers are triggered during a crawl task. A classifier checks whether a given file is in a format it can handle.
          - A classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema.
          - AWS Glue provides a set of built-in classifiers, but you can also create custom classifiers.
          - AWS Glue provides built-in classifiers for various formats, including JSON, CSV, web logs, and many database systems.
          - You use classifiers when you crawl a data store to define metadata tables in the AWS Glue Data Catalog
        Crawler:
          - You can use a crawler to populate the AWS Glue Data Catalog with tables.
          - A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates
            one or more tables in your Data Catalog.
         ETL jobs:
           - Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources
             and targets.
           - The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables.

        AWS Console ---> AWS Glue  ---> Data Cataloog --> Databases  ---> Tables
                                                                     --->  Connections

        AWS Console ---> AWS Glue  ---> Data Cataloog --> Crawlers  ---> Classifiers
           - includes Bul



  Choosing the Right Approach:

     Datasource                      Migration Tool               Why
     --------------------            -------------------          ------------------------------------
     PostgreSQL RDS Instance with    AWS Data Pipeline            Specify SqlActivity query and places the
       Training data                                                 output into S3

     Unstructured log files in S3    AWS Glue                     Create custom classifier and output results
                                                                      into S3

     Clustered Redshift Data         AWS Data Pipeline            Use the 'unload' command to return results of
                                                                     a query to CSV file in S3
                                       AWS Glue                   Create Data catalog describing data and
                                                                     load it into S3

     On-premise MySQL instance       AWS Database Migration       DMS can load data in CSV format onto S3
       with Training data               Service

     PostgreSQL RDS                  AWS Data Pipeline            Specify

------------------------------------------------------
2.7 AWS Helper Tools

  Resources:

     Using AWS Glue Data Catalog as metastore for Hive
       https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html

  Elastic Map Reduced (EMR)
    - EMR is a fully managed Hadoop cluster eco-system that runs on multiple EC2 instances.
    - EMR allows pick and choose different frameworks that you want to include within the cluster. This allows you to
      run distributed workloads over many EC2 instance
    - some of the supported frameworks include (most open source):
        Spark Ml and Mlib:
          - ETL and ML Library
        Presto
          - SQL Query Engine
        Mahout
          - ML framework
        Hive
          - ETL Service
        Juptyer Notebooks
          - code sharing
        TensorFlow
          - ML framework
        Hadoop Distributed File System (HDFS)
          - Persistent Datastore
        MXNet:
          - ML Framework
    - EMR allows you to assemble all or some of these frameworks together to do different things.
    - could use EMR to store mass amounts of files in a distributed file system to use as our input or training data.
    - if our data is already within an EMR cluster, we can use some of the frameworks and services to migrate the data into S3

  Athena:
   - a serverless platform that allows you to run sql queries on your S3 data
   - you can set up a table within our data catalog within AWS Glue and use Athena to query our S3 data.
   - within the console, we can select tables that are defined in our AWS Glue data catalog, write a query, and
     then output the results onto S3 or output the results

   Redshift Spectrum vs Athena S3 queries:
     Redshift Spectrum S3 Queries
       - Query S3 Data
       - must have a Redshift cluster spun up.
       - Made for existing Redshift customers
     Athena S3 Queries
       - Query S3 Data
       - No need for Redshift cluster
       - New customers quickly want to query S3 data
       - just need a table defined in your AWS Glue Catalog

   Get Data into S3:
     Data ---> S3 ---> SageMaker

------------------------------------------------------
2.8 Exam Tips

  Resources:
    Whitepaper - Sizing Cloud Data Warehouses
      download a pdf

    Whitepaper - Big Data Analytics Options on AWS
      download a pdf

    Whitepaper - Data Warehousing on AWS
      download a pdf

    Whitepaper - Migrating to Apache HBase on Amazon S3 on Amazon EMR
      download a pdf

    Whitepaper - AWS Database Migration Service Best Practices
      download a pdf

    AWS re:Invent 2018: Modern Cloud Data Warehousing (video)
      https://www.youtube.com/watch?v=QZ4LAZCbsrQ

    AWS re:Invent 2018: Effective Data Lakes: Challenges and Design Patterns (video)
      https://www.youtube.com/watch?v=v5lkNHib7bw

    Build a Data Lake Foundation with AWS Glue and S3
      https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/


  Data Collection:

    Before you Begin:
      - Understand that before we gather input data we must formulate the problem we are trying to solve
      - know how we can measure success and what are actual goals are
      - Determine if Machine Learning is even necessary
      - Understand what type of data is available ot help solve problem
        - open source data, in-house data, etc.
     Good Data:
       - understand what makes up 'good' data and why having 'good' data is important
       - understanding what 'good' and 'bad' data looks like

  Good Data
    Traits of good data           Traits of bad data               Why
    --------------------------    -----------------------------    ---------------------------------
    Large datasets                Small Datasets (less than 100    Generally, more data means better
                                  rows)                            model training

    Precise attribute types,      Useless attributes, not needed   Models need to train on important
      feature rich                  for solving problem at hand     features

    Complete fields, no missing   Missing values, null fields      Models can skew results when
      values                        for solving problem at hand     data points are missing

    Values are consistent         Inconsistent values              Models like clean and consistent
                                                                    data

    Solid distribution of         Lots of positive outcomes,       Models cannot learn with skewed
      outcomes                      few negative outcomes           distributions of outcomes

    Fair sampling                 Biased sampling                  Models will skew results with
                                                                    biased data


   Data Terminology

     - know how to identify columns/attributes and rows/observations within a dataset
        Datasets
         - collection of data
          dataset = input data = training/testing data

        Datasets parts
           column = attribute = feature
           row = obseration = sample = data point

        Datasets formats include:
           - JSON, CSV, tables
           - images, video, or audio

     - know the difference in structured, semi-structured, and unstructured data
        Structured Data
          - has a defined schema
          - a schema is the information needed to interpret the data, including attribute names,
            and their assigned data types
          - examples include SQL databases, rational databases data stored in tables

        Unstructured Data
          - no defined schema or structured properties
          - makes up majority of data collected
          - examples: PDF, images, video, audio, tweets, social media streams

        Semi-Structured Data
          - unstructured for relational data, but has some organizational structure
          - examples: CSV, JSON, or XML
          - DB examples: no-SQL data, non-relational data, most public datasets

     - know the different types of data repositories (databases, data warehouses, data lakes)
        Databases
          - traditional relational databases
          - transactional
          - strict defined schema
        Data Warehouse
          - prepocessing performed on import (schema-on-write)
          - data is classified/stored with user in mind
          - ready to uses with BI tools (query and analysis)
        Data Lakes
          - preprocessing done on export (schema-on-read)
          - many different sources and formats
          - raw data may not be ready for use

     - understand the differences between labeled data and unlabeled data

     - be able to recognize categorical features (qualitative) and continuous features (quantitative)
        Categorical features
          - values that are associated with a group [fits into a finite group of values] or category
          - Qualitative (memory trick - has an 'l' like in Categorical)
          - Discrete
          - examples: spam/not spam; dog breed
        Continuous features
          - values that are expressed as measurable number
          - Quantitative (memory trick - has an 'n' like in Continuous)
          - if you can place the attribute value on a number line, the attribute is a continuous feature
          - examples: home price
          - can be infinite (potentially be as large or small depending on how it is defined)

     - know terms like corpus (text), ground truth, time series, and image data
        Text Data (Corpus Data)
          - datasets collected from text
          - Uses in Natural Language Processing (NLP), speech recognition, text to speech, and more
          - examples: Text Paper, text dialogue, newspaper clipping
        Ground Truth Data
          - refers to factual data that has been observed or measured
          - this data has successfully been labeled and can be trusted as 'truth' data
        Image Data
          - refers to dataset sets of tagged images
          - free image datasets include MNIST (labeld handwritten chars/numbers)
               and ImageNet (hierarchy of image classifications)
        Time Series Data
          - refers to datasets that capture changes over time
          - example: stock market prices

     - know the difference between 'ground truth data' and AWS Ground Truth Service
        Amazon Sagemaker Ground Truth Service
          - tool that helps build ground truth datasets by allowing different type os tagging/labeling processes
          - easily create labeled data


  AWS Data Stores Tools:
    - know the different AWS Services where data can be stored
      Data Stores
         - S3, RDS, DynamoDB, RedShift, Timestream, Document DB

    - know what types of data is stored in different AWS services

  AWS Migration Tools:
    - know the different AWS services we can use to migrate data
    - now when to uses one migration tool over another

     Datasource                      Migration Tool               Why
     --------------------            -------------------          ------------------------------------
     PostgreSQL RDS Instance with    AWS Data Pipeline            Specify SqlActivity query and places the
       Training data                                                 output into S3

     Unstructured log files in S3    AWS Glue                     Create custom classifier and output results
                                                                      into S3

     Clustered Redshift Data         AWS Data Pipeline            Use the 'unload' command to return results of
                                                                     a query to CSV file in S3
                                       AWS Glue                   Create Data catalog describing data and
                                                                     load it into S3

     On-premise MySQL instance       AWS Database Migration       DMS can load data in CSV format onto S3
       with Training data               Service

     PostgreSQL RDS                  AWS Data Pipeline            Specify



  AWS Helper Tools:
    - know what EMR is and how we could use it as a migration tool
      Elastic Map Reduced (EMR)
        - a managed service that lets you process and analyze large datasets using the latest versions of big data
           processing frameworks such as Apache Hadoop, Spark, HBase, and Presto on fully customizable clusters.
        - EMR is a fully managed Apache Hadoop cluster eco-system that runs on multiple EC2 instances.
        - EMR allows pick and choose different frameworks that you want to include within the cluster. This allows you to
          run distributed workloads over many EC2 instance
        - some of the supported frameworks include (most open source):
            Spark Ml and Mlib:
              - ETL and ML Library
              - a multi-language engine processing framework and programming model for executing data engineering, data science, and
                machine learning on single-node machines or clusters.
            Presto
              - SQL Query Engine
              - An open source, distributed SQL query engine optimized for low-latency, ad-hoc analysis of data.
              - supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions.
              - can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.
            HBase
              - non-relational database  using S3 or HDFS data
              - An open source, non-relational, versioned database that runs on top of Amazon S3 (using EMRFS) or the
                Hadoop Distributed File System (HDFS).
              - HBase is a massively scalable, distributed big data store built for random, strictly consistent, real-time
                access for tables with billions of rows and millions of columns.
            Mahout
              - ML framework
            Hive
              - data warehouse software facilitates reading, writing, and managing largedatasets residing in distributed storage using SQL.
              - ETL Service
              - Allows users to leverage Hadoop MapReduce using a SQL interface, enabling analytics at a massive scale,
                in addition to distributed and fault-tolerant data warehousing.
            Juptyer Notebooks
              - code sharing
            TensorFlow
              - ML framework
            Hadoop Distributed File System (HDFS)
              - Persistent Datastore
            MXNet:
              - ML Framework
        - EMR allows you to assemble all or some of these frameworks together to do different things.
        - could use EMR to store mass amounts of files in a distributed file system to use as our input or training data.
        - if our data is already within an EMR cluster, we can use some of the frameworks and services to migrate the data into S3

    - know what Amazon Athena is how it differs from Redshift Spectrum
      Athena:
       - a serverless platform that allows you to run SQL queries on your S3 data
       - you can set up a table within our data catalog within AWS Glue and use Athena to query our S3 data.
       - within the console, we can select tables that are defined in our AWS Glue data catalog, write a query, and
         then output the results onto S3 or output the results

       Redshift Spectrum vs Athena S3 queries:
         Redshift Spectrum S3 Queries
           - Query S3 Data
           - must have a Redshift cluster spun up.
           - Made for existing Redshift customers
         Athena S3 Queries
           - Query S3 Data
           - No need for Redshift cluster
           - New customers quickly want to query S3 data
           - just need a table defined in your AWS Glue Catalog

------------------------------------------------------
2.9 Quiz AWS Certified Machine Learning - Specialist 2020 - Data Collection Quiz

Question 1 info:

 Amazon's EMR allows you to set up a distributed Hadoop cluster to process, transform, and analyze large amounts of data.
 Apache Spark is a processing framework and programming model that helps you do machine learning, stream processing, or graph
 analytics using Amazon EMR clusters.

Question 4:

  You are a ML specialist working with data that is stored in a distributed EMR cluster on AWS. Currently, your machine
  learning applications are compatible with the Apache Hive Metastore tables on EMR. You have been tasked with configuring
  Hive to use the AWS Glue Data Catalog as its metastore. Before you can do this you need to transfer the Apache Hive
  metastore tables into an AWS Glue Data Catalog. What two answer option workflows can accomplish the requirements with
  the LEAST amount of effort?

  Choose 2

   1 Create DMS endpoints for both the input Apache Hive Metastore and the output data store Amazon EFS, run a DMS
   migration to transfer the data, then create a crawler that creates an AWS Glue Data Catalog.

   2 Create a Data Pipeline job that reads from your Apache Hive Metastore, exports the data to an intermediate format in
   Amazon S3, and then imports that data into the AWS Glue Data Catalog.

   3 Create a second EMR cluster that runs an Apache Spark script to copy the Hive metastore tables from the original
   EMR cluster into AWS Glue.

   4 Run a Hive script on EMR that reads from your Apache Hive Metastore, exports the data to an intermediate format
   in Amazon S3, and then imports that data into the AWS Glue Data Catalog.

   5 Setup your Apache Hive application with JDBC driver connections, then create a crawler that crawls the Apache Hive
   Metastore using the JDBC connection and creates an AWS Glue Data Catalog.

Sorry! (answer 2)

   Data Pipeline is not required. We can simply run a Hive script to query tables and output that data in CSV (or
   other formats) into S3. Once that data is on S3, we can crawl it to create a Data Catalog of the Hive Metastore or
   import the data directly from S3.

Correct Answer (answers 4 & 5)

   We can simply run a Hive script to query tables and output that data in CSV (or other formats) into S3. Once that
   data is on S3, we can crawl it to create a Data Catalog of the Hive Metastore or import the data directly from S3.

   Apache Hive supports JDBC connections that easily can be used with a crawler to create an AWS Glue Data Catalog.
   The benefit of using Data Catalog (over Hive Metastore) is because it provides a unified metadata repository across
   a variety of data sources and data formats, integrating with Amazon EMR as well as Amazon RDS, Amazon Redshift, Redshift
   Spectrum, Athena, and any application compatible with the Apache Hive metastore. We can simply run a Hive script to
   query tables and output that data in CSV (or other formats) into S3. Once that data is on S3, we can crawl it to create
   a Data Catalog of the Hive Metastore or import the data directly from S3.


Question 5 info:

  We need a large, robust, feature-rich dataset. In general, having AT LEAST 10 times as many observations as features
  is a good place to start. So for example, we have a dataset with the following features: id, date, full review, full
  review summary, and a binary safe/unsafe tag. Since id is just an identifier, we have 4 features (date, full review,
  full review summary, and a binary safe/unsafe tag). This means we need AT LEAST 40 rows/observations.


Question 6: info:

  AWS Glue makes it super simple to transform data from one format to another. You can simply create a job that takes
  in data defined within the Data Catalog and outputs in any of the following formats: avro, csv, ion, grokLog, json,
  orc, parquet, glueparquet, or xml.

Question 11: info:

  Because PDF file contents typically include forms (key-value pairs), tables, and free text, the JSON file must
  include nested key-value pairs to represent the PDF file structure and store the extracted data. PDF files are
  unstructured or semi-structured data, which means they don't have a fixed schema. This means that it can be challenging
  to store PDF file contents in a traditional SQL database. However, a NoSQL database is ideal for storing PDF file
  contents because it doesn't require a predefined schema. After PDF file contents are extracted and post-processed,
  you can store them as one record for each PDF file in an Amazon DynamoDB table. AWS Documentation: Data storage phase.


Question 1 (retry)

You have been tasked with converting multiple JSON files within a S3 bucket to Apache Parquet format. Which AWS service can
you use to achieve this with the LEAST amount of effort?

 Choices:
   1. Create a Lambda function that reads all of the objects in the S3 bucket. Loop through each of the objects and convert
   from JSON to Apache Parquet. Once the conversion is complete, output the newly formatted files into S3.

   2. Create an EMR cluster to run an Apache Spark job that processes the data as Apache Parquet. Output the newly formatted files into S3.

   3. Create an AWS Glue job to convert the S3 objects from JSON to Apache Parquet. Output the newly formatted files into S3. <-- correct

   4. Create a Data Pipeline job that reads from your S3 bucket and sends the data to EMR. In EMR, create an Apache Spark job
      to process the data as Apache Parquet and output the newly formatted files into S3.

Sorry!

  choice 1:
   - While you can convert JSON to Parquet using a Lambda function, it does take a lot of effort to write the custom code
     needed to perform the conversion using one of the programming languages supported by AWS Lambda. AWS Glue is a better
     solution because it requires the least amount of effort.

Correct Answer

   - AWS Glue makes it super simple to transform data from one format to another. You can simply create a job that takes
     in data defined within the Data Catalog and outputs in any of the following formats: avro, csv, ion, grokLog, json,
     orc, parquet, glueparquet, or xml.

Question 4 (retry)

An organization needs to store a mass amount of data in AWS. The data has a key-value access pattern, developers need
to run complex SQL queries and transactions, and the data has a fixed schema. Which type of data store meets all of their needs?

 choices:
   1. Athena
   2. DynamoDB
   3. RDS        <--- correct answer
   4. S3

Sorry!
Correct Answer

"Amazon RDS handles all these requirements. Transactional and SQL queries are the important terms here. Although RDS
 is not typically thought of as optimized for key-value based access, using a schema with a primary key can solve this.
 S3 has no fixed schema. Although Amazon DynamoDB provides key-value access and consistent reads, it does not support
 complex SQL based queries. Simple SQL queries are supported for DynamoDB via PartiQL. Finally, Athena is used to query
 data on S3, so Athena is a query tool and not a data store.


Question 4: retry 2

Question 4

You are trying to set up a crawler within AWS Glue that crawls your input data in S3. For some reason after the crawler finishes executing, it cannot determine the schema from your data and no tables are created within your AWS Glue Data Catalog. What is the reason for these results?

The bucket path for the input data store in S3 is specified incorrectly.

The checkbox for 'Do not create tables' was checked when setting up the crawler in AWS Glue.

The crawler does not have correct IAM permissions to access the input data in the S3 bucket.

AWS Glue built-in classifiers could not find the input data format. You need to create a custom classifier.
Good work!

AWS Glue provides built-in classifiers for various formats, including JSON, CSV, web logs, and many database systems. If AWS Glue cannot determine the format of your input data, you will need to set up a custom classifier that helps AWS Glue crawler determine the schema of your input data.

Question 5: retry 2

You are a ML specialist within a large organization who needs to run SQL queries and analytics on thousands of Apache logs files stored in S3. Which set of tools can help you achieve this with the LEAST amount of effort?

Redshift and Redshift Spectrum

Data Pipeline and RDS

Data Pipeline and Athena

AWS Glue Data Catalog and Athena
Good work!

Using Redshift/Redshift Spectrum and Data Pipeline/RDS could work, but require much more effort in setting up and provisioning resources. Using AWS Glue you can use a crawler to crawl the logs files in S3. This will create structured tables within your AWS Glue database. These tables can then be queried using Athena. This solution requires the least amount of effort.


------------------------------------------------------

Chapter 3 Streaming Data Collection

------------------------------------------------------
3.1 Introduction

   - cover streaming data collection
   - continue to talk about 'fetch' data part of the ML cycle

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      Train Model --->  Evaluate Model --->
    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->


------------------------------------------------------
3.2 [Streaming Data Collection] Concepts

Resources:

  kaggle
    https://www.kaggle.com/
    - repository of community-published models, data & code
    - A subsidiary of Google, it is an online community of data scientists and machine learning engineers.

  UCI Machine Learning Respository
    https://archive.ics.uci.edu/
    - UC Irvine Machine Learning Repository
    - maintain 665 [and growing] datasets as a service to the machine learning community

  AWS Open Data Registry
    https://registry.opendata.aws/
    - This registry exists to help people discover and share datasets that are available via AWS resources

  Google's Big Query
    https://cloud.google.com/bigquery/
    - BigQuery is a fully managed, AI-ready data analytics platform that helps you maximize value from your data
      and is designed to be multi-engine, multi-format, and multi-cloud.
    - Store 10 GiB of data and run up to 1 TiB of queries for free per month.


  Early Data Collection
    - the practice of collecting census data began in Egypt 2nd millennium BC (2000 through 1001 BC), where it
      was used for tax gathering and to determine fitness for military service


  Where does Data come from?
    static data
      - data that is not changing
      - data we download and do something with
    streaming data
      - constantly being updated or continuously being added onto during the data collection process

  Public datasets

    Kaggle (kaggle.com)
      - Kaggle is an online platform that allows you to search through different types of datasets, and interact with
        others who are doing Machine Learning, or analyzing different datasets.
      - They also offer different competitions, and provide you with datasets to help analyze, and solve different problems.
      - The datasets are free to download, and come in many different formats

    UCI Machine Learning Repository
      - has many open source and great example datasets that many people use during Machine Learning projects, and during
        Machine Learning examples.

    AWS Open Data Repository
      - open source comprehensive toolkit for sharing and analyzing data at any scale.
      - AWS allows organizations to do is upload their datasets onto AWS, and make them public so anyone can view them,
        and access them, and use them for analytic services.

    Google's BigQuery.
     - allows you to run queries on different datasets, and return the data in whatever format you'd like.

  Your own data
    - for own data, just load it to AWS
    - to load to AWS:
      - upload through the console, via S3,
      - use one of AWS's many SDKs, with code like Java or Node.js, or C#.
      - upload your data via the command line using the AWS CLI

  How to get Streaming Data into AWS
    Kinesis
      - can be used to to stream our data into our Machine Learning process, and create real-time predictions

  Kinesis Family services (covered in more detail following lessons):
    Kinesis
    Kinesis Data Streams
    Kinesis Data Firehose
    Kinesis Video Streams
    Kinesis Data Analytics



------------------------------------------------------
3.3 Kinesis Data Streams

  Resources:

    Kinesis Data Streams Terminology and Concepts
      https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html

    AWS re:Invent 2018: High Performance Data Streaming with Amazon Kinesis: Best Practices
      https://www.youtube.com/watch?v=jKPlGznbfZ0

   Kinesis Data Streams
     - Kinesis Data Streams works by getting data from data producers.
     - these data producers typically have some type of JSON data as the payload, or they can use any type of
       data that they can fit into a data blob.
     - can have multiple consumers consume the data stream or consume those shards, and perform different processes
       depending on the job at hand
     - if you want to just store off your streaming data (i.e S3), you must uses a Data Consumers Service (i.e. Lambda)
       to take the input streaming data and output it onto S3.
     Data Producers:
       - Producers put records into Amazon Kinesis Data Streams.
       - automatically encrypt sensitive data as a producer enters it into a stream
       - AWS SDK, Kinesis Producer Library, Apache Kafka, Kinesis Agent, etc.
     Data Producer examples:
       - application log files or social media streams.
       - real-time user interaction from video games.
       - IoT devices or manufacturing devices in a factory.
       - clickstream data or user interaction for an online website or application.
       - physical devices that produce the data that we want to get into AWS
     Kinesis Streams
       - carries Data Producer data streams into AWS via shards
       - shards are container that holds the data that we want to send into AWS
     Consumers:
       - Consumers get records from Amazon Kinesis Data Streams and process them.
       - These consumers are known as Amazon Kinesis Data Streams Application.
       - There are two types of consumers that you can develop: shared fan-out consumers and enhanced fan-out consumers.
       shared-fanout consumer
         - By default, shards in a stream provide 2 MB/sec of read throughput per shard. This throughput gets shared across
           all the consumers that are reading from a given shard
       enhanced fan-out consumer
         - When a consumer uses enhanced fan-out, it gets its own 2 MB/sec allotment of read throughput, allowing multiple
           consumers to read data from the same stream in parallel, without contending for read throughput with other consumers.
         - consumers do not poll. messages are pushed to the consumer as they arrive
         - SubscribeToShared() API - uses HTTP/2 ; up to a 5 min connection; data pushed to consumer
     Data consumers (processing tools):
       - Consumers get records from Amazon Kinesis Data Streams and process them.
       - EC2
       - Lambda functions
         - example: real time ETL processing
       - Kinesis Data Analytics
         - run realtime SQL queries on our streaming data
       - EMR Spark
         - send our streaming data to an EMR cluster and process it using Apache Spark


  Apache Spark
    - Apache Spark, a distributed data processing framework, has revolutionized the extract, transform, load (ETL)
      process for data engineering, data science, and machine learning.
    - it has libraries like SQL and DataFrames, GraphX, Spark Streaming, and MLlib which can be combined in the same application
    - It provides a high-level API for easy data transformation and a strong ecosystem with many pre-built tools, connectors,
      and libraries.
    - utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size.
    - provides development APIs in Java, Scala, Python and R, and supports code reuse across multiple workloads - batch
      processing, interactive queries, real-time analytics, machine learning, and graph processing.



      Data Producers:                Kinesis Streams          Data Consumers           Storage and
                                                              (Processing Tools)        Analyzation
      --------------------           ---------------          ------------------       ---------------

      Log files               ---->    shard 1                   EC2                      S3
      social media streamsi   ---->    shard 2                   Lambda          --->     DynamoDB
      click stream data       ---->    shard 3         --->      Kinesis Data    --->     Redshift
      physical device         ---->     . . .          --->        Analytics              BI Tools
      gaming streaming data   ---->     . . .                    EMR Spark
                              ---->     shard N                  Kinesis Data Firehose
                                       1-500 shards
                                       default limit

      Kinesis Data Stream Made of:
        Shards
          - A shard is a uniquely identified sequence of data records in a stream.
          - A stream is composed of one or more shards, each of which provides a fixed unit of capacity.
          - Each shard can support up to 5 transactions per second for reads, up to a maximum total data [output] read rate of
            2 MB per second and up to 1000 put records per second for writes, up to a maximum total data [input] write rate of
            1 MB per second (including partition keys).
          - default shard limit 500, but you can request more shards if needed.
          - each shard consists of a sequence of data records. These can be ingested at 1000 records per sec
          - a data record is the unit of data captured
          - Transient Data store - shard retention period for the data records are 24 hours (default), up
            to 7 days with Extended data Retetion, and up to 8760 hours (365 days) ith long term retention
            Note: previous max retention was 7 days
          - A shard's 'data record' is made of up 3 parts: Partition Key, Sequence number, Data Blob (payload, up to 1 MB)
          Parition Key:
            - Partition key is used to segregate and route records to different shards of a stream.
            - A partition key is specified by your data producer while adding data to a Kinesis stream.
            - For example, assuming you have a stream with two shards (shard 1 and shard 2). You can configure your data
              producer to use two partition keys (key A and key B) so that all records with key A are added to shard 1 and
              all records with key B are added to shard 2.
            - unique key with each shard
          Sequence
            - within a shard, each sequence has an increasing squence number
            - each time that we make a request to send streaming data through Kinesia Streams, it creates a sequence
            - Each payload that we make creates a sequence which is associated with a shard.
          Data:
            - payload


        Kinesis Streams
        |--------------------------------------------------------------|
        |                         shard 1                              |
        | |----------------------------------------------------------| |
        | |Parition Key: e55fab...|      | Partition key: e55fab...  | |
        | |Sequence: 1                   | Sequence: 2               | |
        | |Data: <some fancy data>|      | Data: <some fancy data>   | |
        | |----------------------------------------------------------| |
        |                                                              |
        |                         shard 2                              |
        | |----------------------------------------------------------| |
        | |Parition Key: efffab...|      | Partition key: efffab...  | |
        | |Sequence: 1                   | Sequence: 2               | |
        | |Data: <some fancy data>|      | Data: <some fancy data>   | |
        | |----------------------------------------------------------| |
        |                            .                                 |
        |                            .                                 |
        |                            .                                 |
        |--------------------------------------------------------------|

  Interacting with Kinesis Data Streams
    Kinesis Producer Library (KPL)
      - library that allows you to write to a Kinesis Data Stream
      - Higher level access that if you used the Kinesis API. For example, retry mechanisms if the stream didn't get processed;
        optimizing throughput and aggregating records together so you have the most optimal stream.
      - can install KPL on your EC2 instance or integrate it directly with your JAVA application
    Kinesis Client Library (KCL)
      - Integration directly with KPL for consumer application to consume and process data from Kinesis Data Stream
      - Higher level access that if you used the Kinesis API.
      - Kinesis Client Library (KCL) for Java | Python | Ruby | Node.js | .NET is a pre-built library that helps you easily
        build Kinesis Applications for reading and processing data from a Kinesis stream.
      - KCL handles complex issues such as adapting to changes in stream volume, load-balancing streaming data, coordinating
        distributed services, and processing data with fault-tolerance.
    Kinesis API (AWS SDK)
      - used for low level API operations to send records to a Kinesis Data Stream
      - can perform all of the same actions as with KPL and KCL, but it's used for more of low level API operations

   KPL vs API
    Kinesis Producer Library (KPL)
      - provides a layer of abstraction specifically for ingesting data
      - automatic and configurable retry mechanism
      - additional processing delay can occur for higher packing efficiencies and better performance
      - Java wrapper (of a C++ module)
    Kinesis API (AWS SDK)
      - low-level API calls (PutRecords - used by producers to put data in a stream; and GetRecords - used by consumers
        to return data from stream)
      - GetRecord() limits: 5 transactions per second, per shard; Data: 2 MB per sec per shard (thus, 1 call per 200 millisec)
      - Stream creations, resharding, and putting getting records are manually handled
      - No delays in processing, but you may not be packaging and optimizing the Kinesis Data Streams
        - if you need your data stream to be available immediately (in millisec), it may be better to use the Kinesis API
          because of the additional processing that could delay your streaming data with the KPL
      - Any AWS SDK (e.g. C#, javascript, etc.)

  Creating Kinesis Data Stream
    AWS Console -> Kinesis  -> Create Kinesis Stream -> specify number of shards
    Estimate number of Shards tool:
      AWS Console -> Kinesis  -> Create Kinesis Stream -> select "Estimate the number of shards you'll need"
        - dependent on the payload size and the number of requests

  Adding data to Kinesis Stream:
    - You can add data to a Kinesis stream via PutRecord and PutRecords operations, or Kinesis Producer Library (KPL).
    - PutRecord operation allows a single data record within an API call and PutRecords operation allows multiple data
     records within an API call.

  Resharding
    - enables you to increase or decrease the number of shards in a stream.
    - used to adapt for any changes in the rate of data flowing through the stream.
    - resharding is typically done by a separate application that monitors the metrics from the producers and
      consumers and makes an API call to add more shards if needed.
    - resharding is considered an advanced option but you do have the option to reshard or add or decrease the
      number of shards after you create a stream by using the Kinesis API.

  When should you use Kinesis Data Streams
    - need data to be processed by consumers (e.g. data transformation performed or some analytics ran on the data)
    - real time analytics
    - feed data into other services in real time
    - some action needs to occur on your data
    - storing data is optional
    - data retention is important (data retention is built into amd allows store data from default of 24 hours
      and up to 7 days with Extented retention, and up to 365 with Long term retention)

  Kinesis Data Streams - Use Cases
    Process and evaluate logs immediately
      Example: Analyze system and application logs continuously and process within seconds
    Real-time data analytics
      Example: Run real-time analytics on click stream data and process it with seconds
          - e.g., to offer real time suggestions on other products they may like

  Kinesis Capacity Mode
    - in Kinesis Data Streams, you can choose between an 'on-demand' mode and a 'provisioned' mode for your data streams
    On-demand mode
      - With the on-demand mode, Kinesis Data Streams automatically manages the shards in order to provide the
        necessary throughput.
      - You are charged only for the actual throughput that you use and Kinesis Data Streams automatically accommodates
        your workloads throughput needs as they ramp up or down.
      - Each shard provides a default capacity of four megabytes per second or 4,000 records per second for ingestion.
        Pricing is based on the streams hourly usage and data in/out per gigabyte.
    Provisioned mode
      - With the provisioned mode, you must specify the number of shards for the data stream.
      - The total capacity of a data stream is the sum of the capacities of its shards.
      - You can increase or decrease the number of shards in a data stream as needed and you are charged for the number
        of shards at an hourly rate.
      - Each shard supports one megabyte per second or 1,000 records per second for ingestion. The overall throughput per
        shard is two megabytes per second. Pricing is based on the number of shards provisioned per hour.


------------------------------------------------------
3.4 Kinesis Firehose

  Resources:

    Streaming ETL for Data Lakes using Amazon Kinesis Firehose
    https://www.youtube.com/watch?v=0AGNcZfYkzw

    What Is Amazon Data Firehose?
    https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html

  Kinesis Data Firehouse
    - Data Firehose is a fully managed streaming ETL solution that automatically scales to match the throughput of your data
    - can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service,
      Snowflake, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards
    Data record
      - A record is the data of interest your data producer sends to a Firehose stream.
      - The maximum size of a record (before Base64-encoding) is 1024 KB if your data source is Direct PUT or Kinesis Data Streams.
      - The maximum size of a record (before Base64-encoding) is 10 MB if your data source is Amazon MSK (Managed Streaming
        for Apache Kafka).
    Amazon Kinesis Agent
      - Amazon Kinesis Agent
      - pre-built Java application that offers an easy way to collect and send data to your Firehose stream.
      - You can install the agent on Linux-based server environments and Windows such as web servers, log servers, and
        database servers.
      - The agent monitors certain files and continuously sends data to your Firehose stream
    PutRecord and PutRecordBatch operations?
      - You can add data to a Firehose stream through Kinesis Agent or Firehoses PutRecord and PutRecordBatch operations.
      - PutRecord operation allows a single data record within an API call and PutRecordBatch operation allows multiple
        data records within an API call.
    Data Producers (data input)
      - application log files
      - real time clickstream data from ecommerce website
      - IoT or manufacturing sensor devices
      - any type of streaming data can be considered a data producer for Firehouse
      - Firehose API is available in AWS SDKs.
    Processing Tools (optional)
      - Lambda - can use Lambda as an preprocessing ETL services of data prior to data store
    Storage / distinations
      - Firehose currently supports S3, Redshift, OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic,
        LogicMonitor, MongoDB, and HTTP End Point as destinations
      - redshift
      - S3
         - can use S3 events to call a lambda function to push the data to DynamoDB
      - Amazon Elastic Search (previously - now: AWS Opensearch)
      - Splunk


      Data Producers:        Kinesis         Processing Tools          Storage
                             Firehouse       (optional)
      --------------------   ----------      ---------------          ---------------
                                                                                       S3 Event
      Log files               -------->                                S3              ------->   DynamoDB
      social media streams    -------->          Lambda*      --->     Redshift**
      click stream data       -------->          (for data    --->     AWS Opensearch***
      physical device         -------->          transformation)       Splunk***
      gaming streaming data   -------->
      Kinesis Data Streams    -------->


      * Lambda transformed records sent to S3 Destination bucket; If data transformation is enabled, you can optionally
        back up source data to another S3 bucket.

      ** For Redshift destinations, streaming data is delivered to your S3 bucket first. Data Firehose then issues an
        Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster.

      *** For OpenSearch Service & Splunk destinations, streaming data is delivered to your OpenSearch Service cluster (after
          Lambda transformation, if enabled), and it can optionally be backed up to your S3 bucket concurrently.


  Kinesis Firehose vs Kinesis Data Streams
    Kinesis Data Streams
      - has shards and data retention so if a failure, data can be reprocessed
      - storing data is optional
      - stores the data in a buffer for default 24 hours, and you must write custom code to take the data
        off the stream prior to it expiring (data sits in the stream until it expires)
    Kinesis Firehouse
      - no shards
      - storing data is the end goal
      - used to stream data directly to some storage
      - has a managed consumer to take the data off the stream and put it into S3, Splunk, Redshift, AWS Opensearch

  When to use Kinesis Firehose
    - easily collect streaming data
    - processing is optional
    - final destination is S3 (or other data store)
    - [streaming] Data retention is not important
    - easy to use: just point your input data to Kinesis Firehose and set your output destination

  Kinesis Firehose Use Cases:
    Stream and store data from devices
      - Example: Capturing important data from IoT devices, embedded systems, comsunmer applications and
        storing it into a data lake
    Create ETL jobs on streaming data
      - Example: Running ETL jobs on streaming data before data is stored into a data warehousing solution

------------------------------------------------------
3.5 Kinesis Video Streams

  Resources:

    Deep Dive - Amazon Kinesis Video Streams (video - 37 min)
      https://www.youtube.com/watch?v=EzxRtfSKlUA

    Analyze live video at scale in real time using Amazon Kinesis Video Streams and Amazon SageMaker
      https://aws.amazon.com/blogs/machine-learning/analyze-live-video-at-scale-in-real-time-using-amazon-kinesis-video-streams-and-amazon-sagemaker/


   Kinesis Video Streams
     Data Producers:
       - Web Cameras
       - Security Cameras
       - radar data
       - audio feeds
     Data Consumers:
       Data Format for Consumers:
         - data consumers get the data is in fragments and frames from a Kinesis Video Streams to view process and analyze it
       Kinesis Video Stream Applications
         - these consumers are called Kinesis Video Streams applications.
         - you can write applications that consume and process the video streams in real-time or after
            data has been stored onto S3.
         - can write our consumer applications to run on EC2 instances
       - EC2 continuous Consumer
       - EC2 batch Consumer
     Storage:
       - S3


      Data Producers:         Kinesis Streams    Data Consumers       Storage
                              Video Streams
      --------------------    ---------------    --------------       ---------------

      Web Cameras               -------->        EC2 Continuous
      Security Cameras          -------->        Consumer          --->
      audio feeds               -------->                                   S3
      radar data                -------->        EC2 Batch         --->
                                -------->        Consumer
                                -------->

  When should you use Kinesis Video Streams
    - needs to process real-time streaming video data (audio, images, radar)
    - Batch-process and store streaming video
    - Feed streaming data into other AWS Services (e.g. SageMaker to run some machine learning processes
      or use that data for testing or training data)

  Kinesis Video Streams Use Case
     AWS Cloud Cameras
       - when movement detected by camera, sends a text message and records video
       camera -> Kinesis Video Streams --> EC2 ----> ML Process ---> detects person ---> EC2 ---> sends text
                                                                                    ---> S3  (stores video)
------------------------------------------------------
3.6 Kinesis Data Analytics

  Resources:
    Analyzing Data Streams in Real Time with Amazon Kinesis (video - 45 min)
      https://www.youtube.com/watch?v=dNp1emFFGbU

    Create real-time clickstream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena
      https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/

    Joining and Enriching Streaming Data on Amazon Kinesis
      https://aws.amazon.com/blogs/big-data/joining-and-enriching-streaming-data-on-amazon-kinesis/


  Kinesis Data Analytics:
    - real-time data analysis prior to sending it its destination (e.g. S3)
    - continuously read and process streaming data in real time.
    - can do this by using SQL queries to process the incoming streaming data and produce some output data from that.
    - it gets its streaming input data from services like kinesis data streams, and kinesis data firehose,
      run it through kinesis data analytics, run real time SQL queries, and output those into S3 or redshift
      or other visualization tools or BI tools that you may choose to use.


      Stream Inputs                              Kinesis Data         Storage and
                                                  Analytics           visual
      --------------------                       --------------       ---------------

      Kinesis Data Streams      -------->        Kinesis Data             S3
                                -------->        Analtics          --->   Redshift
      Kinesis Data Firehose     -------->                          --->   BI Tools
                                -------->                                 Visualization tools
                                -------->


  Setting Kinesis Data Analytics
      AWS Console  -> Amazon Kinesis -> Data Analytics -> specify streaming input sources from either firehose or data streams                        -> write SQL queries and output those results on to S3 or just view them in real time.


  Kinesis Data Analytics 2 types:
    SQL based:
     -  you write SQL to analyze the data (e.g. report most purchased product over last 30 min)

  When should you use Kinesis Data Analytics
     - run SQL queries or Apache Flink processing on streaming data
     - Construct applications that provide insight on your data
     - Create metrics, dashboards, monitoring, notifications, and alarms
     - output queruy results into S3 (or other AWS datasources)
     Apache Flink
       - Apache Flink is an open-source, distributed engine for stateful processing over unbounded (streams) and
         bounded (batches) data sets.

  Kinesis Data Analytics Use Cases:
    Responsive real-time analytics
      - Example: Send real-time alarms notifications when certain metrics reach a predfined threshold
    Stream ETL jobs
      - Example: Stream raw sensor data then, clean, enrich, organize, and transform it before it lands into
         data warehouse or data lakes

  The  kinesis Family Use Cases:

    Task at Hand                         Kinesis Service        Why
    ---------------------------------    ------------------     ----------------------------------
    Stream Apache log files directly     Kinesis Firehose       Firehose is for streaming data directly to a final
      from (100) EC2 instances and                              destination. First the data is loaded into S3, then
      store them into Redshift                                  copied into Redshift

    Stream live video coverage of a      Kinesis Video Streams  Kinesis Video Streams processes real-time streaming
     sporting event to distribute to                            video data (audio, images, radar) and can be fed into
     customers in near real-time                                other AWS services

    Transform real-time streaming        Kinesis Data Streams   Kinesis Streams allows streaming hugh amounts of
    data and immediately feed into a                            data, process/transform it, and then store it or feed
    custom ML Application                                       into custom applications or other AWS services

    Query real-time data, create metric  Kinesis Data           Kinesis Analytics gives you the ability to run SQL
    graphs, and store output into S3       Analytics            queries on streaming data, then store or feed the
                                                                output into other AWS Services


  Summary:
   kinesis data streams
     - uses shards and has data retention.
   kinesis firehose
     - don't have to worry about shards and there is no data retention.
     - mainly used to directly output the streaming data into some data store like S3.

------------------------------------------------------
3.7 Exam Tips

  Resources:

    whitepaper-streaming-data-solutions-on-aws-with-amazon-kinesis
      download PDF (in resources/resources_3_7)

    Real-Time Machine Learning Using Amazon Kinesis (video 12 min)
      https://www.youtube.com/watch?v=M8jVTI0wHFM


  Streaming Data Collections:

    Loading Data into AWS
      - understand how to get data from public or in house data sets and load it into AWS

      - know the different ways to upload into S3 by using the console, the S3 API, or AWS cli
        Your own data
          - for own data, just load it to AWS
          - to load to AWS:
            - upload through the console, via S3,
            - use one of AWS's many SDKs, with code like Java or Node.js, or C#.
            - upload your data via the command line using the AWS CLI

        How to get Streaming Data into AWS
          Kinesis
            - can be used to to stream our data into our Machine Learning process, and create real-time predictions

    Kinesis Family
      - know what each services is and how to preprocess / handle streaming data

        Kinesis Family services (covered in more detail following lessons):
          Kinesis Data Streams
          Kinesis Data Firehose
          Kinesis Video Streams
          Kinesis Data Analytics

      - know what shards are, what a data record is, and the retention period for a shard

            Kenesis Data Stream Made of:
              Shards
                - default shard limit 500, but you can request more shards if needed.
                - each shard consists of a sequence of data records. These can be ingested at 1000 records per sec
                - a data record is the unit of data captured
                - Transient Data store - shard retention period for the data records are 24 hours (default), up
                  to 7 days with Extended data Retetion, and up to 8760 hours (365 days) ith long term retention
                  Note: previous max retention was 7 days
                - A shard data record is made of up 3 parts: Partition Key, Sequence number, Data Blob (payload, up to 1 MB)
                Parition Key:
                  - unique key for each shard (each data record in shard has the same unique partition key value)
                Sequence
                  - within a shard, each sequence has an increasing squence number
                  - each time that we make a request to send streaming data through Kinesia Streams, it creates a sequence
                  - Each payload that we make creates a sequence which is associated with a shard.
                Data:
                  - payload

      - know the difference between KPL, KCL, and Kinesis API

        Interacting with Kinesis Data Streams
          Kinesis Producer Library (KPL)
            - library that allows you to write to a Kinesis Data Stream
            - Higher level access that if you used the Kinesis API. For example, retry mechanisms if the stream didn't get processed;
              optimizing throughput and aggregating records together so you have the most optimal stream.
            - can install KPL on your EC2 instance or integrate it directly with your JAVA application
          Kinesis Client Library (KCL)
            - Integration directly with KPL for consumer application to consume and process data from Kinesis Data Stream
            - Higher level access that if you used the Kinesis API.
          Kinesis API (AWS SDK)
            - used for low level API operations to send records to a Kinesis Data Stream
            - can perform all of the same actions as with KPL and KCL, but it's used for more of low level API operations

      - For a given scenario, know which streaming Kinesis service to use

------------------------------------------------------
3.8 Demo: Streaming Data Collection

  Resources:

  Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\3_8_streaming_data_collection_demo

     Random User Generator
       https://randomuser.me/
       - A free, open-source API for generating random user data.

    Put-record-python-program.py
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter3/put-record-python-program.py

    CloudFormation Template (setup data producer)
      https://raw.githubusercontent.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/master/Chapter3/setup-data-producer.yml

    Create Subset Transformation Query
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter3/create-subset-transformation-query.sql

    Lambda function (add-newline-function)
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter3/index.js



  Demo Use Case:
    - So you work for a company who has thousands of users interacting with the company's application.
      You have been tasked with capturing real-time data about the users for a marketing campaign,
      and what you need to capture is information like the person's name, their age, their gender,
      and the location of the users who are all 21 and older.

  Dummy User Data:
    - use the randomuser.me API that allows you to call the API and generate random user data
      https://randomuser.me/
    - call this API, you'll get information about a user back, and then you can use that within this lab.
    -  when you make a call to this API, you'll get things like their name, their gender, their location, etc

  Final Results
    - JSON files that are stored into S3, and the only information that we want to capture is the person's first name,
      their last name, their age, gender, latitude, and longitude. Only for users > 21 years old
    - run ingestion for 10 - 15 mins so you will have plenty of records to work with
    - keep generated data for future lab

  Using PutRecord
    - create a simple python program that simulates streaming data.
    - Call the PutRecord to send data to Kinesis streams for ingestion


    - to run local requires python boto3 installed
        !pip install boto3



  Using PutRecord:
    - Create a simple python program that simulates streaming data.
    - Call the PutRecord to send data to Kinesis Streams for Ingestion

    Code: put-record-python-program.py

         >>> import requests
         >>> import boto3
         >>> import uuid
         >>> import time
         >>> import random
         >>> import json
         >>> import sys
         >>>
         >>>
         >>> kinesis_stream="my-data-stream"
         >>> kinesis_arn="arn:aws:kinesis:us-east-1:012345678910:stream/my-data-stream"
         >>> region='us-east-1'
         >>>
         >>> client = boto3.client('kinesis', region_name=region)
         >>> partition_key = str(uuid.uuid4())
         >>>
         >>>
         >>> # Added 08/2020 since randomuser.me is starting to throttle API calls
         >>> # The following code loads 500 random users into memory
         >>> number_of_results = 500
         >>> r = requests.get('https://randomuser.me/api/?exc=login&results=' + str(number_of_results))
         >>> data = r.json()["results"]
         >>>
         >>> cntmsg = 0
         >>> errcnt = 0
         >>> errmax = 3
         >>> print (f'\nStarting producer put_record stream\n')
         >>> while True:
         >>>     # The following chooses a random user from the 500 random users pulled from the API in a single API call.
         >>>     random_user_index = int(random.uniform(0, (number_of_results - 1)))
         >>>     random_user = data[random_user_index]
         >>>     random_user = json.dumps(data[random_user_index])
         >>>     # you likely need either the StreamName or StreamARN, but not both
         >>>     response = client.put_record(
         >>>         StreamName=kinesis_stream,
         >>>         StreamARN=kinesis_arn,
         >>>         Data=random_user,
         >>>         PartitionKey=partition_key)
         >>>     cntmsg += 1
         >>>
         >>>     print('Message sent #' + str(cntmsg))
         >>>
         >>>     # If the message was not sucssfully sent print an error message
         >>>     if response['ResponseMetadata']['HTTPStatusCode'] != 200:
         >>>         print('\nError!\n')
         >>>         print(response)
         >>>         errcnt += 1
         >>>         if ( errcnt > errmax ):
         >>>                print (f'\nEXITING: Error Count > Error Max {errmax}\n')
         >>>                sys.exit(1)
         >>>     time.sleep(random.uniform(0, 1))


  Transform and Load to S3:
    - transform the streaming data using Lambda and output the results onto S3
    - final results are JSON files with First Name, Last Name, Age (>=21), Gender, Latitude, & Longitude


   Data Producer:
     - Use CloudFormation to start EC2 as a simulated data producer

      NOTE: change AMI to: ami-00beae93a2d981137 (Amazon Linux - from community)
            - use 'setup-data-producer_fixed.yml
               - updates AMI and setting up python with Boto3


  Flow:
    Data producer
       -> cloudformation EC2 instances running python script

       Data Producer --> Kinesis Data Streams --> Kinesis Analytics --> Kinesis Firehose  --> Lambda --> S3

  Setup:

  AWS Console -> Kinesis -> Get Started -> select "Kinesis Data Streams" -> create data stream ->
         Data Stream Name: my-data-stream, capacity mode: provisioned, provisioned shards: 1
         -> Create data stream

         Shard calculator:
         Stream size: 1KB at 5/sec rate; consumer applications: 1 -> recommends 1 Shard

  AWS Console -> Cloudformation ->  Create Stack -> Upload a template -> Choose file -> "setup-data-producer_fixed.yml"
     -> Next -> Stack Name: data-producer-stack , Parmater: KinesisDataStream: my-data-stream -> Next -> Next ->
       -> approved ... my create IAM resources ... -> Submit -> wait for Stack to be created


  Create EC2 instance

  create policy:
    policy name: kinesis-put-records-policy-lab-ml-specialty-course
    policy JSON:
        {
         "Version": "2012-10-17",
         "Statement": [
         {
           "Effect": "Allow",
           "Action": [
             "kinesis:DescribeStream",
             "kinesis:PutRecord",
             "kinesis:PutRecords"
            ],
           "Resource": "arn:aws:kinesis:us-east-1:012345678910:stream/my-data-stream"
         }
         ]
        }

  create role:
    role name: data-producer-role-kinesis-lab-ml-specialty-course
    attach policy: kinesis-put-records-policy-lab-ml-specialty-course
    trusted instance: ec2 (sts:AssumeRole)

  Attach Role:
     EC2 -> select instance -> Action -> Security -> Modify IAM Role  ->
       IAM Role: data-producer-role-kinesis-lab-ml-specialty-course -> Update IAM Role

  Setup python producers on EC2:
    Connect ec2:
      # install pip
      cd tmp; curl -O https://bootstrap.pypa.io/get-pip.py; sudo python3 get-pip.py
      # install boto3




  AWS Console -> Kinesis -> 'my-data-stream' -> Monitoriing
    -> verify data is being received

  Create Kinesis Data Analytics:


  AWS Console -> Kinesis Analytics for SQL -> NO LONGER EXISTS!!!!!


  AWS Console -> S3 -> bucket name: my-prod-userdata -> Create Bucket

  # create Lambda function to process firehose records for example blueprint fcn

  AWS Console -> Lambda -> Create Functions -> Use a blueprint" -> blueprint name: "Process records sent to Data Firehose stream" ->
    Function Name: "my-prod-firehose-fcn (runtime python 3.10) -> Create function -> Create function
    -> increase the timeout time to 1-min (changed from 3 to 30 sec)

  AWS Console -> Kinesis Firehose -> Source:  Kinesis Data Streams, Destination : S3 ->
       Kinesis data stream: my-data-stream -> Choose
       -> Firehose steam-name -> my-prod-firehose
       -> Transform source records with AWS Lambda -> select "Turn on data transformation
       -> AWS Lambda fcn: my-prod-firehose-fcn
      -> Destination settings: S3 Bucket: my-prod-userdata, New line delimiter: Enabled
      -> Buffer hints, compression, file extension and encryption -> S3 buffer hints:
         # set hints low so we can see the delivery:
         Buffer size: 1 MiB (default: 5), Buffer interval: 60 sec (from 300)
         -> Service access: Create or update IAM role firehoseService ...
         -> Create firehose stream

 -> worked except I did not use Lambda function to perform previously SQL transformation

------------------------------------------------------
3.9 Quiz AWS Certified Machine Learning - Specialist 2020 - Streaming Collection Quiz


Question 2

Your organization needs to find a way to capture streaming data from certain events that customers are performing. These
events are a crucial part of the organization's business development and cannot afford to be lost. You've already set up a
Kinesis Data Stream and a consumer EC2 instance to process and deliver the data into S3. You've noticed that the last few days
of events are not showing up in S3 and your EC2 instance has been shut down. What combination of steps can you take to ensure
this does not happen again?

correct answer:
Set up CloudWatch monitoring for your EC2 instance as well as Auto Scaling if your consumer EC2 instance is shut down. Next,
ensure that the maximum amount of hours are selected (8760 hours) for data retention when creating your Kinesis Data Stream.
Finally, write logic on the consumer EC2 instance that handles unprocessed data in the Kinesis Data Stream and failed writes to S3.

incorrect answer:
Set up CloudWatch monitoring for your EC2 instance as well as Auto Scaling if your consumer EC2 instance is shut down. Next,
set up a Lambda function to poll the Kinesis Data Stream for failed delivered records and then send those requests back into
the consumer EC2 instance.

Correct Answer info:

In this setup, the data is being ingested by Kinesis Data Streams and processes and delivered using an EC2 instance. It's best
practice to always set up CloudWatch monitoring for your EC2 instance as well as Auto Scaling if your consumer EC2 instance is
shutdown. Since this data is critical data that we cannot afford to lose, we should set the retention period for the maximum
number of hours (8760 hours or 365 days). Finally, we need to have reprocessed the failed records that are still in the data
stream and that fail to write to S3.


Question 3

You have been tasked with capturing two different types of streaming events. The first event type includes mission-critical data
that needs to immediately be processed before operations can continue. The second event type includes data of less importance, but
operations can continue without immediately processing. What is the most appropriate solution to record these different types of
events?

incorrect answer:
Capture the mission critical events with the Kinesis Producer Library (KPL) and the second event type with the Putrecords API call.

incorrect answer:
Capture both events with the PutRecords API call.

correct answer:
Capture the mission critical events with the PutRecords API call and the second event type with the Kinesis Producer Library (KPL).

incorrect answer:
Capture both event types using the Kinesis Producer Library (KPL).


Correct Answer Info

The question is about sending data to Kinesis synchronously vs. asynchronously. PutRecords is a synchronous send function, so
it must be used for the first event type (critical events). The Kinesis Producer Library (KPL) implements an asynchronous send
function, so it can be used for the second event type. In this scenario, the reason to use the KPL over the PutRecords API call
is because: KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable).
Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot
tolerate this additional delay may need to use the AWS SDK directly. For more information about using the AWS SDK with Kinesis
Data Streams, see Developing Producers Using the Amazon Kinesis Data Streams API with the AWS SDK for Java. For more information
about RecordMaxBufferedTime and other user-configurable properties of the KPL, see Configuring the Kinesis Producer Library.


Question 4

Which service in the AWS Kinesis family is specifically designed to continuously capture gigabytes of data per second, making
the collected data available in milliseconds?

correct answer:
Kinesis Data Streams

Correct answer info:
Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously
capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams,
financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds
to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.



Question 5

You work for a farming company that has dozens of tractors with build-in IoT devices. These devices stream data into AWS using
Kinesis Data Streams. The features associated with the data is tractor Id, latitude, longitude, inside temp, outside temp, and
fuel level. As a ML specialist you need to transform the data and store it in a data store. Which combination of services can
you use to achieve this?

Choose 3

incorrect answer:
Use Kinesis Data Firehose to run real-time SQL queries to transform the data and immediately write the transformed data into S3.

correct answer 1:
Set up Kinesis Data Analytics to ingest the data from Kinesis Data Stream, then run real-time SQL queries on the data to transform
it. After the data is transformed, ingest the data with Kinesis Data Firehose and write the data into S3.

correct answer 2:
Immediately pull the data from Kinesis Data Streams using Lambda. Transform the data in Lambda and write the transformed data
into S3.

incorrect answer:
Use Kinesis Data Streams to immediately write the data into S3. Next, set up a Lambda function that fires any time an object
is PUT onto S3. Transform the data from the Lambda function, then write the transformed data into S3.

correct answer 3:
Set up Kinesis Firehose to ingest data from Kinesis Data Streams, then have Lambda read the data. Transform the data in Lambda
and write the transformed data into S3.
Sorry!

incorrect info:
Amazon Kinesis Data Streams cannot load data directly into Amazon S3.

Correct Answer info:

Amazon Kinesis Data Analytics can query, analyze and transform streaming data from Amazon Kinesis Data Streams and use Amazon
Kinesis Data Firehose as a destination for loading data into Amazon S3.

Amazon Kinesis Data Streams can ingest and store data streams for Lambda processing, which can transform and load the data
into Amazon S3.

Amazon Kinesis Data Firehose can ingest streaming data from Amazon Kinesis Data Streams, which can leverage Lambda to transform
the data and load into Amazon S3.


Question 6

What are valid options for uploading objects to Amazon S3?

Choose 3

incorrect answer:
PutRecordBatch API call

correct answer 1:
Amazon S3 Console

incorrect answer:
UploadObject API call

correct answer 2:
PutObject API call

correct answer 3:
AWS SDK

incorrect answer:
PutRecords API call

Sorry!
incorrect info:
PutRecords is an Amazon Kinesis Data Streams Service API to writes multiple data records into a Kinesis data stream in a
single call.

Correct Answer info:

With the Amazon S3 Console, you can upload a single object up to 160 GB in size. AWS Documentation: Uploading objects.

You can send a PUT request to upload an object of up to 5 GB in a single operation. For more information, see the PutObject
example in the AWS CLI Command Reference.

You can use the AWS SDK to upload objects in Amazon S3. The SDK provides wrapper libraries for you to upload data easily. For
information, see the List of supported SDKs.



Question 7

Your organization has a standalone Javascript (Node.js) application that streams data into AWS using Kinesis Data Streams. You
notice that they are using the Kinesis API (AWS SDK) instead of Kinesis Producer Library (KPL). What might be the reasoning
behind this?

incorrect answer:
The Kinesis API (AWS SDK) runs faster in Javascript applications over the Kinesis Producer Library.

correct answer:
The Kinesis Producer Library must be installed as a Java application to use with Kinesis Data Streams.

incorrect answer:
The Kinesis Producer Library cannot be integrated with a Javascript application because of its asynchronous architecture.

incorrect answer:
The Kinesis API (AWS SDK) provides greater functionality over the Kinesis Producer Library.

Sorry! incorrect info:

The KPL must be installed as a Java application before it can be used with your Kinesis Data Streams. There are ways to process
KPL serialized data within AWS Lambda, in Java, Node.js, and Python, but none of these answers mentions Lambda.

Correct Answer info:

The KPL must be installed as a Java application before it can be used with your Kinesis Data Streams. There are ways to process
KPL serialized data within AWS Lambda, in Java, Node.js, and Python, but none of these answers mentions Lambda.

Correct Answer Info:

The Kinesis Producer Library (KPL) is designed to run as a Java application, which means it isn't natively compatible
with a Node.js environment. To use KPL with a Javascript (Node.js) application, you'd need to set up and manage a
separate Java-based service, adding unnecessary complexity. Since your team is using the Kinesis API (AWS SDK), which can be directly integrated into a Node.js application, allowing for a simpler and more efficient workflow.


Question 8

Which service in the Kinesis family allows you to easily load streaming data into data stores and analytics tools?

incorrect answer:
Kinesis Streams

incorrect answer:
Kinesis Video Streams

incorrect answer:
Kinesis Data Analytics

correct answer:
Kinesis Firehose

Sorry!

Amazon Kinesis Data Streams is used to collect and process large streams of data records in real time.  Kinesis Firehose on
the other hand is great for streaming data into data stores and analytics tools.

Correct Answer info:

Kinesis Firehose is perfect for streaming data into AWS and sending it directly to its final destination - places like S3,
Redshift, OpenSearch, and Splunk Instances.


Question 10

You have been tasked with capturing data from an online gaming platform to analyze and process through a machine learning
pipeline. The data consists of player controller inputs captured every second for up to 10 players per game, and each input
is represented as a 100 KB JSON blob. This data needs to be ingested through Kinesis Data Streams. What is the minimum number
of shards required to successfully ingest this data?


correct answer:
1 shard

Good work! correct answer info:

In this scenario, there will be a maximum of 10 records per second with a max payload size of 1000 KB (10 records x 100 KB = 1000KB)
written to the shard. A single shard can ingest up to 1 MB of data per second, which is enough to ingest the 1000 KB from the
streaming game play. Therefor 1 shard is enough to handle the streaming data.




Question 14

You are collecting clickstream data from an e-commerce website using Kinesis Data Firehose. You are using the PutRecord API
from the AWS SDK to send the data to the stream. What are the required parameters when sending data to Kinesis Data Firehose
using the API PutRecord call?

correct answer:
DeliveryStreamName and Record (containing the data)

Good work!

Kinesis Data Firehose is used as a delivery stream. We do not have to worry about shards, partition keys, etc. All we need is
the Firehose DeliveryStreamName and the Record object (which contains the data).



Question 16

You are collecting clickstream data from an e-commerce website to make near-real time product suggestions for users actively
using the site. You need to continuously query data and produce insights, but persisting clickstream data is not a requirement.
What is the best solution to analyze streaming data and make real-time product suggestions to users, based on specific conditions?

correct answer:
Use Amazon Kinesis Data Streams to ingest clickstream data. Then, use Amazon Kinesis Data Analytics to run real-time SQL
queries to gain actionable insights and trigger real-time recommendations to users with AWS Lambda functions based on conditions.



incorrect answer:
Use Amazon Kinesis Data Analytics to collect and process the clickstream data, and use the processed data to update a DynamoDB
table in real time. Use DynamoDB Streams and AWS Lambda to trigger product suggestion updates.
Sorry!

Amazon Kinesis Data Analytics is great for processing streaming data in real time. However, DynamoDB, despite being a fast
and flexible NoSQL database service for any scale, is not best suited for handling real-time product suggestion updates as
this involves complex querying and machine learning models which DynamoDB is not designed for. Additionally, using DynamoDB
streams and Lambda for triggering product suggestion updates introduces unnecessary complexity and latency to the process.

Correct Answer info:

Amazon Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and it scales
automatically to match the volume and throughput of your incoming data from Amazon Kinesis Data Streams. There are no servers
to manage, no minimum fee, and no setup cost, and you only pay for the resources that your streaming applications consume. AWS
Lambda functions can be integrated with Amazon Kinesis Data Analytics to make real-time product suggestions to users, once
certain conditions are met. AWS Documentation: Amazon Kinesis Data Analytics.

Question 15 (retry)

A local university wants to track cars in a parking lot to determine which students are parking in the lot. The university is
wanting to ingest videos of the cars parking in near-real time, use machine learning to identify license plates, and store that
data in an AWS data store. Which solution meets these requirements with the LEAST amount of development effort?

Choices:
  Use Amazon Kinesis Data Streams to ingest videos in near-real time, call Amazon Rekognition to identify license plate
  information, and then store results in DynamoDB.

  Use Amazon Kinesis Data Streams to ingest the video in near-real time, use the Kinesis Data Streams consumer integrated
  with Amazon Rekognition Video to process the license plate information, and then store results in DynamoDB.

  Use Amazon Kinesis Video Streams to ingest the videos in near-real time, use the Kinesis Video Streams integration with
  Amazon Rekognition Video to identify the license plate information, and then store the results in DynamoDB.      <--- Correct Answer

  Use Amazon Kinesis Firehose to ingest the video in near-real time and outputs results onto S3. Set up a Lambda function
  that triggers when a new video is PUT onto S3 to send results to Amazon Rekognition to identify license plate information,
  and then store results in DynamoDB.

Sorry!

Correct Answer

  Kinesis Video Streams is used to stream videos in near-real time. Amazon Rekognition Video uses Amazon Kinesis Video
  Streams to receive and process a video stream. After the videos have been processed by Rekognition we can output the
  results in DynamoDB.

  Setting up your Amazon Rekognition Video and Amazon Kinesis resources
    https://docs.aws.amazon.com/rekognition/latest/dg/setting-up-your-amazon-rekognition-streaming-video-resources.html

    Streaming video into Amazon Rekognition Video

      To stream video into Amazon Rekognition Video, you use the Amazon Kinesis Video Streams SDK to create and use a
      Kinesis video stream. The PutMedia operation writes video data fragments into a Kinesis video stream that Amazon Rekognition
      Video consumes

------------
[Rekognition] Working with streaming video events
https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html


  - You can use Amazon Rekognition Video to detect and recognize faces or detect objects in streaming video.
  - Amazon Rekognition Video uses Amazon Kinesis Video Streams to receive and process a video stream.
  - You create a stream processor with parameters that show what you want the stream processor to detect from the video stream.
  - Rekognition sends label detection results from streaming video events as Amazon SNS and Amazon S3 notifications.
  - Rekognition outputs face search results to a Kinesis data stream.


      Data Producers:         Kinesis Streams    Rekognition              Detection
                              Video Streams      Video                    Results
      --------------------    ---------------    --------------           ---------------

      Web Cameras               -------->        Processes Video
      Security Cameras          -------->        to detect faces   --->   SNS
      audio feeds               -------->        and objects              S3 [event] Notifications
      radar data                -------->        objects           --->
                                -------->
                                -------->

------------
Question 16 (retry)

Which service built by AWS makes it easy to set up a retry mechanism, aggregate records to improve throughput, and automatically
submits CloudWatch metrics?

Choices:
  Kinesis Client Library (KCL)     <- incorrect answer

  Kinesis Producer Library (KPL)   <- correct answer

  Kinesis Consumer Library

  Kinesis API (AWS SDK)

Sorry!
   Note 'retry' is about writing to the Kinesis Data stream via KPL (not reading from it via KCL)
      - KPL provides Higher level access that if you used the Kinesis API. For example, retry mechanisms if the stream
        didn't get processed; optimizing throughput and aggregating records together so you have the most optimal stream.
Correct Answer

  Although the Kinesis API built into the AWS SDK can be used for all of this, the Kinesis Producer Library (KPL) makes
  it easy to integrate all of this into your applications.


Question 8 - retry 2

Which service in the Kinesis family allows you to securely stream video from connected devices to AWS for analytics, machine
learning (ML), and other processing?

Choices:
  Kinesis Firehose

  Kinesis Data Analytics

  Kinesis Video Streams          <- correct answer

  Kinesis Streams

Good work!

  Kinesis Video Streams allows you to stream video, images, audio, radar into AWS to further analyze, build custom application around, or store in S3.


------------------------------------------------------
3.10 Hands-On Lab Performing Real-Time Data Analysis with Kinesis
------------------------------------------------------
About this lab

Real-time streaming data underpins many modern businesses and solutions, and one of the most useful services for handling streaming data is
Amazon Kinesis. In this lab, we will build a Kinesis Data Stream, develop both a producer and consumer of events, and also deliver them to
a destination using Kinesis Data Firehose.

Learning objectives

 1. Create an Amazon Kinesis Data Stream
 2. Develop a Lambda Function to produce streaming events
 3. Develop a Lambda Function to consume streaming events
 4. Setup Kinesis Data Firehose to send events to an S3 Bucket


  Lambda -> Producer --> Kinesis Data Streams --> Consumer -> Lambda
                            |
                            ->  Firehose ------> S3





   -----------------------------------
LambdaKinesisRolePolicy policy which was assigned to the KinesisLambdaRole Role:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*",
            "Effect": "Allow",
            "Sid": "CloudWatchLogsAccess"
        },
        {
            "Action": [
                "kinesis:*"
            ],
            "Resource": "*",
            "Effect": "Allow",
            "Sid": "KinesisAccess"
        }
    ]
}
   -----------------------------------


Introduction

Amazon Kinesis is a service that helps handle real-time streaming data by processing and analyzing it at scale. This service is very commonly
used in many different architectures, including IoT and telemetrics.

In this lab, we're going to build a Kinesis stream which will receieve events we're producing with an AWS Lambda function, and then consume
them with another Lambda function. Finally, we'll build a delivery stream using Amazon Kinesis Data Firehose to push our events into an S3 bucket.

Solution
Log in to the Lab Environment

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account
    credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Create a Kinesis Data Stream

    In the search bar on top, type "Kinesis" to search for the Amazon Kinesis service.
    Click on the Amazon Kinesis result to go directly to the Kinesis service.
    Click on the Data streams button in the sidebar.
    Click on the Create data stream button to launch the wizard to create the Data Stream.
    Enter the following details for the new Data Stream:
        Data stream name: TelemetricsStream
        Capacity mode: On-demand
    Click on the Create data stream button to confirm the creation of the new Data Stream.

Develop the Kinesis Producer Lambda Function

    In the search bar on top, type "Lambda" to search for the AWS Lambda service.

    Click on the Lambda result to go directly to the Lambda service on a new tab.

    Click on the Create function button to create a new AWS Lambda function

    Enter the following details for the new Lambda function:
        Function name: produceKinesisEvents
        Runtime: Python 3.11 (or the latest)

    Click the Change default execution drop toggle to manually change the execution role.

    Change the radio button to Use an existing role.

    From the dropdown, select the IAM role containing the name KinesisLambdaRole, which will provide the necessary privileges to the Lambda function.

    Click on the Create function button to create the function.

    Once the page has loaded, update the following code into the Lambda function:

    import json
    import boto3

    def lambda_handler(event, context):
        client = boto3.client('kinesis')

        data = {
            "id": "0",
            "latitude": "0",
            "longtitude": "0"
        }

        response = client.put_record(
            StreamName="TelemetricsStream",
            PartitionKey="geolocation",
            Data=json.dumps(data)
        )

        return response

    Click on the Deploy button to save and deploy the new version of the Lambda function.

    Click on the Test button to prepare to test the functions execution.

    Enter a short name for the test event (like test), and click on the Save button to save the test event.

    Click on the Test button again to trigger the test

    Confirm that the execution results returned with a HTTPStatusCode of 200.

        Help!

        Did your function return an error? Double-check the execution role you set when creating your Lamdba Function. Since the Lambda Function is sending a record to our Kinesis Data Stream, it needs the extra permissions, or it will trigger an error.

    Return to the Lambda function code, and update the code with the following to make it return a continuous stream of events:

    import json
    import boto3
    import uuid
    import random
    import time

    def lambda_handler(event, context):
        client = boto3.client('kinesis')

        while True:
            data = {
                "id": str(uuid.uuid4()),
                "latitude": random.uniform(-90, 90),
                "longtitude": random.uniform(0, 180)
            }

            response = client.put_record(
                StreamName="TelemetricsStream",
                PartitionKey="geolocation",
                Data=json.dumps(data)
            )

            print(response)

            time.sleep(random.random())

    Navigate to the Configuration tab of your Lambda Function, and ensure the General configuration sidebar tab is selected.

    Click on the Edit button to change the configuration settings of your Lambda function.

    Change the Timeout value to 30 seconds, then click on the Save button to update the function configuration.

    Test the function again, and ensure it runs for the 30 seconds before timing out.

Develop the Kinesis Consumer Lambda Function

    Open a new tab on the Lambda Functions page.

    Click on the Create function button to create a new AWS Lambda function

    Enter the following details for the new Lambda function:
        Function name: consumeKinesisEvents
        Runtime: Python 3.11 (or the latest)

    Click the Change default execution drop toggle to manually change the execution role.

    Change the radio button to Use an existing role.

    From the dropdown, select the IAM role containing the name KinesisLambdaRole, which will provide the necessary privileges to the Lambda function.

    Click on the Create function button to create the function.

    Once the page has loaded, click on the Add trigger button.

    From the dropdown, select Kinesis.

    Enter the following details:
        Kinesis stream: kinesis/TelemetricsStream
        Batch size: 10

    Click on the Add button to confirm the creation of the new trigger.

    Update the following code into the Lambda function:

    import json

    def lambda_handler(event, context):
        print(json.dumps(event))

    Click on the Deploy button to save and deploy the new version of the Lambda function.

    Return to the tab containing the produceKinesisEvents function.

    Click on the Test button again to trigger the test.

    Return to the tab containing the consumeKinesisEvents function.

    Navigate to the Monitoring tab of the Lambda function.

    Click on the View CloudWatch logs button to launch CloudWatch Logs in a new tab.

    Open the current log stream.

    Verify that the records are present, with the data encoded in Base64.

    Return to the tab containing the consumeKinesisEvents function.

    Update the following code into the Lambda function:

    import json
    import base64

    def lambda_handler(event, context):
        records = []

        for record in event["Records"]:
            data = base64.b64decode(record["kinesis"]["data"]).decode()
            records.append(json.loads(data))

        output = {
            "count": str(len(records)),
            "data": records
        }

        print(json.dumps(output))

    Click on the Deploy button to save and deploy the new version of the Lambda function.

    Return to the tab containing the produceKinesisEvents function.

    Click on the Test button again to trigger the test.

    Return to the tab containing the consumeKinesisEvents function.

    Navigate to the Monitoring tab of the Lambda function.

    Click on the View CloudWatch logs button to launch CloudWatch Logs in a new tab.

    Open the current log stream.

    Verify that the records are present, with the data decoded in JSON format, containing the various GPS coordinates.

Setup Kinesis Data Firehose

    In the search bar on top, type "Kinesis" to search for the Amazon Kinesis service.
    Click on the Amazon Kinesis result to go directly to the Kinesis service.
    Click on the Data Firehose button in the sidebar.
    Click on the Create delivery stream button to launch the wizard to create the Data Firehose delivery stream.
    Enter the following details for the new Data Stream:
        Source: Amazon Kinesis Data Streams
        Destination: Amazon S3
        Kinesis data stream: Use Browse to find your TelemetricsStream
        S3 bucket: Use Browse to find your firehosedeliverybucket
        Buffer interval: 60 seconds
    Click on the Create deliverys stream button to confirm the creation of the new Data Firehose.
    Return to the tab containing the produceKinesisEvents function.
    Navigate to the Configuration tab of your Lambda Function, and ensure the General configuration sidebar tab is selected.
    Click on the Edit button to change the configuration settings of your Lambda function.
    Change the Timeout value to 5 minutes, then click on the Save button to update the function configuration.
    Click on the Test button again to trigger the test.
    In the search bar on top, type "S3" to search for the Amazon S3 service.
    Click on the Amazon S3 result to go directly to the S3 service.
    Open the firehosedeliverybucket in S3.
    Wait up to a minute or so for the first events to arrive
    Navigate through the date-based folders to find the delivered events
    Download the file to confirm that it contains the GPS coded events.

Conclusion

Congratulations - you just learned how to create an Amazon Kinesis Data Stream, along with publishing events into the stream, and consuming
events from the stream! And as a bonus, you've learned about using Kinesis Data Firehose to deliver your events to a destination, like Amazon S3.

   -----------------------------------

Chapter 4 Data Preparation

------------------------------------------------------
4.1 Introduction


   - cover data preparation
   - talk about techniques to clean and prepare our data model

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      Train Model --->  Evaluate Model --->
    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
4.2 Concepts

  Data preparation process is to handle things like:
    - ML algorithms prefer numbers (not text)
    - need to handle missing values
    - find inaccuracies or outliers upfront
    - remove incomplete and incorrectly formatted values, irrelevant data and remove duplicate data.
    - Sometimes we need to change text based values into numeric values, group different attributes together, combine
      different attributes, take text strings and break them into different tokens and redesign features in general.

  Data Understanding
    - need to understand our data (after fetching) before cleaning it

  Data Preparation
    - the process of transforming a dataset using different techniques to prepare it for model training and testing
    - changing our dataset so it is ready for Machine Learning

  Data Preparation types:
    Categorical Encoding
      - converting categorical values into numeric values using mappings and one-hot techniques
    Feature Engineering
      - Transforming features so that they are ready for ML algorithms
      - Ensures the relevant features are used for the problem at hand
    Handling Missing Values
      - removing incomplete, incorrect formatted, irrelevant or duplicate data

  Options for Data Preparation
    SageMaker & Juypter Notebooks
      - use for adhoc data preparation
    ETL jobs in AWS Glue
      - use for reusable data preparation

------------------------------------------------------
4.3 Categorical Encoding

  Resources:

    Pandas mapping values
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html
        Series.map()
          - Map values of Series according to an input mapping or function.
          - Used for substituting each vals = pd.Series(list('abca')ue in a Series with another value, that may be derived from a function, a dict or a Series.
        example:
         >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])
         >>> s
             0      cat
             1      dog
             2      NaN
             3   rabbit
             dtype: object

    Pandas one-hot encoding
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html
      pandas.get_dummies()
        - Convert categorical variable into dummy/indicator variables.
        - Each variable is converted in as many 0/1 variables as there are different values. Columns in the output are
          each named after a value; if the input is a DataFrame, the name of the original variable is prepended to the value.

        example 1:
         >>> s = pd.Series(list('abca'))
         >>> pd.get_dummies(s)
                    a      b      c
             0   True  False  False
             1  False   True  False
             2  False  False   True
             3   True  False  False

        example 2:
         >>> pd.get_dummies(pd.Series(list('abc')), dtype=float)
                  a    b    c
             0  1.0  0.0  0.0
             1  0.0  1.0  0.0
             2  0.0  0.0  1.0


  Categorical Encoding
    - the process of manipulating categorical variables when ML algorithms expects numerical values as inputs
    - changing categorical values in your dataset to numbers
    categorical variable = categorical feature = discrete feature



  When to encode:

      Problem                                     Algorithm                       Encoding
      ------------------------------------        ---------------                 ------------------
      Predicting the price of a home              Linear Regression               Encoding necessary

      whether text is about sports or not         Naive Bayes                     Encoding not necessary

      Detecting malignancy in radiologic imagaes  Convolutional Neural Networks   Encoding necessary

  Categorical Encoding Examples:
     - all categorical values are discrete (finite number of values)
     Nominal - order does not matter
        color: {green, purple, blue}
        Evil: (true, false}
     Ordinal - order does matter
        Size {Large > Medium > Small}

  One-hot encoding
    - use one-hot encoding on categorical values,
    - assign a one to the observation that had the value, and a zero to the observations that didn't have the value
    One-hot or not?
      - one-hot encoding is not always a good choice when there are many, many categories
    Grouping
      - Using techniques like group by simarity could create fewer overall categories before encoding
    Mapping Rare values
      - mapping rare values to "other" can help reduce overall number of new columns created


  Categorical Encoding Summary:

    ML Algorithm Specific
      - in general, categorical encoding is used when the ML algorithm can not support categorical data
    Text into Numbers
      - we must find a way to turn text attributes into numeric attributes within our dataset
    There is no "Golden Rule"
      - there is no "golden rule" on how to encode your categories (or transform your data in general)
    Many different Approaches
      - There are many different approaches and each approach can have a different impact on the outcome
        of your analysis

   -> make sure you remember things like ordinal, nominal,
     Nominal - order does not matter
     Ordinal - order does matter  (definition: of a specified order or rank in a series)

   -> how to map values and the technique of one-hot encoding

------------------------------------------------------
4.4 Test Feature Engineering

  Resources (I found):

    AWS Data Transformations Reference
    https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html
    -> includes:  N-gram Transformation, Orthogonal Sparse Bigram (OSB) Transformation, Lowercase Transformation
                  Remove Punctuation Transformation, Quantile Binning Transformation, Normalization Transformation,
                  Cartesian Product Transformation

    Amazon Machine Learning Key Concepts
      https://docs.aws.amazon.com/machine-learning/latest/dg/amazon-machine-learning-key-concepts.html

  Resources:
    tf-idf vectorizer
      https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

      Scikit-Learn TfidfVectorizer() class
        - Convert a collection of raw documents to a matrix of TF-IDF features.

        example:
         >>> from sklearn.feature_extraction.text import TfidfVectorizer
         >>>
         >>> corpus = [
         >>>     'This is the first document.',
         >>>     'This document is the second document.',
         >>>     'And this is the third one.',
         >>>     'Is this the first document?',
         >>> ]
         >>>
         >>> vectorizer = TfidfVectorizer()
         >>> X = vectorizer.fit_transform(corpus)
         >>> vectorizer.get_feature_names_out()
             array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'], ...)
         >>> print(X.shape)
             (4, 9)

      TF-IDF (Term Frequency and Inverse Document Frequency):
        - TF-IDF will transform the text into meaningful representation of integers or numbers which is used to fit
          machine learning algorithm for predictions.

        TF-IDF = TF(t, d) x IDF(t),
            where, TF(t, d) = Number of times term "t" appears in a document "d".
                   IDF(t)   = Inverse document frequency of the term t.

  TF-IDF  Term Frequency-Inverse Document Frequency
    https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/
      - measures how important a term is within a document relative to a collection of documents (i.e., relative to a corpus).
      - importance of a term is high when it occurs a lot in a given document and rarely in others.
      - In short, commonality within a document measured by TF is balanced by rarity between documents measured by IDF
      Term Frequency (TF):
        - TF of a term or word is the number of times the term appears in a document compared to the total number of words in the document.

          TF = 'number of times the term appears in the document' / 'total number of terms in the document'

      Inverse Document Frequency (IDF):
        - IDF of a term reflects the proportion of documents in the corpus that contain the term. Words unique to a small
          percentage of documents (e.g., technical jargon terms) receive higher importance values than words common across
          all documents (e.g., a, the, and).

          IDF = ln( 'number of the documents in the corpus' / 'number of documents in the corpus containing the term' + 1)

        TF-IDF:
            TF-IDF = TF * IDF


  Feature Engineering
    - transforming attributes within our data to make them more useful within our model for the problemm at hand
    - feature engineering is often compared to an art

  Text Feature Engineering
    - transforming text within our data so Machine Learning algorithms can better analyze it
    - splitting text into bite size pieces
    - these transformations or techniques are done on text data or corpus data which essentially just refers to
      a data set that's just a collection of texts.  This type data is used in natural language processing, speech recognition,
      text-to-speech etc

  Text (Corpus) data examples:
    - a research paper, a dialogue between two people, and a newspaper clipping

  Text Feature Engineering example - breaking apart data to bitsize pieces (or tokenize these values)
      Raw Text: {"123 Main Street, Seattle, WA 98101"} tranformed to
      addresss          city          State          Zip
      123 Main Street   Seattle       WA             98101

  Bag-of-Words
    - tokenizes raw text and creates a statistical representation of the text
    - breaks up text by white spaces into single words
    - simpliest way  to breakup text into tokens
    - from bag-of-words tokenization, we can create word counts and see the different distributions
      of how words are produced throughout the text

  N-Gram
    - an extension of Bag-of-words which produces groups of n size
    - breaks up text by white space into group of words
    - N-gram, size=1 is the same as bag-of-words
    when it is useful:
     - if we're trying to find groups of words and two words beside each other, we could take the N-Gram of size
       equals two or three or four and actually find all the combinations of the words throughout the text.
       - for example: we could use this technique in a spam filter to try to find pairs of words like
         'click here' or 'you're a winner'.

    example: 2-gram:
     Raw Text {"he is a jedi and he will save us"}

     N-gram, size=2: {"he is", "is a", "a jedi", "jedi and", "and he", "he will", "will save", "save us",
                      "he", "is", "a", "jedi", "and", "he", "will", "save", "us"}

     Note: Produces an 2-gram using a slide window, but also produces N-grams;
           When we apply N-Gram it also produces any N-gram less than its size, so it also produces N-Gram of size equals one

    example: 2-gram with punctuation marks:
       - n-grams are generated by breaking the input data on whitespace characters.
       - punctuation characters will be considered a part of the word tokens
       - You can use the punctuation remover processor

          Raw Text {"red, green, blue"}

          N-gram, size=2: {"red,", "green,", "blue,", "red, green", "green, blue"}.

  N-gram types:
    unigram = 1 word tokens
    bigram  = 2 word tokens
    trigram = 3 word tokens

  Orthogonal Sparse Bigram (OSB)
    - creates groups of words on size n and outputs every pair or words that include the first word
    - creates groups of words that always include the first word
    - OSB is not "better" or "worse than N-Gram, it's just another technique to use

    meaning:  Orthogonal: when two things are independent of each other
              Sparse:     scattered or thinly distributed  (spreads them out)
              Bigram:     2-gram or 2 words

   example: OSB, size=4
     Raw Text {"He is a jedi and he will save us"}

     OSB, size=4:
      {"he_is",     "he__a",    "he___jedi"}
      {"is_a",      "is__jedi", "is___and"}
      {"a_jedi",    "a__and",   "a___he"}
      {"jedi_and",  "jedi__he", "jedi___will"}
      {"and_he",    "and__will", "and___save"}
      {"he_will",   "he__save",  "he___us"}
      {"will_save", "will__us"}
      {"save_us"}

     Notes: Has a sliding window of 4 words (see each row);
            1st word is paired with each of the next 3 words, with 1 underscore between 1st word & 2nd word,
            2 underscores between 1st word & 3rd word, and 3 underscores between 1st word & 4th word


  Term Frequency - Inverse Document Frequency (tf-idf)
    - represents how important a word or words are to a give set of text by providing appropriate weights to terms
      that are common and less common in the text
    - shows us the popularity of a word or words in text data by making common words like 'the' or "and" less
      important

    meaning:
         Term Frequency:     How frequent does a word appear
         Inverse:            Makes common words less meaningful
         Document Frequency: Number of documents which terms occur

     Example:
         - Since the tokens "the" and "a" showed up in both documents many times, these are deemed as less important

         Document 1                 Document 2
         word        count          word         count
         ---------   -------        ---------    -------
         the           3             the           2
         force         1             jedi          1
         Luke          1             a             1
         Skywalker     1             empire        1
         a             2


   Tf-idf vectorized (potential exam question):
             (number of ducments, number of unique n-grams)
                              (2, 7)  <- for above example, 2 documents, and 7 unique 1-grams

  Use Cases:

    Problem                       Transformation            Why
    -----------------             ---------------           ---------------------------------
    Matching phrases in spam       N-Gram                   Easier to compare whole phrases like
    emails                                                  "click here now" or "you're a winner"

    Determine subject matter      tf-idf                    Filter leas important words in PDFs.
    of multiple PDFs              Orthogonal Sparse Bigram   Find common word combinations repeated throughout PDFs

    Notes:
       N-Gram:
         - can be used to match whole words or phrases,
       OSB
         - used to find common word combinations or bigrams
       Term Frequency- Inverse Document Frequency
         - helps us determine which words are more important in documents than others.
         - it essentially helps us filter out words like 'the" or 'and'.



  Text Feature Engineering (simple upfront transformations)
    Remove Punctunation:
      - somethimes removing punctuation is a good idea if we do not need them
    Lowercase Transformation
      - Using lowercase transformation can help standardized raw text

     Removing prunctuation:
        Example:
         Raw Text: {"Help me, Obi-Wan Kenobi. You're my only hope.")

        Remove punctuation  (note: don't remove punctuation inside words):
                {"Help", "me", "Obi-Wan", "Kenobi", "You're", "my", "only", "hope")


     Lowercase transformation:
      Example:
         Raw Text: {"I Find Your Lack of Faith Distubrbing")

        After lowercase transformation:
                   {"i find your lack of faith distubrbing")


  Cartesian Product Transformation
    - creates a new feature from the combination of two or more text or categorical values
    - combining sets of words together
    - multipling a set or words by another set of words to create a new feature
    - This transformation is used when an interaction between variables is suspected

    Meaning:
      Cartesian: of, relating to, or used in Descartes' mathematical system
                - Rene Descartes created the Cartesian Co-ordinate System ( X-horizontal and Y-vertical accesses)
      Product : multiplication


    Cartesian Product Example:
         - concatentates each word in the textbook name with an underscore and binding type

      ID       textbook                         binding
      ----    ---------------------------      ------------
      1       Python Data Science Handbook     Softcover              ------> remove
      2       Visualization Analysis & Design  Hardcover                      punctuation
      3       Machine Learning Algorithms      Softcover                         |
                                                                                 V
                                                                      Cartesian product (textbook, binding)
      ID       cartesian_product                                                 V
      ----    ---------------------------
      1       {"Python_Softcover", "Data_Softcover", " Science_Softcover", " Handbook_Softcover"}
      2       {"Visualization_Hardcover", "Analysis_Hardcover", "Design_Hardcover"}
      3       {"Machine_Softcover", "Learning_Softcover", "Algorithms_Softcover"}


  Feature Engineering Dates
    - translating dates into useful information

    Date Format examples:
     Date
     --------------------------
     2013-02-08T09
     6 Mar 17 21:22 UT
     Mon 06 Mar 2017 21:22:23 z
     9/28/2019

    Extracted information:
      - was it a weekend, or a week day?
      - was the date the end of a quarter
      - what was the season?
      - what the day a holiday?
      - was it during business hours?
      - was important event like the world cup taking place on this date?



     Date                       is_weekend   day_of_week    month    year
     -----------                ----------   -----------   -------   ------
     2015-06-17                      0            2           6       2015
     2015-04-24      --------->      0            4           4       2015
     2015-02-12      date            0            3           2       2015
     2015-12-16      Feature         0            2          12       2015
     2015-03-14      Engineering     1            5           3       2015



  Text Feature Engineering Summary (more common techniques that may be on the exam):

    Technique                    Function
    -------------                -------------------------------------
    N-Gram                       Splits text by whitespaces with window size n


    Orthogonal Sparse Bigram     Splits text by keeping 1st word in the sliding window and uses delimiter
    (OSB)                          with remaining whitespaces between 2nd word with window size n

    Term Frequency - Inverse     Helps us determine how important words are within multiple documents
    Document Frequency (tf-idf)     - helps filter out common words like 'the' and 'and'

    Removing punctuation          Removes punctuation from text (but not within a word)

    Lowercase                     Lowercase all word in the text

    Cartesian Product             Combines words together to create new features

    Feature Engineering Dates     Extracts information from dates and creates new features
                                    (i.e. day_of_week, is_weekend, month, year)


  Additional Data Transformations in AWS doc:
  https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html

  Quantile (or percentile) Binning Transformation
    - The quantile binning processor takes two inputs, a numerical variable and a parameter called bin number, and outputs
      a categorical variable.
    - The purpose is to discover non-linearity in the variable's distribution by grouping observed values together.
    - used to instruct ML to establish n bins of equal size based on the distribution of all input values
    - Quantiles are values that divide the data into equal portions.

    example panda code:
        pd.qcut():
          - the number of elements in each bin will be roughly the same, but this will come at the cost of differently sized interval widths

         >>> large_counts = [296, 8286, 64011, 80, 3, 725, 867, 2215, 7689, 11495, 91897, 44, 28, 7971, 926, 12]
         >>> import pandas as pd
         >>> ### Map the counts to quartiles
         >>> pd.qcut(large_counts, 4, labels=["s","m","l","xl"], retbins=True)
             (['m', 'xl', 'xl', 'm', 's', ..., 's', 's', 'l', 'l', 's']
             Length: 16
             Categories (4, object): ['s' < 'm' < 'l' < 'xl'],
             array([3.00000e+00, 7.10000e+01, 8.96500e+02, 8.04975e+03, 9.18970e+04]))

         >>> large_counts_series = pd.Series(large_counts)
         >>> large_counts_series.quantile([0.25, 0.5, 0.75])
             0.25      71.00
             0.50     896.50
             0.75    8049.75
             dtype: float64


  Normalization Transformation:
    - normalizes numeric variables to have a mean of zero and variance of one.
    - Normalization of numeric variables can help the learning process if there are very large range differences between numeric
      variables because variables with the highest magnitude could dominate the ML model, no matter if the feature is informative
      with respect to the target or not.

------------------------------------------------------
4.5 Numeric Feature Engineering


  Resources:
     scikit-learn MinMaxScaler (Normalization)
       https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html

       - This estimator scales and translates each feature individually such that it is in the given range on the training set,
         e.g. between zero and one.
        - The transformation is given by:
             X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
             X_scaled = X_std * (max - min) + min
             where min, max = feature_range.

     scikit-learn StandardScaler (Standardization)
       https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html

       - Standardize features by removing the mean and scaling to unit variance.
       - The standard score of a sample x is calculated as:
          z = (x - u) / s
            where u is the mean of the training samples or zero if with_mean=False, and
                  s is the standard deviation of the training samples or one if with_std=False.

      pandas.qcut (Quantile-based discreteization function)
        https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.qcut.html

        pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')[source]
          - Quantile-based discretization function.
          - Discretize variable into equal-sized buckets based on rank or based on sample quantiles.
          - For example 1000 values for 10 quantiles would produce a Categorical object indicating quantile membership for each data point.
            (creates 10 bins of varying widths, each bin containing of ~100 values, and each bin with a Categorical label)

  Resources (I found):

    Normalization vs Standardization - Whats The Difference?
      https://www.simplilearn.com/normalization-vs-standardization-article


  Numeric Feature Engineering
    - transforming numeric values within our data so ML algorithms can better analyze them
    - change numeric values in out dataset so they are easier to work with

  Common Techniques
    Feature scaling
      - Changing numeric value so all values are on the same scale.
      - Techniques:
        Normalization
        Standardization
    Binning
      - changes numerica values into  groups or buckets of similar values
      Technique:
        Quantile Binning
          - aims to assign the same number of features to each bin


  equialent meanings:
    scaling = feature scaling = normalization

  Normalization
    - scales values in feature set to a given range (e.g. 0 to 1)
    - most common and easiest technique to use to scale down your values
    - the problem with normalization is that outliers can throw off normalization


     Example:
        home prices (X):       250555    243566     125700      345000    120900
        scaled prices (X_norm): 0.578559  0.547372    0.021419    1.0000   0.00000

        Normalization assigns
           smallest value to 0: 120900
           largest value  to 1: 345000
           scales other values between 0 and 1

        formula:
            x_norm =  (x - min(X))  / (max(X)  - min(X))
                      (125700 - 120900)  / (345000 - 120900)  = 0.021419

  Standardization
    - scales values in feature set to a given range (e.g. -1 to 1) using the mean value set to mid range value (e.g. 0)
      and using mean and std deviation values to calculate where the standardized values fall within this range
    - standardization helps with handling of outliers


     Example:
        home prices (X):       250555    243566     125700      345000      120900
        scaled prices (X_std): 0.394923  0.312311   -1.080890   1.511283   -1.137627

        u = price mean     = 217144.2      =  (1/N) SUM_i_N x_i
        s  = std deviation =  84600.81     = ( (1/N) SUM_i_N (x_i - u)**2 )**1/2


        Standardization assigns
           assigns average price to 0: 120900
           uses the z-score for the other values
             where z-score uses the price average and the std deviation to smooth out the values

        formula:
              z = (x - u) / s    = (125700 - 217144.2)  / 84600.81   = -1.080890


  Scaling Summary
    Scaling Features
      - some machine learning algorithms can't perform precise calculations on large number scales
      - required for many ML algorithms like linear / non-linear regression, clustering, neural networks, and more
      - scaling features depends on the algorithm you use
    Normalization
      - rescales values from 0 to 1 but doesn't handle outliers very well
    Standardization
      - rescaling the values that meet the characteristics of the standard normal distribution
      - rescales values by making the values of each feature in the data have zero mean and is much less affected by outliers
      - has average value of zero and other values are calculated by their z-score where z-score = (x - mean) / std_dev
    Translating back
      - when you're done you can always scale the data back to the original representation


  Binning
   - use when a numeric value in your dataset does not have a linear role with your prediction or target attribute
   - age is often a feature that binning is applied (e.g. more concerned about the person's life stage than specific age)
   Fixed binning:
     - using fixed widths for bins
     - fixed bin example:  3 age bins:  0 - 30,   30 - 50, and 50 and above
     - can end up with irregular bin which are not uniform (data points in each bin vary significantly)

  Quantile Binning:
    - grouping in equal parts binning
    - example: 12 ages and 3 bins, then quantile binning would place 4 values in each bin

  Quantile Binning Summary:
    Binning
      - used to group together values to reduce the effects on minor observation errors
    Quantile Binning
      - bins equal number values into each bin
    Optimum Number of Bins
      - depends on the characteristics of the variables and its relationship to the target
      - this is best determined through experimentation

  Numeric Feature Engineering Summary

       Technique                             Function
       ----------------------            -----------------------------
       Normalization                     From 0 to 1, 0 - min, 1 - max

       Standardization                   0 is the average, values are the z-score where z-score = (x - mean) / std_dev

       Quantile Binning                  places roughly equal number of values in each bin


------------------------------------------------------
4.6 Other Feature Engineering

  Resources:

    The MNIST Database of handwritten digits
    http://yann.lecun.com/exdb/mnist/
       - The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a
         test set of 10,000 examples.

    AWS Common Data Formats for Training
      https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html


  Image Feature Engineering
    - Extracting useful information from images before using them with ML algorithms
    - transforming images to find out useful information
    - example: break image of handwritten digit black with images into grids, label grids containing black with '1'
      else label '0'


  MNIST (Modified National Institute of Standards and Technology) database
    - public dataset of handwritten digits

  Audio Feature Engineering
    - extracting useful information from sounds and audio before using them with ML algorithms
    - transforming audio data into useful info
    - example: break audio into time series data of its amplitude

  Dataset Formats
    File
      - loads all of the data from S3 [bucket] directly onto the training instance volume
      file formats:
         - CSV, JSON, Parquet, image files (.png or jpg)
    Pipe
      - your datasets are streamed directly from S3
      - to provides for faster training and better throughput
      - file type used: recordIO-protobuf (creates tensor - multi-D array)
      - can convert other files formats (e.g. CSV,...), you can used AWS SDK to convert to recordIO-protobuf

         Code: convert S3 bucket file to recordIO-protobuf

         >>> from sagemaker.amazon.common import write_numpy_to_dense_tensor
         >>> import io
         >>> import boto3
         >>>
         >>> bucket = 'bucket-name'  # use the name for your s3 bucket where to store recordIO-protobuf
         >>> data_key = 'kmeans_lowlevel_example/data'
         >>> data_location = 's3://{}/{}'.format(bucket, data_key)
         >>>
         >>> # Convert the training data into the format required by the algorithm
         >>> #   io.ByteIO converts text to binary bytes in an in-memory buffer
         >>> buf = io.BytesIO()
         >>> # data format conversion:
         >>> write_numpy_to_dense_tensor(buf, train_set[0], train_set[1])
         >>> buf.seek(0)
         >>>
         >>> # location to upload to recordIO-protobuf data after conversion
         >>> boto3.resource('s3').bucket(bucket).object(data_key).upload_fileobj(buf)

      Note:
      Parquet File format:
        Column-based format
        Binary format (can using encoding, compressing, and optimizing data storage)
        Data compression (support various compression algorithms, such as Snappy, Gzip, and LZ4)
        embedding metadata (include metadata that provide information about the schema, compression settings, number of values,
            location of columns, minimum value, maximum value, number of row groups and type of encoding)

------------------------------------------------------
4.7 Handling Missing Values


  Resources:

    pandas mean
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html

      pandas.DataFrame.mean((axis=0, skipna=True, numeric_only=False, **kwargs)
        - Return the mean of the values over the requested axis.
        example:
         >>> s = pd.Series([1, 2, 3, 7, 10])
         >>> s.mean()
             4.6

    panda median
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html

      DataFrame.median(axis=0, skipna=True, numeric_only=False, **kwargs)
         - Return the median of the values over the requested axis.

        example:
         >>> s = pd.Series([1, 2, 3, 7, 10])
         >>> s.median()
             3.0

         >>> s = pd.Series([1, 2, 3, 7, 10, 13])
         >>> s.median()
             5.0                # between 3 and 7

    pandas mode
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html

      DataFrame.mode(axis=0, numeric_only=False, dropna=True)
        - Get the mode(s) of each element along the selected axis.
        - The mode of a set of values is the value that appears most often. It can be multiple values.


    pandas replace missing values
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html

      DataFrame.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=_NoDefault.no_default)[source]
        - Fill NA/NaN values using the specified method.

         arguments:
            method: {backfill, bfill, ffill, None}, default None
              - Method to use for filling holes in reindexed Series:
                 ffill: propagate last valid observation forward to next valid.
                  bfill: use next valid observation to fill gap.


    pandas drop missing values
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html

      DataFrame.dropna(*, axis=0, how=_NoDefault.no_default, thresh=_NoDefault.no_default, subset=None, inplace=False, ignore_index=False)
        - Remove missing values.

        arguments:
          how: {any, all}, default any
            - Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.
               any : If any NA values are present, drop that row or column.
               all : If all values are NA, drop that row or column.



  Missing Data
    - missing data can be represented in many different ways (null, NaN [Not a Number], NA, None, etc)
    - handling missing values is an important data preparation step
    - most machine learning algorithms are going to skew results if you have a lot of missing data.


  1st:
    Understand: Why are the values missing?
    - if you know why the data is missing, then you maybe able to apply specific techniques to repopulated
      the missing data
    - to handle missing values, it really depends on how much of your data is missing and what you want to do is create
      the least amount of impact on your data or create the least amount of bias when you replace these missing values.


  Missingness Mechanisms
    Missing at Random (MAR)
      - means that the propensity for a data point to be missing is not related to the missing data, but it is
        related to some of the observed data
    Missing Completely  at Random (MCAR)
      - the fact that certain value is missing has nothing to do with the hypothetical value and the values of
        other variables
    Missing Not at Random (MNAR)
      - two possible reasons are that the missing value depends on the hypothetical value or missing value is dependent
        on some other variable's value


  How to handle missing values:

    Technique            Why this works                          Ease of Use
    -----------------    ----------------------------------      --------------------------------
    Supervised learning  Predicts missing values based on the    Most difficult, can yield best results
                          values of other features

    Mean                 The average value                       Quick and easy, results can vary

    Median               Orders values then chooses value in     Quick and easy, results can vary
                          the middle

    Mode                 Most common value                       Quick and easy, results can vary

    Dropping rows        Removing missing values                 Easiest but can dramatically change datasets



   Which missing value technique to  use?
     - if you have a very small portion of values that are missing, then using statistical methods like mean, median,
       and mode may be the best option because they are quick and easy.
     - But if you have a large amount of values that are missing, then using supervised learning algorithms may be
       the best bet
     - you can always drop the rows, but this is last case scenario

  Imputation:
    - a technique used for replacing the missing data with some substitute value to retain most of the data/information of
      the dataset.
    - anytime that we use statistical methods like mean, median, and mode and supervised learning algorithms to replace
      our missing data, this is known as imputation

------------------------------------------------------
4.8 Feature Selection


  Resources:

    pandas drop columns
      https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html

      DataFrame.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')
        - Drop specified labels from rows or columns.
        - Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names


  Feature Selection
    - Selecting the most relevant features from your data to prevent over-complicating the analysis, resolving potential
      inaccuracies, and removes irrelevant features or repeated information
    - deciding what to keep and what to get rid of
    - feature selection is an intuitive step that humans take to reduce the number of features

    example: Can old dogs learn new tricks?
      Features: ID, age, fur_color, born_month, born_month_num, breed
      target: trick_learned
      features to remove:
         fur_color: fur color isn't going to help, so remove it.
         born_month: born month and born month num are the exact same feature, so remove born_month
         born_month_num: it doesn't really matter which month the dog was born, so remove it.


  Principal Component Analysis (PCA)
    - an unsupervised learning algorithm that reduces the number of features while still retaining as much information
      as possible
    - reduces the total number of features in a dataset
    - if we had a dataset with hundreds of features, using techniques like PCA upfront can cut down the number of features
      because what it does is find linear combinations of features and removes the rest
    - from OReilly Hands-On ML:
      page: 243: Principal component analysis (PCA) is by far the most popular dimensionality reduction algorithm. First, it
        identifies the hyperplane that lies closest to the data, and then it projects the data onto it, just like
        Figure 8-2 [A 3D dataset lying close to a 2D subspace].


  Feature Selection Use Case

    Problem                          Technique                       Why
    -------------------------        ----------------------------    ----------------------------------------------
    Data too large due to the        Principal Component Analysis    Algorithm that reduces total number of features
     large number of features          (PCA)

    Useless features that do not     Feature Selection               Remove features that do not help solve the problem
     help solve ML problem

   PCA
     - is an unsupervised learning algorithm that we use during the modeling and algorithm selection process


------------------------------------------------------
4.9 Helper Tools

  Resources

    AWS Glue Release Review (link no longer valid)

    Found instead:
      AWS Glue: Overview, Review, and Comparison
      https://www.integrate.io/blog/aws-glue-overview-review-and-comparison/

    AWS Glue - How it works
      https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html

    What is AWS Glue? (video: 4 min)
      https://www.youtube.com/watch?v=qgWMfNSN9f4

        ETL service
        Key differences with other ETL
          Serverless

          Schema-Inference
            - crawlers with automatic schema inference for simi-structered and structured data sets
            - crawlers discover your datasets, your file types, extract the schema and store all this information
              in a centralized metadata catalog for later query and analysis

          Autogen ETL scripts
            - generates the scripts that you need to extract, transform, and load your data

       Example: Starting an AD campaign and want to know where to invest your dollars

            - your ad-click semi-structured logs are stored in an S3 bucket (JSON)
            - move your S3 ad-click data to a redshift warehouse so you can analysis which demographics are
              actually contributing to your adoption
            - your user profile data is stored in RDS in structured format
            - can point your Glue crawlers to the RDS user profile database and S3 ad-click bucket, infer the
              data structure inside your files and extract the schemas , then store this info in tables inside
              the data catalog
              - table definitions will refer to your source data and will have the schema info to read, parse
                and query your source data
            - you can then point Glue to these Catalog tables, and it will automatically generate scripts that
              are needed to extract and transform the data into tables in redshift
                - these script will flatten all semi-structure data
                - you can customize these scripts using a Graph interface in the console or edit scripts directly

    Getting Started with AWS Glue ETL (video: 6 min)
      https://www.youtube.com/watch?v=z3HeHlWg88M

          Serverless
            - runs your job on a serverless SPARK platform

    Using Apache Spark with SageMaker - AWS Online Tech Talks (video presentation: 54 min)
      https://www.youtube.com/watch?v=dada2WzCNPM

    AWS re:Invent 2018: Integrate SageMaker with Apache Spark


  Data Preparation tools:
    AWS Glue
    SageMaker
    EMR
    Athena
    Data Pipeline



  AWS Glue
    - fully managed ETL service that allo
    - allows you to run Python or Scala code on your datasets to transform them
    - It allows you to create jobs that transforms your data and gives you the ability to run them on demand or
      schedule them or when another service is triggered.
    - When a transformation process needs to happen it automatically spins up the necessary resources, does the
      transformation and outputs the data into your desired output target.
    Scala:
      - Scala is a general-purpose programming language built on the Java virtual machine.

  AWS Glue overview
    data sources:
     - S3, DynamoDB, RDS, Redshift, Database on EC2
     - input data can be structured, semi-structured, or unstructured data
    Glue Crawler:
     - you set up a crawler that crawls the dataset, looks at the different data types and determines the
       schema or the structure of your dataset.
     - It then creates a data catalog
    Data Catalog
      - it is the metadata or the data types and the important information about your dataset.
      - contains the structure of your dataset that AWS Glue can use to do data preparation jobs or determine
        important information about your data.
      - Once a data catalog has been created we can then set up job
     ETL Job
       - run Scala or Python to transform your data.
       - apply our categorical encoding, our numeric feature engineering, our text feature engineering, apply
         feature selection or any other data preparation technique
       - this Python and Scala code can manually upload or generated by Glue which allows you to edit it directly
         or from within the console.
       - these jobs to run on demand, to run on some schedule, or to run whenever another AWS service is triggered.
         For example, if we have some object that lands onto S3, we can set up a job that automatically runs when
         any new object is put on to a specific S3 bucket.


   AWS Glue flow:

     Data Sources
     ------------
     S3              ------|                  |----------|
     DynamoDB        ------|                  |  Data    |
     RDS             ------| --> Crawler  --> |  Catalog |
     Redshift        ------|                  |          |
     Database on EC2 ------|                  |----------|
                                                   |
                                                   |
                                                   V                     Data Destinations
                                               |----------|              ----------------
                                               |   JOB    |               Athena
                                               |          |               EMR
                                               | Python   | ---->         S3
                                               |          |               Redshift
                                               | Scala    |
                                               |----------|



  AWS Glue Console:
    - within the console, it consists of two major parts, a data catalog and the ETL section
    Data Catalog
      Databases
      Crawlers
    Data Integration and ETL -> ETL jobs
      - choice to choose the type job that we want to create.
      Spark job
        - a Spark job is a fully managed cluster of Apache Spark servers that AWS Glue spins up in the background
          and allows us to run our transformation code on.
        - default job type is Spark (before).
        - Now: three example job types: Visual ETL jobs to join multiple sources; Spark notebook using Pandas; Spark notebook
          using SQL (???)
        - Now; Create 3 types: Visual ETL (default?), Using an interactive code notebook (default: Spark (Python) with
            Ray (Python) option; Script (default: Spark, options: Python Shell or Ray)
        - If we choose the type Spark, then we have the choice of choosing between Python and Scala as our ETL language.
      ETL options:
        - AWS Glue to generate a script for us, provide it with a script or start a brand-new script from scratch.
        - if Python is selected as my ETL language, it generates PySpark code
        AWS Glue PySpark Job Type
           PySpark option supports a list of transform classes to use in the PySpark ETL operations
             - for example, if we wanted to include categorical encoding or the Cartesian product we could choose the
               appropriate PySpark transformation reference to include within our script.
             - can also choose from different output formats (e.g. csv, io, paraquet, json, xml, ...) if we wanted to
               keep or change the output of our dataset.
             - if you choose Python to set up your job, it generates PySpark code for you.
        AWS Glue Scala Job Type
          - if you chose Scala, you can write Scala code to do similar transformations.
          - Choosing Spark as the job type allows you to create PySpark or Scala code to run as your ETL language.
         Python Shell Job Type
           - if you're more comfortable with Python scripts using libraries like NumPy, Scikit-Learn, Pandas, etc.,
             then the job type that you can choose is Python shell.
           - Python Shell allows you to do is create traditional Python scripts to run on your datasets.
           - supports Python libraries include Boto3, collections, NumPy, pandas, SciPy, sklearn, sklearn.feature_extraction,
             sklearn.preprocessing, etc.
         ETL jobs -> notebooks
           - support Apache Zeppelin and Jupyter Notebooks
           - if you create a notebook within AWS Glue, it's actually hosted within the SageMaker service
           - not hooked into jobs but they allow you to do ad-hoc transformations and simple transformations on your datasets

  SageMaker
    Jupyter Notebooks
      - SageMaker allows you to create Jupyter Notebooks that are directly integrated within the SageMaker service.
      - You can spin up Jupyter Notebook instances that are on a fully managed server all within SageMaker.
      - From these notebooks you can use many of the Python libraries that are most common within data preparation
        and data analysis.
      - You can also install other Python libraries by using package managers like conda and pip to install the necessary
        Python packages

  AWS Glue vs SageMaker & Juypter Notebooks for ML data preparation
    AWS Glue
      - use for longstanding transformations or repetitive transformations that you may have to do on some schedule or
       when some action occurs
       - AWS Glue is going to be your go-to service for any transformation or any ETL and data preparation jobs
    SageMaker & Juypter Notebooks
      - use for run quick transformations on your data or ad-hoc commands on your dataset right before you use them within your model


  EMR for data preparation
    - the entire ETL process or data preparation process could be done within the EMR ecosystem
    - EMR is a fully managed Hadoop cluster ecosystem that runs on multiple EC2 instances.
    - EMR allows you to do is pick and choose different frameworks that you want to include within the cluster.
      This allows you to run distributed workloads over many EC2 instances if you have petabytes and petabytes worth of data
    - allows you to pick from different frameworks (or combinations) including Apache Spark, Apache Hive, Presto,
      and Apache HBase.
   - Data scientists use EMR to run deep learning and machine learning tools such as TensorFlow, Apache MXNet, ...
   - could do all of our data preparation directly in EMR but other services within AWS make the job much easier
  EMR with SageMaker
     -  if you already had some ETL processes or some Spark jobs that are already running within our EMR cluster.
        Then, you could directly integrate the SageMaker SDK for Spark within our EMR cluster so we can run all of
        our SageMaker and Spark jobs together within the EMR ecosystem.

  Exam Questions
   1. we have some data that's set up in our EMR cluster, how do we use it within SageMaker?
      - we can use Apache Spark to integrate directly within SageMaker
   2. how do we perform the least amount of effort for ETL jobs?
      - using AWS Glue is going to be the least amount of effort in terms of infrastructure that you have to set up.
      - Since AWS Glue is fully managed we don't have to spin up the infrastructure like we would have to do in EMR.
      - we could use tools like Hive and Apache Spark to transform our data but we would have to manage all of the
        clusters ourselves within EMR

  Elastic Map Reduce (EMR) cluster
   - suppored Kernels incude PySpark, Python, Spark, SparkR that Jupyter Notebooks offers within EMR

  Athena
    - allows you to is run SQL queries on your S3 data.
    - a serverless platform that's all managed by AWS
    - as long as you have your data catalog set up for any of your data sources, you can then query it with Athena.
    - You would setup a data catalog in AWS Glue for your S3 data and then using Athena to query that data.
    - in Athena, you can write your queries, and then see the output results.  You can save results or perform SQL
      transformation on the data and save it

  Data Pipeline
   - allows you to process and move data between different AWS compute services.
   - For example, if you need to move data from DynamoDB, RDS, Redshift, sending it through Data Pipeline,
     doing our ETL jobs on EC2 instances or within EMR, and then landing the output dataset on one of our selected
     target data sources.
   - in most cases Data Pipeline wouldn't be used for an ETL job but for some reason if you didn't want to use Python
     or Scala but you wanted to use programming languages like Java or JavaScript, you could always load these onto EC2
     instances and have the Data Pipeline transform your data or prepare your data for you.
   - whenever you're creating a data pipeline we can choose from several of the different built-in templates that AWS
     offers to migrate our data from one service to another.

   Data Pipeline Flow:
                                                  ETL Jobs             Target Data Sources
                                                --------------         -------------
     DynamoDB   ------->  Data Pipeline  ---->                             Athena
                                                    EC2                    EMR
     RDS        ------->  Data Pipeline  ---->                 ------>     S3
                                                    EMR                    Redshift
     Redshift   ------->  Data Pipeline  ---->



  Which Service to use?

      Data Source                           Data Preparation Tool                Why
      -----------------------
      S3, Redshift, RDS, DynamoDB,          AWS Glue                     Use Python or Scala to transform data and
        on Premise DB                                                    output data into  S3

      S3                                    Athena                       Query data and output results into S3


      EMR                                   PyShark/Hive In EMR          Transform petabytes of distributed data and
                                                                           output data into S3

      RDS, EMR, DynamoDB, Redshift          Data Pipeline                Setup EC2 instances to transform data and
                                                                           output data into S3

   Notes:
     AWS Glue
       - most cases AWS Glue is your go-to tool
       - its a fully managed within AWS so it scales appropriately, you can transform mass amounts of data on demand or
         on some schedule or have a job triggered from another AWS service
     AWS Athena
       - could always use Athena if you knew some SQL queries that you could do to prepare your data.
       - You could simply run the SQL queries on your S3 data and have the results output onto S3
     EMR
       - for petabytes of data you might want to use services like EMR, and use tools like PySpark and Hive to transform
         petabytes of distributed data, and then output the results onto S3
     Data Pipeline
       - could use Data Pipeline if we wanted to set up EC2 instances to transform our data and then output it onto S3.
       - If you wanted to use a language outside of Python or Scala you could set up a data pipeline to output the data
         in the correct format onto S3.


------------------------------------------------------
4.10 Exam Tips

  Resources:

    Whitepaper - Power Machine Learning at Scale
      download PDF (in resources/resources_4_10/aws-power-ml-at-scale.pdf)

    AWS re:Invent 2018: Building Serverless Analytics Pipelines with AWS Glue (video: 61 min)
      https://www.youtube.com/watch?v=S_xeHvP7uMo


    AWS re:Invent 2018: Integrate Amazon SageMaker with Apache Spark
     (provide link returned private view)

    Using Apache Spark on Amazon EMR with SageMaker for End-to-End ML and Data Science Workflows
       (I found: video 41 min)
       https://www.youtube.com/watch?v=RxRENYQBxZU

    Building Serverless ETL Pipelines with AWS Glue (video: 35 min)
      https://www.youtube.com/watch?v=PHYWI4Y9mzs

    Build a Data Lake Foundation with AWS Glue and Amazon S3
      https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/


  Data Preparation

    Data Preparation
      - know what data preparation is and why it is important in ML
      - Understand the different techniques for preparing data

        Data Preparation
          - the process of transforming a dataset using different techniques to prepare it for model training and testing
          - transforming our dataset so that it is ready for Machine Learning

        ----------------------
        Data Preparation types:
          Categorical Encoding
            - converting categorical values into numeric values using mappings and one-hot techniques
          Feature Engineering
            - Transforming features so that they are ready for ML algorithms
            - Ensures the relevant features are used for the problem at hand
          Handling Missing Values
            - removing incomplete, incorrect formatted, irrelevant or duplicate data

        Options for Data Preparation
          SageMaker & Juypter Notebooks
            - use for adhoc data preparation
          ETL jobs in AWS Glue
            - use for reusable data preparation

        ----------------------

    Categorical Encoding
      - know  why categorical encoding is used for certain ML algorithms
      - understand the difference between ordinal and norminal categorical features
      - understand that categorical data is qualitative and continuous data is quantitative
      - Know what one-hot encoding is and when to use it

        ML Algorithm Specific
          - in general, categorical encoding is used when the ML algorithm can not support categorical data
        Text into Numbers
          - we must find a way to turn text attributes into numeric attributes within our dataset
        There is no "Golden Rule"
          - there is no "golden rule" on how to encode your categories (or transform your data in general)
        Many differe Approaches
          - There are many different approaches and each approach can have a different impact on the outcome
            of your analysis

        ----------------------
        Categorical features
          - values that are associated with a group [fits into a finite group of values] or category
          - Qualitative (memory trick - has an 'l' like in Categorical)
          - Discrete
          - examples: spam/not spam; dog breed
        Continuous features
          - values that are expressed as measurable number
          - Quantitative (memory trick - has an 'n' like in Continuous)
          - if you can place the attribute value on a number line, the attribute is a continuous feature
          - examples: home price
          - can be infinite (potentially be as large or small depending on how it is defined)

        Nominal Vs Ordinal
          Nominal - order does not matter
          Ordinal - order does matter  (definition: of a specified order or rank in a series)

        One-hot encoding
          - use one-hot encoding on categorical values,
          - assign a one to the observation that had the value, and a zero to the observations that didn't have the value
          One-hot or not?
            - one-hot encoding is not always a good choice when there are many, many categories
          Grouping
            - Using techniques like group by simarity could create fewer overall categories before encoding
          Mapping Rare values
            - mapping rare values to "other" can help reduce overall number of new columns created
        ----------------------

    Numeric Feature Engineering
      - know what numeric feature engineering is and why it is important
      - know the different techniques used for feature engineering numeric data
      - know the different types of feature scaling and when they should be used
        - Normalization
        - Standardization
      - know what binning is when it should be used

        ----------------------
        Scaling Summary
          Scaling Features
            - some machine learning algorithms can't perform precise calculations on large number scales
            - required for many ML algorithms like linear / non-linear regression, clustering, neural networks, and more
            - scaling features depends on the algorithm you use
          Normalization
            - rescales values from 0 to 1
            - doesn't handle outliers very well
          Standardization
            - rescaling the values that meet the characteristics of the standard normal [Gaussian] distribution
            - less affected by outliers
            - rescales values such that the average/mean value is zero and other values are calculated by their
              z-score where z-score = (x - mean) / std_dev
          Translating back
            - when you're done you can always scale the data back to the original representation

        Quantile Binning Summary:
          Binning
            - used to group together values to reduce the effects on minor observation errors
            - used with numeric values when the small increments of the numeric values really aren't important to ML
            - example: bin person age into certain age groups or life stages.
          Quantile Binning
            - bins equal number values into each bin
          Optimum Number of Bins
            - depends on the characteristics of the variables and its relationship to the target
            - this is best determined through experimentation
        ----------------------

    Text Feature Engineering
      - know what text feature engineering is an why it is important
      - Know the different techniques used for feature engineering text data
        - N-Gram
        - Orthogonal Sparse Bigram (OSB)
        - Term Frequency-Inverse Document Frequency (tf-idf)
        - Removing punctuation
        - lowercase transformation
        - Cartesian product
      - Understanding why feature engineering dates is important
      - Know the questions we can answer when dates are transformed


        ----------------------
       Text Feature Engineering
         - transforming text within our data so Machine Learning algorithms can better analyze it
         - splitting text into bite size pieces
         - these transformations or techniques are done on text data or corpus data which essentially just refers to
           a data set that's just a collection of texts.  This type data is used in natural language processing, speech recognition,
           text-to-speech etc

       Text (Corpus) data examples:
         - a research paper, a dialogue between two people, and a newspaper clipping

       Text Feature Engineering example - breaking apart data to bitsize pieces (or tokenize these values)
           Raw Text: {"123 Main Street, Seattle, WA 98101"} tranformed to
           addresss          city          State          Zip
           123 Main Street   Seattle       WA             98101

       Bag-of-Words
         - tokenizes raw text and creates a statistical representation of the text
         - breaks up text by white spaces into single words
         - simpliest way  to breakup text into tokens
         - from bag-of-words tokenization, we can create word counts and see the different distributions
           of how words are produced throughout the text

       N-Gram
         - an extension of Bag-of-words which produces groups of n size
         - breaks up text by white space into group of words
         - N-gram, size=1 is the same as bag-of-words
         when it is useful:
          - if we're trying to find groups of words and two words beside each other, we could take the N-Gram of size
            equals two or three or four and actually find all the combinations of the words throughout the text.
            - for example: we could use this technique in a spam filter to try to find pairs of words like
              'click here' or 'you're a winner'.

         example: 2-gram:
          Raw Text {"he is a jedi and he will save us"}

          N-gram, size=2: {"he is", "is a", "a jedi", "jedi and", "and he", "he will", "will save", "save us",
                           "he", "is", "a", "jedi", "and", "he", "will", "save", "us"}

          Note: Produces an 2-gram using a slide window, but also produces N-grams;
                When we apply N-Gram it also produces any N-gram less than its size, so it also produces N-Gram of size equals one


       Term Frequency - Inverse Document Frequency (tf-idf)
         - represents how important a word or words are to a give set of text by providing appropriate weights to terms
           that are common and less common in the text
         - shows us the popularity of a word or words in text data by making common words like 'the' or "and" less
           important

         meaning:
              Term Frequency:     How frequent does a word appear
              Inverse:            Makes common words less meaningful
              Document Frequency: Number of documents which terms occur

          Example:
              - Since the tokens "the" and "a" showed up in both documents many times, these are deemed as less important

              Document 1                 Document 2
              word        count          word         count
              ---------   -------        ---------    -------
              the           3             the           2
              force         1             jedi          1
              Luke          1             a             1
              Skywalker     1             empire        1
              a             2


        Tf-idf vectorized (potential exam question):
                  (number of ducments, number of unique n-grams)
                                   (2, 7)  <- for above example, 2 documents, and 7 unique 1-grams


       Cartesian Product Transformation
         - creates a new feature from the combination of two or more text or categorical values
         - combining sets of words together
         - multipling a set or words by another set of words to create a new feature
         - This transformation is used when an interaction between variables is suspected

         Meaning:
           Cartesian: of, relating to, or used in Descartes' mathematical system
                     - Rene Descartes created the Cartesian Co-ordinate System ( X-horizontal and Y-vertical accesses)
           Product : multiplication


         Cartesian Product Example:
              - concatentates each word in the textbook name with an underscore and binding type

           ID       textbook                         binding
           ----    ---------------------------      ------------
           1       Python Data Science Handbook     Softcover              ------> remove
           2       Visualization Analysis & Design  Hardcover                      punctuation
           3       Machine Learning Algorithms      Softcover                         |
                                                                                      V
                                                                           Cartesian product (textbook, binding)
           ID       cartesian_product                                                 V
           ----    ---------------------------
           1       {"Python_Softcover", "Data_Softcover", " Science_Softcover", " Handbook_Softcover"}
           2       {"Visualization_Hardcover", "Analysis_Hardcover", "Design_Hardcover"}
           3       {"Machine_Softcover", "Learning_Softcover", "Algorithms_Softcover"}


        Feature Engineering Dates
          - translating dates into useful information

          Extracted information:
            - was it a weekend, or a week day?
            - was the date the end of a quarter
            - what was the season?
            - what the day a holiday?
            - was it during business hours?
            - was important event like the world cup taking place on this date?

       Text Feature Engineering Summary (more common techniques that may be on the exam):

         Technique                    Function
         -------------                -------------------------------------
         N-Gram                       Splits text by whitespaces with window size n


         Orthogonal Sparse Bigram     Splits text by keeping 1st word in the sliding window and uses delimiter
         (OSB)                          with remaining whitespaces between 2nd word with window size n

         Term Frequency - Inverse     Helps us determine how important words are within multiple documents
         Document Frequency (tf-idf)     - helps filter out common words like 'the' and 'and'

         Removing punctuation          Removes punctuation from text (but not within a word)

         Lowercase                     Lowercase all word in the text

         Cartesian Product             Combines words together to create new features

         Feature Engineering Dates     Extracts information from dates and creates new features
                                         (i.e. day_of_week, is_weekend, month, year)


       Use Cases:

         Problem                       Transformation            Why
         -----------------             ---------------           ---------------------------------
         Matching phrases in spam       N-Gram                   Easier to compare whole phrases like
         emails                                                  "click here now" or "you're a winner"

         Determine subject matter      tf-idf                    Filter leas important words in PDFs.
         of multiple PDFs              Orthogonal Sparse Bigram   Find common word combinations repeated throughout PDFs

         Notes:
            N-Gram:
              - can be used to match whole words or phrases,
            OSB
              - used to find common word combinations or bigrams
            Term Frequency- Inverse Document Frequency
              - helps us determine which words are more important in documents than others.
              - it essentially helps us filter out words like 'the" or 'and'.
             Cartesian Product
               - used when an interaction between variables is suspected

          AWS Data Transformations Reference
          https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html
          -> includes:  N-gram Transformation, Orthogonal Sparse Bigram (OSB) Transformation, Lowercase Transformation
                        Remove Punctuation Transformation, Quantile Binning Transformation, Normalization Transformation,
                        Cartesian Product Transformation
        ----------------------



    Other Feature Engineering
      - know other types of feature engineering covered
        - feature engineering can be used with other data including for audio and images

    Handling Missing Values
      - know why handling missing values is an important step in data preparation
      - know the different techniques used for handling missing values
      - Understand implications of dropping rows
      - Understand what data imputation is

        ----------------------


        Missing Data
          - missing data can be represented in many different ways (null, NaN [Not a Number], NA, None, etc)
          - handling missing values is an important data preparation step
          - most machine learning algorithms are going to skew results if you have a lot of missing data.

        How to handle missing values:

          Technique            Why this workds                         Ease of Use
          -----------------    ----------------------------------      --------------------------------
          Supervised learning  Predicts missing values based on the    Most difficult, can yield best results
                                values of other features

          Mean                 The average value                       Quick and easy, results can vary

          Median               Orders values then chooses value in     Quick and easy, results can vary
                                the middle

          Mode                 Most common value                       Quick and easy, results can vary

          Dropping rows        Removing missing values                 Easiest but can dramatically change datasets



         Which missing value technique to  use?
           - if you have a very small portion of values that are missing, then using statistical methods like mean, median,
             and mode may be the best option because they are quick and easy.
           - But if you have a large amount of values that are missing, then using supervised learning algorithms may be
             the best bet
           - you can always drop the rows, but this is last case scenario

        Imputation:
          - a technique used for replacing the missing data with some substitute value to retain most of the data/information of
            the dataset.
          - anytime that we use statistical methods like mean, median, and mode and supervised learning algorithms to replace
            our missing data, this is known as imputation

        ----------------------


    Feature Selection
      - Know what feature selection is and why it is important
      - Understand the difference in feature selection and Principle Component Analysis (PCA)

        ----------------------

        Feature Selection
          - Selecting the most relevant features from your data to prevent over-complicating the analysis, resolving potential
            inaccuracies, and removes irrelevant features or repeated information
          - used during the modeling and the algorithm selection step
          - deciding what to keep and what to get rid of
          - feature selection is an intuitive step that humans take to reduce the number of features

        Principal Component Analysis (PCA)
          - an unsupervised learning algorithm that reduces the number of features while still retaining as much information
            as possible
          - reduces the total number of features in a dataset
          - if we had a dataset with hundreds of features, using techniques like PCA upfront can cut down the number of features
            because what it does is find linear combinations of features and removes the rest
          - from OReilly Hands-On ML:
            page: 243: Principal component analysis (PCA) is by far the most popular dimensionality reduction algorithm. First, it
              identifies the hyperplane that lies closest to the data, and then it projects the data onto it, just like
              Figure 8-2 [A 3D dataset lying close to a 2D subspace].


        Feature Selection Use Case

          Problem                          Technique                       Why
          -------------------------        ----------------------------    ----------------------------------------------
          Data too large due to the        Principal Component Analysis    Algorithm that reduces total number of features
           large number of features          (PCA)

          Useless features that do not     Feature Selection               Remove features that do not help solve the problem
           help solve ML problem

         PCA
           - is an unsupervised learning algorithm that we use during the modeling and algorithm selection process

        ----------------------

    Data Preparation Tools
      - Know the different AWS services that allow you to transfor data
      - Know what a Data Catalog, Crawlers, and Jobs are in AWS Glue
      - Be able to identify the different services and when to use one transformation tool over another

        ----------------------

        AWS Glue
          https://docs.aws.amazon.com/prescriptive-guidance/latest/serverless-etl-aws-glue/aws-glue-etl.html
          Data Catalog
            - is a centralized metadata repository for all your data assets across various data sources.
            - provides a unified interface to store and query information about data formats, schemas, and sources.
            - When an AWS Glue ETL job runs, it uses this catalog to understand information about the data and ensure
              that it is transformed correctly.
          Crawlers
            - automatically discovers and extracts metadata from a data store, and then it updates the AWS Glue Data
              Catalog accordingly.
            - The crawler connects to the data store to infer the schema of the data. It then creates or updates tables
              within the Data Catalog with the schema information that it discovered.
            - A crawler can crawl both file-based and table-based data stores.
          ETL Jobs
            - supports extracting data from various sources, transforming it to meet your business needs, and loading
              it into a destination of your choice.
            - This service uses the Apache Spark engine to distribute big data workloads across worker nodes, enabling
              faster transformations with in-memory processing.

        Which Date Preparation Service to use?

            Data Source                           Data Preparation Tool                Why
            -----------------------
            S3, Redshift, RDS, DynamoDB,          AWS Glue                     Use Python or Scala to transform data and
              on Premise DB                                                    output data into  S3

            S3                                    Athena                       Query data and output results into S3


            EMR                                   PyShark/Hive In EMR          Transform petabytes of distributed data and
                                                                                 output data into S3

            RDS, EMR, DynamoDB, Redshift          Data Pipeline                Setup EC2 instances to transform data and
                                                                                 output data into S3

         Notes:
           AWS Glue
             - most cases AWS Glue is your go-to tool
             - its a fully managed within AWS so it scales appropriately, you can transform mass amounts of data on demand or
               on some schedule or have a job triggered from another AWS service
           AWS Athena
             - could always use Athena if you knew some SQL queries that you could do to prepare your data.
             - You could simply run the SQL queries on your S3 data and have the results output onto S3
           EMR
             - for petabytes of data you might want to use services like EMR, and use tools like PySpark and Hive to transform
               petabytes of distributed data, and then output the results onto S3
           Data Pipeline
             - could use Data Pipeline if we wanted to set up EC2 instances to transform our data and then output it onto S3.
             - If you wanted to use a language outside of Python or Scala you could set up a data pipeline to output the data
               in the correct format onto S3.

        ----------------------

------------------------------------------------------
4.11 Demo: Data Preparation


  Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\4_11_data_preparation_demo


  From previously collected data, you need to find the following:
    - which percentage of users are male vs female?
    - What are the sages of most of the users?
    - of the users, how many are in their 20s, 30s, 40s, etc.?
    - Convert data from JSON to CSV and store it in S3
    - Transform 'gender' feature to binary value (e.g. male to 1, female to 0)


  Answering Business Questions:
    - Use Athena to query the data collected to answer the business questions
    - CSV format stored in S3 containing First Name, Last Name, Age, Gender (male: 1, female: 0), Longitude, Latitude

  Transform data:
    - Setup an AWS Glue job to run Apache Spark code (or Python shell code or Scala code) to transform
      the data and convert it to CSV formatto transform data and convert to CSV format
    - also apply the mapping to the gender attribute with this AWS Glue job


   AWS Glue & Athena:
      S3  --> Crawler   --->   Data Catalog          --->  Athena
                               Database Tables
                                                     ---> AWS Glue (python and Apache Spark)  --> S3




  # create a AWS Glue crawler to search S3 bucket
  AWS Console  -> AWS Glue --> Data Catalog (left tab) -> Tables -> Add tables using Crawler ->
      Crawler Details: Name: my-user-data-crawler -> Next
      Data Source -> Add a data source: Data Source: S3, S3 path: s3://my-prod-userdata -> Add an S3 data source
      -> Next -> Create New IAM Role: Name: AWSGlueServiceRole-UserDataRole  ->  Create
      -> Next -> Output Configuration:
                     Target Database -> Add Database: Database Details: Name: my-user-database -> Create database
                Target Database: my-user-database
                Crawler Schedule: Frequency: On demand -> Next  -> Create Crawler
                -> Run Crawler
                -> Go to AWS GLue -> Table -> my_prod_userdata -> shows table with columns from files (e.g. geneder, name,
                   location, email, dob, ....) along with data types
                   Also create colunms with names "partition_0" ... "partition_3" as way to separate various folders in bucket

  # use Athena to query data
  AWS Console  -> AWS Athena -->  Query editor (left tab) -> Now shows Data Source: AwsDataCatalog , Database: my-user-database
     -> Tables (left side) -> right click on 3-dots, select "Preview table"
       -> this place the following doe in Query 1: SELECT * FROM "my-user-database"."my_prod_userdata" limit 10;
       -> Run
       -> Create Query 2: to have specific columns:
          old: SELECT "name", "gender", "dob", "location" FROM "my-user-database"."my_prod_userdata" limit 10;
          new: SELECT "name"."first", "name"."last", "dob"."age", "gender", "location"."coordinates"."latitude",
                 "location"."coordinates"."latitude" FROM "my-user-database"."my_prod_userdata" limit 10;
       -> Create Query 3:  report number of records in S3 data:
       SELECT count(*) FROM "my-user-database"."my_prod_userdata"

       -> Create Query 4: percentage of male and female
       -> Create Query 5 : top 5 ages
         New:
          SELECT "dob"."age", COUNT("dob"."age") AS occurances
          FROM my_prod_userdata
          GROUP BY "dob"."age"
          ORDER BY occurances DESC
          LIMIT 5;

       -> Create Query 5 : group ages to 5 bins:
          New:
            SELECT SUM(CASE WHEN "dob"."age" BETWEEN 21 AND 29 THEN 1 ELSE 0 END) AS "21-29",
            SUM(CASE WHEN "dob"."age" BETWEEN 30 AND 39 THEN 1 ELSE 0 END) AS "30-39",
            SUM(CASE WHEN "dob"."age" BETWEEN 40 AND 49 THEN 1 ELSE 0 END) AS "40-49",
            SUM(CASE WHEN "dob"."age" BETWEEN 50 AND 59 THEN 1 ELSE 0 END) AS "50-59",
            SUM(CASE WHEN "dob"."age" BETWEEN 60 AND 69 THEN 1 ELSE 0 END) AS "60-69",
            SUM(CASE WHEN "dob"."age" BETWEEN 70 AND 79 THEN 1 ELSE 0 END) AS "70-79"
            FROM my_prod_userdata;

  # use AWS Glue to query data
  # Create an ETL job to transform data to CSV and converts gender attribute from male/female to 1/0


  # first create output bucket
  AWS Console -> S3 -> bucket name: my-userdata-glue -> Create Bucket
  arn: arn:aws:s3:::my-userdata-glue

  # next UserDataRole-xxxx-Policy attached to UserDataRole  to include new bucket
  AWS Console -> S3 -> bucket name: my-userdata-glue -> Create Bucket

  updated policy:
     {
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"s3:GetObject",
				"s3:PutObject"
			],
			"Resource": [
				"arn:aws:s3:::my-prod-userdata*",
				"arn:aws:s3:::my-userdata-glue"
			]
		}
	]
     }

  AWS Console  -> AWS Glue -->  ETL jobs (left tab) -> select "Visual ETL" ->  Start Fresh, Engine: Spark -> Create Script
     -> Job Details <tab>  -> Name: my-userdata-transformation-job,  Type: Spark, Language: Python3, IAM Role: xxx-UserDataRole

     -> PROVIDED AWS GLUE LAB flow does not work in the current AWS GLUE environment



------------------------------------------------------
4.12 Quiz AWS Certified Machine Learning - Specialist 2020 - Data Preparation Quiz


Question 10

We are analyzing the following text { Hello cloud gurus! Keep being awesome! }. We apply lowercase transformation, remove punctuation
and n-gram with a sliding window of 3. What are the unique trigrams produced? What are the dimensions of the tfidf vector/matrix?

Choices:
 ['hello cloud gurus', 'cloud gurus keep', 'gurus keep being', 'keep being awesome'] and (2, 4)

 ['hello cloud gurus!', 'cloud gurus keep', 'gurus keep being', 'keep being awesome.'] and (1, 4)

 ['hello cloud gurus', 'cloud gurus keep', 'gurus keep being', 'keep being awesome'] and (1, 4)       <-- Correct answer

 ['hello cloud gurus', 'cloud gurus keep', 'keep being awesome'] and (1, 3)

Sorry!

  There is only 1 sentence (or corpus data we are vectorizing) with 4 unique trigrams ('hello cloud gurus', 'cloud gurus keep',
  'gurus keep being', 'keep being awesome'). So the vectorized matrix would be (1, 4).

Correct Answer

  There is only 1 sentence (or corpus data we are vectorizing) with 4 unique trigrams ('hello cloud gurus', 'cloud gurus keep',
  'gurus keep being', 'keep being awesome'). So the vectorized matrix would be (1, 4). Also, remember, since we removed punctuation
  and performed lowercase transformation, those cannot be part of the unique trigrams.


Question 14

A term frequencyinverse document frequency (tfidf) matrix using both unigrams and bigrams is built from a text corpus
consisting of the following two sentences: { Hello world } and { Hello how are you }. What are the dimensions of the tfidf vector/matrix?

Choices:
  (2, 6)

  (2, 9)                  <- correct answer

  (2, 10)

  (2, 5)

  (5, 9)
Sorry!

  There are 2 sentences (or corpus data we are vectorizing) with 5 unique unigrams ('are', 'hello', 'how', 'world', 'you')
  and there are 4 unique bigrams ('are you', 'hello how', 'hello world', 'how are'). Thus, the vectorized matrix would be (2, 9).

Correct Answer

  There are 2 sentences (or corpus data we are vectorizing) with 5 unique unigrams ('are', 'hello', 'how', 'world', 'you')
  and there are 4 unique bigrams ('are you', 'hello how', 'hello world', 'how are'). Thus, the vectorized matrix would be (2, 9).


Question 7 (retry)

What are the programming languages offered in AWS Glue for Spark job types?

Choose 2

  R

  C#

  Java

  Python     <--- Correct answer

  Scala     <--- Correct answer

Good work!

  When choosing Spark as the job type for AWS Glue jobs, you can write code in Scala or Python (Pyspark). You can have the code
  generated for you by AWS or you can provide your own scripts.


Question 10

You are a ML specialist that has been tasked with setting up an ETL pipeline for your organization. The team already has a EMR cluster
that will be used for ETL tasks and needs to be directly integrated with Amazon SageMaker without writing any specific code to connect
EMR to SageMaker. Which framework allows you to achieve this?

Choices:

  Apache Mahout

  Apache Flink

  Apache Spark     <--- Correct answer

  Apache Pig

  Apache Hive

Good work!

  Apache Spark can be used as an ETL tool to preprocess data and then integrate it directly with Amazon SageMaker for model training and hosting.


Question 14

You are a ML specialist preparing some labeled data to help determine whether a given leaf originates from a poisonous plant. The target
attribute is poisonous and is classified as 0 or 1. The data that you have been analyzing has the following features: leaf height (cm),
leaf length (cm), number of cells (trillions), poisonous (binary). After initial analysis, you do not suspect any outliers in any of the
attributes. After using the data given to train your model, you are getting extremely skewed results. What technique can you apply to
possibly help solve this issue?

Choices:

  Standardize the "number of cells" attribute.

  Drop the "number of cells" attribute.

  Normalize the "number of cells" attribute.    <--- Correct Answer

  Apply one-hot encoding to each of the attributes, except for the "poisonous" attribute (since it is already encoded).

Good work!

  Since the "number of cells" attribute is on a scale of trillions and we do not suspect any outliers, we can normalize the values
  within the "number of cells" features so all of our values are between 0 and 1.


------------------------------------------------------

Chapter 5 Data Analysis and Visualization

------------------------------------------------------
5.1 Introduction

  - already fetched, cleaned, and prepared the data
  - now we need to Analysis and Visualize our Data to make sure it is ready to train our model
  - may determined that we need to do additional data preparation or even fetch more data

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->

     -->  Data Analysis and Visualization -->

    Train the Model
      Train Model --->  Evaluate Model --->
    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
5.2 Concepts

  Resources:

    Gapminder Tool:
      https://www.gapminder.org/tools/#$chart-type=bubbles&url=v2


  Gapminder (gapminder.com):
    - Hans Rosling's, PhD, passion for data analysis and visualization, him, his son, and his daughter
      created a site called Gapminder
    - website that allows you to analyze world data with graphics.
    - Using this site, you can analyze and visualize all important data about the world
    - Gapminder is an independent educational non-profit highlighting global misconceptions.


  Seeing your Data
    Relationships
      - do we want to find important relationships within our data?
      - are there any trends or outliers?
    Comparisons
      - Are we comparing different values within our data
    Distributions
      - Do we want to know more about the distributions of our data?
      - are there any outliers?
    Compositions
      - do we want to know what makes up our data?
      - What are the different parts of our data as a whole?
      - use visualizations to understand the composition of our data

   Visualization tools
     Developer tools
       -  consist of tools like Jupyter Notebooks and Python Libraries (pandas, matplotlib, sci-learn)
          that allow you to create visualizations by using Python code.
       - SageMaker
       - You can also use tools like R (not covered)
     Business Intelligence (BI) tools
       Tableau
         - a business intelligence tool that allows you to import data sets and create visualizations
           and interactive graphs with your datasets.
       Amazon Quicksight
         - QuickSight is very similar, but it's built in to AWS
         - Business Intelligence (BI) tool that makes it easy to create visualization from your data
         - Create awesome visualization in the AWS Console


------------------------------------------------------
5.3 Seeing Your Data Relationships

  Resources:

    matplotlib Scatter plot
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html

      matplotlib.pyplot.scatter (x, y, ...)
        - A scatter plot of y vs. x with varying marker size and/or color.

    matplotlib Line Charts
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html

      matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]
       - Plot y versus x as lines and/or markers.

    Kaggle Swedish Insurance dataset
      https://www.kaggle.com/datasets/redwankarimsony/auto-insurance-in-sweden?resource=download


  Relationships
    - visualizing relationships in your data can provide a good general overview, show distrbution,
      and correlation between attributes
    - visualizaint relationships can also help find outliers and extreme values

  Plot types:
    Scatter Plots (scatter charts)
      - graphs plot points along the x and you axis for two values
    Bubble Plots (bubble charts)
      - graphs plot points along the x and y axix for 3 values where bubbles size is the 3rd value

  Use Cases
    Scatter Plots (scatter charts)
      - Example: Is there any relationship between the size of a home and the price?
      - shows the relationships between 2 values
    Bubble Plots (bubble charts)
      - Example: Is there any relationship between the size of a home, the age of the home, and
      - shows the relationships between 3 values
        the price?

  Correlation:
    - use scatter plots to see if there is a:
       - Positive Correlation
         - increasing scatter line
       - Negative Correlation
         - decreasing scatter line
       - No Correlation
         - random values with no visual correlation

   Code: swedish-auto-scatter plot example (demos/5_3_scatterplot_example/scatterplot_example.ipynb)

         >>> # set dir
         >>> cd "pat-cloud-ml-repo/machine-learning-training/acloudguru-machine-learning-aws-certified-course/demo/5_3_scatterplot_example"

         >>> # import python libraries
         >>> # %matplotlib inline
         >>> import sys
         >>> import numpy as np
         >>> import pandas as pd
         >>> import matplotlib.pyplot as plt

         >>> # read in data
         >>> df = pd.read_csv ("swedish_insurance.csv")
         >>> df.head

         >>> # plot graph
         >>> plt.scatter(x=df['X'], y=df['Y'])
         >>> plt.title('Scatter Plot Example')
         >>> plt.xlabel('Number of Claims')
         >>> plt.ylabel('Total Payed Out (in thousands)')
         >>> plt.savefig('Scatter_Plot_Example.png', format='png', dpi=1200)
         >>> plt.show()


Summary:

  scatter plots (charts) vs bubble plots (charts):
    - scatter plots show the relationship between two attributes,
    - bubble plots show the relationship between three attributes.

------------------------------------------------------
5.4 Seeing Your Data Comparisons


  Resources:

    matplotlib Bar Charts
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html

    matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)
      - Make a bar plot.
      - The bars are positioned at x with the given alignment. Their dimensions are given by height and
        width. The vertical baseline is bottom (default 0).
      - Many parameters can take either a single value applying to all bars or a sequence of values,
        one for each bar.

    matplotlib Line Charts
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html

      matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]
       - Plot y versus x as lines and/or markers.


  Comparisons
    - Visualizing comparisons in your data can provide a static snapshot of how different variables
      compare and show how different variables change over time


  Comparison Charts
    Bar Charts
      - graphs that use lines (bars) to mark single variable values.
      - provides a way to lookup and compare values

      - A bar chart (aka bar graph, column chart) plots numeric values for levels of a categorical feature as bars.
      - Levels are plotted on one chart axis, and values are plotted on the other axis.
      - Each categorical value claims one bar, and the length of each bar corresponds to the bars value.
      - Bars are plotted on a common baseline to allow for easy comparison of values.
    Line Charts
      - graphs that use lines to show one or more variables changeing over time

  Bar Chart example1:
      US muscle cars (camaro, covette, mustang) vs price
      - take average price vs car name and show 1 bar for each car
      - bar charts can be vertical (car - x-axis; price y-axis) or horizontal (price - x-axis; car y-axis)

   Bar Chart example2 :
     - comparing the number of star ratings for a mobile application in the app store
     - a bar for each star rating (1-star, 2-star, ..., 5-star)

  Line Chart example2:
      US muscle cars (camaro, covette, mustang) vs price over time
      - price on Y-axix, date on x-axis, and 1 line per car for price over time created by
        plotting the car price points over time and connecting points with different color lines
        for each car

   Line Chart example2 :
     - Ploting the price of Bitcoin over the past 3 years


  Line Chart vs Bar chart Summary
    - can use bar charts to represent a single point in time, or a lookup value for our data,
    - can use line charts to show how our data changes over time.

------------------------------------------------------
5.5 Seeing your Data Distributions


  Resources:

    matplotlib histograms
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html

      matplotlib.pyplot.hist(x, bins=None, range=None, ...)

        - Compute and plot a histogram.

         - This method uses numpy.histogram to bin the data in x and count the number of values in
           each bin, then draws the distribution either as a BarContainer or Polygon. The bins, range,
           density, and weights parameters are forwarded to numpy.histogram.

    matplotlib Box Plots
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html

      matplotlib.pyplot.boxplot(x, notch=None, sym=None, vert=None, ...)

       - Draw a box and whisker plot.

       - The box extends from the first quartile (Q1) to the third quartile (Q3) of the data, with a
         line at the median. The whiskers extend from the box to the farthest data point lying within
         1.5x the inter-quartile range (IQR) from the box. Flier points are those past the end of the
         whiskers. See https://en.wikipedia.org/wiki/Box_plot for reference.

                               median
               Q1-1.5IQR   Q1     :    Q3    Q3+1.5IQR
                            |-----:-----|
            o      |--------|     :     |--------|    o  o
                            |-----:-----|
          flier             <----------->            fliers


    matplotlib Scatter plot
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html

      matplotlib.pyplot.scatter (x, y, ...)
        - A scatter plot of y vs. x with varying marker size and/or color.


  Distributions:
    - visualizing distributions in your data can show how your data is grouped or clustered over
      certain intervals

  Distribution graph types:
    Histograms
      - these graphs put values into buckets or bins and determine a measurement (amount, frequency,
        duration, density, etc)
         - example: quantity vs price with 50 bins
       - a chart that plots the distribution of a numeric variable's values as a series of bars. Each bar typically
         covers a range of numeric values called a bin or class; a bar's height indicates the frequency of data points
         with a value within the corresponding bin.
    Box Plots (box-and-whisker plot)
      - these graphs show a wealth of distribution information
      - you can see things like lowest and highest values outliers and where most of the values fall
      - example: muscle car price distribution, one box plot per car
    Scatter Plots (scatter charts)
      - These graphs plot points along the x and y axis for two values.
      - can show clustering and distribution of your data
      - example: price vs date

   Histogram example:
     - a muscle car's price vs quanity (x-axis: price, y-axis: quantity; with 50 bins)
     - can see a bell curve around the price range where most of the cars were sold

   Histogram vs Bar Chart:
     https://www.storytellingwithdata.com/blog/2021/1/28/histograms-and-bar-charts
     - Histograms visualize quantitative data or numerical data, whereas bar charts display categorical variables.
     - In most instances, the numerical data in a histogram will be continuous (having infinite values). Attempting to display all
       possible values of a continuous variable along an axis would be foolish. Instead, the continuous data is grouped into ranges
       called bins.
     - Unlike histograms, bar charts have a finite set of categorieslike the ten boroughs displayed in the above bar graph.

   Box plots details:
                 IQR = Q3  Q1 = q_n (0.75)  q_n(0.25)

                Q1-1.5IQR   Q1 median  Q3   Q3+1.5IQR
                            |-----:-----|
            o      |--------|     :     |--------|    o  o
                            |-----:-----|

            ^      ^        ^     ^     ^         ^      ^
            |      |        |     |     |         |      |
      outlier(s)   min      |   median  |       max      extreme values
                   value    |   value   |       value    outlier(s)
                          lower        upper
                       quartile        quartile
                         value         value


      - The left edge of the box represents the lower quartile; it shows the value at which the first 25%
        of the data falls up to.
      - The right edge of the box shows the upper quartile; it shows that 25% of the data lies to the
        right of the upper quartile value.
      - The values at which the horizontal lines stop at are the values of the upper and lower values
        of the data.
      - The single points on the diagram show the outliers.

       Note: IQR: inter-quartile range
               - the distance between the upper and lower quartiles

        - box plots can be viewed vertically or horizontally (above)
        - a great way to show distribution of your data

   Scatter Plot example
      - this time, x-axis: year,  y-axis: all car price points
      - will have many price points along most years
      - could use different color for different car models (camaro, corvette, mustang)
      - can distribution, clustering, and outliers

 Distribution Plots Use Cases:

   Histograms & Box plots
     example: showing the distribution of test scores for a given exam

   Scatter plots
      example: Showing the return on investment (ROI) for the amount of money spent and total time
               invested

------------------------------------------------------
5.6 Seeing Your Data Compositions

  Resources:

    matplotlib Pie Charts
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html

      matplotlib.pyplot.pie(x, explode=None, labels=None, ...)
        - Plot a pie chart.
        - Make a pie chart of array x. The fractional area of each wedge is given by x/sum(x).
        - The wedges are plotted counterclockwise, by default starting from the x-axis.


    matplotlib Stacked Area Charts
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.stackplot.html

      matplotlib.pyplot.stackplot(x, *args, labels=(), colors=None, ...)
        - Draw a stacked area plot or a streamgraph.

    matplotlib Stacked Bar Charts
      https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_stacked.html

      Stacked bar chart
        - This is an example of creating a stacked bar plot using bar.


  Compositions
    - visualizing composition of your data show the various elements and what your data is made of

  Composition Visualization types:
    Pie Charts
      - show the composition of your data in a pie format with percentages of the data as a whole
      - example: muscle car dataset with pie representation of the percentage made of each car
                 (camaro, corvettee, mustang)

    Stacked Area Charts
      - show the measurement of various items over longer periods of time
      - example: muscle car dataset using 3 dealership muscle car sales over each month of 2018
                 on stack area chart with: x-axis: 2018 months, y-axis:number of cars sold by each
                 dealership each shown with a different color

    Stacked Column Charts (stacked bar charts)
      - show quantity of various items over [discrete] short periods of time
      - example: muscle car dataset using 3 dealership muscle car sales over each month of 2018
                 on stack bar chart with: x-axis: 5 dealerships, y-axis:number of cars sold
                 by dealership with each month's car sales shown with a different color

  Stack Area Chart Vs Stack Bar Chart
    https://inforiver.com/insights/stacked-area-vs-stacked-column-charts-comparison/

    Choose a stacked area chart when:
      - You want to show a whole and its parts evolving in time
      - The intention is to track changes in multiple cumulative variables
      - There is data for at least 10 points in time
      - You want to show a quick, broad, and rough overview of the numbers

    Choose a stacked column chart when:
      - You want to compare totals and changes in their contributing parts
      - The focus is to showcase comparisons within one category of the total
      - The x-axis category is a short [discrete] time series, or is not a time series
      - You want to enable accurate comparisons of the totals


  Composition Visualizations Use Cases:
    Pie Charts
      Example: Showing the sales figures for each region in a pie chart

    Stacked Area Charts
      Example: Showing the number of products sold by different departments on a weekly basis
               we can see values as a whole during a specific period of time (e.g. week)

    Stacked Bar (Column) Charts
      Example: Showing the quarterly revenue totals for each region


------------------------------------------------------
5.7 Choosing a Visualization
------------------------------------------------------

  Choosing a Visualization
    - picking the right graph, chart, or visualization depends on what you want to see

  Choosing a Visualization type based on what you want to show:
   Relationship
     - use to see relationships or the correlation between two (or 3) values as one value goes up does another value
       go up or does it go down?

     Scatter Plot
       - for relationship between 2 variables
     Bubble Charts
       - for relationship between 3 variables

   Comparison
     - to compare values and see which one is the max or the minimum, or see values changing over time
     Bar Chart
       - value look ups
     Line Chart
       - change over time

   Distribution
     Histogram
       - Single distribution (using bins)
     Box Plots
       - Multi distribution
       - can see the min and max values, median values, any outliers, and where the majority of the dataset values sit
     Scatter Plots
       - Multi distribution
   Composition
     Stacked Bar Chart
       - changing over time
     Stacked Area Chart
       - changing over time
     Pie Chart
       - static

  Heatmaps
    - graphs that represent values as color
    - as the values change, the color representation of the data changes too
    - great way to show all types of variations within your data,
    - it can compare data, show the distribution and show the composition of your data

    - Example: NASA world population density shown a world map -
       - color darkens as population density increases (white to dark red)

    - Example: US drought level over time on US Map
       - color darkens as drought level increases (white to dark red)

    - Example: Cars sold heatmap. x-axis: dealeships, y-axix: car sold by model;
       - color darkens as number cars sold for a model increases (white to dark green)

   Visualization tools
     Developer tools
       -  consist of tools like Jupyter Notebooks and Python Libraries (pandas, matplotlib, sci-learn)
          that allow you to create visualizations by using Python code.
       - SageMaker
       - You can also use tools like R (not covered)
     Business Intelligence (BI) tools
       Tableau
         - a business intelligence tool that allows you to import data sets and create visualizations
           and interactive graphs with your datasets.
       Amazon Quicksight
         - QuickSight is very similar, but it's built in to AWS
         - Business Intelligence (BI) tool that makes it easy to create visualization from your data
         - Create awesome visualization in the AWS Console

------------------------------------------------------
5.8 Data Analysis and Visualization Exam Tips


  Resources:

    Amazon QuickSight Announces General Availability of ML Insights
    https://aws.amazon.com/blogs/big-data/amazon-quicksight-announces-general-availability-of-ml-insights/

    AWS re:Invent 2018: [NEW LAUNCH!] Introducing ML Insights with Amazon QuickSight (video: 61 min)
    https://www.youtube.com/watch?v=FhI1kVABMF0

    Matplotlib: Visualization with Python
    https://matplotlib.org/


  Data Analysis and Visualization
    - Know what data analysis and visualization is and why it is important
    - Understand what QuickSight is and how it can be used

      ---------------------------------------
       Amazon Quicksight
         - Business Intelligence (BI) tool that makes it easy to create visualization from your data
         - Create awesome visualization in the AWS Console
         - It allows you visualize graphs and data within our AWS account that's stored in S3 and other data repositories.
      ---------------------------------------

    - Be able to recognized different types of visualizations and what each visualization represents
      - Represents
      - Comparisons
      - Distributions
      - Compositions

      ---------------------------------------
          Relationships
            - do we want to find important relationships within our data?
            - are there any trends or outliers?
          Comparisons
            - Are we comparing different values within our data
          Distributions
            - Do we want to know more about the distributions of our data?
            - are there any outliers?
          Compositions
            - do we want to know what makes up our data?
            - What are the different parts of our data as a whole?
            - use visualizations to understand the composition of our data
      ---------------------------------------

    - Know what heatmaps are and what they can represent
      ---------------------------------------
        Heatmaps
          - graphs that represent values as color
          - as the values change, the color representation of the data changes too
          - great way to show all types of variations within your data,
          - it can compare data, show the distribution and show the composition of your data

          - Example: NASA world population density shown a world map -
             - color darkens as population density increases (white to dark red)
      ---------------------------------------

   Data Analysis and Visualization Types and usages

      ---------------------------------------

        Choosing a Visualization type based on what you want to show:
         Relationship
           - use to see relationships or the correlation between two (or 3) values as one value goes up does another value
             go up or does it go down?

           Scatter Plot
             - for relationship between 2 variables
           Bubble Charts
             - for relationship between 3 variables

         Comparison
           - to compare values and see which one is the max or the minimum, or see values changing over time
           Bar Chart
             - value look ups
             - bar charts display categorical variables.
           Line Chart
             - change over time

         Distribution
           Histogram
             - Single distribution (using bins)
             - Histograms visualize quantitative data or numerical data
           Box Plots
             - Multi distribution
             - can see the min and max values, median values, any outliers, and where the majority of the dataset values sit
           Scatter Plots
             - Multi distribution
         Composition
           Stacked Bar Chart
             - changing over time
           Stacked Area Chart
             - changing over time
           Pie Chart
             - static
      ---------------------------------------


   Data Analysis and Visualization Tools

      ---------------------------------------
         Visualization tools
           Developer tools
             -  consist of tools like Jupyter Notebooks and Python Libraries (pandas, matplotlib, sci-learn)
                that allow you to create visualizations by using Python code.
             - SageMaker
             - You can also use tools like R (not covered)
           Business Intelligence (BI) tools
             Tableau
               - a business intelligence tool that allows you to import data sets and create visualizations
                 and interactive graphs with your datasets.
             Amazon Quicksight
               - QuickSight is very similar, but it's built in to AWS
               - Business Intelligence (BI) tool that makes it easy to create visualization from your data
               - Create awesome visualization in the AWS Console
      ---------------------------------------


------------------------------------------------------
5.9 Demo: Data Analysis and Visualization
------------------------------------------------------

  Resources:


  Note: Downloaded demo files to:
    C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\5_9_data_analysis_and_visualization_demo
     -> for Car Dataset (car_data.csv)
     -> my-manifest.json
     -> Box Plot Jupyter Notebook (box-plot-example.ipynb)

     jupyter notebook:
      https://jupyter.org/

  Use Case:
    You work for a company that owns several car dealerships.  You've been tasked with analyzing and visualizing
    some different data aspects about sales and possibly use it to predict future sales.
      - which month generates the most sales?
      - which salesman sold the most cars?
      - which dealership sold the most cars.
      - in what year was the average price of a Corvette greater than $100K?
      - which of those cars has several data points in the box plot's far right upper quartile?
     Final results
       - answer above business questions
       - visualization showing your results

    Use Quicksight or Jupyter Notebooks to analyze and visualize the data
      -> drag and drop method, then use Quicksight
      -> more hands on, then use Jupyter Notebooks

    Flow:

       S3 (csv)  ---> Amazon Quicksight (answer most questions)
                 ---> SageMaker / Jupyter Notebook for last question



  # first create car-analysis bucket and upload car_data.csv
  AWS Console -> S3 -> bucket name: car-analysis-data-lab -> Create Bucket
    -> Upload -> Add files -> car_data.csv -> upload


  AWS Console -> Quicksight
       -> Sign up for quicksight (note: 4 free users for 30 days, $250+/mo)
       -> after QS account is created

       -> New analysis -> New Dataset -> Data Source: S3, Data source name: car-data-source,
             Upload a manifest file: my-manifest.json -> Upload
              -> Connect

              -> Edit and Preview -> Examine data
                  -> Shows each colunm (car, year, engine_hp, ...) along with their data type (string, int, decimal, date, ...)
                  -> Could "add a calculated field", to combine two values or creates new fields,
                            add a filter which gets rid of some of these attributes (by unchecking fields).
                  -> Save & Visualize
                     -> (in top center you can name analysis, e.g,: Car Data Anaysis
                     -> left tab options: Visualize, filter (to exclude/include attributes),
                        Story (create stories that are interactive videos or interactive storyboards) ,
                        Parameters (create parameters that allow you to interact with your dashboard features)
                    -> Visualization
                        -> Visual Types (include bar charts, stacked bar charts, Pivot table, Pie Chart, ....)
                        -> 1st business questions: which month generates the most car sales
                           -> select "vertical bar chart" (since comparison question)
                              X-axis: sold month
                              value : car (count)
                                  -> now it shows the number of cars sold each month
                                  -> sold most cars in may (417)

                        -> 2nd business questions: which salesman sold the most car
                           -> Add -> Visual
                           -> select "pie chart" (since composition of data question)
                              Group/Color: saleman
                              value : car (count)
                                  -> now it shows the number of cars sold by each salesman
                                  -> Saleman 3 sold the most cars (1054)

                        -> 2nd-b business questions: which salesman sold the most car
                           -> Add -> Visual
                           -> select "vertical bar chart" (since composition of data question)
                              x-axis: saleman
                              Group/color for bars: car
                                  -> now it shows the number of each car type sold by each salesman
                                  -> Saleman 3 sold most cars by all 3 car types

                        -> 2nd-c business questions: which salesman sold the most car
                           -> Add -> Visual
                           -> select "Cluster bar combo chart" (since composition of data question)
                              x-axis: saleman
                              Group/color for bars: car
                              lines: car(count)
                                  -> now it shows the number of each car type sold by each salesman
                                  -> Saleman 3 sold most cars by all 3 car types

                        -> 3rd business questions: which dealership sold the most car (by car type)
                           -> Add -> Visual
                           -> select "Heat Map" (since composition of data question)
                              rows: dealership
                              columns: car
                              values (measure): car(count)
                                  -> right click -> Format visual -> Data Lables -> check: show data labels
                                  -> now heat map by dealership and by the 3 car types with labels showing number of cars
                                  -> Uptown car sold the most cars

                        -> 3rd-b business questions: which dealership sold the most car
                           -> Add -> Visual
                           -> select "Pie chart" (since composition of data question)
                              Group/Color: dealership
                              value (measure): car(count)
                                  -> Uptown car sold the most cars

                        -> 4th business questions: which year was average corvette sold price over $100k
                           -> Add -> Visual
                           -> select "Line chart" (since ??? question)
                              x-axis: sold (year)
                              value (measure): price(average)
                              color: car
                                  -> in 2005, the  average corvette price was ~$102k

         Quicksight Notes:
           - Quicksight stores all of the datasets in memory in a data store called SPICE.
             SPICE capactiy for region shown in upper left corner.
           - Data source for data set include upload file, S3, Athena, RDS, Redshif, Aurora, Spark, ...
           - Manifest file (JSON): info on what your data file's location (e.g. S3 bucket) and its format (e.g. CSV, ...)

            my-manifest.json contents:
            {
                "fileLocations": [
                    {
                        "URIPrefixes": [
                            "s3://<YOUR_BUCKET_NAME>/"
                        ]
                    }
                ],
                "globalUploadSettings": {
                    "format": "CSV",
                    "delimiter": ",",
                    "textqualifier": "'",
                    "containsHeader": "true"
                }
            }


  AWS Console -> SageMaker
     # missing from lab - creating a domain
     -> Getting Started -> Set up SageMaker Domain -> Set up for Single user (Quick setup)
     -> Add User: pat -> User profile: Name: pat-profile, Execution Role: AmazonSageMaker-ExecutionRole-20240518
        -> ...

    -> Open Studio   -> Notebook (left tab) -> Notebook Instances -> Create notebook instance
       Notebook instance name: my-notebook-inst -> Create an IAM role -> S3: Any S3 bucket -> Create role
       -> Create notebook instance
       -> Open Jupyter -> Files (tab) -> New -> (show all the different frameworks available) -> conda_python3
          -> (rename) car-data-box-plot-ex

       Note:
         SageMaker Examples (tab): lots of example Jupyter Notebooks for various ML datasets
         Conda (tab): can install additional Conda libraries

       car-analysis-data-lab jupyter notebook code:

         >>> # Importing the important libraries
         >>> import boto3
         >>> import pandas as pd
         >>> from sagemaker import get_execution_role
         >>> import numpy as np
         >>> import matplotlib.pyplot as plt
         >>> %matplotlib inline
         >>>
         >>> #Getting the car data from S3
         >>> role = get_execution_role()
         >>> bucket='car-analysis-data-lab'
         >>> data_key = 'car_data.csv'
         >>> data_location = 's3://{}/{}'.format(bucket, data_key)
         >>> print(data_location)
         >>>
         >>> # load data
         >>> df = pd.read_csv(data_location)
         >>> df.head()
         >>>
         >>> df_vet = df[df['car'] == 'Corvette']
         >>> df_mustang = df[df['car'] == 'Mustang']
         >>> df_camaro = df[df['car'] == 'Camaro']
         >>>
         >>> # create box-plot Engine HP for each car
         >>> data = [df_camaro['engine_hp'], df_vet['engine_hp'], df_mustang['engine_hp']]
         >>> plt.boxplot(data, vert=False)
         >>> plt.title('Engine HP Box Plot')
         >>> plt.xlabel('Engine HP')
         >>> plt.yticks([1, 2, 3], ['Camaro', 'Corvette', 'Mustang'])
         >>>
         >>> #Mustang HP shows 4 large HP outliers
         >>> plt.show()

  Clean up:

      -> File -> Close and halt (notebook) -> Logout
         my-notebook-inst -> Action -> Stop -> could also delete

------------------------------------------------------
5.10 Quiz AWS Certified Machine Learning - Specialist 2020 - Data Analysis and Visualization
------------------------------------------------------


Question 3:
  Which visualization types are recommended for displaying the distribution of data?
  choose3

answer info:
  A scatter plot is a good visualization type for displaying multi-distribution data, as it easily shows data clusters,
  minimum and maximum values, and outliers.

  A box plot is a good visualization type for displaying multi-distribution data, as it easily shows the minimum, maximum,
  and mean values of data, as well as outliers.

  A histogram is a good visualization type for displaying the single distribution of data.



Question 1 (retry 1)

You are a ML specialist working for a retail organization. You are analyzing data that has different items at different costs.
You decide to choose the top 5 most expensive items and visually compare their prices. Which visualization can help you achieve this?

Scatter chart

Bar chart

Histogram

Pie chart
Sorry!
Correct Answer

Bar charts can show single values really well. Each product is represented by a bar extended up the the price of the item.




Question 3 (retry 1)

You are a ML specialist working for a retail organization. You are analyzing customer spending data for particular locations
and comparing how it changes over time. You want to visualize the monthly total amount spent at each location over the last 5 years.
Which visualization can you use to help you see this?

Choices:
  Bar chart

  Histogram

  Line chart    <--- Correct Answer

  Scatter chart

Correct Answer

  The key words in this question is how the data changes over time. We can sum up the total amount spent by all customers for
  each month.  Place the months on the x axis and the dollar amount on the y axis. Plot a point for each month and connect each
  point creating a line chart, where each line represent a different location.


Question 9 (retry 1)

Which visualizations help show relationships?

Choose 2

  Scatter plot     <-- Correct Answer

  Bar chart

  Bubble chart     <-- Correct Answer

  Histogram

  Pie chart

  Stacked area chart

  Stacked bar chart

Good work!

  Visualizing relationship in data is important because it shows how different attributes can effect one another. They can also show
  trends and outliers within our data.


Question 10 (retry 1)

Which visualizations help show composition?

Choose 3

  Stacked bar chart   <-- Correct Answer

  Stacked area chart  <-- Correct Answer

  Bar chart

  Histogram

  Bubble chart

  Pie chart           <-- Correct Answer

  Box plot

Good work!

  Visualizing the composition of our data is a great way to show what our data is made of.


Question 11 (retry 1)

Which visualization types are recommended for displaying the distribution of data?

Choose 3

  Histogram              <-- Correct Answer

  Stacked bar chart

  Stacked area chart

  Box plot               <-- Correct Answer

  Line chart

  Scatter plot           <-- Correct Answer

Good work!

  A histogram is a good visualization type for displaying the single distribution of data.

  A box plot is a good visualization type for displaying multi-distribution data, as it easily shows the minimum, maximum, and mean
  values of data, as well as outliers.

  A scatter plot is a good visualization type for displaying multi-distribution data, as it easily shows data clusters, minimum and
  maximum values, and outliers.


Question 1 (retry 2)

Which type of color coded visualization shows the intersection of two dimensions where values fall in a range.

Choices:
  Bubble plot

  Heatmap                                  <-- Correct Answer

  Scatter chart

  Histogram

Correct Answer
  Heatmaps use color to show values increasing or decreasing. They are used in many different domains and can help show
  distribution, correlation, relationships and much more insightful information.


------------------------------------------------------

Chapter 6 Modeling

------------------------------------------------------
6.1 Introduction


  - already fetched, cleaned, and prepared, encoded, categorized the data
  - now we need to train our model

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      - time to train the model
      Train Model --->  Evaluate Model --->

    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
6.2 Modeling Concepts

  What is a Model?
    - taking a problem as described by lots of data and then using a mathematical formula can infer the outcome
      based on what it's learned that can accurately generalize the problem

  Model Components:
                       Generalization
                           ^
     -> Data --------->    |     <------- computation
     |                   Model
     |          -------->        --------->
     | Algorithm                 <---------  Feedback
     |           <-------------------------
     \-------------------------------------


  Developing a Good model:

    What type of generalization are we seeking?
      - Forecasting a number?
      - decide whether customer is more likely to choose A or B?
      - Detect a quality defect?

    Do we really need machine learning?
      - can simple heuristics handle the job?

    How will my ML generalization be consumed?
      - do you need real-time results or can I process the inferences in batch?
      - will ML service be consumed using API call or other systems which will perform additional processing on the data?

    What do we have to work with?
       - what sort of data accurately and fully capture the inputs and outputs of the target generalization?
       - do we have enough data?

    How can I tell if the generalization is working?
      - What method can I use to test accuracy and effectiveness?
      - Should my model have a higher sensitivity to false positives or false negatives?
      - How about Accuracy, Recall, and Precision?


  Model types (at a high level)

                    Supervised            Unsupervised      Reinforcement
                    Learning              Learning          Learning

      Discrete      Classification        Clustering         Simulation-Based Optimization

      Continuous    Regression            Clustering         Autonomous Devices


  Choosing the Right Approach

        Problem                             Approach                         Why

        Detect whether a Financial          Binary Classification            Only two possible outcomes
          transaction is fraud              Approach                           Fraud or Not Fraud

        Predict the rate of deceleration    Heuristic Approach               Well-know formulas involving speed,
          of car when brackes are applied   (No ML Needed)                     inertia, and Friction predict this

        Determine the most efficient path   Simulation-based                 Must figure out the optimal path
          of surface for a lumar rover      Reinforcement Learning             via trail error and improvement

        Determine the breed of dog in       Multi-Class Classificaiton        There are many options (breeds)
          in a photograph                                                      to choose from


  Cascading Algorithms

    Problem: What is the estimated basket size of shoppers who response to our email promotion?

     potential steps include:
         Remove outliers  ---> Identify relevant attributes ---> cluster into groups ---> predict basket size

     stacking algorithms:
       random cut forest (remove outliers)  ---> PCA (determine attributes) ---> K-means (to cluster)
           ---> Linear Learning (forecast size of basket)

  Confusion Matrix
                                          Actual Outcomes

                                 True                        False

                 True      predictedly correctly         predicted incorrectly
      Predicted             (True Positive)              (False Positive)
      outcomes
                 False     predicted incorrectly         predicted correctly
                            (False Negative)              (True Negative)


------------------------------------------------------
6.3 Data Preparation

  Resoures:

    Evaluate predictors
      https://docs.aws.amazon.com/whitepapers/latest/time-series-forecasting-principles-with-amazon-forecast/step-4-evaluate-predictors.html

      Backtesting
        - Backtesting is a technique used by traders and analysts to evaluate the performance of a specific strategy or
          model using historical data
      time series
        - forecasting characteristic time makes it different, in terms of evaluation and backtesting methodology
        rolling forecast evaluation
          - use a series of splits over multiple time points and output the average result leads to more robust and reliable
            backtest results. In each split (covering various time periods), the testing data sequentially follows the training data
       ...


  ML Objective
    - want to Generalize, Not memorize

  Data breakdown:
    Training data: 70 - 80%
    Testing data:  20 - 30%

  Data Preparation:
    1. Randomize
       - make sure training and testing data are equally mixed of data
       - good statistical sampling between the training data and testing data
    2. Split
    3. Train
    4. Test

  Code: Numpy randomization example

         >>> import numpy as np
         >>> import os
         >>>
         >>> # read raw data
         >>> print("Reading raw data from {}".format(raw_data_file)
         >>> raw = np.loadtxt(raw_data_file, delimiter=",")
         >>>
         >>> # split into train/test with 90/10 split
         >>> np.random.seed(42)
         >>> np.random.shuffle(raw)
         >>> train_size = int(0.9 * raw.shape[0])
         >>> train_features = raw[:train_size, :-1]
         >>> train_labels =   raw[:train_size, :-1]
         >>> test_features =  raw[train_size, :-1]
         >>> test_labels   =  raw[train_size, :-1]
         >>>

  Splitting Time series data:
    - if we pick 20% of that straight out of the middle that's not going to be a very good set of data to test
      with because it's not representative of the entire body of data.
    - if we pick certain points randomly across that time series that's not going to be terribly effective either.
      because it is time series data
    - time series data works pretty well if we just slice off the last few months and reserve that for testing.

  K-Fold Cross-Validation Method
    - K-fold cross-validation means that you're gonna specify how many times we're gonna fold this data
    - for each training round, you will hold out a different fold (cross-section) of data for testing
    1. Randomize
    2. Split
    3. Fold
    4. Train
    5. Test
    6. Repeat

  K-Fold Cross-Validation Error Rate
     Error Rate of Rounds
      - check to see if error rate is consistent across each round
        - if error is consistent, this indicates good randomization
        - if error is not consistent, this indicates data was NOT sufficiently randomization

------------------------------------------------------
6.4 SageMaker Modeling

  Amazon Mechanical Turk
    - a  crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs
      to a distributed workforce who can perform these tasks virtually.
    - includes anything from conducting simple data validation and research to more subjective tasks like survey participation,
      content moderation, and more.

   SageMaker Services
     - Ground Truth, training, algorithms, notebooks, hosting, Marketplace

   SageMaker Basic Areas
     Ground Truth
       - set and manage labeling jobs for training datasets using active learning and human labeling
     Notebook
       - access a managed Jupyter Notebook environment
     Training
       - train and tune models
     Inference
       - package and deploy your machine learning models at scale

  SageMaker Model Creation Options (submitting jobs)
    SageMaker Console
    SageMaker SDK
    SageMaker Jupyter Notebook with python libraries
    SageMaker Apache Spark

  Staging Data flow [except for Spark]
     Training Dataset     -------->  S3     --------> Model
     test Dataset         -------->

   Dataset formats supported
     - check the documentation for the respective algorithms for recommendations
     - include x-recordio-protobuf, csv, json, x-image, image
     CSV
       - most sagemaker algorithms accept CSV
       - the target value should be in the first column with no header
       File Metadata
       - be sure metadata Content-type is "text/csv" in S3
          - if file has ".csv" extension, S3 will generally automatically recognize it as 'text/csv'
     Label Metadata
      - for unsupervised algorithms, we specify the absense of labels
      - the target value should be in the first with no headers
      - specify the label size in metadata value at the end of the content type value (label_size=0 if no labels)
        e.g.: S3 Metadata:   Key: Content-Type    Value: text/csv;label_size=0

     Optimized Protobuf recordIO format
       - use for optimal performance
       - this format can take advantage of Pipe model
       - the  advantage of recordIO and Pipe model is that the data can be streamed from S3 to the learning instance
         without downloading data to EBS volume resulting in requiring less EBS space and faster start-up
       - can convert to recordIO format using python, etc.

  CreateTrainingJob API
    High-Level Python Library
      - provided by SageMaker
    Use SDK for Python

    For both High-level Python Library and SDK flow:
      1. Specify the training algorithm
      2. Supply the  algorithm specific hyperparameters
      3. Specify the input and output configurations

  Hyperparameters vs Parameters
    Hyperparameters
      - values set before the learning process
      - It's how you manage, adjust, or tweak the learning process itself
    Parameters
      - value derived via the learning process


  Code: High-Level Python Library example to submits a KMeans model
        Note: 'k' hyperparameter: the number of clusters to create (e.g. k = 10, for 10 clusters)

         >>> from sagemkaer import KMeans
         >>>
         >>> data_location   = 's3://{}/kmeans_highlevel_example/data'.format(bucket)
         >>> output_location = 's3://{}/kmeans_highlevel_example/output'.format(bucket)
         >>>
         >>> kmeans = KMeans (role = role, train_instance_count = 2, train_instance_type = 'ml.c4.8xlarge'
         >>>                  output_path = output_location, k = 10, data_location = data_location)
         >>>
         >>> # use 'fit()' method to start training job
         >>> kmeans.fit(kmeans.record_set(train_set[0])

------------------------------------------------------
6.5 SageMaker Training

  Why GPUs
    - Machine Learning = Mathematics
    - Graphical Processing Units are optimized for math

  CreateTrainingJob API
    High-Level Python Library
      - provided by SageMaker
    Use SDK for Python

  Elastic Container Repository (ECR)
    - contains all sorts of containers for each algorithm that it supports,
    - it has training images and it has inference images.
    - the training activity is very math intensive often times, but not necessarily the inference activities.
       - it's not unusual to train on a high-powered GPU instance and then deploy to production on a less costly CPU instance
    -  two different API calls here .
       CreateTrainingJob API Call:
          - to create a training job and that will reference the training images
          - python boto3 version: create_training()  -> see tutorials/sagemaker_builtin_series/Readme.Linear_Learner.txt
       CreateModel API call:
          - will use the inference images
          - python boto3 version: create_model(**kwargs)  -> see tutorials/sagemaker_builtin_series/Readme.Linear_Learner.txt
    - AWS has created these ECR image repositories in different regions, so it's most common to use a region near you
    Training Images
      K-means
      Blazing Text
      Image Classification
      ...
    inference Images
      K-means
      Blazing Text
      Image Classification
      ...


    Code: Example python calls for Deploy model steps [create_model(), create_endpoint_config(), & create_endpoint()]
          Note:

        >>> import boto3
        >>> client = boto3.client('sagemaker')

        >>> # create sagemaker model
        >>> create_model_api_response = client.create_model(
        >>>                                     ModelName='my-sagemaker-model',
        >>>                                     PrimaryContainer={
        >>>                                         'Image': <insert the ECR Image URI>,
        >>>                                         'ModelDataUrl': 's3://path/to/model/artifact/model.tar.gz',
        >>>                                         'Environment': {}
        >>>                                     },
        >>>                                     ExecutionRoleArn='ARN for AmazonSageMaker-ExecutionRole'
        >>>                             )

        >>> print ("create_model API response", create_model_api_response)

        >>> # create sagemaker endpoint config
        >>> create_endpoint_config_api_response = client.create_endpoint_config(
        >>>                                             EndpointConfigName='sagemaker-neomxnet-endpoint-configuration',
        >>>                                             ProductionVariants=[
        >>>                                                 {
        >>>                                                     'VariantName': <provide your variant name>,
        >>>                                                     'ModelName': 'my-sagemaker-model',
        >>>                                                     'InitialInstanceCount': 1,
        >>>                                                     'InstanceType': <provide your instance type here>
        >>>                                                 },
        >>>                                             ]
        >>>                                        )

        >>> print ("create_endpoint_config API response", create_endpoint_config_api_response)

        >>> # create sagemaker endpoint
        >>> create_endpoint_api_response = client.create_endpoint(
        >>>                                     EndpointName='provide your endpoint name',
        >>>                                     EndpointConfigName=<insert your endpoint config name>,
        >>>                                 )

        >>> print ("create_endpoint API response", create_endpoint_api_response)


    Code: Example python calls for estimator.deploy which does these 3 steps: create_model(), create_endpoint_config(), & create_endpoint()
            # for jumpstart models, estimator.deploy() must also include source uri and entry point file
        >>> clf_predictor = clf_estimator.deploy(
        >>>     initial_instance_count=1,
        >>>     instance_type=infer_instance_type,
        >>>     entry_point="inference.py",
        >>>     image_uri=deploy_image_uri,
        >>>     source_dir=deploy_source_uri,
        >>>     endpoint_name=endpoint_name,
        >>>     model_name=model_name,
        >>> )


   ECR - Common information about Built-in Algorthms
     https://docs.aws.amazon.com/sagemaker/latest/dg/common-info-all-im-models.html
     Algorithm
       - specify ML algorithm being used
     Channel Name
       - named input source for algorithm consumption
       - whether it is for training or testing, etc
     Training IMage and Inference Image Registry Path
       - path to the ECR image for a given algorithm
       - if using the SageMaker python library, it will automatically know most paths
       - e.g. <ecr_path>/blazingtext<tag>
          - <tag> is a form of versioning;
              - use ':1' for latest stable version; AWS recommends using ':1' for production
              - use ':latest' for absolute latest version
     Training Input Mode
       - 'File', 'Pipe', or 'File or Pipe'
     File Type
       - data format (JSON, Parquet, CSV, recordIO (for best performance), ...
     Instance Class
       - GPU, GPUCommon, Single GPU device on one or more instances, CPU, ...



  SageMaker Training Jobs

        Elastic Container Repository         Elastic Container Service
              (ECR)                                (ECS)
                                                                             inputs
             Training Images        ----->        Your Training Job        <----------    S3
                                                  (e.g. K-means)           ---------->
                                                                              outputs
  SageMaker Training Jobs Using Custom Algorithms

        Elastic Container Repository         Elastic Container Service
              (ECR)                                (ECS)
            Your Own Repo
                                                                             inputs
             Your Training     ----------->        Your Training Job        <----------    S3
     ------->   Image        CreateTrainingJob     (e.g. K-means)           ---------->
     Your                     API Call with ECR                               outputs
     Dockerfile               path to your image


    Note:
      nvidia-docker is supported as well for GPU-optimized images


  SageMaker Inference Jobs

        Elastic Container Repository         Elastic Container Service
              (ECR)                                (ECS)
                                                                             inputs
            Inference Images        ----->        Your Model Hosted        <----------    S3  or
                                                                           ---------->    API Gateway

  Information Logged during SageMaker Training/Inference process
    - logged in cloudWatch
    - can be accessed from SageMaker console which has click through to CloudWatch console
    - includes: Arguments provided, Errors during training, Algorithm accuracy statistics, timing of the algorithm
    Common Errors
      - specifying hyperparameter errors (e.g. specifying extra one)
      - invalid hyperparameter value
      - Incorrect protobuf file format
      - Note: SageMaker console is not very verbose when errors occurs, so you need to drill down to cloudwatch


  SageMaker Training Jobs with Apache Spark
    - integrates really well with SageMaker, using the library that SageMaker provides for Apache Spark
    - Using Apache spark with SageMaker may be useful if you happen to already have a fairly large investment in Spark,
      and you want to use that data to create a training job

         Elastic Container          Elastic Container                         Apache Spark
          Repository (ECR)            Service (ECS)                              ---> SageMaker Spark Library
                                                           inputs
           Training Images  -- 2 -->  Training Job        <----------    S3     Convert to protobuf
             Images                                       ---------->    S3     <--- 1 ----
                                                           outputs                     Spark DataFrame
                                                                                       Format
                                     3 Inference    <----------------- 4 -------------------
                                                    ------------------ 4 ------------------->


------------------------------------------------------
6.6 Exam Tips

  Resources:

    Use your own Algorithms with SageMaker [Use Docker containers to build models]
      https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html


    Amazon Machine Learning Key Concepts
      https://docs.aws.amazon.com/machine-learning/latest/dg/amazon-machine-learning-key-concepts.html

       This section summarizes the following key concepts and describes in greater detail how they are used within Amazon ML:
         - Datasources:           contain metadata associated with data inputs to Amazon ML
         - ML Models:             generate predictions using the patterns extracted from the input data
         - Evaluations:           measure the quality of ML models
         - Batch Predictions:     asynchronously generate predictions for multiple input data observations
         - Real-time Predictions: synchronously generate predictions for individual data observations

        ML Model
          An ML model is a mathematical model that generates predictions by finding patterns in your data.
          Amazon ML supports three types of ML models: binary classification, multiclass classification and regression

          The following table defines terms that are related to ML models.
            Term              Definition
            --------------    -------------------------------------------------------------------------------
            Regression        The goal of training a regression ML model is to predict a numeric value.
            Multiclass        The goal of training a multiclass ML model is to predict values that belong to a limited,
                                pre-defined set of permissible values.
            Binary            The goal of training a binary ML model is to predict values that can only have one of two
                                states, such as true or false.
            Model Size        ML models capture and store patterns. The more patterns a ML model stores, the bigger it will be.
                                ML model size is described in Mbytes.
            Number of Passes  When you train an ML model, you use data from a datasource. It is sometimes beneficial to
                                use each data record in the learning process more than once. The number of times that you
                                let Amazon ML use the same data records is called the number of passes.
            Regularization    Regularization is a machine learning technique that you can use to obtain higher-quality
                                models. Amazon ML offers a default setting that works well for most cases.


        Evaluations
          An evaluation measures the quality of your ML model and determines if it is performing well.

          The following table defines terms that are related to evaluations.
            Term              Definition
            --------------    -------------------------------------------------------------------------------
            Model Insights    Amazon ML provides you with a metric and a number of insights that you can use to evaluate
                                the predictive performance of your model.

            AUC               Area Under the ROC Curve (AUC) measures the ability of a binary ML model to predict a
                                higher score for positive examples as compared to negative examples.

            Macro-averaged    The macro-averaged F1-score is used to evaluate the predictive performance of
             F1-Score           multiclass ML models.

            RMSE              The Root Mean Square Error (RMSE) is a metric used to evaluate the predictive performance
                                of regression ML models.

            Cut-off           ML models work by generating numeric prediction scores. By applying a cut-off value, the
                                system converts these scores into 0 and 1 labels.

            Accuracy          Accuracy measures the percentage of correct predictions.

            Precision         Precision shows the percentage of actual positive instances (as opposed to false positives)
                               among those instances that have been retrieved (those predicted to be positive). In other words,
                               how many selected items are positive?

            Recall            Recall shows the percentage of actual positives among the total number of relevant instances
                                (actual positives). In other words, how many positive items are selected?

  From ORiely Hands ON Machine Learning (chapter 3):

       Confustion Matrix:

                        |  Predicted Class     |     Predicted Class
                        |  Negative            |     Positive
                        |  (NOT FRAUD)         |     (FRAUD)
          --------------|----------------------|----------------------
          Actual Class  | True Negative (TN)   |  False Positive (FP)
                        |                      |
          Negative      | NOT FRAUD was        | FRAUD was incorrectly
          (NOT FRAUD)   | correctly predicted  | predicted as NOT FRAUD    ^
                        | as NOT FRAUD         |                           |
          --------------|----------------------|----------------------     |
          Actual Class  | False Negative (FN)  |  True Positive (TP)       |
                        |                      |                           |
          Postive       | FRAUD was incorrectly| FRAUD was correctly       |
          (FRAUD)       | predicted as         | predicted as FRAUD       Precision
                        | NOT FRAUD            |
          --------------|----------------------|----------------------
                                                  <-------- Recall

        Metric for Classification Problems

           Accuracy: (TP + TN)  / (TP + FP + TN + FN)
              - percentage of predictions that were correct:
              - less effective with a lot of true negatives
                 - example: predicting fraud with little to no fraud data

           Precision: (TP)  / (TP + FP)
              - accuracy of positive predictions
              - percentage of positive predictions that were correct:
              - Use when the cost of false positives is high
                 - example: an email is flagged and deleted as spam when it really isn't

           Recall: (TP)  / (TP + FN)
              - also called sensitivity or true positive rate (TPR)
              - percentage of actual positive predictions that were correctly identified:
              - Use when the cost of false negatives is high
                 - example: someone has cancer, but screening does not find it

           F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
             - combined precision and recall score
             - harmonic mean of the precision and recall
             - regular mean treats all values equally, the harmonic mean give more weight to low values
             - classifiers will only get high F1 Score if both recall and precision values are high

           Equation 3-3: F1 score:

           F1 = 2 / [ (1/precision) + (1/recall)]  =  2 x [( precision x recall) / (precision + recall)]

              = (TP)  / [TP + ((FN + FP) / 2)]

  From ORiely Hands ON Machine Learning (chapter 3):

    The ROC Curve:

      The receiver operating characteristic (ROC) curve plots the True Positive rate (TPR) (aka recall) (y-axis) against
        the False Positive Rate (FPR) (x-axis).
      The FPR (aka fall-out) is the ratio of negative instances incorrectly classified as positive.
      The TNR (true negative rate) (aka specificity) is the ration of negative instances that are correctly classified as negative.

               ROC Curve: plots  TPR (aka recall)  versus   FPR
                          -> equivalent: sensitivity (recall) versus 1 - specificity

               TPR or Recall or sensitivity:       TP  / (TP + FN)

               TNR or specificity:                 TN / (FP + TN)

               FPR (or fall-out):                  FP  / (FP + TN) = 1 - TNR
                 - the probability that a false alarm will be raised: that a positive result will be given when the true value is negative

               FNR              :                  FN  / (TP + FN)
                 - the probability that a true positive will be missed by the test.



  From ORiely Hands ON Machine Learning (chapter 3):

      AUC (Area Under the Curve):
      - one way to compare classifiers is to measure the AUC.
      - a perfect Classifier will have and ROC AUC equal to 1.
      - a purely random Classifier will have and ROC AUC equal to 0.5.

      Code:
        from sklearn.metrics import roc_auc_score
        roc_auc_score(y_train_5, y_scores)

    ROC (receiver operating characteristic) curve vs PR (Precision/recall) curve:

      As a rule of thumb, we should prefer the PR curve whenever the positive class is rare or when we care more
      about the false positives than the false negatives.
      Otherwise, use the ROC curve.


  Exam Tips

    Model Design:
      - select a model that's a good fit for your objective.
      - choosing the proper machine-learning approach for your objective (regression, binary classification,
        multi-classification, etc)
      - Choose proper evaluation strategies for your model,
      - steps for training a model

    Data Preparation:
      - Understand the concepts of Training Data and Testing Data
      - Identify potential biases introduced in an insufficient split strategy
      - Know when to use sequential splits (e.g. with time series data) versus randomized splits versus K-fold cross-validation
        and what additional measures coud be used to increase training data value

    Model Training:
      - Multiple Options for training: SageMaker Console, Apache Spark, Custom Code via SDK, Jupyter Notebook

      - Be familair with default data types SageMaker algorithms support and the recommended format for best performance
        ---------------------------------------

         Dataset formats supported
           - check the documentation for the respective algorithms for recommendations
           - include x-recordio-protobuf, csv, json, x-image, image
           CSV
             - most sagemaker algorithms accept CSV
             - the target value should be in the first column with no header
             File Metadata
             - be sure metadata Content-type is "text/csv" in S3
                - if file has ".csv" extension, S3 will generally automatically recognize it as 'text/csv'
           Label Metadata
            - for unsupervised algorithms, we specify the absense of labels
            - the target value should be in the first column with no headers
            - specify the label size in metadata value at the end of the content type value (label_size=0 if no labels)
              e.g.: S3 Metadata:   Key: Content-Type    Value: text/csv;label_size=0

           Optimized Protobuf recordIO format
             - use for optimal performance
             - this format can take advantage of Pipe model
             - the  advantage of recordIO and Pipe model is that the data can be streamed from S3 to the learning instance
               without downloading data to EBS volume resulting in requiring less EBS space and faster start-up
             - can convert to recordIO format using python, etc.
        ---------------------------------------
      - Know the difference between Hyperparameter and Parameter

        ---------------------------------------
        Hyperparameters vs Parameters
          Hyperparameters
            - values set before the learning process
            - It's how you manage, adjust, or tweak the learning process itself
          Parameters
            - value derived via the learning process
        ---------------------------------------

      - Understand the repository and container image concept for SageMaker Training
      - Understand the process if you wish to provide your own algorithm
      - Understand the procress for Apache Spark to interact with SageMaker
        ---------------------------------------

        SageMaker Training Jobs

              Elastic Container Repository         Elastic Container Service
                    (ECR)                                (ECS)
                                                                                   inputs
                   Training Images        ----->        Your Training Job        <----------    S3
                                                        (e.g. K-means)           ---------->
                                                                                    outputs
        SageMaker Training Jobs Using Custom Algorithms

              Elastic Container Repository         Elastic Container Service
                    (ECR)                                (ECS)
                  Your Own Repo
                                                                                   inputs
                   Your Training     ----------->        Your Training Job        <----------    S3
           ------->   Image        CreateTrainingJob     (e.g. K-means)           ---------->
           Your                     API Call with ECR                               outputs
           Dockerfile               path to your image


          Note:
            nvidia-docker is supported as well for GPU-optimized images


        SageMaker Inference Jobs

              Elastic Container Repository         Elastic Container Service
                    (ECR)                                (ECS)
                                                                                   inputs
                  Inference Images        ----->        Your Model Hosted        <----------    S3  or
                                                                                 ---------->    API Gateway

        Information Logged during SageMaker Training/Inference process
          - logged in cloudWatch
          - can be accessed from SageMaker console which has click through to CloudWatch console
          - includes: Arguments provided, Errors during training, Algorithm accuracy statistics, timing of the algorithm
          Common Errors
            - specifying hyperparameter errors (e.g. specifying extra one)
            - invalid hyperparameter value
            - Incorrect protobuf file format
            - Note: SageMaker console is not very verbose when errors occurs, so you need to drill down to cloudwatch


        SageMaker Training Jobs with Apache Spark
          - integrates really well with SageMaker, using the library that SageMaker provides for Apache Spark
          - Using Apache spark with SageMaker may be useful if you happen to already have a fairly large investment in Spark,
            and you want to use that data to create a training job

               Elastic Container          Elastic Container                         Apache Spark
                Repository (ECR)            Service (ECS)                              ---> SageMaker Spark Library
                                                                 inputs
                 Training Images  -- 2 -->  Training Job        <----------    S3     Convert to protobuf
                   Images                                       ---------->    S3     <--- 1 ----
                                                                 outputs                     Spark DataFrame
                                                                                             Format
                                           3 Inference    <----------------- 4 -------------------
                                                          ------------------ 4 ------------------->

        ---------------------------------------


------------------------------------------------------
6.7 Demo: Modeling

  Resouces:

    Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\6_7_demo_modeling

    UFO Sightings Dataset (uso_fullset.csv)
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter6/ufo_fullset.csv

    Jupyter Notebook (ufo-modeling-lab.ipynb)
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter6/ufo-modeling-lab.ipynb

    K-Kmeans SageMaker Documentation (no longer at provided location - now at below location)
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html

    K-Kmeans Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html


    SageMaker trained model Deserialization
      https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
      SageMaker Output model
        - SageMaker models are stored as 'model.tar.gz" in the S3 bucket specified in 'OutputDataConfig' S3OutputPath parameter
          of the create_training_job call.
       -  When model.tar.gz is untarred, it contains model_algo-1, which is a serialized Apache MXNet object.
       - use the following to load the k-means model into memory and view it
         import mxnet as mx
         print(mx.ndarray.load('model_algo-1'))


  From ORiely Hands ON Machine Learning (chap 9):

    The K-Means Algorithm
      The K-Means algorithm is one of the fastest clustering algorithms, and also one of the simplest:
        - First initialize k-centroids randomly: e.g., k-distinct instances are chosen randomly from the dataset and
          the centroids are placed at their locations.
        - Repeat until convergence (i.e., until the centroids stop moving):
           - Assign each instance to the closest centroid.
           - Update the centroids to be the mean of the instances that are assigned to them.


  Identify Sensor Locations
    - deploy a network of 10 sensors across the globe
    - locate these sensors in the center of the 10 most likely locations for UFO sightings
    - What type of generalization are trying to make?
    - Do we have enough data? what does our data look like?
    - What algoritm can help use solve this problem?
    - Where should we launch these sensors?

  Dataset attributes:
      "reportedTimestamp","eventDate","eventTime","shape","duration" (how many seconds),"witnesses","weather",
      "firstName","lastName", "latitude","longitude","sighting" (always 'yes'),"physicalEvidence" (Y=Yes, N=No),
      "contact" (Y=Yes, N=No),"researchOutcome ("explained", "unexplained", "probable")

  Final Results:
    - 10 locations (latitude and longitude
    - Map of locations

  SageMaker and K-means
    - use SageMaker and the K-means clustering algorithm to locate teh 10 best locations
    - Visualize teh locations in QuickSight
    - K-means is an unsupervised learning algorithm that attempts to find discrete groupings within data
      - use latitude and longitude as the values we want to group together (find similarity in)
      - set K-Means hyperparameter k = 10 since we want to find 10 locations


  Steps:
    - create S3 bucket and upload ufo data
    - In SageMaker:
       - Create Jupyter Notebook and prepare the data
       - train model using K-means
       - inference from model the 10 locations
    - In QuickSight:
       - visualize the data

  # first create modeling-ufo-lab bucket and upload ufo_fulset.csv and ufo-modeling-lab.ipynb
  AWS Console -> S3 -> bucket name: modeling-ufo-lab1 -> Create Bucket
    -> Create folder -> ufo_dataset -> create
    -> ufo_dataset folder -> Upload -> Add files -> ufo_fulset.csv -> upload
    -> Upload -> Add files -> ufo-modeling-lab.ipynb -> upload


  AWS Console -> SageMaker
    # actually, I reused last lab instance
    -> Notebook (left tab) -> Notebook Instances -> Create notebook instance
       Notebook instance name: my-notebook-inst,  instance type: ml.t3.medium, platform: AL2, Jupyter Lab 3
          -> Create an IAM role -> S3: Any S3 bucket -> Create role
       -> Create notebook instance
       -> Open Jupyter -> Upload -> ufo-modeling-lab.ipynb -> select


  Code: UFO modeling lab code

         >>> # Import libraries
         >>> import pandas as pd
         >>> import numpy as np
         >>> from datetime import datetime

         >>> import boto3
         >>> from sagemaker import get_execution_role
         >>> import sagemaker.amazon.common as smac

         >>> # Step 1: Loading the data from Amazon S3
         >>> role = get_execution_role()
         >>> bucket = 'modeling-ufo-lab1'
         >>> prefix = 'ufo_dataset'
         >>> data_key = 'ufo_fullset.csv'
         >>> data_location = 's3://{}/{}/{}'.format(bucket, prefix, data_key)

         >>> df = pd.read_csv(data_location, low_memory=False)
         >>> df.head()

         >>> df.shape()
             (18000, 15)

         >>> # Step 2: Cleaning, transforming, and preparing the data
         >>> # Create another DataFrame with just the latitude and longitude attributes
         >>> df_geo = df[['latitude', 'longitude']]
         >>> df_geo.head()
         >>> df_geo.info()
             <class 'pandas.core.frame.DataFrame'>
             RangeIndex: 18000 entries, 0 to 17999
             Data columns (total 2 columns):
              #   Column     Non-Null Count  Dtype
             ---  ------     --------------  -----
              0   latitude   18000 non-null  float64
              1   longitude  18000 non-null  float64
             dtypes: float64(2)
             memory usage: 281.4 KB

         >>> # check for missing values
         >>> missing_values = df_geo.isnull().values.any()
         >>> print('Are there any missing values? {}'.format(missing_values))
         >>> if(missing_values):
         >>>     df_geo[df_geo.isnull().any(axis=1)]

             Are there any missing values? False

         >>> # Next, let's go ahead and transform the pandas DataFrame (our dataset) into a numpy.ndarray.
         >>> # When we do this each row is converted to a Record object.
         >>> # According to the documentation, this is what the K-Means algorithm expects as training data.
         >>> # This is what we will use as training data for our model.

         >>> # dataframe.values.astype('float32') returns numpy.ndarray of float32 objects
         >>> data_train = df_geo.values.astype('float32')
         >>> data_train
             array([[  47.329445, -122.57889 ],
                   [  52.664913,   -1.034894],
                   [  38.951668,  -92.333885],
                   ...,
                   [  36.86639 ,  -83.888885],
                   [  35.385834,  -94.39833 ],
                   [  29.883055,  -97.94111 ]], dtype=float32)

         >>> # Step 3: Create and train our model
         >>> # In this step we will import and use the built-in SageMaker K-Means algorithm. We will set the number of cluster to 10 (for
         >>> # our 10 sensors), specify the instance type we want to train on, and the location of where we want our model artifact to live.
         >>>
         >>> # See the documentation for input training
         >>> #   https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html

         >>> from sagemaker import KMeans
         >>>
         >>> num_clusters = 10
         >>> output_location = 's3://' + bucket + '/model-artifacts'
         >>>
         >>> kmeans = KMeans(role=role,
         >>>                instance_count=1,
         >>>                instance_type='ml.c4.xlarge',
         >>>                output_path=output_location,
         >>>                k=num_clusters)


         >>> job_name = 'kmeans-geo-job-{}'.format(datetime.now().strftime("%Y%m%d%H%M%S"))
         >>> print('Here is the job name {}'.format(job_name))
             Here is the job name kmeans-geo-job-20240624172337

         >>> # use kmeans.record_set() to make sure training data is in correct format
         >>> %%time
         >>> kmeans.fit(kmeans.record_set(data_train), job_name=job_name)

             INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.
             INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.
             INFO:sagemaker:Creating training-job with name: kmeans-geo-job-20240620221726
             2024-06-20 22:17:31 Starting - Starting the training job...
             2024-06-20 22:17:48 Starting - Preparing the instances for training...
             2024-06-20 22:18:15 Downloading - Downloading input data...
             2024-06-20 22:18:55 Downloading - Downloading the training image.......
             . . .
             2024-06-20 22:20:39 Completed - Training job completed
             Training seconds: 144
             Billable seconds: 144
             CPU times: user 708 ms, sys: 44.7 ms, total: 753 ms
             Wall time: 3min 43s

             # SageMaker store output at:
             # Amazon S3 -> modeling-ufo-lab1 model-artifacts/kmeans-geo-job-20240624172337/output/model.tar.gz


         >>> # Step 4: Viewing the results
         >>> # In this step we will take a look at the model artifact SageMaker created for us and stored onto S3. We have to do a few special things
         >>> # to see the latitude and longitude for our 10 clusters (and the center points of those clusters)
         >>>
         >>> # See the documentation of deserilization here
         >>> #     https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization
         >>>
         >>> # At this point we need to "deserilize" the model artifact. Here we are going to open and review them in our notebook instance.
         >>> # We can unzip the model artifact which will contain model_algo-1. This is just a serialized Apache MXNet object. From here we
         >>> # can load that serialized object into a numpy.ndarray and then extract the clustered centroids from the numpy.ndarray.
         >>>
         >>> # After we extract the results into a DataFrame of latitudes and longitudes, we can create a CSV with that data, load it onto S3 and
         >>> # then visualize it with QuickSight.

         >>> import os
         >>> model_key = 'model-artifacts/' + job_name + '/output/model.tar.gz'

         >>> # download model output from S3 to notebook instance, then unzip to extract "model_algo-1" output file
         >>> boto3.resource('s3').Bucket(bucket).download_file(model_key, 'model.tar.gz')
         >>> os.system('tar -zxvf model.tar.gz')
         >>> os.system('unzip model_algo-1')

         >>> !pip install mxnet

         >>> # convert model output to ndarray
         >>> import mxnet as mx
         >>> Kmeans_model_params = mx.ndarray.load('model_algo-1')


         >>> # convert model output ndarray to panda DF and add column names
         >>> cluster_centroids_kmeans = pd.DataFrame(Kmeans_model_params[0].asnumpy())
         >>> cluster_centroids_kmeans.columns=df_geo.columns
         >>> cluster_centroids_kmeans

              	latitude 	longitude
             0 	35.336853 	-98.741165
             1 	49.873829 	-3.797668
             2 	-4.999058 	112.205666
             3 	34.935562 	-118.706398
             4 	31.744030 	-82.604263
             5 	41.340214 	-75.298149
             6 	46.147224 	-119.763893
             7 	62.170712 	-148.799774
             8 	40.933262 	-87.650185
             9 	1.034187 	-67.699471

         >>> # Let's go ahead and upload this dataset onto S3 and view within QuickSight

         >>> # Note: StringIO model is an in-memory file-lib object. This object can be used as input or output to the most function
         >>> #   that would expect a standard file object
         >>> from io import StringIO

         >>> # store CSV results data on S3 in bucket at: 'results/ten_locations_kmeans.csv
         >>> csv_buffer = StringIO()
         >>> cluster_centroids_kmeans.to_csv(csv_buffer, index=False)
         >>> s3_resource = boto3.resource('s3')
         >>> s3_resource.Object(bucket, 'results/ten_locations_kmeans.csv').put(Body=csv_buffer.getvalue())


AWS console -> QuickSight -> create account (again)

   QuickSight -> Point Dataset to S3 results file -> Points on Map (visualization) -> Geospatial Data -> add longitude & latitude


------------------------------------------------------
6.8 Quiz AWS Certified Machine Learning - Specialist 2020 - Modeling

Question 5

  You want to be sure to use the most stable version of a training container. How do you ensure this?

incorrect answer:
  Use the :latest tag when specifying the ECR container path.

correct answer
  Use the :1 tag when specifying the ECR container path.
Good work!

answer info:
  When specifying a training or inference container, use the :1 tag at the end of the path to use the stable version. If you want the
  latest version, use :latest but that might not be backward compatible.


  https://docs.aws.amazon.com/sagemaker/latest/dg-ecr-paths/sagemaker-algo-docker-registry-paths.html
  For the registry path, use the :1 version tag to ensure that you are using a stable version of the algorithm/DLC. You can reliably
  host a model trained using an image with the :1 tag on an inference image that has the :1 tag. Using the :latest tag in the registry
  path provides you with the most up-to-date version of the algorithm/DLC, but might cause problems with backward compatibility. Avoid
  using the :latest tag for production purposes


Question 8

  You are working on a model that tries to predict the future revenue of select companies based on 50 years of their historical data
  (from public financial filings). What might be a strategy to determine if the model is reasonably accurate?

correct answer
  Use a set of the historic data as testing data to back-test the model, then compare the results to the actual historical results.

anwser info:
Good work!

  Time-series data should be typically training and validated in the existing order. A common method to validate time-series data is
  backtesting. Backtesting is the replaying the historical data as if it were new data, then evaluating the model on how successful it
  predicted the historic values.



Question 1 (retry 1)

  Which of the following mean that our algorithm predicted false but the real outcome was true?

Choices:

  False Negative    <--- Correct Answer

  True Negative

  True Positive

  False Affirmative

  False Positive
Good work!

  A false negative is when the model predicts a false result but the real outcome was true.


Question 4 (retry 1)

  When you issue a CreateModel API call using a built-in algorithm, which of the following actions would be next?

Choices:

  Sagemaker provisions an EC2 instances using the appropriate AMI for the algorithm selected from the regional container registry.

  SageMaker provisions an EMR cluster and prepares a Spark script for the training job.

  SageMaker launches an appropriate inference container for the algorithm selected from the global container repository.

  SageMaker launches an appropriate training container from the algorithm selected from the regional container repository.   <--- incorrectly selected

  SageMaker launches an appropriate inference container for the algorithm selected from the regional container repository.   <--- Correct Answer

  Sagemaker provisions an EC2 instances using the appropriate AMI for the algorithm selected from the global container registry.

Sorry!
Correct Answer

  CreateModel API call is used to launch an inference container. When using the built-in algorithms, SageMaker will automatically reference
  the current stable version of the container.


Question 5 (retry 1)

  We are designing a binary classification model that tries to predict whether a customer is likely to respond to a direct mailing of our
  catalog. Because it is expensive to print and mail our catalog, we want to minimize the number of potential sales that we lose. When
  considering if the customer will buy something, what outcome would we want to minimize in a confusion matrix?

Choices:
  False positive

  False negative   <--- Correct Answer

  True negative

  False affirmative
Good work!

  A false negative occurs when the model predicts that a customer won't buy, but in reality, they would have. In the context of the
  question, by not sending a catalog to these customers, the company misses out on potential sales. Minimizing false negatives means
  the company is trying to ensure they don't overlook potential buyers. This aligns with the goal of minimizing the number of potential
  sales that are lost.


Question 8 (retry 1)

  Your company currently has a large on-prem Hadoop cluster that contains data you would like to use for a SageMaker training job.
  Your cluster is equipped with Mahout, Flume, Hive, Spark and Ganglia. How might you most efficiently use this data?"

Choices:
  Use Data Pipeline to make a copy of the data in Spark DataFrame format. Upload the data to S3 where it can be accessed by the
  SageMaker training jobs.

  Using EMR, create a Scala script to export the data to an HDFS volume. Copy that data over to an EBS volume where it can be read
  by the SageMaker training containers.

  Use Mahout on the Hadoop Cluster to preprocess the data into a format that is compatible with SageMaker. Export the data with Flume
  to the local storage of the training container and launch the training job.

  Using the SageMaker Spark Library, convert the Spark DataFrame format into protobuf and load onto S3. Then, use SageMaker as normal. <--- Correct

Good work!

  SageMaker provides an Apache Spark library, in both Python and Scala, that you can use to easily train models in SageMaker using
  org.apache.spark.sql.DataFrame data frames in your Spark clusters. You can convert the input DataFrame to the protobuf format by
  selecting the features and label columns from the input DataFrame and uploading the protobuf data to an Amazon S3 bucket. The protobuf
  format is efficient for model training in SageMaker.

  Reference: Use Apache Spark with Amazon SageMaker


Question 9 (retry 1)

  We are using a CSV dataset for unsupervised learning that does not include a target value. How should we indicate this for training
  data as it sits on S3?

Choices:
  Include label_size=0 appended to the Content-Type key.

  Enable pipe mode when we initiate the training run.

  CSV data format should not be used for unsupervised learning algorithms.

  SageMaker will automatically detect the data format for supervised learning algorithms.

  Include a reserved word metadata key of "ColumnCount" for the S3 file and set it to the number of columns.

Good work!

  To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type.
  For example, in this case 'text/csv;label_size=0'



Question 11 (retry 1)

  We are running a training job over and over again using slightly different, very large datasets as an experiment. Training is taking a
  very long time with your I/O-bound training algorithm and you want to improve training performance. What might you consider?

Choose 2

  Make use of pipe mode to stream data directly from S3.       <--- Correct Answer

  Convert the data format to an Integer32 tensor.

  Use the SageMaker console to change your training job instance type from an ml.c5.xlarge to a r5.xlarge.

  Make use of file mode to stream data directly from S3.

  Convert the data format to protobuf recordIO format.       <--- Correct Answer

Good work!

  The combination of using the protobuf recordIO format and pipe mode will result in improved performance for I/O-bound algorithms
  because the data can be streamed directly from S3 versus having to be first copied to the instance locally.


------------------------------------------------------
6.9 Introducing Jupyter Notebooks (Amazon SageMaker)
------------------------------------------------------



In this lab, we're going to use a Jupyter Notebook with Amazon SageMaker to use an existing notebook, execute existing code
written by others, and also write and execute our own code. Basic knowledge of Python will be helpful, but not required.



  Note: Downloaded demo files to:
    C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\6_9_introducing_juypter_notebooks_demo

The files used in this lab can be found on our GitHub.
   https://github.com/pluralsight-cloud/AWS-Certified-Machine-Learning-Specialty-Labs/tree/main/Introducing-Jupyter-Notebooks-Amazon-SageMaker

Learning objectives
  - Open the existing Jupyter Notebook
  - Execute the Demonstration code
  - Write a new section to the code
  - Perform basic data analytics


    Lab Diagram

                                ---> Markdowns
       SageMaker ---> Notebook
                                ---> Code Cells     --->  Data Objects
                                                    --->  pandas
                                                          External Libraries


Solution
Log in to the Lab Environment

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your
    personal account credentials, which may be active in your main window, are not used for the lab.  Log in to the AWS Management
    Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Navigate to the Jupyter Notebook

    In the search bar on top, type "Sagemaker" to search for the Sagemaker service.
    Click on the Amazon SageMaker result to go directly to the Sagemaker service.
    Click on the Notebook Instances button to look at the notebook provided by the lab.
    Check to see if the notebook is marked as InService. If so, click on the Open Jupyter link under Actions.
    Click on the AssessmentReviewNotebook.ipnyb file.
    Wait for the kernel to spin up. You'll see a green button that reads Kernel ready in the upper right momentarily when the kernel is finished spinning up.

Demo Orientation

The first section of the lab is intended to demonstrate the basic functionality of Jupyter Notebooks, and provide some familiarization with the basics of Python.

    Run the first code cell to execute basic variable assignment, perform simple arithmetic, and print the result to the console.
    Run the second code cell to do the same, but with multiplication using the earlier assigned variables.
    Run the third code cell to reassign the two previously assigned variables.
    Run the second code cell again to see that the result has changed, since the variables have changed.
    Run the fourth code cell to import the random library, and define the function is_odd_or_even, which we'll use in the next cell.
    Run the fifth code cell to use our earlier function to check whether both our earlier assigned variables, and a random number are either odd or even numbers.

Initializing our Notebook

    Under the Initializing our Notebook section, run the first code cell to import the necessary pandas and matplotlib packages. Both are installed in our environment by default.
    Run the second code cell to import the existing CSV into the dataset variable, and to print how many records (rows) are in the CSV.
    Run the third cell to print out the first several rows in the table.

Basic Data Analysis

    Under the Basic Data Analysis section, run the only code cell to confirm how many unique attempts (rows) exist for each unique question ID.

Diving Deeper

    Under the Diving Deeper section, select the empty code cell, use the dropdown in the toolbar to change it from a Code cell, to a Markdown cell, and add a description about what we're about to do. For example:

        Let's begin by evaluating the success rate of each question and dientify any potential outliers. We can then further examine these outlier questions to ascertain the quality of the question itself, or determine if the original training material needs enhancement.

    Run the Markdown cell to render the text.

    Create a new cell, and enter the following code to perform the necessary analysis:

    success_rate = dataset.groupby('questionId')['isCorrect'].mean()

    plt.bar(success_rate.index, success_rate.values)

    for i, rate in enumerate(success_rate):
        plt.text(i, rate, f'{rate:.0%}', ha='center', va='bottom')

    plt.xlabel('Question ID')
    plt.ylabel('Success Rate')
    plt.title('Question Success Rate')

    plt.xticks(rotation=90)

    plt.show()

    Run the cell to perform the analysis by generating the bar chart graphic.

Conclusion

Congratulations - you just learned how to work with Jupyter Notebooks and create your own notebook!

------------------------------------------------------

Chapter 7 Algorithms

------------------------------------------------------
7.1 Introduction

   ------------------
   Jayendra's Cloud Certification Blog - AWS SageMaker Built-in Algorithms Summary
     https://jayendrapatil.com/aws-sagemaker-built-in-algorithms-summary/
     Need to review because it list some algorithm NOT covered!!!!

  SageMaker Built-in Algorithms:
    Text-Based
      BlazingText
        - provides highly optimized implementations of the (unsupervised) Word2vec and (supervised) text classification algorithms.
        Word2vec algorithm
          - useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition,
           machine translation, etc.
          - maps words to high-quality distributed vectors, whose representation is called word embeddings
          - word embeddings capture the semantic relationships between words.
          - provides the Skip-gram and continuous bag-of-words (CBOW) training architectures
         Text classification
           - is an important task for applications performing web searches, information retrieval, ranking, and document classification

    Forecasting
      DeepAR
        - is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN).
        - use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on.

    Recommendations
      Factorization Machines
        - is a general-purpose supervised learning algorithm used for both classification and regression tasks.
        - extension of a linear model designed to capture interactions between features within high dimensional sparse datasets
          economically, such as click prediction and item recommendation.

    Clustering
      - unsupervised
      K-Means (kMeans)
        - an unsupervised learning algorithm for clustering [no labels are provided]
        - Note: you need to specify the value of 'k' to specify to the number of clusters to split the data into during training
        - attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and
          as different as possible from members of other groups

    Classification
      Linear Learner
        - supervised learning algorithms used for solving either classification or regression problems
      XGBoost
        - is a popular and efficient open-source implementation of the gradient boosted trees algorithm.
        - Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining
          an ensemble of estimates from a set of simpler, weaker models
      K-Nearest Neighbors (KNN or K-NN)
       - is an index-based algorithm.
       - uses a non-parametric method for classification or regression.
       - For classification problems, the algorithm queries the k points that are closest to the sample point and returns the most
         frequently used label of their class as the predicted label.
       - For regression problems, the algorithm queries the k closest points to the sample point and returns the average of their
         feature values as the predicted value.

    Regression:
      Linear Learner
        - supervised learning algorithms used for solving either classification or regression problems
      XGBoost
        - is a popular and efficient open-source implementation of the gradient boosted trees algorithm.
        - Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining
          an ensemble of estimates from a set of simpler, weaker models

    Topic Modeling
      Latent Dirichlet Allocation (LDA)
        - is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories.
        - used to discover a user-specified number of topics shared by documents within a text corpus.
      Neural Topic Mode (NTM)
        - is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings
          based on their statistical distribution
        - Topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or
          recommend content based on topic similarities.

    Feature Reduction
      PCA (Principal Component Analysis)
        - is an unsupervised ML algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still
          retaining as much information as possible.
      Object2Vec
        - is a general-purpose neural embedding algorithm that is highly customizable
        - can learn low-dimensional dense embeddings of high-dimensional objects.

    Anomaly Detection
      Random Cut Forest
        - is an unsupervised algorithm for detecting anomalous data points within a data set.
      IP Insights
        - is an unsupervised learning algorithm that learns the usage patterns for IPv4 addresses.
        - designed to capture associations between IPv4 addresses and various entities, such as user IDs or account numbers

    Sequence Translation
      Sequence to Sequence  seq2seq
        - is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio), and the output
          generated is another sequence of tokens.
        - key uses cases are machine translation (input a sentence from one language and predict what that sentence would be in another
          language), text summarization (input a longer string of words and predict a shorter string of words that is a summary),
          speech-to-text (audio clips converted into output sentences in tokens)

    Computer Vision
      Image Classification
        - a supervised learning algorithm that supports multi-label classification
        - takes an image as input and outputs one or more labels
        - uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when
          a large number of training images are not available.
        - recommended input format is Apache MXNet RecordIO. Also supports raw images in .jpg or .png format.
      Object Detection
        - detects and classifies objects in images using a single deep neural network.
        - is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene.
      Semantic Segmentation
        - provides a fine-grained, pixel-level approach to developing computer vision applications.
        - tags every pixel in an image with a class label from a predefined set of classes and is critical to an increasing number
          of CV applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.
        - also provides information about the shapes of the objects contained in the image. The segmentation output is represented
          as a grayscale image, called a segmentation mask.
   ------------------

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      - time to train the model
      Train Model --->  Evaluate Model --->

    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
7.2 OPTIONAL - Why do we call them algorithms
------------------------------------------------------
7.3 Algorithm Concepts

  what is an algorithm?
   - unambiguous specification of how to solve a class of problems

  Algorigthm
    - set of steps to solve a specific problem intended to have a very repeatable outcome

  Heuristic
    - a mental shortcut or "rule of thumb" thats provides some guidance on doing a task but doesn't
        guarantee a consistent outcome.
    - think it as an educated guess


  Model types (at a high level)

                    Supervised            Unsupervised      Reinforcement
                    Learning              Learning          Learning

      Training      Training Data and     No Training       How to Maximize Reward
                      Testing Data        (no labels)

      Discrete      Classification        Clustering         Simulation-Based Optimiazation

      Continuous    Regression            Reduction of       Autonomous Devices
                                          Dimensionality

  SageMaker Algorithm Sources
     - SageMaker Built-in Algorithms
     - Purchased from AWS Marketplace
     - Build your Own Via Docker image

------------------------------------------------------
Loss Functions in Machine Learning Explained
  https://www.datacamp.com/tutorial/loss-function-in-machine-learning


  What is a loss function (error function)?
    - quantifies the difference between the predicted outputs of a machine learning algorithm and the actual target values.

    Optimizing Model predictions:
      Calculate loss
        - determine the error between the prediction and actual values
      Compute the Gradient
        - calculate the gradient of the loss function
      Adjust Parameters
        - Update model parameters to minimize loss


  Loss Functions for Regression

    Mean Square Error (MSE) / L2 Loss
       MSE = (1/n) * (y_i_predicted - y_i_target)

          where:
             n              the number of samples in the dataset
             y_i_predicted: the predicted value for the i-th sample
             y_i_target:    the target value for the i-th sample

     When to use MSE
       - MSE is a standard loss function utilized in most regression tasks
       - when it is conducive to penalize significantly the presence of outliers.

    Mean Absolute Error (MAE) / L1 Loss
       MAE = (1/n) * |y_i_predicted - y_i_target|

     When to use MAE
       - MAE measures the average absolute difference between the predicted and actual values.
       - minimizes outliers impact
       - where we don't want to penalize outliers considerably or at all, for example, predicting delivery times
         for a food delivery company.


    Huber Loss / Smooth Mean Absolute Error
      - combines Mean Absolute Error and Mean Squared Error loss functions into a single loss function.

           L(, y, f(x)) = (1/2) * (f(x) - y)^2           if |f(x) - y| <= 
                         =  * |f(x) - y| - (1/2) * ^2   if |f(x) - y| > 


              where:
                L   : represents the Huber Loss function
                   : the delta parameter, which determines the threshold for switching between the quadratic and
                     linear components of the loss function
                y    : the true value or target value
                f(x) : the predicted value


      When to use Huber Loss / Smooth Mean Absolute Error
          - uses Quadratic Component for Small Errors: For errors smaller than , it uses the quadratic component
                (1/2) * (f(x) - y)^2
          - uses Linear Component for Large Errors:    For errors larger than , it applies the linear component
                 * |f(x) - y| - (1/2) * ^2

  Loss Functions for Classification

    Binary Cross-Entropy Loss (BCE) / Log Loss
      - (BCE) is a performance measure for classification models that outputs a prediction with a probability value
         typically between 0 and 1, and this prediction value corresponds to the likelihood of a data sample belonging
         to a class or category.

         L(y, f(x)) = -[y * log(f(x)) + (1 - y) * log(1 - f(x))]

            Where:
                L    : represents the Binary Cross-Entropy Loss function
                y    : the true binary label (0 or 1)
                f(x) : the predicted probability of the positive class (between 0 and 1)

      When to use Binary Cross-Entropy Loss / Log Loss
        - The equation above specifically applies to a scenario where the machine learning algorithm will make a
          classification between two classes. This is a binary classification scenario.
        - The BCE loss function penalizes inaccurate predictions, which are predictions that have a significant difference
          from the positive class or, in other words, have a high quantification of entropy.

    Hinge Loss
      - Hinge Loss is a loss function utilized within machine learning to train classifiers that optimize to increase
        the margin between data points and the decision boundary. Hence, it is mainly used for maximum margin classifications.

        L(y, f(x)) = max(0, 1 - y * f(x))

            Where:
                L    : represents the Hinge Loss
                y    : the true label or target value (-1 or 1)
                f(x) : the predicted value or decision function output


  Factors to consider when selecting a loss function

    Classification vs Regression
      Classification:
        - cross-entropy loss function
      Regression:
        - mean squared error (MSE) or mean absolute error (MAE)

    Binary vs Multiclass Classification
      Binary Classification
        - binary cross-entropy loss function is best
      Multiclass Classification
        - categorical cross-entropy should be utilized.

    Sensitivity to Outliers
      penalize Outliers
        - mean squared error (MSE)
      less sensitive to Outliers
        - mean absolute error (MAE)


      Loss Function           Applicability      Applicability    Sensitivity
                              to Classification     Regression       to Outliers

      Mean Squared Error (MSE)      no                yes               High

      Mean Absolute Error (MAE)     no                yes               Low

      Huber Loss                    no                yes               Medium

      Cross-Entropy                 yes               no                Medium

      Hinge Loss                    yes               no                Low

      Log Loss                      yes               no                Medium
      (Binary Cross-Entropy)
      (logistic loss)


------------------------------------------------------
7.4 Regressions

  Linear Learner Algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html
    - linear models are supervised learning algorithm for regression, binary classification or multiclass
      classification problems
    - you give the  model labels (x,y) with 'x' being high dimensional vector and y is numeric label
    - the algorithm learns a linear function, or classification problems, a linear threshold function, and
      maps a vector 'x' to an approximation of label 'y'

    - to use this algorithm you need a number of list of numbers which yields some other number ... the
      answer you're after
    - you can use it to predict a specific value or a threshold for grouping purposes

    Adjusts to Minimize Error
      - the algorithm wants the equation to be as good of a fit as possible ... meaning the sum of all the
        distances from the training data point and line is as low as possible
      - one method is "Stochastic Gradient Descent" (SGD)
          Stochastic: outcomes based upon random probability
     Stochastic Gradient Descent (SGD):
       local minimum
         as low as it can go within the local area, but the not the absolute lowest in error
       global minimum
        - as low as it can

     Classification:
       - can handle classification problems, but data needs to be converted to numeric data

  Linear Learner Algorithm
    very flexible
      - can be used to explored different training objectives and choose the best one
      - Well suited for discrete or continuous inferences
    Built-in Tuning
      - has an internal mechanism for tuning hyperparameters separate from automatic model tuning feature
    Good First choice
      - if your data and objective meets the requirements, Linear learner is a good first choice to try for
        your model if your model involves predicting numeric values.
   input formats
     - For training, the linear learner algorithm supports both recordIO-wrapped protobuf (only Float32) and CSV formats.
     - For inference, the linear learner algorithm supports the application/json, application/x-recordio-protobuf,
       and text/csv formats.
   predicted label/class:
    - For binary classification, predicted_label is 0 or 1, and score is a single floating point number that indicates
      how strongly the algorithm believes that the label should be 1.
    - For multiclass classification, the predicted_class will be an integer from 0 to num_classes-1, and score will be a
      list of one floating point number per class.

  Linear Learner Algorithm Use-cases
    Predict quantitative value based on given numeric input
      - Example: Based on the last five years of ROI from marketing spend, what can we expect to
        be this year's ROI
    Discrete Binary Classification Problems
      - provided the data is  structured in numeric format
      - Example: Based on past customer response, should I mail this particular customer? Yes or No?
    Discrete Multiclass Classification Problems
      - provided the data is  structured in numeric format
      - Example: Based on past customer response, how should I reach out to this customer?
        Email, Direct Mail, or Phone Call?
    Score:
      set by 'loss' hyperparameter
        for regression: squared_loss, absolute_loss, eps_insensitive_squared_loss, eps_insensitive_absolute_loss,
          quantile_loss, and huber_loss.
       for binary classifier: logistic, and hinge_loss
       for multiclass classifier: softmax_loss


  Factorization Machines Algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html
    - general purpose supervised learning algorithm for both binary classification and regression
    - It is an extension of a linear model that is designed to capture interactions between features within high dimensional
      sparse datasets economically.
    - For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads
      from a certain ad-category are placed on pages from a certain page-category.
    - is a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction
      and item recommendation.
    - test channel:
       - In regression mode, the testing dataset is scored using Root Mean Square Error (RMSE).
       - In binary classification mode, the test dataset is scored using Binary Cross Entropy (Log Loss),
         Accuracy (at threshold=0.5) and F1 Score (at threshold =0.5).

    - For the binary classification problem, the algorithm predicts a score and a label.
    - For the regression problem, just a score is returned and it is the predicted value.
      For example, if used to predict a movie rating, score is the predicted rating value.
    - To use this algorithm you need a number or list of numbers which yields some other number ...
      the answer you're after
    - you can use it to predict specific value or a threshold for placing into one of two groups
    - It is a good choice when you have 'holes' in your data (e.g. sparse datasets)


  Things to know about 'Factorization Machines Algorithm'
    Considers only pair-wise features
      - SageMaker's implementation of 'factorization machines' will only analyze relationships of two
        pairs of features at a time
    CSV is NOT Supported
      - CSV is not good choice for sparse dimensions
      - for training, File and Pipe modes are support using recordIO-protobuf format with Float32 tensors
      - for inference, it supports the application/json and x-recordio-protobuf formats
    Doesn't work for Multiclass classification
      - 'Factorization Machines' algorithm can be run in either binary classificaiton mode regression mode
    Needs LOTS of data
      - to make up for the data sparseness, it needs lots of data
      - AWS recommended dimension of input feature space is between 10K and 10M rows in the dataset
    CPUs rock sparse data better than GPUs
      - AWS recommends CPUs with 'factorization machines' for the most efficient experience
    Doesn't perform well on dense data
      - other algorithms are much more performant when you have a full set of data


   Sparse Data example for Factorization Machine:
     - rating movies based on feedback from various movie watchers, but movie watchers only watched some movies
     - use one-hot to encoded movie watcher liked results (e.g. 100K movie watchers and 10k movies)
     - CSV results would be mostly zeros, so not a good format for sparse data

   Factorization Machine Algorithm - Use-Cases
     High Dimensional Sparse Data sets
       - lots of rows of data, but many rows with missing features
       Example: Click stream data on when ads on a webpage ten to be clicked given known information abou the
         person viewing the page
     Recommendations
       Example: What sort of movies should we recommend to a person who has watched and rated some other movies


------------------------------------------------------
7.5 Clustering


  Clustering
    - unsupervised algorithms that aim to group things such that they are with other thing more similar
      than different

  K-Means
    - unsupervised algorithm that attempts to find discrete groups within data, where members of a group
      are as similar as possible to one another and as different as possible from members of other groups
    - unsupervised algorithm used for partitioning a dataset into K distinct, non-overlapping subsets or clusters
    - The Euclidean distance between these points represents the similarity of the corresponding observations
    - K-means chooses k random points as the initial cluster centers, and assign each data point to the closest center.
      Then, the algorithm updates the cluster centers by taking the mean of the data points in each cluster, and repeats
      the process until the centers do not change much

    - K-Means will take a in a list of things with attributes
    - you specifiy which attributes indicate similarity and the algorithm will attempt to group them together
      such that they are with other similar things
    - 'Similarity' is calculated based on the distance between the identify attributes

  SageMaker K-Means
    https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html
    Expects Tabular Data
      - rows represent the observations that you want to cluster and the columns represent attributes
        of the observations
    Define the Identifying Attributes
      - you must know enough about the dataset to propose attributes that will define similarity
      - if you have no idea, there are ways around this too
    SageMaker uses a Modified K-Means
      - uses a modified version of the Web-scale K-Means algorithm, which it claims to be more accurate
    CPU Instances recommended
      - GPU instances can be used but SageMaker's K-Means can only use one GPU
      - more cost effective to use many CPUs than one GPU
    Training is Still a Thing
      - you want to be sure your model is still accurate and using the best identifying attributes to identify
        the clustering
      - your data just doesn't have labels
    Define the Number of Features and Clusters
      - you must define the number of features for the algorithm to analyze and the number of clusters you want
    input format:
      - an use either File mode or Pipe mode to train models on data that is formatted as recordIO-wrapped-protobuf or as CSV

   Clustering Use Cases
     Pulse Code Modelulation K-means use case
       Analog Audio Stream
         - label anything above the line as 1 and anything below the line as 0
     MNIST - handwritting recognition
       - cluster similar looking number images

------------------------------------------------------
7.6 Classification

  K-Nearest Neighbor (KNN)
    - a supervised algorithm
    - an index-based, non-parametric method for classification or regression
    - For classification, the algorithm queries the k points that are closest to the sample point, and
      returns the most frequently used label as the predicted label
    - For regression, the algorithm queries the k points that are closest to the sample point, and
      and returns the average of the feature values as predicted value

    - Predicts the value or classification based on that which you are closet
    - it can be used to classify of to predict a value (average value of nearest neighbors)

  K-Nearest Neighbor classification (KNN or K-NN)
    Choose the number of neighbors
      - include a value for 'k', or in other words, the number of closest neighbors to use for classifying
    KNN is a lazy algorithm
      - does not use training data points to generalize (memorize) but rather uses them to figure out who's nearby
    Training data stays in memory
      - KNN doesn't 'learn' but rather uses the training dataset to decide on similar samples

  K-Nearest Neighbor Use Cases
    Credit Ratings
      Example: Group people together for credit risk based on attributes they share with other know credit usage
    Product Recommendation
      Example: Based on what someone likes, recommend similar items they might also like

   K-means vs K-NN
    supervised vs unsupervised:
      - KNN is a supervised learning algorithm so you need labelled data,
      - K-means is an unsupervised learning algorithm, so it discovers the structure of the data, or example how many
        groups you should divide your data into.
    Goals:
     - KNN is a predictive algorithm, which means that it uses the existing data to make predictions or classifications for new data.
     - K-means is a descriptive algorithm, which means that it uses the data to find patterns or structure within it.
    Output
       - The output of KNN is a label or a value for each query point
       - K-means output is a set of k clusters and their centers.

------------------------------------------------------
7.7 Image Analysis


  Image Analysis
    - returns predict label along with confidence level
    - if predict confidence (e.g. 35%) is below your threshold (e.g. 70%), then return I don't know what the
      image is

  Amazon Recognition
    - AWS service that does image analysis
    - does a lot of the functions that we're going to be talking about


  Image Analysis Algorithms

    Image Classification Algorithm
      - supervised algorithm
      - Determine the classification of an image (e.g. hotdog / not hotdog)
      - it uses convolutional neural network (ResNet) that can be trained from scratch of make use of transfer
        learning

      Image Dataset Resources
        ImageNet (https://www.image-net.org/)
          - large database (~14M classified images over 21K topics) of hierarchically labeled images
          - image database organized according to the 'WordNet' hierarchy (currently only the nouns), in which
            each node of the hierarchy is depicted by hundreds and thousands of images

    Object Detection Algorithm
      - supervised algorithm
      - Detects specific objects in an image and assigns classification with confidence score
      - look for various objects in an image (e.g. lamp, glasses, book, calculator along with confidence scores)

    Semantic Segmentation Algorithm
      - supervised algorithm
      - supports transfer learning
      - low level analysis of individual pixels and identifies shapes with an image
      - identifies shapes and edges
      - use case: creating a autonomous robot and it needs to be able to identify geographic structures, rocks,
        hazards,  etc.


  Semantic Segmentation Algorithm overview
    Accepts PNG file Input
      - you can submit files for training or inference in uncompressed PNG format
    Only support GPU instances for training
      - due to have computational power required, GPU instances must be used
    Can deploy on either CPU or GPU instances
      - After training is done, model artifacts are output to S3
      - the model can be deployed as an endpoint with either CPU or GPU instances


  Cityscapes Dataset (https://www.cityscapes-dataset.com/)
    - Semantic Understanding of Urban Street Scenes
    - captured many images of cities, and classified elements within those cities an such as roads, sidewalk,
      person, motorcycle, etc.
    - Cityscapes Dataset can be used to help train and test self-driving models to follow the road for example
      and avoid things like people and other cars.
    - can use this dataset to help train

  Image Analysis Use Cases
    Image Metadata Extraction
      Example: Extract scene metadata from an image provided an store it in a machine readable format
        - useful for cataloging
        - If you're building a website or a service where to sell pictures, you can run them through this
          metadata extraction process and then people can search on certain keywords and we can return images
          that contain those certain objects

    Computer Vision Systems
      Example: Recognize orientation of a part of any assembly line and, if required, issue a command to a
        robotic are to re-orient the part
        - if you need to inspect a part or look for certain defects, and rather than having a human do that,
          you can train a model and put a video camera on the production line and train the model what good parts
          look like and what reject parts look like
        - use case for semantic segmentation because you have a lot of control over how you evaluate that image

------------------------------------------------------
7.8 Anomaly Detection

  Random Cut Forest (RCF)
    https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html
    https://docs.aws.amazon.com/quicksight/latest/user/what-is-random-cut-forest.html
    - A random cut forest (RCF) is a special type of random forest (RF) algorithm
    - is an unsupervised algorithm for detecting anomalous data points within a data set.

    - detects anomalous data points within a set, like spikes in time series data, breaks in periodicity
      or unclassifiable points
    - Useful way to find outliers when it's not feasible to plot graphically.
    - RCF is designed to work with n-dimensional input

    - Find occurences in the data that are significantly beyond 'normal' (usually more than 3 stardard
      deviations) that could mess up your model training



  Random Cut Forest Overview
    Gives an Anomaly Score to Data Points
      - low scores indicates that a data point is consider 'normal' while high scores indicate the presence
        of an anomaly
    Scales Well
      - RCF scales very well with respect to number of features, data set size, and number of instances
    Does not benefit from GPU
      - AWS recommends normal compute instances (ml.m4, ml.c5)

    Algorithm high-level
      - the algorithm's makes a series of random cuts, it doesn't really matter where, it's random.
      - the key here is that the outlier is going to tend to lie on one side of that cut, where the majority
        of the normal data, quote unquote, will live on the other side of that cut.
      - the algorithm analyzes what resides on one side of the cut versus the other, and does some statistical
        analysis to try to resolve that maybe those items, if there's few enough of them on the other side of
        the cut, maybe they're just outliers.

  Random Cut Forest (RCF) Use Cases:
    Quality Control
      Example: Analyze an audio pattern played by a high-end speaker system for any unusual frequencies
    Fraud Detection
      Example: If a financial transaction occurs for an unusual amount, unusual time or from an unusual
        place, flag the transaction for a closer look
    Quality Control


  IP Insights
    - unsupervised algorithm that consumes observed data in the form of (entity, IPv4 address) pairs that associates entities
      with IP addresses. IP Insights determines how likely it is that an entity would use a particular IP address by learning
      latent vector representations for both entities and IP addresses.
    - learns usage patterns for IPv4 addresses by capturing associations between IP addresses and various
      entities such as user IDs or account numbers

    - can potentially flag odd online behavior that might require closer review

  IP Insights Overview
    Ingests Entity/IP address Pairs
      - historic data can be used to learn baseline patterns
    Returns Inferences via a Score
      - When queried, the model will return a score that indicates how anomalous the entity/IP combination is,
        based on the baseline
    Uses a Neural Network
      - Uses a neural network to learn latent vector representations for entities and IP addresses (learns patterns)
    GPUs recommended for Training
      - Generally, GPU are recommended bu if the dataset is large, distributed CPU instances might be more cost
        effective

  IP Insights Use Cases
    Tieried Authentication Models
      Example: If a user tries to log into a website from an anomalous IP address (e.g. from a different country),
        you might dynamically trigger an additional two factor authentication routine
    Fraud Detection
      Example: On a banking website, only permit certain activities if the IP address is usual for a given
        user login

------------------------------------------------------
7.9 Text Analytics

  Topic Modeling
    https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2
    - method for unsupervised classification of documents, similar to clustering on numeric data, which
      finds some natural groups of items (topics)
    - Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing
      large electronic archives

  Corpus
    - collection of written texts, especially the entire works of a particular author or a body of writing on
      a particular subject

  Latent Dirichlet Allocation (LDA)
    https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2
    - one of the most popular topic modeling methods
    - LDA algorithm is an unsupervised learning algorithm that attempts to describe a set of observations
      as a mixture of distinct categories
    - The aim of LDA is to find topics a document belongs to, based on the words in it
    - LDA is most commonly used to discover a user-specified number of topics shared by documents within
      a text corpus
    - Here each observation is a document, the features are the presence (or occurence count) of each word,
      and the categories are the topics

    - Used to figure out how similar documents are based on the frequency of similar words


  Latent Dirichlet Allocation (LDA) Assumptions:
     - Each document is just a collection of words or a "bag of words".
        - the order of the words and the grammatical role of the words (subject, object, verbs, ...) are not
          considered in the model.
     - Words like am/is/are/of/a/the/but/... dont carry any information about the "topics" and therefore can be
       eliminated from the documents as a preprocessing step
     - common words to all documents can be eliminated (if our corpus contains only medical documents, words like human,
       body, health, ... can be eliminated)


  Latent Dirichlet Allocation (LDA) Use Cases:
    Article Recommendation
      Example: Recommend articules on similar topics which you might have read or rated in the past
    Musical Influence Modeling
      Exmaple: Explore which musical artists over time were truly innovative and those who were influenced
        by those innovators

  Neural Topic Model (NTM)
    https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html
    - unsupervised learning algorithm that is used to organize a corpus of documents into topics that
      contain word groupings based on their statistical distribution
    - topic model can be used to classify or summarize documents based on the topics detected or to
      retrieve information or recommend content based on topic similarities

    - similar uses and function to LDA in that both NTM and LDA can perform topic modeling
    - However, NTM uses a different algorithm which yield different results than LDA

  Sequence to Sequence (seq2seq)
    https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html
    - Supervised learning algorithm where the input is a sequence of tokens (for example, text, audio)
      and the output generated is another sequence of tokens

    - Seq2Seq is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in
      another domain (e.g. the same sentences translated to French).
    - think a language translation engine that can take in some text and predict what that text might
      be in another language
    - must supply training data and vocabulary


  from OReilly Hands-On ML:
    Convolutional Neural Networks (CNN)
      - used in computer image recognition since the 1980's
      - they are also used for other tasks, such as voice recognition and natural language processing

     Recurrent Neural Networks (RNNs):
      - a class of net that can predict the future (well, up to a point)
      - RNNs can analyze time series data , such as the number of daily active users on your website, the hourly temperature
        of your city, your home's power consumption, etc.
      - RNNs can work on sequences of arbitrary lengths, rather than on fixed-size inputs

  Sequence to Sequence (seq2seq) overview
    https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html
    Steps consist of embedding, encoding and decoding
      - Using a neural network model (RNN and CNN), the algorithm uses layers for embedding, encoding and
        decoding into the target
    Commonly initialized with pre-trained word libraries
      - a standard practice is initializing the embedding layer with pre-trained word vector like FastText
        or GloVe or to initialize it randomly and learn the parameters during training
    Only GPU instances are supported
      - SageMaker seq2seq is only supported on GPU instance types and is only set up to train on a single
        machine
      - However, you can use single instances with multiple GPUs

  FastText (text classification algorithm)
    https://fasttext.cc/
    https://www.analyticsvidhya.com/blog/2023/01/introduction-to-fasttext-embeddings-and-its-implication/
    - FastText is an open-source, free, lightweight library that allows users to learn text representations
      and text classifiers. It works on standard, generic hardware
    - FastText is a word embedding technique that provides embedding to the character n-grams.
    - It is the extension of the word2vec model.
    - Word2Vec model provides embedding to the words, whereas fastText provides embeddings to the character n-grams.
    - FastText can also handle out-of-vocabulary words (OOV), i.e., the fast text can find the word embeddings that are not
     present at the time of training.

  Character n-gram
    - Consider the word "equal" and n = 3, then the word will be represented by character n-grams:
       < eq, equ, qua, ual, al > and < equal >
    - So, the word embedding for the word equal can be given as the sum of all vector representations of all of
      its character n-gram and the word itself.

  Word2Vec vs FastText
    - Word2Vec works on the word level, while fastText works on the character n-grams.
    - Word2Vec cannot provide embeddings for out-of-vocabulary words, while fastText can provide embeddings for OOV words.
    - FastText can provide better embeddings for morphologically rich languages compared to word2vec.
    - FastText uses the hierarchical classifier to train the model; hence it is faster than word2vec.


  Global Vectors (GloVe)
    - Global Vectors for Word Representation, or GloVe for short, is an unsupervised learning algorithm that
      generates vector representations, or embeddings, of words.
    - GloVe is intended to capture the semantic relationships between words.

  Sequence to Sequence (seq2seq) Use Cases
    Language Translation
      Example: Using a vocabulary, predict the translation of a sentence into another language
    Speech to Text
      Example: Give an audio "vocabulary", predict the textual representation of spoken words
      Example: audio clips converted into output sentences in tokens
    Text Summarization
      Example: input a longer string of words and predict a shorter string of words that is a summary


  Word2Vec
    https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work
    - architectures that can find words with similar contexts and group them together.
    - The vectors representing words with similar meanings are positioned close to each other in this
      high-dimensional space.
    - two-layer neural network that processes text by taking in batches of raw textual data, processing them and producing
      a vector space of several hundred dimensions. Each unique word in the data is assigned a corresponding vector in the space.
      The positioning of these vectors in the space is determined by the words semantic meanings and proximity to other words.
   Architectures
     Continuos Bag of Words (CBOW) Model
       - predicts the target word from its surrounding context words.
       - In other words, it uses the surrounding words to predict the word in the middle
       - This model is beneficial when we have a small dataset, and its faster than the Skip-Gram model
     Skip-Gram Model
       - predicts the surrounding context words from a target word.
       - In other words, it uses a single word to predict its surrounding context.
       - This model works well with a large dataset and with rare words.
       - advantages, including an improved ability to capture semantic relationships, handle rare words, and be
         flexible to linguistic context

  CBOW vs Skip-gram
    https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314
    - Both are architectures to learn the underlying word representations for each word by using neural networks.
    Continuous Bag of Words (CBOW) Model
      - In the CBOW model, the distributed representations of context (or surrounding words) are combined to
        predict the word in the middle.
    Skip-gram Model
      -  in the Skip-gram model, the distributed representation of the input word is used to predict the context.

  BazingText
    - both supervised and unsupervised
    - highly optimized implementation of the Word2vec and text classification algorithms
    - the Word2vec algorithm is useful for many downstream natural language processing (NLP tasks, such
      as sentiment analysis, name entity recognition, machine translation, etc)

    - optimized way to determine contextual semantic relationship between words in a body of text

    - based on FastText, which is an algorithm that was developed by Facebook
    - where FastText may have taken days to train models, BlazingText can do that in minutes
    - inference can be performed in real time where before they may had to be done in batch


  BlazingText - Modes

     Modes                    Word2Vec                      Text Classification (extended FastText)
                              (Unsupervised)                (Supervised)

     Single CPU Instance      Continuous Bag of Words       Supervised
                              Skip-gram
                              Batch Skip-gram

     Single GPU Instance      Continuous Bag of Words       Supervised with 1 GPU
      (1 or more GPUs)        Skip-gram


     Multiple CPU Instances  Batch Skip-gram                 None

  BlazingText Overview
    Expects single pre-processed text file
      - each line in the file should contain a single sentence
      - if you need to train on multiple text files, concatenate them into one file and upload the file
        in the respective channel
    Highly Scalable
      - improves on tranditional Word2vec algorithm by supporting scale-out for multiple CPU instances
      - FastText [text] classifier can leverage GPU acceleration
      - improved on the original Word2Vec algorithm, as well as the original Facebook FastText text classifier
    Around 20x faster than FastText
      - supports pre-trained FastText models, but also can perform training abut 20x faster that FastText

  BlazingText Use Cases
    Sentiment Analysis
      - its Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment
         analysis, named entity recognition, machine translation, etc
      Example: Evaluate customer comments in social media posts to evaluate whether they have a positive
        or negative sentiment
      - uses sentimental analysis with Amazon Comprehend
    Document Classification
      - Text classification (extended FastText) is an important task for applications that perform web searches, information
         retrieval, ranking, and document classification.
      Example: Review a large collection of documents and detect whether the document should be classified
        as containing sensitive data like personal information or trade secrets
      - uses doc classification (finding PI in docs) with Amazon Macie

  Object2Vec
    - supervised algorithm
    - general purpose neural enbedding algorithm that can learn low-dimensional dense enbeddings of
      high-dimensional objects while preserving the semantics of the relationship between the pairs in
      the original embedding space

    - a way to map out things in a d-dimensional space to figure out how similar they might be to one another

  Word2Vec vs Object2Vec
    Word2Vec
      - way to map out the relationship between a pair of two things to try to determine how similar
        they might be or how related they might be.
      - use Word2Vec to feed in some words and try to figure out how closely they are related based on maybe
        how often they appear in a certain document together.
    Object2Vec
      - figure out the relationship between objects so it doesn't have to be words, it can be customers,
        it can be products, movies, movie reviews, what have you.
      - let's say we have some music albums, and we can specify a rating or a relationship between the
        person and the album.


  Object2Vec Overview
    Expects Pairs of Things
      - looking for pairs of items and whether they are 'positive' or 'negative' from a relationship standpoint
      - Accepts categorical labels or ratings/score-based labels
    Feature Engineering
      - embedding can be used for downstream supervised tasks like classification or regression
      - For example, we might wanna figure out how similar products are based on their product description.
        And then we might have another process that tries to suggest those products based on what the customer has
        purchased in the past
    Training data Required
      - Officially, Object2Vec requires labeled data for training, but there are ways to generate the relationship
        labels from natural clustering

  Object2Vec Use Cases
    Movie Rating Predictions
      Example: Predict the rating a person is likely to give a movie based on similarity of ratings they
        gave to other's movie ratings
    Document Classification
      Example: Determine which genre a book is based on its similarity to known genres (history, thriller,
        biography, etc)

------------------------------------------------------
7.10 Reinforcement Learning

  Reinforcement Learning Strategies
    Positive
      - provide a positive reward thereby motivating the subject to repeat the behavior, presumably
        for another positive reward
    Negative
      - provide a displeasurable experience or response thereby motivating the subject to NOT repeat the
        undesired behavior

  Reinforcement Learning (RL)
    - RL is a machine learning technique that attempts to learn a strategy, called a policy, that optimizes
      for an agent acting in an environment
    - well suited for solving problems where an agent can make autonomous decisions

    - find the path to the greatest rewards

  Markov Decision Process (MDP)
    Agent:
      - the thing that's going to be doing the activity
    Environment
      - the agent is placed in an environment
      - the environment could be real-world or simulated
    Reward
    State
      - state is the information about the environment and maybe any past steps that might be relevant
        to any sort of future steps
    Observation
      - the information that's available to the agent at each state or each step
    Episodes
      - episodes are just iterations from start to finish that the agent takes while it's accumulating reward
    Policy
      - the goal is to create a policy that will help it make those decisions in the future, given a similar objective
      - in developing that path, it's going to learn some lessons that will help it make those decisions
        in the future, given a similar objective.  And this is called the policy.
      - It's the decision making part of the reinforcement learning model that will make choices which will
        hopefully maximize our reward.


  Markov Decision Process info:
    https://www.geeksforgeeks.org/markov-decision-process/
    - In the problem, an agent is supposed to decide the best action to select based on his current state.
      When this step is repeated, the problem is known as a Markov Decision Process.
    - A Markov Decision Process (MDP) model contains:
        - A set of possible world states S.
           state: A State is a set of tokens that represent every state that the agent can be in.
        - A set of Models.
        - A set of possible actions A.
        - A real-valued reward function R(s,a).
        - A policy the solution of Markov Decision Process.


  DeepRacer
    - good example of reinforcement learning
    - has a python reward function that it defines how we're going to reward the car for progressing along the track.
      (closer to the center the higher the reward)

  Reinforcement Learning - Use Cases
    Autonomous Vehicles
      Example: A self-driving car model can 'learn' to stay on the road through iterations of trail and error in
        a simulation. Once the model is good enough, it can be tested on a real vehicle on a test track
    Intelligent HVAC Control
      Example: A RL model can learn patterns and routines of building occupants, impact of sunlight as it
        transitions across the sky and equipment efficiency to optimize the temperature control for lowest
        energy consumption

------------------------------------------------------
7.11 Forecasting

  DeepAR
    https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html
    - supervised forecasting algorithm for scalar times series using Recurrent Neural Networks (RNN)
    - DeepARM outperforms standard AutoRegRessive Integrated Moving Average (ARIMA) and Exponential Smoothing
      (ETS) by training a single model over multiple time series as opposed to each individual time series

    - Can predict both point-in-time values and estimated values over a timeframe by using multiple sets of
      historic data

  Example Case
     How many can we expect to sell of a brand new shoe?
       - has patterns of multiple shoes but NOT a pattern of a  specific shoe
       -  create a model to combine those patterns of multiple shoes to create an aggregate forecast
          that's probably a more realistic model for our brand new shoe
       Cold Start Problem
         - little or no history to use for building a forecasting model

  DeepAR Forecast Types

      Forecast Type               Example
      ----------------------      -------------------------------------------------
      Point Forecast              Number of sneakers sold in a week is 'X'

      Probabilistic Forecast      Number of sneakers sold in a week is between 'X' and 'Y' with 'Z' probability


  DeepAR overview
    Support for various time series
      - time series can be numbers, counts, or values in an interval (such as temperature readings
        between 0 and 100)
    More time series data is better
      - recommend training a model on as many time series as are available
      - DeepAR really shines with hundreds of related time series
    Must supply at least 300 observations
      - DeepAR requires a minimum number of observations across all time series
    Must Supply some Hyperparameters
      - required hyperparameters:
        Context lenth:
          - number of time points that the model gets to see before making the prediction.
        Epochs:
          - the number of passes over the training data.
        Prediction length (or Forecast Horizon)
          - the number of time steps that the model is trained to predict
        Time Frequency:
          - the granularity of the time series data that we're passing in. Maybe it's in months,
             weekdays, hours, minutes, ...
    Automatic Evaluation of the Model
      - uses a some of the training data as a backtest after training to evaluate the accuracy of the
        model automatically

  DeepAR Use Cases
    Forecasting New Product Performance
      Example: Incorporate historical data from other products to create a model that can predict performance
        of a newly released product
    Predict Labor needs for Special Events
      Example: Use labor utilization rates at other distribution centers to predict the required level
        of staffing for a brand new distribution center
        - predict level staffing at new distribution center for a certain amount of throughput


------------------------------------------------------
7.12 Ensemble Learning

  from OReilly Hands-On ML:

    Ensemble Methods:
      - try a combination of models that perform best. The group (or 'ensemble') will often perform better than
        the best individual models

    page 222: Boosting (originally called hypothesis boosting) refers to an ensemble method that can combine
      several weak learners into a strong learner. The general idea of most boosting methods is to train predictors
      sequentially, each trying to correct it predecessor. There are many boosting methods available, by far the most
      popular are 'AdaBoost' (...) (short for adaptive boosting) and 'gradient boosting'.

    page 226: Another popular boosting algorithm is 'gradient boosting' (...). Just like AdaBoost, gradient boosting works by
      sequentially adding predictors to an ensemble, each one correcting it predecessor. However, instead of tweaking the instance
      weights at every interation like AdaBoost does, this method tring to fit the new predictor to the 'residual errors' make
      by the predictor.

    7. If your gradient boosting ensemble overfits the training set should you increase or decrease the learning rate?

     -> If gradient boosting is overfitting, you can try to decrease the learning rate. Instead you may try to the reduce
        the number of estimators and add early stopping.

    page 228: The 'learning_rate' hyperparameter scales the contributions of each tree. If you set to a low value, such as 0.05,
      you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This
      is a regularization technique called 'shrinkage'. Figure 7-10 [GBRT ensembles with not enough predictors (left) and just
      enough (right)] shows two GBRT [Gradient Boosted Regression Trees] ensembles trained with different hyperparameters: the one
      on the left does not have enough trees to fit the training set, while the one on the righ has about the right amount. If we
      added more trees, the GBRT would start to overfit the training set.


  Ensemble Learning
    - using multiple learning algorithms and models collectively to improve the model accuracy

  Extreme Gradient Boosting (XGBoost)
   https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html
    - supervised open-source implementation of the gradient boosted trees algorithm that attempts to accurately
      predict a target variable by combining the estimates of a set of simple, weaker models

    - A virtual "Swiss army knife" for all sorts of regression, classification and ranking problems, with 2 required
      and 35 optional hyperparameters to tune

  Extreme Gradient Boosting (XGBoost) Overview
    - a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library.
    - It provides parallel tree boosting and is the leading machine learning library for regression, classification,
      and ranking problems.
    Accepts CSV and libsvm for training and inference
      - Uses tabular data with rows representing ovservastions, one colum representing the target variable or label
        and the remaining columns representing features
      - done  to be consistent with XGBoost supported data formats
    Only trains on CPU and memory bound
      - currently only trains on CPU instances and is memory bound as opposed to compute-bound
    Recommends lots of Memory
      - AWS recommends using an instance with enough memory to hold the entire training data for optimal performance
    Spark Integration
      - Using the SageMaker Spark SDK, you can call XGBoost direct from within the Spark Environment


   libsvm format:
     - developed by National Taiwan University as a library to Support Vector Machines (SVM) classifications and
       regressions
     - each row looks like this:
       <label> <index1>:<value1> <index2>:<value2> ... <indexN>:<valueN>
     - This format is advantageous to use when the data is sparse and contain lots of zeroes. All 0 values are not
       saved which will make the files both smaller and easier to read.

  Decision Tree Ensembles overview example:

    What price should I ask for when selling my house?

      We can create a set of Classification and Regression Trees (CART) for each one of the following features:
        location, age, size, number of bedrooms, number of bathrooms, condition, walk-up or lift access,
        economic climate, lending climate, ...

      When was it built - create age bucket groups with these bucket groups having a tree of modifiers to
        indicate how each age bucket is more desirable (positive value) or less desireable (negative values)

      Similarly build modifier tree  for how many bedrooms, location, ...

      Sum values found under each feature's tree for the specific case, to determine each Home's value modifier

      - using multiple trees to create a more realistic estimation model than any one tree could have provided
         Asking Price = Median Home Price * (<default Home value Modifier> + <+/- modifier sum>)
                      = Median Home Price * (100 + -19)

  XGBoost Use Cases

    Ranking
      Example: On an e-commerce website, you can leverage data about search results, clicks, and successful
        purchases, and then using XGBoost to train a model that can return relevance scores for searched products

    Fraud Detection
      Example: When XGBoost is given a dataset of past transactions and whether or not they were fraudulent, it can
        learn a function that maps input transactions data to the proability that transaction was fradulent

------------------------------------------------------

7.13 Exam Tips

  Resources:
    Using SageMaker Built-in Algorithms or pre-trained models
      https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html


  Algorithms:

    Concepts
      - difference between an algorithm and a heuristic
      - be aware of how bias can foul out models
      - Understand the difference between a discrete model and a continuous model
      - understand the difference and characteristics of supervised learning, unsupervised learning, and
        reinforcement learning
      - know the options SageMaker provides for algorithms (built-in, buy from Marketplace, and bring your own)


    Regressions
      - understand the types of problems best suited for regression
      - Linear Learner algorithm seeks to minimize error via Stochastic Gradient Descent (SGD) with regression
        problems
      - Linear Learner can also be used with classification problems too if you set up your input data properly
        (structured in numeric format)
      - know that Factorization Machines are best suited for sparse datasets and don't perform well on
        dense data at all

    Clustering
      - know that clustering algorithms are usually unsupervised
      - Understand that K-Means can perform clustering similar items based on identifying attributes
      - Must define the identifying attributes, number of features, and number of clusters

    Classification
      - understand the difference between Classification and Clustering
      - K-NN can be used for classification or regression problems based on the nearest K data points
        - user normally specifies in the "k" value to use
      - K-NN considered 'lazy algorithm' because it does not seek to generalize rather looks for who's nearest

    Image Analysis
      - Know that image analysis services are usually classifier models which require training
      - Understand the difference between the SageMaker algorithms or Image Classification, Object Detection, and
        Semantic Segmentation
          Image Classification
            - recognize objects in an image (e.g. dog, cat, ...)
            - if multi-label classifer, it could recognize multiple objects (dog & cat) in the image, but not the location
            - Use SageMaker Image Classification Algorithm
          Image Localization
            - identifies location of single object in a image
          Object Detection
            - used when multiple objects may be present in an image
            - focuses on localizing and classifying specific objects within an image, providing bounding boxes around them.
            - Use SageMaker Object Detection Algorithm
          Image Segmentation
            - divide or partition an image into  various parts called segments
          Semantic Segmentation
            - involves dividing an image into semantically meaningful regions, assigning class labels to each pixel.
            - Essentially, it provides detailed information about object boundaries and regions of interest.
            - Use SageMaker Semantic Segmentation Algorithm

      - Be Familiar with high-level Amazon Rekognition service

    Anomaly Detection
      - Understand that Random Cut Forest is best used to detect unusual and out-of the-ordinary events
      - Know that IP Insights is used to detect anomalies between IPv4 addresses and various entities such as user IDs
        or account numbers

    Text Analysis
      - Latent Dirichlet Allocation (LDA) most commonly used to figure out similarity of documents but that it also
        has uses in other clustering problems
      - Know that a Neural Topic Model (NTM) and LDA can both perform topic model but use different algorithms
      - Sequence to Sequence (seq2seq) is often used in language translation and speech to text by using an
        embedding, encoding, and decoding process (layers)
      - understand BlazingText is highly optimized and ca be used to cluster (word2vec) as well as classify text
        - BlazingText can be used in two ways - in a Word2vec manner or text classification manner.

         text classification:
           - Text Classification is the task of assigning a label or class to a given text.
           - Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness
         Word2vec:
           - is a technique in natural language processing (NLP) for obtaining vector representations of words.
           - These vectors capture information about the meaning of the word based on the surrounding words

    Reinforcement Learning
      - know that Rl seeks to find the policy that optimizes an agent acting in an environment
      - Reinforcement Learning seeks to find the policy that optimizes the agent's ability to collect reward
      - understand the components of the Markov Decision Process (MDP)
      - RL is best suited for situations where the agent an aor must make autonomous decisions

    Forecasting
      - DeepAR is SageMaker go to algorithm for forecasting
      - Know why DeepAR is considered to outperform other regression methods of forecasing
      - Understand the 'Cold Start Problem' and how DeepAR can help

        Cold Start Problem
         - little or no history to use for building a forecasting model
        To get around Cold Start Problem:
          - aggregate multiple related datasets

      - Understand the difference between Point Forecasts and Probabistic Forecasts


    Ensemble Learning
      - Understand Ensemble Learning from a conceptual standpoint
      - XGBoost can be used for regression, classification, and ranking problems
      - Know how XGBoost used decision trees to create an improvement over linear regression
      - XGBoost is "memory-bound" versus "compute bound"


------------------------------------------------------
7.14 Demo: Algorithms


  Resources

    Note: Downloaded github demo files to:
     C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\7_14_algorithms_demo

     -> Jupyter Notebook:
        ufo-algorithms-lab.ipynb
     -> dataset data:
          ufo_fullset.csv

  Identifying UFO Sightings Legitimacy
    - build model that can predict the legitimacy of a UFO sighting based on the information supplied by
      the submitter
    - can use the ground truth dataset used in the previous lab on whether sightings could be explained
      (as a hoax or other natural explanations), unexplained, or probable

  To do:
    - build a model to determine whether newly reported UFO sightings are legitimate or not (explained,
      unexplained, or probable)
    - require the model be at least 90% accurate
    - Which algorithm would you choose?
    - What does our data look like? Do we need to transform and prepare our data? Are we missing an values?
    - How accurate is our model?

  Final Results (for each algorithm):
    - Model artifact store in S3
    - Model validation metrics (accuracy, recall, precision, and the F1 score)
    - model error rate

   Additional Suggested Steps:
     - build a model usingn the XGBoost algorithm as a multi-classification problem with 'researchOutcomes' as
       the target attribute
         - choosing XGBoost because: simple to implement, and it only needs two hyperparameters with 35 optional
           hyperparameters as well.  It's really simple to implement
         - XGBoost can be used for different types of problems, like ranking problems or regression problems,
         - we're going to use it as a multi-classification problem.
         - since we're not exactly sure which attributes are most important, then we need to use a classification
           algorithm over a clustering algorithm
     - Goal is to minimize the training and validation error

     - also build a model usingn the Linear Learner algorithm as a multi-classification problem with 'researchOutcomes'
       as the target attribute
         - train different models and determine which attributes from our data are the most determining factors.
         - Linear Learner has built-in hyperparameter tuning
     - Goal is to maximize the training accuracy (and other metrics like precision, F1 score, and recall)


     Flow:
         S3 (dataset)  ---> SageMaker  --> Notebook  --> train  --> create model --> upload to S3

         Convert data to numeric format
           - XGBoost and Linear Learner both expect dataset to be in numeric format
         Split dataset to training, validation, and testing sets

         Train via algorithm

         logs
            - in cloudwatch and in Notebook after training job completes


  # first create modeling-ufo-lab bucket and upload ufo_fulset.csv  - performed in previous lab
  AWS Console -> S3 -> bucket name: modeling-ufo-lab1 -> Create Bucket
    -> Create folder -> ufo_dataset -> create
    -> ufo_dataset folder -> Upload -> Add files -> ufo_fulset.csv -> upload


  AWS Console -> SageMaker
    # actually, I reused last lab instance
    -> Notebook (left tab) -> Notebook Instances -> Create notebook instance
       Notebook instance name: my-notebook-inst,  instance type: ml.t3.medium, platform: AL2, Jupyter Lab 3
          -> Create an IAM role -> S3: Any S3 bucket -> Create role
       -> Create notebook instance
       -> Open Jupyter -> Upload -> ufo-algorithms-lab.ipynb -> select
          -> (rename) car-data-box-plot-ex

  Code: UFO algorithm lab code

         >>> # First let's go ahead and import all the needed libraries.
         >>> import pandas as pd
         >>> import numpy as np
         >>> from datetime import datetime
         >>> import io
         >>> import sagemaker.amazon.common as smac
         >>>
         >>> import boto3
         >>> from sagemaker import get_execution_role
         >>> import sagemaker
         >>>
         >>> import matplotlib.pyplot as plt
         >>> import seabomodelingrn as sns

         >>> # Let's get the UFO sightings data that is stored in S3 and load it into memory.
         >>> role = get_execution_role()
         >>> bucket='modeling-ufo-lab1'
         >>> sub_folder = 'ufo_dataset'
         >>> data_key = 'ufo_fullset.csv'
         >>> data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)
         >>>
         >>> df = pd.read_csv(data_location, low_memory=False)
         >>> df.head()
         >>>
             	reportedTimestamp	eventDate	eventTime	shape	duration	witnesses	weather	firstName	lastName	latitude	longitude	sighting	physicalEvidence	contact	researchOutcome
             0	1977-04-04T04:02:23.340Z	1977-03-31	23:46	circle	4	1	rain	Ila	Bashirian	47.329444	-122.578889	Y	N	N	explained
             1	1982-11-22T02:06:32.019Z	1982-11-15	22:04	disk	4	1	partly cloudy	Eriberto	Runolfsson	52.664913	-1.034894	Y	Y	N	explained
             . . .

         >>> # Step 2: Cleaning, transforming, analyize, and preparing the dataset
         >>>
         >>> # Let's check to see if there are any missing values
         >>> missing_values = df.isnull().values.any()
         >>> if(missing_values):
         >>>     display(df[df.isnull().any(axis=1)])
         >>>
         >>> # Note: Found just 2 missing values for 'shape' in row 1024 and 2048
         >>>
         >>> # determine most common shape, and use it to replace missing values
         >>> df['shape'].value_counts()

             shape
             circle      6047
             disk        5920
             light       1699
             . . .

         >>> # Replace the missing values with the most common shape
         >>> #    Note: pandas.DataFrame.fillna()    Fills NA/NaN values using the specified method
         >>> df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])


         >>> # Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types.
         >>> # Here is what we are going to take care of.
         >>>
         >>> #  1. Convert the reportedTimestamp and eventDate to a pandas datetime data types.
         >>> #  2. Convert the shape and weather to a pandas category data type.
         >>> #  3. Map the physicalEvidence and contact from 'Y', 'N' to 0, 1.
         >>> #  4. Convert the researchOutcome to a pandas category data type (target attribute).


         >>> df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])
         >>> df['eventDate'] = pd.to_datetime(df['eventDate'])
         >>>
         >>> df['shape'] = df['shape'].astype('category')
         >>> df['weather'] = df['weather'].astype('category')
         >>>
         >>> df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})
         >>> df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})
         >>>
         >>> df['researchOutcome'] = df['researchOutcome'].astype('category')

         >>> df.dtypes

         >>> # Let's visualize some of the data to see if we can find out any important information.
         >>>
         >>> %matplotlib inline
         >>> sns.set_context("paper", font_scale=1.4)
         >>>
         >>> m_cts = (df['contact'].value_counts())
         >>> m_ctsx = m_cts.index
         >>> m_ctsy = m_cts.to_numpy()
         >>> f, ax = plt.subplots(figsize=(5,5))
         >>>
         >>> sns.barplot(x=m_ctsx, y=m_ctsy)
         >>> ax.set_title('UFO Sightings and Contact')
         >>> ax.set_xlabel('Was contact made?')
         >>> ax.set_ylabel('Number of Sightings')
         >>> ax.set_xticklabels(['No', 'Yes'])
         >>> plt.xticks(rotation=45)
         >>> plt.show()

             # Seaborn barplot() displayed visualizing 'was contacted made?' yes or no (no contact made in most sightings)
             #  contact No:    16600  Yes:   1400

         >>> m_cts = (df['physicalEvidence'].value_counts())
         >>> m_ctsx = m_cts.index
         >>> m_ctsy = m_cts.to_numpy()
         >>> f, ax = plt.subplots(figsize=(5,5))
         >>>
         >>> sns.barplot(x=m_ctsx, y=m_ctsy)
         >>> ax.set_title('UFO Sightings and Physical Evidence')
         >>> ax.set_xlabel('Was there physical evidence?')
         >>> ax.set_ylabel('Number of Sightings')
         >>> ax.set_xticklabels(['No', 'Yes'])
         >>> plt.xticks(rotation=45)
         >>> plt.show()

             # Seaborn barplot() displayed visualizing 'was there physical evidence?' yes or no (no physical evidence in most sightings)
             #   physicalEvidence  No:    15313  Yes:     2687


         >>> m_cts = (df['shape'].value_counts())
         >>> m_ctsx = m_cts.index
         >>> m_ctsy = m_cts.to_numpy()
         >>> f, ax = plt.subplots(figsize=(9,5))
         >>>
         >>> sns.barplot(x=m_ctsx, y=m_ctsy)
         >>> ax.set_title('UFO Sightings by Shape')
         >>> ax.set_xlabel('UFO Shape')
         >>> ax.set_ylabel('Number of Sightings')
         >>> plt.xticks(rotation=45)
         >>> plt.show()

             # Seaborn barplot() displayed visualizing 'UFO Shapes?'
             #  shape: circle: 6049, disk: 5920, light: 1699, square: 1662, triangle: 1062, sphere: 1020, box: 200, oval: 199, pyramid: 189

         >>> m_cts = (df['weather'].value_counts())
         >>> m_ctsx = m_cts.index
         >>> m_ctsy = m_cts.to_numpy()
         >>> f, ax = plt.subplots(figsize=(5,5))
         >>>
         >>> sns.barplot(x=m_ctsx, y=m_ctsy)
         >>> ax.set_title('UFO Sightings by Weather')
         >>> ax.set_xlabel('Weather')
         >>> ax.set_ylabel('Number of Sightings')
         >>> plt.xticks(rotation=45)
         >>> plt.show()

             # Seaborn barplot() displayed visualizing 'UFO Sightings by Weather'
             # weather: clear: 3206, mostly_cloudy: 3079, partly_cloudy: 2704, rain: 2605, stormy: 2162, fog: 2123, snow: 2121

         >>> m_cts = (df['researchOutcome'].value_counts())
         >>> m_ctsx = m_cts.index
         >>> m_ctsy = m_cts.to_numpy()
         >>> f, ax = plt.subplots(figsize=(5,5))
         >>>
         >>> sns.barplot(x=m_ctsx, y=m_ctsy)
         >>> ax.set_title('UFO Sightings and Research Outcome')
         >>> ax.set_xlabel('Research Outcome')
         >>> ax.set_ylabel('Number of Sightings')
         >>> plt.xticks(rotation=45)
         >>> plt.show()

             # Seaborn barplot() displayed visualizing 'UFO Sightings and Research Outcome'
             # researchOutcome explained: 12822, unexplained: 3308, probable: 1870

         >>> ufo_yr = df['eventDate'].dt.year  # series with the year exclusively
         >>>
         >>> ## Set axes ##
         >>> years_data = ufo_yr.value_counts()
         >>> years_index = years_data.index  # x ticks
         >>> years_values = years_data.to_numpy()
         >>>
         >>> ## Create Bar Plot ##
         >>> plt.figure(figsize=(15,8))
         >>> plt.xticks(rotation = 60)
         >>> plt.title('UFO Sightings by Year')
         >>> plt.ylabel('Number of Sightings')
         >>> plt.xlabel('Year')
         >>>
         >>> years_plot = sns.barplot(x=years_index[:60],y=years_values[:60])

             # Seaborn barplot() displayed 'UFO Sightings By Year'
             # shows fairly even distribution of sightings for each year

         >>> # display pandas correlation between attributes (needed to add 'numeric_only=True':
         >>> df.corr(numeric_only=True)
             .  .  .
             	                duration	witnesses	latitude	longitude	physicalEvidence	contact
             duration	        1.000000	0.020679	0.000243	-0.010529	0.016430	        0.015188
             witnesses	        0.020679	1.000000	0.010229	0.003449	0.009186	        -0.000651
             latitude	        0.000243	0.010229	1.000000	-0.394536	0.006465	        0.004284
             longitude	        -0.010529	0.003449	-0.394536	1.000000	-0.004519	        -0.004828
             physicalEvidence	0.016430	0.009186	0.006465	-0.004519	1.000000	        0.693276
             contact	        0.015188	-0.000651	0.004284	-0.004828	0.693276	        1.000000

             # df.corr() in video found:  some strong correlations with physical evidence and contact.
             # whenever there is contact, that there is physical evidence at least 69% of the time.

         >>> # Let's drop the columns that are not important.
         >>> #
         >>> # 1. We can drop sighting becuase it is always 'Y' or Yes.
         >>> # 2. Let's drop the firstName and lastName becuase they are not important in determining the researchOutcome.
         >>> # 3. Let's drop the reportedTimestamp because when the sighting was reporting isn't going to help us determine the
         >>> #    legitimacy of the sighting.
         >>> # 4. We would need to create some sort of buckets for the eventDate and eventTime, like seasons for example, but since
         >>> #    the distribution of dates is pretty even, let's go ahead and drop them.
         >>>
         >>> df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)
         >>> df.head()

             	shape	duration	witnesses	weather	latitude	longitude	physicalEvidence	contact	researchOutcome
             0	circle	4	1	rain	47.329444	-122.578889	0	0	explained
             1	disk	4	1	partly cloudy	52.664913	-1.034894	1	0	explained
             . . .

         >>> # Let's apply one-hot encoding
         >>> #
         >>> # We need to one-hot both the weather attribute and the shape attribute.
         >>> # We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the alogrithm
         >>> #   is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2.


         >>> # Let's one-hot the weather and shape attribute
         >>> #  Note: pandas.get_dummies() Convert categorical variable into dummy/indicator variables. (converts to one-hot values)
         >>> #  Note: FIXED! needed to add 'dtype='int' because get_dummies dtype default is now 'bool'
         >>>
         >>> df = pd.get_dummies(df, columns=['weather', 'shape'], dtype='int')
         >>>
         >>> # Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable
         >>> df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})


         >>> # Let's randomize and split the data into training, validation, and testing.
         >>>
         >>> # 1. First we need to randomize the data.
         >>> # 2. Next Let's use 80% of the dataset for our training set.
         >>> # 3. Then use 10% for validation during training.
         >>> # 4. Finally we will use 10% for testing our model after it is deployed.



         >>> # Let's go ahead and randomize our data.
         >>> # pandas sample() returns are random sample of the size of 'frac' (fraction), since '1' returns full randomized dataframe
         >>> #   and "reset_index(drop=True)" drop previous dataframe index values
         >>> df = df.sample(frac=1).reset_index(drop=True)
         >>>
         >>> # Next, Let's split the data into a training, validation, and testing.
         >>> rand_split = np.random.rand(len(df))
         >>> train_list = rand_split < 0.8                       # 80% for training
         >>> val_list = (rand_split >= 0.8) & (rand_split < 0.9) # 10% for validation
         >>> test_list = rand_split >= 0.9                       # 10% for testing
         >>>
         >>>  # This dataset will be used to train the model.
         >>> data_train = df[train_list]
         >>>
         >>> # This dataset will be used to validate the model.
         >>> data_val = df[val_list]
         >>>
         >>> # This dataset will be used to test the model.
         >>> data_test = df[test_list]


         >>> # Next, let's go ahead and rearrange our attributes so the first attribute is our target attribute researchOutcome.
         >>> #  This is what AWS requires and the XGBoost algorithms expects. You can read all about it here in the documentation.
         >>> #      https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost
         >>>
         >>> #  After that we will go ahead and create those files on our Notebook instance (stored as CSV) and then upload them to S3.

         >>> # Simply moves the researchOutcome attribute to the first position before creating CSV files
         >>> pd.concat([data_train['researchOutcome'], data_train.drop(['researchOutcome'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)
         >>> pd.concat([data_val['researchOutcome'], data_val.drop(['researchOutcome'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)

         >>> # Next we can take the files we just stored onto our Notebook instance and upload them to S3.
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_train/train.csv').upload_file('train.csv')
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/xgboost_validation/validation.csv').upload_file('validation.csv')

         >>> # Step 3: Creating and training our model (XGBoost)
         >>> # This is where the magic happens. We will get the ECR container hosted in ECR for the XGBoost algorithm.

         >>> from sagemaker import image_uris
         >>> container = image_uris.retrieve('xgboost', boto3.Session().region_name, '1')

         >>> # Next, because we're training with the CSV file format, we'll create inputs that our training function can use as a pointer
         >>> #  to the files in S3, which also specify that the content type is CSV.
         >>>
         >>> s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/algorithms_lab/xgboost_train'.format(bucket), content_type='csv')
         >>> s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/algorithms_lab/xgboost_validation'.format(bucket), content_type='csv')


         >>> # Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to
         >>> #   create a XGBoost model.
         >>> #
         >>> # First I like to always create a specific job name.
         >>> #
         >>> # Next, we'll need to specify training parameters.
         >>> #
         >>> # 1. The xgboost algorithm container
         >>> # 2. The IAM role to use
         >>> # 3. Training instance type and count
         >>> # 4. S3 location for output data/model artifact
         >>> # 5. XGBoost Hyperparameters
         >>> #     https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html

         >>>
         >>> # Finally, after everything is included and ready, then we can call the .fit() function which specifies the S3 location
         >>> #  for training and validation data.

         >>> # Create a training job name
         >>> job_name = 'ufo-xgboost-job-{}'.format(datetime.now().strftime("%Y%m%d%H%M%S"))
         >>> print('Here is the job name {}'.format(job_name))
         >>>
         >>> # Here is where the model artifact will be stored
         >>> output_location = 's3://{}/algorithms_lab/xgboost_output'.format(bucket)

             Here is the job name ufo-xgboost-job-20240701184208

         >>> sess = sagemaker.Session()
         >>>
         >>> xgb = sagemaker.estimator.Estimator(container, role, instance_count=1, instance_type='ml.m4.xlarge',
         >>>                                     output_path=output_location, sagemaker_session=sess)
         >>>
         >>> xgb.set_hyperparameters(objective='multi:softmax', num_class=3, num_round=100)
         >>>
         >>> data_channels = { 'train': s3_input_train, 'validation': s3_input_validation }
         >>> xgb.fit(data_channels, job_name=job_name)

             INFO:sagemaker:Creating training-job with name: ufo-xgboost-job-20240701191325
             2024-07-01 19:13:26 Starting - Starting the training job...
             . . .
             [98]#011train-merror:0.00893#011validation-merror:0.056299
             [19:16:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 68 extra nodes, 0 pruned nodes, max_depth=6
             [19:16:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=6
             [19:16:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6
             [99]#011train-merror:0.008581#011validation-merror:0.055741

             2024-07-01 19:16:25 Uploading - Uploading generated training model
             2024-07-01 19:16:25 Completed - Training job completed
             Training seconds: 119
             Billable seconds: 119

         >>> # note: the training accuracy was (1 - 0.0085)*100 = 99.9925, but the validation accuracy was (1 - 0.0557)*100 = 95.53
         >>> #         so we may an unfitting issue
         >>>
         >>> print('Here is the location of the trained XGBoost model: {}/{}/output/model.tar.gz'.format(output_location, job_name))

             Here is the location of the trained XGBoost model: s3://modeling-ufo-lab1/algorithms_lab/xgboost_output/ufo-xgboost-job-20240701191325/output/model.tar.gz

         >>> # After we train our model we can see the default evaluation metric in the logs. The merror is used in multiclass
         >>> #   classification error rate. It is calculated as #(wrong cases)/#(all cases). We want this to be minimized (so we
         >>> #   want this to be super small).
         >>>
         >>>
         >>> # Step 4: Creating and training our model (Linear Learner)
         >>> #
         >>> #  Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready
         >>> #  for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects
         >>> #  the first column to be the target attribute)

         >>> np.random.seed(0)
         >>> rand_split = np.random.rand(len(df))
         >>> train_list = rand_split < 0.8
         >>> val_list = (rand_split >= 0.8) & (rand_split < 0.9)
         >>> test_list = rand_split >= 0.9
         >>>
         >>>  # This dataset will be used to train the model.
         >>> data_train = df[train_list]
         >>>
         >>> # This dataset will be used to validate the model.
         >>> data_val = df[val_list]
         >>>
         >>> # This dataset will be used to test the model.
         >>> data_test = df[test_list]
         >>>
         >>> # This rearranges the columns
         >>> cols = list(data_train)
         >>> cols.insert(0, cols.pop(cols.index('researchOutcome')))
         >>> data_train = data_train[cols]
         >>>
         >>> cols = list(data_val)
         >>> cols.insert(0, cols.pop(cols.index('researchOutcome')))
         >>> data_val = data_val[cols]
         >>>
         >>> cols = list(data_test)
         >>> cols.insert(0, cols.pop(cols.index('researchOutcome')))
         >>> data_test = data_test[cols]
         >>>
         >>> # Breaks the datasets into attribute numpy.ndarray and the same for target attribute.
         >>> train_X = data_train.drop(columns='researchOutcome').values
         >>> train_y = data_train['researchOutcome'].values
         >>>
         >>> val_X = data_val.drop(columns='researchOutcome').values
         >>> val_y = data_val['researchOutcome'].values
         >>>
         >>> test_X = data_test.drop(columns='researchOutcome').values
         >>> test_y = data_test['researchOutcome'].values

         >>> #  Next, Let's create recordIO file for the training data and upload it to S3.

         >>> train_file = 'ufo_sightings_train_recordIO_protobuf.data'
         >>>
         >>> f = io.BytesIO()
         >>> smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))
         >>> f.seek(0)
         >>>
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)
         >>> training_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_train/{}'.format(bucket, train_file)
         >>> print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))

             The Pipe mode recordIO protobuf training data: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data


         >>> #  Let's create recordIO file for the validation data and upload it to S3

         >>> validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'
         >>>
         >>> f = io.BytesIO()
         >>> smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))
         >>> f.seek(0)
         >>>
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('algorithms_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)
         >>> validate_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_validation/{}'.format(bucket, validation_file)
         >>> print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))

             The Pipe mode recordIO protobuf validation data: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data

         >>> # Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call
         >>> #   the Linear Learner algorithm.



         >>> from sagemaker import image_uris
         >>> container = image_uris.retrieve('linear-learner', boto3.Session().region_name, '1')

         >>> container
             '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1'


         >>> # Create a training job name
         >>> job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime("%Y%m%d%H%M%S"))
         >>> print('Here is the job name {}'.format(job_name))
         >>>
         >>> # Here is where the model-artifact will be stored
         >>> output_location = 's3://{}/algorithms_lab/linearlearner_output'.format(bucket)

             Here is the job name ufo-linear-learner-job-20240701194728


         >>> # Next we start building out our model by using the SageMaker Python SDK and passing in everything that is
         >>> #   required to create a Linear Learner model.
         >>> #
         >>> # First I like to always create a specific job name.
         >>> #
         >>> # Next, we'll need to specify training parameters.
         >>>
         >>> # 1. The linear-learner algorithm container
         >>> # 2. The IAM role to use
         >>> # 3. Training instance type and count
         >>> # 4. S3 location for output data/model artifact
         >>> # 5. The input type (Pipe)
         >>> #     https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html
         >>> # 6. Linear Learner Hyperparameters
         >>> #     https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html
         >>> #
         >>> # Finally, after everything is included and ready, then we can call the .fit() function which specifies the S3 location
         >>> #  for training and validation data.

         >>> data_train.shape
             (14430, 23)

         >>> print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))

             The feature_dim hyperparameter needs to be set to 22.


         >>> sess = sagemaker.Session()
         >>>
         >>> # Setup the LinearLeaner algorithm from the ECR container
         >>> linear = sagemaker.estimator.Estimator(container, role, instance_count=1, instance_type='ml.c4.xlarge',
         >>>                                        output_path=output_location, sagemaker_session=sess, input_mode='Pipe')
         >>> # Setup the hyperparameters
         >>> linear.set_hyperparameters(feature_dim=22, # number of attributes (minus the researchOutcome attribute)
         >>>                            predictor_type='multiclass_classifier', # type of classification problem
         >>>                            num_classes=3)  # number of classes in out researchOutcome (explained, unexplained, probable)
         >>>
         >>>
         >>> # Launch a training job. This method calls the CreateTrainingJob API call
         >>> data_channels = { 'train': training_recordIO_protobuf_location, 'validation': validate_recordIO_protobuf_location }
         >>> linear.fit(data_channels, job_name=job_name)


             INFO:sagemaker:Creating training-job with name: ufo-linear-learner-job-20240701194728
             2024-07-01 19:47:36 Starting - Starting the training job...
             2024-07-01 19:47:51 Starting - Preparing the instances for training...
             2024-07-01 19:48:19 Downloading - Downloading input data...
             2024-07-01 19:48:59 Downloading - Downloading the training image.........
             2024-07-01 19:50:20 Training - Training image download completed. Training in progress..Docker entrypoint called with argument(s): train
              . . .
              . . .
             [07/01/2024 19:50:55 INFO 139723470354240] #validation_score (algo-1) : ('multiclass_balanced_accuracy', 0.9270477490089298)
             [07/01/2024 19:50:55 INFO 139723470354240] #validation_score (algo-1) : ('multiclass_log_loss', 0.6215193271011654)
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation multiclass_cross_entropy_objective <loss>=0.177802592028485
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation multiclass_accuracy <score>=0.9469825155104343
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation multiclass_top_k_accuracy_3 <score>=1.0
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation dcg <score>=0.978291278630411
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation macro_recall <score>=0.9270477294921875
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation macro_precision <score>=0.9028711915016174
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation macro_f_1.000 <score>=0.9140763282775879
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation multiclass_balanced_accuracy <score>=0.9270477490089298
             [07/01/2024 19:50:55 INFO 139723470354240] #quality_metric: host=algo-1, validation multiclass_log_loss <score>=0.6215193271011654
             [07/01/2024 19:50:55 INFO 139723470354240] Best model found for hyperparameters: {"optimizer": "adam", "learning_rate": 0.03578924546103785, "l1": 0.3730851993396367, "wd": 0.0020367449461466797, "lr_scheduler_step": 153, "lr_scheduler_factor": 0.9895518165671255, "lr_scheduler_minimum_lr": 2.294182951069527e-05}
             [07/01/2024 19:50:55 INFO 139723470354240] Saved checkpoint to "/tmp/tmpxy_r1r8a/mx-mod-0000.params"
             [07/01/2024 19:50:55 INFO 139723470354240] Test data is not provided.
             #metrics {"StartTime": 1719863432.4299383, "EndTime": 1719863455.1427574, "Dimensions": {"Algorithm": "Linear Learner", "Host": "algo-1", "Operation": "training"}, "Metrics": {"initialize.time": {"sum": 1342.7329063415527, "count": 1, "min": 1342.7329063415527, "max": 1342.7329063415527}, "epochs": {"sum": 15.0, "count": 1, "min": 15, "max": 15}, "check_early_stopping.time": {"sum": 7.383584976196289, "count": 9, "min": 0.16188621520996094, "max": 4.042625427246094}, "update.time": {"sum": 18203.925371170044, "count": 8, "min": 2251.8746852874756, "max": 2326.157569885254}, "finalize.time": {"sum": 2108.978033065796, "count": 1, "min": 2108.978033065796, "max": 2108.978033065796}, "setuptime": {"sum": 1.8444061279296875, "count": 1, "min": 1.8444061279296875, "max": 1.8444061279296875}, "totaltime": {"sum": 22805.270433425903, "count": 1, "min": 22805.270433425903, "max": 22805.270433425903}}}

             2024-07-01 19:51:08 Completed - Training job completed
             Training seconds: 170
             Billable seconds: 170

         >>> print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))

             Here is the location of the trained Linear Learner model: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_output/ufo-linear-learner-job-20240701194728/output/model.tar.gz

             # Note: Linear Learner model had: multiclass_accuracy <score>=0.9469825155104343


             From here we have two trained models to present to Mr. K. Congratulations!

------------------------------------------------------
7.15 Creating a TensorFlow Image Classifier in Amazon SageMaker

  Resources

    Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\7_15_creating_a_tensorflow_demo

     -> Jupyter Notebook:
        CreateATensorFlowImageClassifier_completed.ipynb
     -> dataset data:
          lego-simple-train-images.npy
          lego-simple-train-labels.npy
          lego-simple-test-images.npy
          lego-simple-test-labels.npy

TensorFlow

  - TensorFlow is an end-to-end, open source platform for machine learning.
  - It has a comprehensive, flexible ecosystem of tools, libraries, and community resources.
  - This lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.
    (Source: https://www.tensorflow.org/)

Keras

  - Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.
  - It was developed with a focus on enabling fast experimentation.
  - Being able to go from idea to result with the least possible delay is key to doing good research.
    (Source: https://keras.io/)

Or, to put it another way:
  - TensorFlow is a complex and powerful tool, but Keras provides a high-level interface that allows us to rapidly
    prototype models without dealing with all of the complexity TensorFlow offers.

About this lab

TensorFlow is the biggest name in machine learning frameworks. In this lab, you will use TensorFlow to create
a neural network that performs a basic image classification task: deciding which LEGO brick is in an image
to help you sort your giant pile of blocks.

Learning objectives
  - Navigate to the Jupyter Notebook
  - Load and Prepare the Data
  - Train the TensorFlow Model
  - Evaluate the Model
  - Make a Batch Prediction


  Creating a TensorFlow Image Classification in SageMaker Flow:

                             Lego
  SageMaker Notebook ------- Image --------------------------> Keras   --- 2x3 Brick (94.2%) ---> TensorFlow


     SageMaker                          Datasets
                          Training 80%           Testing 20%


     Notes:
        TensorFlow: underlying framework
        Keras:      API

     ------------------------------------------------------
Creating a TensorFlow Image Classifier in AWS SageMaker
Introduction

TensorFlow is the biggest name in machine learning frameworks. In this lab, you will use TensorFlow to create a neural network that performs a basic image classification task: deciding which LEGO brick is in an image to help you sort your giant pile of blocks.
Solution
Log in to AWS Console

    Open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Navigate to the Jupyter Notebook

    In the search bar, type "Sagemaker" to search for the Sagemaker service.
    Click on the Amazon SageMaker result to go directly to the Sagemaker service.
    Click on the Notebook Instances button to look at the notebook provided by the lab.
    Check to see if the notebook is marked as InService. If so, click on the Open Jupyter link under Actions.
    Click on the CreateATensorFlowImageClassifier.ipnyb file.
    Wait for the kernel to spin up. You'll see a green button that reads Kernel ready in the upper right momentarily when the kernel is finished spinning up.

Load and Prepare the Data

    Make sure your kernel can support TensorFlow 2.
        Check the bar in the upper right corner to see if it contains tensorflow2.
        If it does not, click on Kernel in the menu, select Change kernel, and then select conda_tensorflow2_p36 from the list. The version number available to you may differ.

    Under 1) Import Libraries, select the cell containing the import code and click the Run button in the menu or press Ctrl + Enter to run the cell.
        You will see a warning that no CUDA-capable devices are detected. This is expected, since our notebook doesn't have access to any GPU's, and won't impact our lab.

    Under 2) Load the Data, update the code as below, and run the first code cell to load the training images and labels into numpy arrays. The images and labels are provided in the respective files.

    train_images = np.load('lego-simple-train-images.npy')
    train_labels = np.load('lego-simple-train-labels.npy')
    test_images = np.load('lego-simple-test-images.npy')
    test_labels = np.load('lego-simple-test-labels.npy')

    Run the second code cell to add in the human-readable class names for the labels.

    Run the rest of the code cells to visualize the first few images from the training data set and testing set to better understand the data.

Train the TensorFlow Model

    Under Create the Model, update and run the first code cell to create a flatten layer and two dense layers for a neural network model using Keras.

    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(48,48)),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(10, activation='softmax')
    ])

    Update and run the second code cell to compile the model, using adam as the optimizer, sparse categorical cross-entropy for the loss function, and accuracy as the metric.

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    Update and run the third code cell to train the model using the training data and training labels.

    history = model.fit(train_images, train_labels, epochs=4)

    Run the fourth code cell to save and plot the history of the training process in terms of accuracy values and loss values.

Test and Analyze the Model

    Under Evaluate the Model, update and run the first code cell to calculate the loss and accuracy of the model on the testing data.

    test_loss, test_acc = model.evaluate(test_images, test_labels)

    print('Test accuracy:', test_acc)

    Under Single Prediction, run the first code cell to pick a random image in the test set.

    Run the next code cell to transform the image into a collection of one image.

    Run the third code cell to pass the image into the predict method.

    predictions_single = model.predict(img)
    predictions_single

    Run the fourth code cell to use the argmax function to find the highest-probability prediction result found by the predict method.

    Run the fifth code cell to look up the class name of the prediction result and obtain the model's probability of being correct.

    Run the sixth code cell to determine the label that the model predicted for the image and compare that to the actual label.

    Run the seventh code cell to run functions to display the image and the results as a graph.

    Run the eighth code cell to run a function to display the results as a bar chart.

Make a Batch Prediction Using the Testing Data

    Under Batch Prediction, run the first and second code cell to predict the labels for all of the test images.
    Run the third code cell to summarize the results with bar charts measuring the probability of the predictions for the first 16 images of our test data.

Conclusion

Congratulations - you've completed this hands-on lab!
     ------------------------------------------------------

------------------------------------------------------
7.16 Creating an MXNet Image Classifier in Amazon SageMaker


  Resources

    Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\7_16_creating_an_mxnet_image_classifier_demo

     -> Jupyter Notebook:
        CreateAnMXNetImageClassifier.ipynb
        -> original notebook: need to rename without '.orig' to use else you may have a load error
           CreateAnMXNetImageClassifier.orig.ipynb
     -> dataset data (pickled files):
          lego-simple-mx-train
          lego-simple-mx-test


  python pickle:
     - The pickle module implements binary protocols for serializing and de-serializing a Python object structure.
     - "Pickling" is the process whereby a Python object hierarchy is converted into a byte stream, and
       "unpickling" is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted
        back into an object hierarchy.

  mxnet transforms
    Gluon provides pre-defined vision transformation and data augmentation functions in the mxnet.gluon.data.vision.transforms module.
    https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/transforms.html
  mxnet transforms.compose()
    With Compose we can choose and order the transforms we want to apply.
  mxnet transforms.compose ToTensor & Normalize:
    transforms.Compose([ transforms.ToTensor(), transforms.Normalize(0.13, 0.31)])
    Normalize
     - We scaled the values of our data samples between 0 and 1 as part of ToTensor but we may want or need to normalize
       our data instead: i.e. shift to zero-center and scale to unit variance


Frameworks

For this lab, we will be using Apache MXNet to build and train a model to classify images, and specifically take advantage of the Gluon API provided with MXNet to make that process really easy.
MXNet

A flexible and efficient library for deep learning.

    MXNet provides optimized numerical computation for GPUs and distributed ecosystems, from the comfort of high-level environments like Python and R.
    MXNet automates common workflows, so standard neural networks can be expressed concisely in just a few lines of code.

(Source: https://mxnet.apache.org/)
Gluon

Based on the the Gluon API specification, the Gluon library in Apache MXNet provides a clear, concise, and simple API for deep learning. It provides basic building blocks for neural networks that make it easy to prototype, build, and train deep learning models without sacrificing training speed. Install a recent version of MXNet to get access to Gluon.

(Source: AWS Blog)



Apache MXNet is an open-source machine learning framework focusing on deep learning with neural networks.
In this lab, you will use MXNet to create a neural network that performs a basic image classification task:
deciding which LEGO brick is in an image to help you sort your giant pile of blocks. MXNet supports many
programming languages, but we will use Python.

Learning objectives
  - Navigate to the Jupyter Notebook
  - Load and Prepare the Data
  - Train the MXNet Model
  - Evaluate the Model
  - Make a Batch Prediction


  Creating an MXNet Image Classification in SageMaker Flow:

                             Lego
  SageMaker Notebook ------- Image -------------------------->  MXNet
                                                                Apache MXNet

                                                                  Model
     SageMaker                          Datasets
                          Training 80%           Testing 20%

      ------------------------------------------------------
Solution
Log in to the Lab Environment

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Navigate to the Jupyter Notebook

    In the search bar on top, type "Sagemaker" to search for the Sagemaker service.
    Click on the Amazon SageMaker result to go directly to the Sagemaker service.
    Click on the Notebook Instances button to look at the notebook provided by the lab.
    Check to see if the notebook is marked as InService. If so, click on the Open Jupyter link under Actions.
    Click on the CreateAnMXNetImageClassifier.ipnyb file.
    When prompted, select the conda_python3 kernel. In the notebook, the code line of !pip install mxnet installs the needed mxnet package.
    Wait for the kernel to spin up. You'll see a green button that reads Kernel ready in the upper right momentarily when the kernel is finished spinning up.

Load and Prepare the Data

    Under 1) Import Libraries, select the cell containing the installation command for Apache MXNet, and run the cell either by clicking the Run button in the menu or press Ctrl + Enter to run the cell.

    Run the second cell to import the necessaries libraries for this lab, including the now-installed Apache MXNet

    Run the third cell to set the random seed for the lab to ensure reproducibility.

    Run the fourth cell to tell MXNet that it's using a CPU rather than a GPU for this lab.

    Under Load the Data, update and run the first cell to load the training and testing images and labels into an NDArray using Pickle. The images and labels are provided in the lego-simple-mx-train file and the lego-simple-mx-test file.

    train_fh = open('lego-simple-mx-train', 'rb')
    test_fh = open('lego-simple-mx-test', 'rb')

    train_data = pickle.load(train_fh)
    test_data = pickle.load(test_fh)

    Run the second cell to Add in the human-readable class names for the labels.

    Under Convert to MXNet Tensors, run the first cell to convert the training and testing NDArrays to MXNet Tensors. For better results, normalize the data using the mean of 0.13 and standard deviation of 0.31, which have been precomputed for this dataset.

    Run the second cell to visualize the first few images from the training data set to better understand the data.

    Run the third cell to look at more of the data formatted into a graph.

Train the MXNet Model

    Under 3) Create the Model, update and run the first cell to create a neural network model with one flatten layer and three dense layers using Gluon.

    net = nn.HybridSequential(prefix='MLP_')
    with net.name_scope():
        net.add(
            nn.Flatten(),
            nn.Dense(128, activation='relu'),
            nn.Dense(64, activation='relu'),
            nn.Dense(10, activation=None)
        )

    Run the second cell to load the data using Gluon's data loader with a batch size of 34.

    Run the third cell to initialize the model.

    Run the fourth cell to create a trainer object and use it to maintain the state of the training.

    Run the fifth cell to define the accuracy metric, keep track of accuracy during the training process, and choose a loss function appropriate for classification tasks.

    Run the sixth cell to train the model using the training data and training labels for 10 epochs, and save the history of the training process.

Evaluate the Model

    Under 4) Evaluate the Model, run the first cell to calculate the accuracy of the model on the testing data and output the data as a graph.

    Update and run the second cell to apply the model to the testing data.

    test_loader = mx.gluon.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)

    Run the third cell to measure the accuracy of the model.

Test the Model

    Under 5) Test the Model, run the cell to define a couple of functions to display the model's results as a graph.

    Under Single Prediction, run the first cell to choose an image from the test set.

    Update and run the second cell to make a prediction using the model.

    predictions_single = net(prediction_image)
    predictions_single

    Run the third cell to look at the resulting predictions as a bar chart.

    Run the fourth cell to visualize the highest-accuracy prediction as an image.

Make a Batch Prediction

    Under Batch Prediction, run the first cell to predict the classes for 16 images from the testing data.
    Run the second cell to compare the predictions against the actual data.

Conclusion

Congratulations - you've completed this hands-on lab!

      ------------------------------------------------------

------------------------------------------------------
7.17 Creating a scikit-learn Random Forest Classifier in Amazon SageMaker


  Resources

    Note: Downloaded github demo files to:
     C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\7_17_creating_scikit_learn_random_forest_classifier_demo

     -> Jupyter Notebook:
        CreateAScikit-LearnRandomForestClassifier_completed.ipynb
     -> dataset data:
          data.csv
     -> generated:
        random forest decision tree graph: tree.png


   sklearn.metrics.roc_curve(y_true, y_score, ...)
     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html
     - Compute Receiver operating characteristic (ROC).
     - Note: this implementation is restricted to the binary classification task.
     Returns:
    fpr: ndarray of shape (>2,)
        Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].
    tpr: ndarray of shape (>2,)
        Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].
    thresholds: ndarray of shape (n_thresholds,)
        Decreasing thresholds on the decision function used to compute fpr and tpr.

   sklearn.metrics.auc(x, y)
     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html
     - Compute Area Under the Curve (AUC) using the trapezoidal rule.
     - This is a general function, given points on a curve.
     Parameters:
       x: array-like of shape (n,)
           X coordinates. These must be either monotonic increasing or monotonic decreasing.
       y: array-like of shape (n,)
           Y coordinates.
     Returns:
       auc: float
          Area Under the Curve.

  scikit-learn, a machine learning framework.
    - Designed to be native to Python, scikit-learn contains various classification, regression, and clustering algorithms,
      including random forests which we use in this lab.



About this lab

Scikit-learn is a great place to start working with machine learning. In this lab, we will use scikit-learn to create a
Random Forest Classifier to determine if you prefer cats or dogs. The data set being used is entirely made up, but could
easily be swapped with one of your own!

Learning objectives
  - Navigate to the Jupyter Notebook
  - Load and Prepare the Data
  - Train the Random Forest Model
  - Evaluate the Model
  - Predict for Yourself



  Creating an MXNet Image Classification in SageMaker Flow:

                             Survey
  SageMaker Notebook ------- Data  -------------------------->  Scikit Learn

                                                                  Model

     SageMaker                          Datasets                  Survey Questions
                          Training 80%           Testing 20%      1. Do you like walking?
                                                                  2. Do you like running?
                                                                  3. What is your favorite 'color'?
                                                                  4. What 'distance' do you walk?
                                                                  -> Do you prefer 'cats' or 'dogs'?
      ------------------------------------------------------
Solution
Log in to the Lab Environment

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Navigate to the Jupyter Notebook

    In the search bar, type "Sagemaker" to search for the Sagemaker service.
    Click on the Amazon SageMaker result to go directly to the Sagemaker service.
    Click on the Notebook Instances button to look at the notebook provided by the lab.
    Check to see if the notebook is marked as InService. If so, click on the Open Jupyter link under Actions.
    Click on the CreateAScikit-LearnRandomForestClassifier.ipnyb file.

Load and Prepare the Data

    Make sure you have the conda_python3 kernel.
        Check the bar in the upper right corner to see if it contains conda_python3.
        If it does not, click on Kernel in the menu, select Change kernel, and then select conda_python3 from the list. The version number available to you may differ.

    Under 1) Import Libraries, run the cell containing the import code by either clicking the Run button in the menu or pressing Ctrl + Enter to import the standard Python libraries as well as scikit-learn.

    Under 2) Prepare the Data, update and run the first cell to load the survey data from the data.csv file.

    df = pd.read_csv("data.csv")

    Under 2) Prepare the Data, run the second cell to look at the top ten results.

    Run the third cell to look at the data types for the different columns.

    Run the fourth cell to change the column names for more clarity.

    Run the fifth cell to describe the data as a whole and obtain statistical information about the data.

    Update and run the sixth cell to format the data so that the model will understand it better, specifically by changing some answer values into boolean data type or categorical data type.

    df['walk'] = df['walk'].astype('bool')
    df['run'] = df['run'].astype('bool')
    color_type = CategoricalDtype(categories=['red','green','blue'])
    df['color'] = df['color'].astype(color_type)
    df['label'] = df['label'].astype('bool')

    Run the seventh cell to look at the data types for the different columns.

    Update and run the eighth cell to perform a one hot encoding process to format the categorical data type.

    df = pd.get_dummies(df, prefix=['color'])

    Run the ninth cell to look at the top ten results again, now that the data has been reformatted.

    Update and run the tenth cell to split the data into training and testing sets, using 20% of the data for testing and 80% of the data for training.

    X_train, X_test, y_train, y_test = train_test_split(df.drop(labels='label', axis=1), df['label'], test_size = .2, random_state = 10)

Create the scikit-learn Model

    Under 3) Create the Model, run the first cell to create a Random Forest Classifier model using scikit-learn.

    Under 3) Create the Model, update and run the second cell to train the model using the training data.

    model.fit(X_train, y_train)

Evaluate the Model

    Under 4) Evaluate the Model, run the first cell to grab the estimator from the trained model and look at one of the decision trees.

    Under 4) Evaluate the Model, run the second cell to generate a graph of that decision tree.

    Update and run the third cell to run the testing data through the model.

    y_predict = model.predict(X_test)

    Run the fourth cell to generate a confusion matrix for the testing data and see how well the model performed.

    Run the fifth cell to format the confusion matrix using the seaborn library.

    Run the sixth cell to calculate the sensitivity and specificity from the confusion matrix.

    Run the seventh cell to plot the ROC curve.

    Run the eighth cell to calculate the area under the curve.

Predict for Yourself

    Under 5) Predict for yourself, type in answers for yourself in the first cell to create your own survey response.
    Under 5) Predict for yourself, run the first cell to have the model predict if you prefer cats or dogs.

Conclusion

Congratulations - you've completed this hands-on lab!

      ------------------------------------------------------

------------------------------------------------------
7.18 Quiz AWS Certified Machine Learning - Specialist 2020 - Algoritms

Question 6

Which of the following are good candidate problems for using XGBoost?

Choose 3

 - Evaluating handwritten numbers on a warranty card to detect what number they represent.  Correct

 - Mapping a text string to an n-gram vector.

 - Creating a policy that will guide an autonomous robot through an unknown maze.

 - Providing a ranking of search results on an e-commerce site customized to a customer's past purchases.  Correct

 - Deciding whether a transaction is fraudulent or not based on various details about the transaction.  Correct
Good work!

Answer Info:
XGBoost is an extremely flexible algorithm which can be used in regression, binary classification, and multi-class classification.
XGBoost can be a good choice here if speed is key and the dataset isn't too large.

XGBoost can be a good choice here, but it will largely depend on the number of products. XGBoost can be considered for many types
of regression, binary classification, and multi-class classification problems.

XGBoost is an extremely flexible algorithm which can be used in regression, binary classification, and multi-class classification.


Question 10

Which of these examples would be considered as introducing bias into a problem space?

Choose 2

 - Not randomizing a dataset even though you were told the data should be random.   Correct

 - Deciding to use a supervised learning method to estimate missing values in a dataset.

 - Filtering out outliers in a dataset which are greater than 4 standard deviations outside the mean. Incorrect

 - Removing records from a set of customer reviews that were not fully complete.  Correct

 - Omitting records before a certain date in a forecasting problem.

Sorry!
Correct Answer

Failing to randomize records even when you think they are already random can lead to introducing bias in the model when splitting.

Additionally, removing customer review records could unintentionally filter out statistically significant parts of the dataset,
biasing towards those customers who have the time or energy to fully complete the reviews.

The other answers are reasonable and do not explicitly introduce bias.


Question 1 (retry 1)

  You have been asked to help develop a vision system for a manufacturing line that will reorient parts to a specific position using
  a robotic arm. What algorithm might you choose for the vision part of this problem?

Choices:
  AWS Comprehend

  Object Detection

  Seq2Seq

  Image Analysis

  Semantic Segmentation   <--- Correct Answer

  Object2Vec
Good work!

  Semantic Segmentation can perform edge detection, which could be used to identify orientation. Object Detection and Image Analysis
  are better used for full images where as Semantic Segmentation can create a segmentation mask or outline of a part of the image.


Question 2 (retry 1)

  You are on a personal quest to design the best chess playing model in the world. What might be a good strategy for this objective?

Choices:
  Use a Factorization Machine approach to analyze the winning series of moves across thousands of chess matches to find the best series of moves.

  Use an unsupervised learning strategy to analyze similarities across thousands of the best chess matches.

  Use Mechanical Turk to crowdsource the best chess moves in a variety of scenarios then use those for a supervised learning session.

  Use a reinforcement learning strategy to let the model learn itself.

  Use a supervised learning strategy that is trained by feeding in the chess moves of thousands of famous chess experts.
Good work!

  Chess is a complex game and would require some training to develop a good model. Supervised learning, by definition, can only
  be as good as the training data that it is supplied with so we won't be able to meet our goal of the best chess model using
  supervised learning. Rather, reinforcement learning would have the best chance of becoming better than any other player if given
  enough iterations and a proper reward function.


Question 6 (retry 1)

  Which text analysis algorithm is recommended when trying to compare documents based on the presence of words in each document?

Choose 2

  Sequence to Sequence (seq2seq)

  BlazingText

  Neural Topic Model (NTM)            <--- Correct Answer

  Latent Dirichlet Allocation (LDA)   <--- Correct Answer
Good work!

  NTM is promising for achieving high topic coherence (measuring the semantic similarity between high-scoring words in the topic)
  and can scale more easily than LDA.

  LDA is most commonly used to identify topics in common between documents in a text corpus.



Question 7 (retry 1)

  You are consulting with a retailer who wants to evaluate the sentiment of social media posts to determine if they are positive
  or negative. They have very little understanding of machine learning and want a solution that is as simple and straightforward
  as possible with the least amount of operational overhead. Which approach is best for this analysis?

Choices:
  Use BlazingText in Word2Vec mode for skip-gram.

  Use Object2Vec in sentiment detection mode.

  Use BlazingText in Text Classification mode.

  Use Amazon Comprehend.    <--- Correct Answer

  Use Amazon Macie.
Good work!

  Comprehend provides a pre-trained sentiment analysis model that can classify text into categories like positive, negative, neutral,
  and mixed. This meets the client's requirements for simplicity and low operational overhead, as it is fully managed and does not
  require training or model tuning.


Question 8 (retry 1)

  You are being asked to develop a model to predict the likelihood that a student will pass a certification exam based on hours
  of study. Of the options given, what would be the best approach to this problem?

Choices:
  Build a Logistic Regression model using the hours of study as as a feature.                             <--- Correct Answer

  Build a simulation-based model which will analyze past student performance at varying levels of study.

  Build a model using LDA based on the text of the questions on the exam and predict student outcome.

  Build a clustering mode with K-Means to group students who pass in a cluster.

  Build a model using NLP to classify students into passing and failing groups.
Good work!

  This problem is best described as a binary classification problem as we are trying to understand whether a student will pass or fail.
  The option that most directly provides for a binary classification problem is logistic regression.

--------------------

Logistic Regression in Machine Learning
  https://www.geeksforgeeks.org/understanding-logistic-regression/

  - Logistic regression is a supervised machine learning algorithm used for classification tasks where the goal is to
    predict the probability that an instance belongs to a given class or not.
  - Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent
    variables and produces a probability value between 0 and 1.
--------------------


Question 9 (retry 1)

  Which of the following is an example of unsupervised learning?

Choose 2

  Using Seq2Seq to extract a text string from a segment of a recorded speech.

  Using K-Means to cluster customers into demographic segments.              <--- Correct Answer

  Using a Factorization Machine to provide book recommendations.

  Using NTM to extract topics from a set of scientific journal articles.     <--- Correct Answer

  Using XGBoost to predict the selling price of a house in a particular market.
Good work!

  Both K-Means and Neural Topic Modelling are unsupervised learning methods. XGBoost, Seq2Seq, and Factorization Machines are
  supervised learning methods.


Question 10 (retry 1)

  When analyzing a set of one-hot encoded data you realize that, while there is a massive amount of data, most of the values are absent.
  This is expected given the type of data, but what built-in SageMaker algorithm might you choose to work with this data?

Choices:
  Factorization Machines       <--- Correct Answer

  Semantic Segmentation

  Linear Learner

  K-Nearest Neighbor

  Object2Vec
Good work!

  From the description, it seems that we have a very sparse dataset. Factorization Machines is most often associated with
  high-dimensional sparse datasets that we are likely to see in one-hot encoded datasets.


Question 2 (retry 2)

Which best describes SGD in common terms?

Choices:
  Attempt to find the most efficient path to deliver packages to multiple destinations.

  Calculate the linear distance between arrows shot into a target to determine accuracy.

  Ensure that our sample size in a traffic study has at least 30 drivers.

  Seek to find the lowest point in elevation on a landscape.
Good work!

  Stochastic Gradient Descent (SGD) is a cost function that seeks to find the minimal error. This can be analogous to
  trying to find the lowest point on a landscape.


Question 9

You have created a deterministic, step-by-step process for patching servers to remove inconsistency in the process.
This process always yields the same result when given the same input. How would this best be described?

Choices:
  The procedure is an algorithm, as it will allow the person performing the upgrades to choose the best path.

  The procedure is a heuristic, as it will yield a consistent output.

  The procedure is a workaround until a supervised learning process can be implemented, which will remove the human from the process.

  The procedure is a heuristic, as it permits the person performing the upgrades from making unintended mistakes.

  The procedure is an algorithm, as it will yield a consistent output.
Good work!

  An algorithm is a step-by-step procedure that is used to perform a specific task or solve a specific problem. It is deterministic,
  meaning it will always produce the same output when given the same input. The described process fits this definition.
---------------
Algorithms and heuristics are both problem-solving strategies, but they differ in their approach, accuracy, and efficiency.
  Algorithms
    A set of step-by-step instructions that guarantee a correct solution
  Heuristics
    A mental shortcut or rule of thumb that can help with problem-solving and probability judgments
---------------

------------------------------------------------------

Chapter 8 Evaulation and Optimizations

------------------------------------------------------
8.1 Introduction

  - focus on evaluation of model (is the model any good)

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      - time to train the model
      Train Model --->  Evaluate Model --->

    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
8.2 Concepts


  Model Components:
                       Generalization
                           ^
     -> Data --------->    |     <------- computation
     |                   Model
     |          -------->        --------->
     | Algorithm                 <---------  Feedback
     |           <-------------------------
     \-------------------------------------

  Model Goal:
    - generalization not memorization

  Evaluation Steps
    Define Evaluation
      - decide what metric or metrics we should use to decide if the algorithm is "good enough"
    Evaluate
      - review the metrics during or after the training process
      - this might be manual or automatic, depending on the algorithm
    Tune
      - Adjust hyperparameters, data, the evaluation strategy or even the entire algorithm to bring
        us close to the desired results

    Repeat
      - as needed, repeat the process till meeting end goals

  Validation types
    Offline validation
      - validation done using test sets of data
      - example: Validation datsets and k-fold validation
    online validation
      - validation under real-world conditions
      - Example: Canary deployment or A/B Testing



  Canary Deployment
    - shift a fraction of the real-world traffic to the new version, watch the inference and accuracy rate
      compared to current version, if it is better, then shift more traffic over time, else remove new version

                            --- 99% ---> Current Version    ---->
      Users ----> Route 53                                         cloudWatch
                            ---  1% ---> New "Canary" verion ---->


------------------------------------------------------
8.3 Monitoring and Analyzing Training Jobs

  Metric computed by the linear learning algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html

  Creating a Custom Algorithm Container in SageMaker
    https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-training-container.html

  Metric types
    Training Metrics
      - training metrics you generally use during the training process, to see if that process is going okay.
      - SageMaker docs, the training metrics have a test prefix,
    Validation Metrics
      - Validation metrics are used when you're testing your model.
      - SageMaker docs, the validation metrics have a validation prefix



  Metric storing & viewing
    storing metrics
    - AWS sends the metrics to logs in cloudwatch
    - shows accuracy, speedn
    Viewing metrics
      - can view metrics from SageMaker console
      - can create charts in the cloudwatch console
      - in cloudwatch logs, you can see the job speed and accuracy figures as the job progresses

  Algorithm metrics for custom algorithms
    - can send algorithm metrics for custom algorithms to cloudwatch


     Code: Adding Metrics definition to custom algorithm Example
           - adding regular expression to extract train/validation metrics from output

           from sagemaker.estimator import Estimator

           estimator = Estimator(image_uri='tf-custom-container-test',
                                 role=sagemaker.get_execution_role(),
                                 instance_count=1,
                                 instance_type='m1.c4.xlarge',
                                 k=10
                                 sagemaker_session=sagemaker_session,
                                 metric_definition = [
                                    {'Name': 'train:error', 'Regex': "Train_error=(.*?);'},
                                    {'Name': 'validation:error', 'Regex': "Valid_error=(.*?);'}
                                 ]
                                 )
           estimator.fit()

------------------------------------------------------
8.4 Evaluating Model Accuracy


  Underfitting
    - our model just isn't very reflective of the underlying data shape
    - example: model uses straight line when data is not a straight line
    - mayb we need some more variables to help train our model and achieve a better fit

  Prevent Underfitting
    More Data
      - sometimes more data will provide enough additional entropy to steer your algorithm away
        from overfitting
    Train longer
      - the algorith needs more iterations with the data to minimize error


  Overfitting
    - Our model is too dependent on that specific data that we used to train
    - if it sees any new data, accuracy will likely be poor unless the data is identical to the
      training data
    - we have trained our algorithm to memorize rather than generalize


  Robust Model
    - models that fits the training data but also does reasonably well on new data which it has
      not seen before
    - model can deal with noise in the data
    - it can generalize effectively for that new data

  Data split
    - 70 - 80% for training data
    - 20 - 30% for testing data


  Training and Testing error:

     Training Error    Testing Error
     --------------    ---------------         ---------------------------------
     Low                Low                     You want this!
     Low                high                    Overfitting
     high               high                    Try another approach   (underfitting model)
     high               Low                     Run away - this should not occur!

  Prevent overfitting
    More data
     - sometimes more data will provide enough additional entropy to steer your algorithm away
       from overfitting
    Early stop
      - tell the training process to stop once you've achieved a certain accuracy to prevent overfitting
      - terminate the training process before it has a chance to "over train"
      - many algorithms include this option as a hyperparameter
    Sprinkle in Some Noise
      - you training data could be too clean and you migh need to introduce some noise to generate
        the model
    Regulate
      - Regulation forces your model to be more general by create constraints around weights or smoothing
        the input data
      - this has an effect of trying to make the model more generic, so it's less tied to the actual
        discrete numbers in the input data.
    Ensemble
      - combine different models together to either amplify individual weaker models (boosting) or
        smooth out strong models (bagging)
    Drop some Features
      - (aka Feature Selection) too many irrelevant features can influence the model in a negative way
        by drowning out the signal with noise


  Regression Accuracy
    Residuals (actual number - predicted number)
       - negative residual:  predicted value was higher than my actual value
       - positive residual:  predicted value was lower than my actual value
      Residual histogram
        - like to see a bell curve centered around zero
        - that is, equal amounts of the time we predicted either too high or too low
        - if center around negative value, means predicting consistently too high


  Root Mean Square Error (RMSE)
     - Equation:
             RMSE = ( (1/n) SUM (actual_i - predicted_i)**2 )**-1/2

                 where SUM is from i=1 to i=n

     - a measure of the distance between an actual observation and predicted value
       across all the predicted values
     - lower RMSE is better (means less error, therefore more accurate predictions)



  Binary Classification Accuracy:
       Confusion Matrix
                                          Actual Outcomes

                                 True                        False

                 True      predictedly correctly         predicted incorrectly
      Predicted             (True Positive)              (False Positive)
      outcomes                                            TYPE I ERROR

                 False     predicted incorrectly         predicted correctly
                            (False Negative)              (True Negative)
                             TYPE II ERROR


  Amazon Machine Learning vs SageMaker
    - Amazon has phased out Amazon Machine Learning
    - Amazon ML generally geared for beginners for entry level machine learning activities,
      But now Amazon is encouraging everybody to go straight to SageMaker



  Area Under the Curve (AUC)
    - score can be zero to one, but you want your score to be as close to '1' as possible
      because that means it is highly accurate



     Recall    =   (TP)  / (TP + FN)      Note: Spam gets through
               - measure TP out of all the actual positives
               - percentage of true positives all actual positives
               - measure of a model correctly identifying TPs
               - also called Senstivity or TPR

     Precision =   (TP)  / (TP + FP)      Note: Legitmate emails get blocked
               = measure TP out of all predicted positives
               = percentage predicted positives that were correct
               = ratio between the True Positives and all the actual Positive

     Notation: [was prediction correct: true/false][actual vs predicted: positive(true)/negative(false)]

     Recall vs Precision
       - As recall goes up, precision decreases
       - you can have a 100% recall but that means that we've flagged lots of items that probably
         should not have been flagged

  F1 Score
   - equation:

      F1 Score =  2 * ( Precision * Recall) / (Precision + Recall)

      F1 Score =  2 * ( Precision * Recall) / (Precision + Recall)
   - the balance between precision and recall
   - a larger value indicates better predictive accuracy

  From ORiely Hands ON Machine Learning (chapter 3):

       Confustion Matrix:

                        |  Predicted Class     |     Predicted Class
                        |  Negative            |     Positive
                        |  (NOT FRAUD)         |     (FRAUD)
          --------------|----------------------|----------------------
          Actual Class  | True Negative (TN)   |  False Positive (FP)
                        |                      |
          Negative      | NOT FRAUD was        | FRAUD was incorrectly
          (NOT FRAUD)   | correctly predicted  | predicted as NOT FRAUD    ^
                        | as NOT FRAUD         |                           |
          --------------|----------------------|----------------------     |
          Actual Class  | False Negative (FN)  |  True Positive (TP)       |
                        |                      |                           |
          Postive       | FRAUD was incorrectly| FRAUD was correctly       |
          (FRAUD)       | predicted as         | predicted as FRAUD       Precision
                        | NOT FRAUD            |
          --------------|----------------------|----------------------
                                                  <-------- Recall

        Metric for Classification Problems

           Accuracy: (TP + TN)  / (TP + FP + TN + FN) = Correct Predictions / All Predictions
              - percentage of predictions that were correct:
              - Accuracy shows how often a classification ML model is correct overall.
              - accuracy is useful with balanced classes and you care about overall correctness
              - accuracy is less useful when you have imbalanced classes since it gives equal weight
                to predict all categories
              - less effective with a lot of true negatives
                 - example: predicting fraud with little to no fraud data

           Precision: (TP)  / (TP + FP)
              - accuracy of positive predictions
              - Precision shows how well the model detects the positive class.
              - Precision works well for problems with imbalanced classes since it shows the model correctness in
                identifying the target class.
              - Precision is a suitable metric when you care more about being right when assigning the positive
                class than detecting them all.
              - percentage of positive predictions that were correct:
              - Use when the cost of false positives is high
                 - example: an email is flagged and deleted as spam when it really isn't

           Recall: (TP)  / (TP + FN)
              - also called sensitivity or true positive rate (TPR)
              - percentage of actual positive predictions that were correctly identified:
              - recall is a suitable metric when you care more about being right when assigning the negative class
                than missing some of them.
              - use when false negative errors as more costly than false positives
                 - example: fraud detection
              - Use when the cost of false negatives is high
                 - example: someone has cancer, but screening does not find it

           F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
             - combined precision and recall score
             - harmonic mean of the precision and recall
             - regular mean treats all values equally, the harmonic mean give more weight to low values
             - classifiers will only get high F1 Score if both recall and precision values are high

           Equation 3-3: F1 score:

           F1 = 2 / [ (1/precision) + (1/recall)]  =  2 x [( precision x recall) / (precision + recall)]

              = (TP)  / [TP + ((FN + FP) / 2)]


  Improving Model Accuracy:
    Collect data
      - increase the number training data examples available to the model
      - more (good) data usually means more accurate model
    Feature Processing
      - provide additional quality variables or refine the existing variables so they are more
        representative
    model Parameter Tuning
      - adjust the hyperparameters used by your training algorithm
    Bias [underfitting] and overfitting [high variance]
      - Beware of introducing bias and overfitting
      - you don't want it to achieve a predefined outcome that you have decided either
        consciously or subconsciously - Rather you want a correct outcome


  -------------
  from OReilly Hands-On ML (chapter 4.4):

      Improving underfitting
        - use a better model
        - come up with better features

      Improving Overfitting
         - feed more training data until the validation error reaches the training error

     The Bias / Variance Trade-off
       - a model generalization error can be expressed at the sum of three very different errors:
       Bias:
         - the part of the generalization to wrong assumptions, such as assuming the data is linear
           when it is actually quadratic
         - a high-bias model is most likely underfitting the training data
       Variance:
         - this part is due to the model's excessive sensitivity to small variation in the training data
         - a model with many degrees of freedom (such as a high-degree polynomial model) is likely  to
           have high-variance and thus 'overfit' the training data
       Irreduclibe Error:
         - due to the noiseness of the data
         - the only way to reduce this part of the error is to clean-up the data (fix the data sources, such
           as a broken sensor, or detect and remove outliers)
        Note:
          - Increasing a model's complexity will typically increases its variance and reduce its bias
          - Decreasing a model's complexity will typically increases its bias and reduce its variance
  -------------


------------------------------------------------------
8.5 Model Tuning


  AWS SageMaker Linear learner hyperparameters:
    https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html

  AWS SageMaker Tune a Linear learner model:
    https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html

  AWS SageMaker Tune a BlazingText model:
    https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html


  Model Tuning
    - making small adjustments to hyperparameters to improve the performance of the model
    - note: some hyperparameters are required, others are optional (have defaults)


  Automatic Model Tuning
   - Also known as hyperparameter tuning
   - finds the best version of a model by running many jobs that test a range of hyperparameters
     on your dataset

  Automatic Model Tuning steps
    Choose tunable hyperparameter
      - decide what hyperparameter you want to adjust
      - not all hyperparameters are auto tunable
    choose a range of values
      - specify a range of values (min to max) to use for tuning the hyperparameter, paying
        attention to the allowable maximum
    choose the objective metric
      - specify the objective metric that the auto-tuning jobs will seek to optimize


   Example:
   Linear Learning tunable hyperparameters:
    https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html

    Tuning linear learner hyperparameters

    You can tune a linear learner model with the following hyperparameters.

    Parameter Name 	                Parameter Type 	                Recommended Ranges
    -----------------------             -------------------------       ---------------------------
    wd 	                                ContinuousParameterRanges       MinValue: 1e-7, MaxValue: 1
    l1 	                                ContinuousParameterRanges       MinValue: 1e-7, MaxValue: 1
    learning_rate 	                ContinuousParameterRanges       MinValue: 1e-5, MaxValue: 1
    mini_batch_size 	                IntegerParameterRanges          MinValue: 100, MaxValue: 5000
    use_bias 	                        CategoricalParameterRanges      [True, False]
    positive_example_weight_mult 	ContinuousParameterRanges       MinValue: 1e-5, MaxValue: 1e5


    wd
      - The weight decay parameter, also known as the L2 regularization parameter.
      - If you don't want to use L2 regularization, set the value to 0.
    l1
      - The L1 regularization parameter.
      - If you don't want to use L1 regularization, set the value to 0.
      - Optional; Valid values: auto or non-negative float; Default value: auto
    learning_rate
      - the step size used by the optimizer for parameter updates.
      - Optional; Valid values: auto or positive floating-point integer
      - Default value: auto, whose value depends on the optimizer chosen.
    mini_batch_size
      - The number of observations per mini-batch for the data iterator.
      - Optional; Valid values: Positive integer; Default value: 1000
    use_bias
      - Specifies whether the model should include a bias term, which is the intercept term in the linear equation.
      - Optional; Valid values: true or false; Default value: true
    positive_example_weight_mult
      - The weight assigned to positive examples when training a binary classifier. The weight of negative examples is fixed at 1.
      - If you want the algorithm to choose a weight so that errors in classifying negative vs. positive examples have equal impact
        on training loss, specify balanced. If you want the algorithm to choose the weight that optimizes performance, specify auto.
      - Optional; Valid values: balanced, auto, or a positive floating-point integer; Default value: 1.0

  Optimization Metrics
    Training Metrics
      - training metrics you generally use during the training process, to see if that process is going okay.
      - SageMaker docs, the training metrics have a test prefix,
    Validation Metrics
      - Validation metrics are used when you're testing your model.
      - SageMaker docs, the validation metrics have a validation prefix


  Choosing metric
    - AWS does have certain recommendations on which metrics to use, depending on the algorithm.
    - In the case of Linear Learner, AWS recommends that we use a validation metric as the objective rather
      than a training metric to avoid overfitting.
      https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html


  Choosing metric - example BlazingText
    https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html
    - optimization metrics may differ by how we're using the algorithm.
    - If using BlazingText in the Word2Vec mode, then AWS recommends using training 'mean_rho' metric
    - if using BlazingText for Text Classification, then AWS recommends we use the validation 'accuracy' metric.
    - tunable hyperparameters differ between using BlazingText for Word2Vec or for Text Classification.

  Bayesian Hyperparameter Optimization
   - tries to avoid costly iterations by focusing on what hyperparameter value is likely to give more improvement
   - What Bayesian optimization builds a probability model to more intelligently send in optimization values
     that will likely give better improvement
   - it's more efficient than random or sequential [matrix/grid] attempts

   https://towardsdatascience.com/bayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f
   - Bayesian Optimization has been widely used for the hyperparameter tuning purpose in the Machine Learning world.
   - hyperparameter optimization methods, which are generally 4 types:
     Manual Search, Random Search, Grid Search, and Bayesian Optimization
   - Bayesian Optimization differs from Random Search and Grid Search in that it improves the search speed using past
     performances, whereas the other two methods are uniform (or independent) of past evaluations
   - Bayesian Optimization builds a probability model of the objective function and uses it to select hyperparameter
     to evaluate in the true objective functio


  from OReilly Hands-On ML (chapter 10):
        kt.BayesianOptimization tuner
           - gradually learns which regions of the hyperparameter space are most promising by a fitting a probabilistic
             model called 'Guassian process'
           - this allows it gradually zoom in on the best hyperparameters
           - the downside is this algorithm has its own hyperparameters:
             alpha: represents the level of noise you expect in the performance measures across trial (default: 10**-4)
             beta: specifies how much you want the algorithm to explore, instead of simply exploiting the known
                  good regions of hyperparameter space (default: 2.6)


------------------------------------------------------
8.6 Exam Tips

  Resources:

    Use Amazon SageMaker built-in algorithms or pretrained models
      https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html
      -> use cases for built-in algorithms
      -> summary of all built-in algorthms


    Easily monitor and visualize metrics while training models on Amazon SageMaker
      https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/

      - step-by-step demo provided in the below blog using:

       sample notebook github repo:
         https://github.com/aws/amazon-sagemaker-examples/tree/main/introduction_to_amazon_algorithms/object2vec_sentence_similarity
         -> Provided in demos/8_6_blog_sagemaker_builtin_metric_support

    Amazon SageMaker Automatic Model Tuning: Using Machine Learning for Machine Learning
      https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/

    Use Incremental Training in Amazon SageMaker
      https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html



  Evaluation and Optimization

    Concepts
      - remember you want to Generalize ... not memorize
      - understand the difference between offline validation and online validation
      - know conceptually about canary deployment

      Validation types
        Offline validation
          - validation done using test sets of data
          - example: Validation datsets and k-fold validation
        online validation
          - validation under real-world conditions
          - Example: Canary deployment or A/B Testing

    Monitoring and Analyziing Training jobs
      - know the difference between Training metrics and validation metrics
      - know that cloudwatch integrates well with SageMaker for both logging and metric charting and
        dashboarding
      - understand how to define custom metrics for custom algorithms when you are submitting a job

      Algorithm metrics for custom algorithms
        - can send algorithm metrics for custom algorithms to cloudwatch


         Code: Adding Metrics definition to custom algorithm Example
               - adding regular expression to extract train/validation metrics from output

               from sagemaker.estimator import Estimator

               estimator = Estimator(image_uri='tf-custom-container-test',
                                     role=sagemaker.get_execution_role(),
                                     instance_count=1,
                                     instance_type='m1.c4.xlarge',
                                     k=10
                                     sagemaker_session=sagemaker_session,
                                     metric_definition = [
                                        {'Name': 'train:error', 'Regex': "Train_error=(.*?);'},
                                        {'Name': 'validation:error', 'Regex': "Valid_error=(.*?);'}
                                     ]
                                     )
               estimator.fit()


    Evaluating Model Accuracy
      - Understand underfitting and overfitting as well as potential causes and countermeasures
      - know that regression accuracy is commonly measured by RMSE
      - be able to explain what it means if a histogram of residuals is skewed negatively or positively
      - For binary classification, know what the AUC metric indicates
         - want AUC to be closer to one which indictes your model is more accurate
      - understand the trade-off and how different scenerios might call for different optimizations
        - know the difference between recall and precision
      - Understand how to read a confusion matrix and how what an F1 score and Macro Average F1 score
        metric can tell you
      - Know some ways to improve model accuracy

      ------------
      F1 Score vs Macro Average F1 Score
        - regular F1 score calculates the harmonic mean of precision and recall for a single class
        - "macro average F1 score" calculates the average F1 score across all classes in a multi-class classification problem,
           treating each class equally, regardless of class imbalance; essentially, it gives equal weight to the performance
           on each class when computing the overall F1 score.`
      ------------

    Model Tuning
      - recall hyperparameters and how they can be adjusted to control your learning process
        - some hyperparameters can be auto tuned, but not all of them
      - know the three components you must define for automatic tunning
      - understand that the best metrics to optimize varies by algorithm and sometimes is specific to
        how you use the algorithm (BlazingText for Word2Vec vs Text Classification)
      - Conceptually understand Bayesian Optimization
      - automatic tuning is not the perfect solution and sometimes may result in weaker models

    Concepts

------------------------------------------------------
8.7 Demo: Evaluation and Optimization

  Resources:

    Hyperparameter Tuning with Amazon SageMaker's Automatic Model Tuning - AWS Online Tech Talks
      https://www.youtube.com/watch?v=ynYnZywayC4

    Linear Learning Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html


    Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\8_7_evaluation_and_optimization_demo

    Training Dataset in recordIO protobuf data (ufo_sightings_validatioin_recordIO_protobuf.data)
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter8/ufo_sightings_train_recordIO_protobuf.data

    Validation Dataset in recordIO protobuf data (ufo_sightings_validatioin_recordIO_protobuf.data)
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter8/ufo_sightings_validatioin_recordIO_protobuf.data

    Jupyter Notebook (ufo-evaluation-optimization-lab.ipyynb
    https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter8/ufo-evaluation-optimization-lab.ipynb


    ------------------------------------------------------

  Sending out Team of Scientist Overview

    - Mr K has g given us the go to use the Linear Learner model for identifying the legitimacy
      of a reported UFO sighting.
    - He plans to send out a team of scientists if any reported UFO sighting is probable or unexplained.

    - Before deploying the model, he wants us to make sure that we have the most optimized model,
      improve any performance and training and possibly improve the accuracy.

  Sending out Team of Scientist Steps / Questions to resolve:

    - tune our model to find the most optimized model for our problem
    - determine if the model is less accurate, more accurate, or about the same
    - What is the objective metric woudl you want to monitor to ensure this? How do you plan on
      measuring success?
    - Which hyperparameters need to be tuned? What combinations of hyperparameters need to be used?
      Note: previously, we used the default hyperparameter values
    - How much faster was training time improved [compared to baseline model from previous demo]?

  Final Results
    - best training job hyperparameters
    - difference in time between baseline model and optimized model
    - difference in accuracy between models

  Use SageMaker Hyperparameter Tuning job

    - create a SageMaker Hyperparameter tuning job with different ranges of values for hyperparameters
      to find the best configuration to minimize the validation:objective_loss metric


   validation:objective_loss
     https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html
     - The mean value of the objective loss function on the validation dataset every epoch.
     - By default, the loss is logistic loss for binary classification and squared loss for regression.
     - To set loss to other types, use the loss hyperparameter.

    objective_loss metric
      - this metric is used because this is what's used in multi classification problems.
      - this metric measures the performance of our classification model.
      - repeatedly calculate the difference between the values that our model is predicting
        and the actual values of the label.
      - AWS recommends that we minimize this value when using it as our objective metric.


    ------------------------------------------------------


  AWS Console -> SageMaker
     -> Training <left tab> -> Hyperparameter tuning job ->  Create hyperparameter tuning job ->
       Hyperparameter tuning job name: linear-learner-tuning-job -> Next ->
       -> Add training job definition (must be unique in your AWS account): linear-learning-tuning-070924,
          IAM Role: AmazonSageMaker-ExecutionRole-*,
          Algoritm Options: Built-in, Choose an Algorithm: Tabular - LinearbLearner,
          input mode: Pipe, Objective Metric: validation:objective_loss, type: minimize
          Hyperparameter Configuration:
              Name            Type           Scaling Type     Value / Range
              feature_dim     static                           22
              min_batch_size  Integer        Linear            500  - 5000
              epochs          static                           15
              predictor_type  static                           multiclass_classifier
              wd              Continuous     Logarithmic       .0001  - 1.0
              l1              Continuous     Logarithmic       .0001  - 1.0
              learning_rate   Continuous     Logarithmic       .0001  - 1.0
              num_classes     static                           3

           -> Next -> Channels -> Channel name: train, Input mode: Pipe, Data Source: S3,
              S3 Location: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data

              -> Add Channel:
                Channel name: validation, Input mode: Pipe, Data Source: S3,
              S3 Location: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data

            Output data Configuraiton:
              S3 output path: s3://modeling-ufo-lab1/optimization_evaluation_lab/hyperparameter_tuning_output

            -> Next ->

            Resource Configurations: Instance type: ml.c5.xlarge, Instance count: 1
               Stopping Condition: Maximum duration per training job: 20 min

            -> Next ->  <review Training Job definition -> Next ->

            Resource Limits:
              Maximum training jobs: 50,
              Maximum parallel training jobs: 5


           -> Create hyperparameter tuning job
              ->  took 16 min to complete


    Best training job summary:
    Name                                        Status          Objective metric                value
    linear-learning-tuning-job-030-eda920d9     Completed       validation:objective_loss       0.1782093346118927

    Best training job hyperparameters:

	Name            Type    Value
	l1	        -	0.0005953224641699573
	learning_rate	-	0.04380096472026997
	mini_batch_size	-	2977
	use_bias	-	true
	wd	        -	0.0019134739491125144

      # search for best training job in cloudWatch

        AWS Console -> CloudWatch -> log -> Log Grous -> /aws/sagemaker/TrainingJobs ->
          Search: linear-learning-tuning-job-030-eda920d9 -> select

        [07/09/2024 22:20:57 INFO 139932632377152] #quality_metric: host=algo-1, validation multiclass_accuracy <score>=0.9469825155104343
        . . .
	[07/09/2024 22:20:57 INFO 139932632377152] #quality_metric: host=algo-1, validation macro_recall <score>=0.9270477294921875
	[07/09/2024 22:20:57 INFO 139932632377152] #quality_metric: host=algo-1, validation macro_precision <score>=0.9028711915016174
	[07/09/2024 22:20:57 INFO 139932632377152] #quality_metric: host=algo-1, validation macro_f_1.000 <score>=0.9140763282775879

          examine best job log to find:  accuracy, f1, precision, recall:

           validation multiclass_accuracy <score>=0.9469825155104343
           validation macro_recall <score>=0.9270477294921875
           validation macro_precision <score>=0.9028711915016174
           validation macro_f_1.000 <score>=0.9140763282775879


          previous (7.14 Demo: Algorithms) . to find accuracy, f1, precision, recall:
            multiclass_accuracy <score>=0.9469825155104343
            macro_recall <score>=0.9270477294921875
            macro_precision <score>=0.9028711915016174
            macro_f_1.000 <score>=0.9140763282775879

            -> identical results! (in video, accuracy was slightly improved)

      # re- train using hyperparameters from best model

   AWS Console -> SageMaker -> Notebook -> Notebook Instances -> select "my-notebook-inst" -> start
   # when running:
   -> Open Jupyter
     -> Upload "ufo-evaluation-optimization-lab.ipynb"
        -> start uploaded notebook



code:
         >>> # First let's go ahead and import all the needed libraries.

         >>> import pandas as pd
         >>> import numpy as np
         >>> from datetime import datetime
         >>>
         >>> import boto3
         >>> from sagemaker import get_execution_role
         >>> import sagemaker

         >>> role = get_execution_role()
         >>> bucket='modeling-ufo-lab1'

         >>> # 1. Create and train our "optimized" model (Linear Learner)
         >>>
         >>> # Let's evaluate the Linear Learner algorithm with the new optimized hyperparameters.
         >>> # Let's go ahead and get the data that we already stored into S3 as recordIO protobuf data.


         >>> # Let's get the recordIO file for the training data that is in S3

         >>> train_file = 'ufo_sightings_train_recordIO_protobuf.data'
         >>> training_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_train/{}'.format(bucket, train_file)
         >>> print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))

             The Pipe mode recordIO protobuf training data: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data

         >>> # Let's get the recordIO file for the validation data that is in S3

         >>> validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'
         >>> validate_recordIO_protobuf_location = 's3://{}/algorithms_lab/linearlearner_validation/{}'.format(bucket, validation_file)
         >>> print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))

             The Pipe mode recordIO protobuf validation data: s3://modeling-ufo-lab1/algorithms_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data


         >>> # Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call
         >>> #    the Linear Learner algorithm.

         >>> from sagemaker import image_uris
         >>> container = image_uris.retrieve('linear-learner', boto3.Session().region_name, '1')


         >>> # Let's create a job and use the optimzed hyperparameters.

         >>> # Create a training job name
         >>> job_name = 'ufo-linear-learner-job-optimized-{}'.format(datetime.now().strftime("%Y%m%d%H%M%S"))
         >>> print('Here is the job name {}'.format(job_name))
         >>>
         >>> # Here is where the model-artifact will be stored
         >>> output_location = 's3://{}/optimization_evaluation_lab/linearlearner_optimized_output'.format(bucket)

             Here is the job name ufo-linear-learner-job-optimized-20240709233014

         >>> # Next we can start building out our model by using the SageMaker Python SDK and passing in everything that is
         >>> #   required to create a Linear Learner training job.
         >>>
         >>> # Here are the linear learner hyperparameters that we can use within our training job.
         >>> #     https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html

         >>> # After we run this job we can view the results.

         >>> # note: video shows adding more parameters, I only the tuned hyperparameters

         >>> %%time
         >>> sess = sagemaker.Session()
         >>>
         >>> # Setup the LinearLeaner algorithm from the ECR container
         >>> linear = sagemaker.estimator.Estimator(container,
         >>>                                        role,
         >>>                                        instance_count=1,
         >>>                                        instance_type='ml.c4.xlarge',
         >>>                                        output_path=output_location,
         >>>                                        sagemaker_session=sess,
         >>>                                        input_mode='Pipe')
         >>> # Setup the hyperparameters
         >>> linear.set_hyperparameters( feature_dim=22,
         >>>                             predictor_type='multiclass_classifier',
         >>>                             num_classes=3,
         >>>                             l1=0.0005953224641699573,
         >>>                             learning_rate=0.04380096472026997,
         >>>                             mini_batch_size=2977,
         >>>                             use_bias='true',
         >>>                             wd=0.0019134739491125144
         >>>                           )
         >>>
         >>>
         >>> # Launch a training job. This method calls the CreateTrainingJob API call
         >>> data_channels = {
         >>>     'train': training_recordIO_protobuf_location,
         >>>     'validation': validate_recordIO_protobuf_location
         >>> }
         >>> linear.fit(data_channels, job_name=job_name)


             INFO:sagemaker:Creating training-job with name: ufo-linear-learner-job-optimized-20240709233014

             2024-07-09 23:42:01 Starting - Starting the training job...
             2024-07-09 23:42:17 Starting - Preparing the instances for training...
             2024-07-09 23:42:47 Downloading - Downloading input data...
             2024-07-09 23:43:28 Downloading - Downloading the training image.........
             2024-07-09 23:44:44 Training - Training image download completed. Training in progress.Docker entrypoint called with argument(s): train
             . . .

             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation multiclass_accuracy <score>=0.9469825155104343
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation multiclass_top_k_accuracy_3 <score>=1.0
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation dcg <score>=0.978291244205619
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation macro_recall <score>=0.9270477294921875
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation macro_precision <score>=0.9028711915016174
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation macro_f_1.000 <score>=0.9140763282775879
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation multiclass_balanced_accuracy <score>=0.9270477490089298
             [07/09/2024 23:45:36 INFO 140579665631040] #quality_metric: host=algo-1, validation multiclass_log_loss <score>=0.6202281367401643
             [07/09/2024 23:45:36 INFO 140579665631040] Best model found for hyperparameters: {"optimizer": "adam", "learning_rate": 0.04380096472026997, "l1": 0.0005953224641699573, "wd": 0.0019134739491125144, "lr_scheduler_step": 10, "lr_scheduler_factor": 0.98, "lr_scheduler_minimum_lr": 1e-05}
             [07/09/2024 23:45:36 INFO 140579665631040] Saved checkpoint to "/tmp/tmp67b8ckvo/mx-mod-0000.params"
             [07/09/2024 23:45:36 INFO 140579665631040] Test data is not provided.
             #metrics {"StartTime": 1720568698.8876836, "EndTime": 1720568736.392592, "Dimensions": {"Algorithm": "Linear Learner", "Host": "algo-1", "Operation": "training"}, "Metrics": {"initialize.time": {"sum": 1422.2440719604492, "count": 1, "min": 1422.2440719604492, "max": 1422.2440719604492}, "epochs": {"sum": 15.0, "count": 1, "min": 15, "max": 15}, "check_early_stopping.time": {"sum": 12.825965881347656, "count": 16, "min": 0.29158592224121094, "max": 1.1529922485351562}, "update.time": {"sum": 32834.91826057434, "count": 15, "min": 2167.7138805389404, "max": 2247.562885284424}, "finalize.time": {"sum": 2165.846347808838, "count": 1, "min": 2165.846347808838, "max": 2165.846347808838}, "setuptime": {"sum": 1.8286705017089844, "count": 1, "min": 1.8286705017089844, "max": 1.8286705017089844}, "totaltime": {"sum": 37600.07357597351, "count": 1, "min": 37600.07357597351, "max": 37600.07357597351}}}


             2024-07-09 23:45:53 Uploading - Uploading generated training model
             2024-07-09 23:45:53 Completed - Training job completed
             Training seconds: 185
             Billable seconds: 185
             CPU times: user 905 ms, sys: 68.5 ms, total: 974 ms
             Wall time: 4min 13s


         >>> # Now we can compare the amount of time billed and the accuracy compared to our baseline model.

            -> previous billable secords was 119 (new job is slower!)
    ------------------------------------------------------


    -> stop notebook instance



------------------------------------------------------
8.7+ Hyperparameter Tuning with Amazon SageMaker's Automatic Model Tuning - AWS Online Tech Talks
     https://www.youtube.com/watch?v=ynYnZywayC4

  Regularization
    - force a "simpler model" to avoid memorizing training data
    - the smaller the dataset, the more regularization is needed
    how to measure/test the regularization (determine how much regularization)
      - measure quality on a held-out test set
    setting regularion
      - set via hyperparameters

  Learning Rate
    - set with hyperparameter
    - smaller learning steps/rate generally more accurate, but this increases the learning time
    Iterative learning:
      1. observe difference between predicted answers, and correct answers
      2. Adjust the model a "small amount" to make the predictions closer to the correct answer

  Hyperparameter
    - any decision the algorithm author can't make for you

  Configurable expressiveness via hyperparameters
    - way of making the model more complex (opposite of regularization)
    Deep Learning
      - number of layers in a neural net
      - number of hidden nodes in network layer
      - example, if model is underfitting, you can try to add another layer or increasing the width
        by adding more hidden nodes
    Trees
      - maximum tree depth
      - number of tree


  Types of Hyperparameters
    Expressivenss
      - control how complex of a model (for example, help with underfitting)
    REgulaization
     - force a "simpler model" to avoid memorizing (overfitting) training data
    Optimization
      - hyperparameter like the 'learning rate' and 'learning rate schedule' that control how the
        algorithm learns the model
      - for momentum based optimizer, then 'momentum' is a hyperparameter
      - if you are using 'atom' or msprop' then there even more hyperparameters
    Data Handling
      - for text processing algorithm, you may want to control then number of by grams (bi-grams,
        4-grams, ...)
    Algorithmic
      - hyperparameters used to change/set the algorithms used


  Model Tuning Strategies
    - how to pick hyperparameters

   Tuning Strategies
     Trail & Error
       -  manual tune parameters
     Try everything
       - not practical
       Grid Search
         - choose N values of each hyperparmaters, and try all (hyperparamers ** N) combinations
         - for the unimportant hyperparmaeters you spend much resources with grid search points
       Random Search
         - picking hyperparameters points randomly in the configuration space out performs grid searches
         - by picking randomly, you are not repeating important hyperpameter points (as with a grid search)
           since each randomly pick point is  unique
         - a good baseline tuning strategy

       Use Machine Learning - Bayesian optimizaation

  Hyperparameter tuning using Machine Learning - Bayesian optimizaation
    - used for Automatic Model Tuning in SageMaker

    Surrogate Model
      - essentially a classic supervised learning model
      - tries to predict the quality of the model to be produced
      - no gradient data available (use a ML model based on gradient free optimization
      - use a Gaussian process:
          - are great because the not only predict Y but also an uncertainity range
            (get a mean and std deviation for each Y)
      f(X) -> y
        x: hyperparameter configuration
        y: model quality (e.g accuracy, RMSE, ...)

      Aquisition Function
        - a way of evaluation all of the hyperparameter configurations
        - use Expected Improvement, EI, aquisition function
        - looking got lowest possible value in this uncertainity band which is used
          to pick the next hyperparameter value to try, ...


  Hands on Demo of Automatic Model Tuningin SageMaker

    Forecasing With DeepAR
      Tuning is Important
        - DeepAR is neural network where Tuning is very important
      Lots of Hyperparameter
        - DeepAR has lots of hyperparameter
        - results vary wildly depending on how you tune your model
    Notebook
      - opens Jupyter Notebook on existing notebook instance
      - goes in SageMaker examples to 'Introduction to Amazon Algorithms to 'DPR electricity' example
         - notebook: DeepAR-Electricity.ipynb

         - modified example to tune the hyperparameters instead of running a single training job
           Note: large dataset that will take some time to download to notebook instance


      Modified Notebook (at 30:23 to 31:49)
        - renamed hyperparameters list to static_hyperparameters and tuned_hyperparameters lists
          and commented out the 'learning_rate'

          added:

          from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner

          tuned_hyperparameter_ranges = {
             'num_calls': IntegerParameter{30, 200},
             'num_layers': IntegerParameter{1, 4},
             'dropout_rate': ContinuousParameter{0.0, 0.9},
             'embedding_dimension': IntegerParameter{10, 100},
             'learning_rate': IntegerParameter{1e-5, 0.1},
          }


         added:
         # hyperparameter tuning object

         %%time
         tuner = HyperparameterTuner(estimator,
                                     objective_metric_name='test:RMSE',
                                     objective_type='Minimize',
                                     hyperparameter_ranges=tuned_hyperparameter_ranges,
                                     max_jobs=50,
                                     max_parallel_jobs=10)

         tuner.fit = (inputs=data_channels, job_name='tune-elec-'+str(uuid.uuid4())[:5],
                      include_cls_metadata=False)



         # start job

         Go to SageMaker -> Training -> Hyperparameter tuning jobs -> select 'tune-elec-**' job   # time: 33:08
            -> hyperparameter configuration <tab>
                -> shows all the hyperparameters and value/value ranges
            -> Training jobs <tab>
              -> shows the running hyperparameter tuning training jobs
              -> note takes a 15 hours to run

            # after completion
            -> Training jobs <tab>
              -> sort on 'objective metric value' from lowest value
            -> Best Training job <tab>
               -> shows best hyperparameter results

                     best values:  # time: 34:41
                       num_calls': 195
                       num_layers': 2
                       dropout_rate': 0.2094459601304402
                       embedding_dimension': 28
                       learning_rate': 0.0005376414997129647
               -> to use this model, click on "create model"


  Tips for Model Tuning
    - effectively using Automatic Model Tuning  (time: 35:54)

    Clarify your Goals
      - get a high quality model?
      - Understand your model?
      - Get your results as fast as possible, or using fewest resources?

    Quality vs Time trade-off in hyperparameter tuning
      max_parallel_jobs
        - how many jobs to run in parallel
        - if > 1, can run parallel or clusters of jobs at same time
        max_parallel_jobs=1
          - get you the best surrogete model, but does on 1 job at a time
          - longest wall clock time
        max_parallel_jobs=N  (e.g. N=10)
          - runs N jobs in parallel
          - much faster results
          - often similar find model quality

    Intuition about Hyperparameters
      - in SageMaker examples to 'Introduction to Amazon Algorithms to 'HPO Aanlyze Tuning ..' example
         - notebook: HPO_Analyze_TuningJob_Results.ipynb

      - lets you visualize results from varies combinations of hyperparameters
      - Note: Fewer hyperparameters tuned => Better intuition
      - may want to tune 1 hyperparameter at a time
      - may want to tune 2 hyperparameter at a time, and look at heat map on relationship
      - sample notebook shows you how to do the 1-way correlation plots

    More Tuning Tips
      Avoid categorical hyperparameter tuning
        - does not recommend picking a fix set of values for a continuous parameter
          e.g. not recommended: dropout_rate={0,0.2,05}
          - if you have something that is numeric (or continuous e.g. dropout_rate), treat it as
            numeric (or continuous)
         - if you treat a numeric/continuous as categorical parameter, then surrogate model will
           not understand the relationship beween your selected discrete values
      Keep search ranges reasonable (not too big and not too small)
        - e.g. don't set learning_rate from 1e-99 to 1e+99 or even 1e-10 to 1e+10
        - only search values that you might reasonable think could create a good model
        - use too large of a configuration space makes it difficult for the surrogate
          model to hone in a good results
        if too small
          - if you see all the high-quality results centered around the edge of a hyperparameter
            value range, this indicates this hyperparameters range was too small
      Don't tune everything that is tuneable
        - e.g. XGBoost has over 40 hyperparameters
        - for tuning, pick the parameters that a most likely impact your results

    Automatic Model Tuning is a productivity tool
      - it is not magic
      - can help you explore a range of possibilitys
      - can use model tuner to rule out an approach by trying lots of different variations
        if you find none of the variations are working
      - by searching over a set of configuration, you can gain confidence that the model
        you found is the best possible within that space



  Linear Learning Regularization hyperparameters
    l1
       - The L1 regularization parameter.
       - If you don't want to use L1 regularization, set the value to 0.

    wd
       - The weight decay parameter, also known as the L2 regularization parameter.
       If you don't want to use L2 regularization, set the value to 0.

    bias_wd_mult
       - Allows different regularization for the bias term.
       - The actual L2 regularization weight for the bias is wd * bias_wd_mult.
       - By default, there is no regularization on the bias term.

    num_models
      - The number of models to train in parallel.
      - For the default, auto, the algorithm decides the number of parallel models to train.
      - One model is trained according to the given training parameter (regularization, optimizer,
        loss), and the rest by close parameters.


------------------------------------------------------
8.8 Quiz AWS Certified Machine Learning - Specialist 2020 - Evaluation and Optimizations


Question 3

In a binary classification problem, you observe that precision is poor. Which of the following most contribute to poor precision?

  Type III Error

  Type IV Error

  Type I Error  - correct answer  (False Positive)

  Type II Error                   (False Negative)

  Type V Error


Precision is defined as the ratio of True Positives over the sum of all Predicted Positives, which includes correctly labeled
trues and those that we predicted as true but were really false (false positives). Another term for False Positives is Type I error.


Question 6

After training and validation sessions, we notice that the accuracy rate for training is acceptable but the accuracy rate for
validation is very poor. What might we do?

Choose 3

  Increase the learning rate.

  Add an early stop.                           - Correct Answer

  Run training for a longer period of time.

  Gather more data for our training process.   - Correct Answer

  Reduce dimensionality.                       - Correct Answer

  Encode the data using Laminar Flow Step-up.

Correct Answer

High error rate observed in validation and not training usually indicates overfitting to the training data. We can introduce
more data, add early stopping to the training job and reduce features among other things to help return the model to a generalizer.



Question 7

You are designing a testing plan for an update release of your company's mission critical loan approval model. Due to regulatory
compliance, it is critical that the updates are not used in production until regression testing has shown that the updates
perform as good as the existing model. Which validation strategy would you choose?

Choose 2

  Use a canary deployment to collect data on whether the model is ready for production.

  Make use of backtesting with historic data.                                - correct answer

  Use an A/B test to expose the updates to real-world traffic.

  Use a K-Fold validation method.                                            - correct answer

  Use a rolling upgrade to determine if the model is ready for production.

Good work!

Because we must demonstrate that the updates perform as well as the existing model before we can use it in production, we
would be seeking an offline validation method. Both k-fold and backtesting with historic data are offline validation methods
and will allow us to evaluate the model performance without having to use live production traffic.


Question 8

Which of the following metrics are recommended for tuning a Linear Learner model so that we can help avoid overfitting? (Choose 3).

Choose 3

  validation:precision               - correct answer

  test:objective_loss

  test:recall

  validation:objective_loss          - correct answer

  test:precision

  validation:recall                  - correct answer

Correct Answer

  To avoid overfitting, AWS recommends tuning the model against a validation metric instead of a training metric.


Question 10

After training and validation sessions, we notice that the error rate is higher than we want for both sessions. Visualization of
the data indicates that we don't seem to have any outliers. What else might we do? (Choose 4).

Choose 4

   Gather more data for our training process.                - correct answer

   Encode the data using Laminar Flow Step-up.

   Run a random cut forest algorithm on the data.

   Add more variables to the dataset.                       - correct answer

   Run training for a longer period of time.                - correct answer

   Reduce the dimensions of the data.                       - correct answer

Correct Answer

When both training and testing error is high, it indicates that our model is underfitting the data. We can try to add more
details to the dataset, gather more data for training and/or run the training session longer. We might also need to identify
a better algorithm.

This approach has the potential to reduce the error rate.

BE MINDFUL OF: However, we would argue that during data analysis and feature engineering the data scientist should have done
their due diligence to trim down the features and determine which ones were relevant to the problem theyre trying to solve
(i.e., what theyre trying to predict)


Question 3  (retry 1)

  During several training iterations, you observe that the model's loss function consistently settles at values that are close
  but not identical. You suspect that tweaking the model's hyperparameters could lead to improvements. Which of the following
  actions is most likely to assist in achieving better convergence?

Choices:
  Change to another algorithm.

  Increase the learning rate.

  Decrease the objective rate.

  Decrease the learning rate.      <--- Correct Answer

  Change from a CPU instance to a GPU instance.
Good work!

  The learning rate can be thought of as the "step length" of the training process. A learning rate can be too large if it
  overshoots the true global minimum, causing the model to oscillate around the minimum and settle on different final loss
  function values. This makes decreasing the learning rate our best available option to try next. Decreasing the learning rate
  allows the training process to take more precise steps toward the minimum, potentially leading to a better result, but it can
  also increase the time needed for convergence. It is worth noting that a learning rate that is too low might not converge or
  may get stuck in a local minimum.

Question 11 (retry 1)

  We want to perform automatic model tuning on our linear learner model using the built-in algorithm from SageMaker. We have
  chosen the tunable hyperparameter we want to use. What is our next step?

Choose 2

  Choose a range of values which SageMaker will sweep through for the selected tunable hyperparameter   <--- Correct Answer
  and target objective metric we want to use in the tuning process.

  Decide what hyperparameter we want SageMaker to tune in the tuning process.                           <--- Incorrect Answer

  Submit the tuning job via the console or CLI.                                                         <--- Correct Answer

  Choose an algorithm container from ECR ensuring its tagged with :1

  Choose the SageMaker Notebook instance where the input data is stored.
Sorry!

  This option goes back to an earlier step in the process which has already been completed in the scenario. Since the tunable
  hyperparameter has already been chosen, this step is redundant. The focus should instead be on defining the range for this
  hyperparameter and selecting an objective metric for the tuning process.

Correct Answer

  After selecting a tunable hyperparameter, it's key to define a range of values that SageMaker can explore to optimize the model.
  This range serves as a boundary within which the algorithm will search for the best hyperparameter value. Simultaneously,
  selecting an objective metric is crucial as it guides SageMaker on evaluating the model's performance during the tuning process.

  Once the tunable hyperparameter, its range of values, and the objective metric have been specified, the next step is to initiate the
  tuning process. This is done by submitting the tuning job through SageMaker's console or CLI. The submission triggers SageMaker to
  start automatic tuning, iterating through the specified range of hyperparameter values to find the optimal configuration based on
  the chosen objective metric.



Question 1 (retry 2)

In a regression problem, if we plot the residuals in a histogram and observe a distribution heavily skewed to the right of zero indicating mostly positive residuals, what does this mean?

Choose 1:
  Our model is consistently overestimating.                                                                  <-- Incorrect Answer

  Our model is sufficient with regard to RMSE.

  Our model is sufficient with regard to aggregate residual.

  Our model is consistent underestimating.                                                                   <-- Correct Answer

Answer Info:
  Residual is the actual value minus the predicted value. If most of our residuals are positive numbers, that means that
  our predicted values are mostly less than the actual values. This means that our model is consistently underestimating.

------------------------------------------------------

Chapter 9 Implementation and Operations

------------------------------------------------------
9.1 Introduction

  - focus on model deployment and model monitoring & evaluation

  Machine Learning Cycle:

    Generate Example data
      Fetch --->  Clean  ---> Prepare --->
    Train the Model
      - time to train the model
      Train Model --->  Evaluate Model --->

    Deploy Model
      Deploy to Production --->  Monitor & Evaluation --->

------------------------------------------------------
9.2 Implementation and Operations Concepts

  DeepMind Technologies
    PDF:
    https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&ved=2ahUKEwiJypiXz5iHAxXeMzQIHTz-Ap8QFnoECBQQAQ&usg=AOvVaw3M864Nv-fcyDpY-K9pPqEF
    - in 2013 researches over at DeepMind Technologies created a reinforcement learning project that not only
       learned how to play some of the classic video games, but they soon learned to totally dominate them
       including: Beamrider, Enduro, Seaquest and Space Invaders
    - this research lead to DeepMind being acquired by Google in 2014
    - it also lead to the creation of AlphaGo.

  AlphaGo
    - AlphaGO is an artificial intelligence engine that plays the Chinese game of Go.
    - the system and its attempt to beat a Go master was very dramatically captured in the 2017 movie,
       "AlphaGo" that's available on Netflix

  OpenAI Gym
    https://gym.openai.com
    - you too can build your own reinforcement learning models to try to defeat some of these classic Atari games,
      using something called, OpenAI Gym.
    - the OpenAI Gym is a environment that you can exercise your reinforcement learning algorithms to try to
      figure out if they're going to be able to solve for these classic Atari games and oh,

  Two Types of [model] Usage:

             offline Usage                               Online Usage

    What     Make inferences on datasets in batch        Make inferences on demand as the model
             and return results in a set                 is called and return results immediately

    Why      Entire dataset is needed for inferences;    Need instance response when endpoint is
             Pre-process data before using as an input   is called via an app or service
             for another model

    When     - Predictive models with large historic
               dataset inputs                            - Real-time fraud detection
             - Feature engineering for a follow-on       - Autonomous machines
               model


   Note:
          - some algorithms require the entire dataset to be in memory for them to do their job
             - this type of algorithms is for offline use cases


  Types of Deployment

               "Big Bang"                  Phased Rollout             Parallel Adoption

    time        takes the Least time       Medium amount of time      longest time

    Risk        Most Risk                  Medium Risk               *least Risk

    Cost        function(Risk, Time)       function(Risk, Time)      function(Risk, Time)

    * Sometimes, risk is amplified in a Parallel Adoption due to concurrency such as data
        synchronization issues, multiple processes, temporary integrations, etc.

    Big Bang
      - means that at some point in time, a full cut over from the old to the new.
      - This is a common deployment strategy in a lot of maybe ERP system deployments, where
        you really can't run both systems at the same time
      - it has the highest level of risk,

       Note: ERP: Enterprise Resource Planning

    Phased Rollout
      - gradually deploy the new system and decommission the old system.

    Parallel Adoption
     - running both systems simultaneously
     - generally, this deployment option takes the most amount of time
     - generally, the lowest amount of risk, but sometimes it has a higher risk, because
       you're running multiple systems, or multiple processes at the same time and that
       could lead to errors, or data consistency issues


  Rolling Deployment
    - rather than upgrade all resources at once, the upgrade is one by one to minimize downtime

    example:
                        ----> server_1  --> upgrade version on one server at a time
       load Balancer    ----> server_2
                        ----> server_3
                        ----> server_4

             Risk: Multiple versions are in production at the same time

  Canary Deployment
    - deploy a new version into production so it sees a small portion of the total traffic
      and evaluate the performance and behaviour


    example:
                            -- 99% ---> current version     ----->
       Users ---> Route 53                                          cloudWatch
                            --  1% ---> Canary (new version) ----->

  A/B Testing
     - a canary deployment variation
     - deploy a new version into production and configure a set amount of new inbound traffic
       to use the new (B) version, recording follow-on data about the outcome of those that used
       the new (B) version
     - once enough data is collected, make decision on whether to fully deploy the new version
       or make adjustments

     - A/B Testing is very common in E-Commerce scenarios, where you want to evaluate whether the activities
       that you did on your website have any impact on the customer's willingness to purchase a product,


    example:
                                     -- 99% ---> variant 1 (current model)  ----->
       Users ---> SageMaker Hosting                                                 cloudWatch --> Metrics Evaluation
                                     --  1% ---> variant 2 (new model)      ----->



  ------
  [Blue & Green Deployment, A/B Testing, & Canary Release] Key Differences
    https://medium.com/@kazimozkabadayi/exploring-blue-green-deployment-a-b-testing-and-canary-release-161c5e588de0

    Scope of Deployment:
      Blue & Green Deployment: Instant switch between two environments.
      A/B Testing:             Parallel deployment with distinct versions for different user groups.
      Canary Release:          Gradual rollout to a subset of users.

   Purpose:
      Blue & Green Deployment: Minimization of downtime and risk during deployment.
      A/B Testing:             Evaluation of variations to optimize performance.
      Canary Release:          Early detection of issues through controlled exposure.

    User Engagement:
      Blue & Green Deployment: Entire user base may be involved in a seamless switch.
      A/B Testing:             Users are divided into groups, each experiencing a different version.
      Canary Release:          Involves a subset of users for feedback.
  ------

  CI, CD, and more CD
    Continuous Integration
      - Merge code changes back to main branch as frequently as possible with automated testing as you go
    Continuous Delivery
      - automated release process up to the point you are ready to deploy, but the deployment requires human
        intervention (e.g. the click of a button)
    Continuous Deployment
      - Each code change that passes all stages of the release process is released to production with
        no human intervention required

  Typical Development Lifecycle

     Get Latest   --->  Make       ---> Unit        ---> Commit to    --|
       from Repo        changes         Testing          Repo           |
                                                                        |
      Smoke       <---  Deploy to  <-- Acceptance  <--- Integration  <--|
      Testing           Production     Testing          testing


     previously:
       - required human to do Unit testing, integration testing, acceptance testing, deploy to
         production, and smoke testing

     for continous integration
       -  means that we're going to get the latest version from the repo and we're gonna commit it
          several times during the day and with each check in, we're gonna have some sort of integration
          test that makes sure it's not gonna break things

     For Continuous Delivery
        - we'd still use the same [CI] process, but maybe we're going to be able to release this
          once per day, because all the test automation says that our code is good.
       - may release once per day
       - all steps are automated except for 'deploy to production'

     For Continuous Deployment
        - as soon as all the tests are passed, then we can automatically deploy that into production


------------------------------------------------------
9.3 AI Developer Services


  AWS Artificial Intelligenece Stack

     AI Services
       - App Developers, no ML experience required
       Amazon Comprehend
          - natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text
       Amazon forecast
          - combines time-series data with variables to deliver highly accurate forecasts
       Amazon Lex
         - Build conversational interfaces that can understand the intent and content of natural speech
         - provides response
       Amazon Personalize
         - recommendation engine as a service based on demographic and behavioral data
       Amazon Polly
         - text-to-speech service supporting multiple languages, accents, and voices
       Amazon Rekognition
         - image and video analysis to parse and recognize objects, people, activities, and facial expressions,
       Amazon Textrac
         - Extract text, content, and metadata from scanned documents
       Amazon Translate
          - translate text to and from many different languages
       Amazon Transcribe
          - Speech-to-text as a service

     AI Developer Services
       - Easy to use with no ML knowledge required
       - scalable and robust
       - redundant and fault tolerant
       - pay per usage typically with tiered pricing
       - REST API and SDK


  Amazon Comprehend
    - Natural Language Processing (NLP) service that finds insight and relationship within text
    - Sentiment analysis of social media

  Amazon forecast
    - combines time-series data with variables to deliver highly accurate forecasts
    - Forecast seasonal demand for a specific color of shirt
    - shrink-wrapped version of the DeepAR algorithm

  Amazon Lex
    - Build conversational interfaces that can understand the intent and content of natural speech
    - Creates a customer service chatbot to automatically handle routine requests



  The Trio in Action

               Alexa             Amazon Transcribe             Lex                    Polly
     Person -> talks to Alexa -> converts speech to text  ->  provides response -> converts text reponse to speech


  Amazon Personalize
    - recommendation engine as a service based on demographic and behavioral data
    - takes the service that they use already with amazon.com for recommending certain products,
      and then strip it down just to an API that we can then use ourselves and our own applications.
      And just like Amazon.com, we can use a service to provide recommendations to customers
    - provides potential upsell products at checkout during a web transaction

  Amazon Polly
    - text-to-speech service supporting multiple languages, accents, and voices
    - provide dynamically generated personalized voice response for inbound callers
    - give it some text and it will render an MP3 file of a voice speaking that text.
    - examples:
        - provide automated voice response in a telephone system, where we could read off
          account details dynamically when the person calls in via phone.

  Amazon Rekognition
    - image and video analysis to parse and recognize objects, people, activities, and facial expressions,
      and facial recognition
    - example use: provide an additional form of employee athentication through facial recognition as they can
      as acces badge

  Amazon Textract
    - Extract text, content, and metadata from scanned documents
    - automatically digitize and process physical paper forms
    - consider this a rolled up version of LDA (Latent Dirichlet Allocation), where we can extract certain data
      and metadata from documents.

  Amazon Transcribe
    - Speech-to-text as a service
    - automatically create transcripts of recorded presentations

  Amazon Translate
    - translate text to and from many different languages
    - probably using seq2seq to the translation
    - dynamically create localized web content for users based on their geography


  Recommends using either CLI or console for each of these services
    - AWs console includes demos and examples
    - shows an Comprehend Console example


------------------------------------------------------
9.4 Using AWS Step Functions to Categorize Uploaded Data


About this lab

  AWS Step Functions is a powerful service that lets you build and orchestrate different AWS services to construct
  state machines. This can be done with almost any AWS API action, and with little-to-no code.

  In this lab, we're going to build a Step Functions state machine to process an MP3 call recording, determine
  the sentiment of the conversation, and take a different action depending on the analysis.

Learning objectives
  - Create the Step Function State Machine
  - Implement Amazon Transcribe to the State Machine
  - Implement Amazon Comprehend to the State Machine
  - Develop a Choice state based on Sentiment


Flow
                      |------------------------------------------------|   Positive
                      | Step Function                                  |      |---> SQS
                      |                                                |      |
    Audo File (MP3)   | ---> Transcribe --> Comprehend --> Sentiment ---------|
      S3              |                                      ??        |      |
                      |                                                |      |---> Lambda
                      |------------------------------------------------|    Negative


    ------------------------------------------------------
Solution
Log in to the Lab Environment

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.

Create a Step Functions State Machine

    In the search bar on top, type "Step Functions" to search for the Amazon Kinesis service.
    Click on the Step Functions result to go directly to the Step Functions service.
    Click on the State machines button in the sidebar.
    Click on the Create state machine button to launch the wizard to create the state machine.
    When prompted to select a template, opt for a blank state machine.
    Navigate to the Config section of the state machine.
    Update the Execution role to the pre-created step-function-execution-role, which will provide the necessary permissions for the lab.

    Navigate to the Design section of the state machine.
Setup Amazon Transcribe

    Add the transcribe:StartTranscriptionJob action to the start of the workflow.

    Navigate to Amazon S3 in a new tab.

    Open the bucket named stepfunctionsbucket.

    Follow the following path to the GitHub Repository for this lab, and download the conversation.mp3 file:
        Lab GitHub Repository

    Upload conversation.mp3 to the S3 bucket.

    Select the uploaded mp3 file to display the Properties page.

    Copy the S3 URI for the uploaded file.
       s3://cfst-4234-8119d311182a00943842-stepfunctionsbucket-3spgmpuxapru/conversation.mp3

    Return to the Step Functions tab.

     StartTranscriptionJob API doc:
        https://docs.aws.amazon.com/transcribe/latest/APIReference/API_StartTranscriptionJob.html

    Copy the following details to the API Parameters (replacing the necessary placeholders):

    {
        "LanguageCode": "en-US",
        "Media": {
            "MediaFileUri": "<MP3-File-S3-URI>"
        },
        "TranscriptionJobName": "MyData",
        "OutputBucketName": "<S3-Bucket-Name>"
    }

    Rename the state name to Start Call Transcription Job.

    Add the transcribe:StartTranscriptionJob action after the Start Call Transcription Job state.

    Rename the state name to Check Transcription Job.

    Add a Choice flow state after the Check Transcription Job state.

    Set the conditions for the first Choice state rule to the following condition:
        Variable: $.TranscriptionJob.TranscriptionJobStatus
        Operator: is equal to
        Value: String constant
        COMPLETED

    Add the comment Transcript Completed to the first rule.

    Create a second rule for the Choice state.

    Set the conditions for the first Choice state rule to the following condition:
        Variable: $.TranscriptionJob.TranscriptionJobStatus
        Operator: is equal to
        Value: String constant
        IN_PROGRESS

    Add the comment Transcript Processing to the first rule.

    Add a Wait flow state between the Start Call Transcription Job state, and the Check Transcription Job state.

    Set the wait timer to a fixed interval of 2 seconds.

    Rename the state name to Wait 2 seconds.

    Return to the Choice state.

    Update the next state for the second rule (In Progress) to Wait 2 seconds to create a loop.

    Rename the state name for the Choice state to Transcript Status?.

    Add a Success flow state to the Transcript Completed branch of the Transcript Status? state.

    Add a Fail flow state to the Default branch of the Transcript Status? state, in case of a failure.

    Click on the Create button at the top-right of the screen to save the state machine.

    Click on the Start execution button to trigger the first execution for our state machine.

    Leave all of the contents of the execution window to the defaults, and click Start execution.

    Wait for the job to be completed, and verify that it reaches the Success state.

    Scroll to the top of the page, and click the New execution button to run the state machine again

    Note the error due to the conflicting transcription job name, noting the requirements for a unique name.

    Scroll to the top of the page, and click the Edit state machine button to return to the Workflow Studio.

    Select the Start Call Transcription Job state, and update the API Parameters to the following (replacing the placeholders):

    {
        "LanguageCode": "en-us",
        "Media": {
            "MediaFileUri": "<MP3-File-S3-URI>"
        },
        "TranscriptionJobName.$": "$$.Execution.Name",
        "OutputBucketName": "<S3-Bucket-Name>"
    }

    Select the Check Transcription Job state, and update the API Parameters to the following:

    {
        "TranscriptionJobName.$": "$$.Execution.Name"
    }

    Click on the Save button at the top-right of the screen to save the state machine.

    Click on the Start execution button to trigger the first execution for our state machine.

    Leave all of the contents of the execution window to the defaults, and click Start execution.

    Wait for the job to be completed, and verify that it reaches the Success state.

Setup Amazon Comprehend

    Add the comprehend:DetectSentiment action in between the Transcript Status? state, and the Success state.

    Add the s3:GetObject action in between the Transcript Status? state, and the DetectSentiment state.

    Update the GetObject API Parameters to the following (replacing the placeholders):

    {
        "Bucket": "<S3-Bucket-Name>",
        "Key.$": "States.Format('{}.json', $.TranscriptionJob.TranscriptionJobName)"
    }

    Add a Pass flow state between the GetObject state, and the DetectSentiment state.

    Configure the Parameters of the Pass state, and provide the following definition:

    {
        "Body.$": "States.StringToJson($.Body)"
    }

    Select the DetectSentiment state, and update the API Parameters to the following:

    {
        "LanguageCode": "en",
        "Text.$": "$.Body.results.transcripts[0].transcript"
    }

    Click on the Save button at the top-right of the screen to save the state machine.

    Click on the Start execution button to trigger the first execution for our state machine.

    Leave all of the contents of the execution window to the defaults, and click Start execution.

    Wait for the job to be completed, and verify that it reaches the Success state.

    Return to the Step Functions tab.

    Rename the state name for the GetObject state to Get Transcript File Contents.

    Rename the state name for the Pass state to Parse the JSON.

    Rename the state name for the DetectSentiment state to Detect Sentiment.

    Add a Choice flow state between the Detect Sentiment state, and the Success state.

    Set the conditions for the first Choice state rule to the following condition:
        Variable: $.Sentiment
        Operator: is equal to
        Value: String constant
        NEGATIVE

    Create a second rule for the Choice state.

    Set the conditions for the first Choice state rule to the following condition:
        Variable: $.Sentiment
        Operator: is equal to
        Value: String constant
        POSITIVE

    Add the lambda:Invoke action after the Choice state for the NEGATIVE sentiment path.

    Configure the Function Name for the action to the NegativeInteraction:$LATEST function.

    Set the Next State to Success.

    Add the sqs:SendMessage action after the Choice state for the POSITIVE sentiment path.

    Set the Next State to Success.

    Configure the Queue URL for the action to the PositiveInteractionQueue.

    Add a Pass state to the Default path for the Choice state.

    Click on the Save button at the top-right of the screen to save the state machine.

    Click on the Start execution button to trigger the first execution for our state machine.

    Leave all of the contents of the execution window to the defaults, and click Start execution.

    Wait for the job to be completed, and verify that it reaches the Success state.

Conclusion

Congratulations  you just learned how to create an AWS Step Function to process data and orchestrate events!

    ------------------------------------------------------

------------------------------------------------------
9.5 Amazon SageMaker Deployments

  Resources:

    Logging IP traffic using VPC Flow Logs
      https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html


  SageMaker console includes:
    Ground Truth
      - set up and manage labeling jobs for training datasets using active learningin and human labeling
    Notebook
      - access a managed Jupyter Notebook enviroment
    Training
      - training and tune models
    Inference
      - package and deploy your machine learning models at scale
      - focus for this lesson


  Two Deployment Types:

             offline Usage                               Online Usage

    Usage    Asynchronous or Batch                       Synchronous or Real-time

    when     Generate predictions for whole set of       Generate low-latency predictions
               data all as once

    method   SageMaker Batch Transform                   SageMaker Hosting Services

    input    Varies depending on Algorithm               Varies depending on Algorithm
    format

    output   Varies depending on Algorithm               JSON string
    format



  SageMaker Hosting Services - 3 main steps
    Create a [production deployable] Model
      - this is the inference enging that will provide predictions for your endpoint
    Create an Endpoint Configuration
      - define the model to use, inference instance type, instance count, variant name and weight
      - Also called a Production Variant
    Create an Endpoint
      - publish the model via the endpoint configuration to be called by SageMaker API InvokeEndpoint() method

   Model Training

     AWS Console    ----------->                  Behind the
         or           training                      Scenes:
     SDK            ----------->     SageMaker  ------> Container  -----> ML Training  --------> Model Artifacts
                                                         Registry          Container              (S3)
     Training Data (S3) ------->


   Model Creation

     AWS Console      ----------->
         or           CreateModel
     SDK              ----------->     SageMaker  ------>  Model v1

     Model Artifacts (S3) ------->

     Inference Container  ------->


    Model Creation - SDK or Console
      console
        - for your own model:
          - specify the path to the inference code image
          - specify s3 location for the model artifacts generated by training process
          - Note: console does additional shortcuts that let you create a model by simply selecting a training job
            and it's gonna auto-populate model [e.g. artifacts and inference image]
          OR
       AWS Marketplace
        - Use a model package from AWS Marketplace


    Model versions
      - can create a new version of your model  (have multiple version)


  Endpoint Configuration
    - specify a production variant for each version of the model
    - production variant includes: model version, instance type to run model on, and initial_weights
    initial weight
      - control the amount of traffic going to a particular production variant

            Percent of Traffic = Variant Weight / (sum of all weights)

                                                                   |----------------------------|
                                                                   |   Endpoint Configuration   |
     AWS Console      ----------->                                 | -------------------------  |
         or           CreateWEndpointConfig                        | | Production Variant 1  |  |
     SDK              ----------->            SageMaker  ------>   | |     model v1          |  |
                                                                   | |  instance type =      |  |
                                                                   | |       'ml.m4.xlarge'  |  |
                                                                   | |  initial_weight = 99  |  |
                                                                   | -------------------------  |
                                                                   | -------------------------  |
                                                                   | | Production Variant 2  |  |
                                                                   | |     model v1          |  |
                                                                   | |  instance type =      |  |
                                                                   | |      'ml.m4.xlarge'   |  |
                                                                   | |  initial_weight = 1   |  |
                                                                   | |-----------------------|  |
                                                                   |----------------------------|

  Endpoint Configuration
    - create our endpoint via the SDK or the AWS console
    behind the scenes
       - SageMaker is going to fetch the container image from the local inference container repo and provision
         the resources to spin up each model versions.
       - SageMaker also creates an HTTPS endpoint that that can then given to our apps so that they can call your models
       - The container logs are sent to CloudWatch and the endpoint traffic is logged in CloudTrail
    Application
      - use the SageMaker API with the invokeEndpoint() method to call our endpoint and get back the response
        from one of your production variants depending on the weighting assigned the the variant

              AWS Console      --------------->                 Behind the Scenes
                  or           CreateWEndpoint
              SDK              --------------->                                                             99%
                                                  SageMaker  ----> Container  -----> Inference Container  <-----
     |----------------------------|                                Registery          (model v1)               HTTPS    ------|
     |   Endpoint Configuration   |                                                                         1% Endpoint <-|   |
     | -------------------------  | ---------->                               -----> Inference Container  <-----          |   |
     | | Production Variant 1  |  |                                                   (model v2)             traffic      |   |
     | |-----------------------|  |                                                        |                      |       |   |
     | | Production Variant 2  |  |                                CloudWatch  <-- logs ---|      CloudTrail <----|       |   |
     | -------------------------  |                                                                                       |   |
     |----------------------------|                                                                                       |   |
                                                                                                                          |   |
                                                                                                                          |   |
                                                                                                 Application        HTTPS |   |
                                                                                                   SageMaker API     Post |   |
                                                                                                   InvokeEndpoint() ------|   |
                                                                                                                    <---------|
                                                                                                                 HTTPS 200 Body

  Batch Transform
     - batch processing use is common when you need to evaluate the whole dataset like in a forecasting scenario.

     AWS Console ----------------->               Behind the
         or      CreateTransformJob                 Scenes:                              logs
     SDK         ----------------->  SageMaker  ------> Provision  ----->   node n     --------> CloudWatch
                                                        ML Instances          |
     Dataset and  ---------------->                                           |
     Model Artifacts  --------------------------------->                      v
     (S3)             <--------------------------------------------------- Inference
                                                                            results


  Inference Pipeline example flow:
    - it's not uncommon at all to chain together algorithms and feature engineering steps
    - might use Random Cut Forest to screen out outliers, then PCA to figure out which features to focus on.
      Then K-Means for clustering customers into groups and then maybe a Linear Learner model that's tailored to each
      specific customer cluster.
    - AWS SageMaker offers interface pipelines to help us do this smoothly.  It just is a way for
      us to chain together containers

    Problem: What is the estimated basket size of shoppers who respond to our email promotion

          remove     ------->   Identify relevant  ------> Cluster into   ------>  Predict Basket
          outliers               attributes                 Groups                  Size

      Potential Algorithms:

          Random Cut ------->     PCA              ------>  K-means       ------>  Linear Learner
          Forest

  Inference Pipeline
    - A SageMaker model composed of a sequence of two to five containers which can process data as a flow
    - these can be built-in algorithms or your own custom algorithms in docker containers
    - can be used for both real-time inference and Batch transforms
    - all containers deployed to the same EC2 instance for local speed

    https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html
    Define a pipeline
    - To create an inference pipeline, You can use either the SageMaker Pipelines Python SDK or the drag-and-drop visual
      designer in Amazon SageMaker Studio to author, view, edit, execute, and monitor your ML workflows.


  SageMaker Neo
    - Enables a simplified way to optimize machine learning models for a variety of computing architectures such as
      ARM, Intel, and nVidia processors
    - consists of a compiler to convert the machine learning model into an optimized binary and runtime to execute
      the model on the target architecture

    - for example, if you have created a model using MXNet, you can use Neo to optimize that single model for running
      on an IoT device or similarly an Nvidia CUDA platform without having to change anything about our model.
    - Neo takes care of porting it and optimizing it to whatever target architecture we're deploying to.


                                                                   -----> SageMaker Hosting Services
          TensorFlow   ------->
          MxNet        ------->   SageMaker ------>  Architecture  ------>  AWS IoT Greengrass
          Pytorch      ------->     Neo            optimized Binary
          OnnX         ------->                                    ------->  nVidia Platform
          dmlc XGBoost ------->

  Elastic Inference (EI)
    https://aws.amazon.com/blogs/machine-learning/reduce-inference-costs-on-amazon-ec2-for-pytorch-models-with-amazon-elastic-inference/
    Note: Amazon Elastic Inference is no longer available. Please see Amazon SageMaker for similar capabilities.
    - speeds up throughput and decreases latency of real-time inferences deployed on SageMaker Hosted Services using
      only CPU-based instances but much more cost-effective than a full GPU instance
    - must be configured when you create a deployable model and EI is not available for all algorithms yet

    - If you have periods of heavy demand, using elastic interfaces might be a good option over deploying
      a lot of resources and just having them hanging out there waiting for it demand spike


  Automatic Scaling
    - dynamically add and remove instances to a production variant based on changes in workload
    - you define and apply a scaling policy that uses a CloudWatch metric and target value such as InvocationsPerInstance


    Configure variant automatic scaling:
      you need to define:  Minimum instance count
                           Maximum instance count
                           Target Metric & target value
              optional:    Cool Down - SCale in cool down (sec) & Scale out cool down (sec)


     Cool Down period
       - provide our landscape with a chance to stabilize before adding or removing instances
       - add a delay period between CloudWatch alarm and the scale in or scale out auto scaling events
       - scale out cool down provids a buffer to allow for the newly-introduced instances to spin up and take on
         some load before we start launching another instance

       - Note:
          - if the traffic drops to zero, SageMaker won't scale down. because CloudWatch does not emit metrics
            with a zero value.
          - you either have to figure out some way to send minimal requests to the endpoint so that it ramps down
            or you can change the maximum capacity to match the minimum capacity manually.

  High Availability With SageMaker
    - AWS strongly recommends that you use multiple instances for mission critical endpoints.
    - AWS will automatically distribute those multiple instances over separate AZs to guard against an AZ failure
    - If AZ running an instance goes down, AWS will automatically spin-up another instance in a different AZ


------------------------------------------------------
9.6 Other ML Deployment Options

   Other Deployment Options
     - Elastic Container Service (ECS)
     - EC2
     - Elastic Map Reduce (EMR)
     - On-premise


  SageMaker Hosting Service

              AWS Console      ---------------> |----------|    Behind the Scenes
                  or           CreateWEndpoint  |          |
              SDK              ---------------> |          |
                                                | SageMaker| ----> Container  -----> Inference Container  <-----
     |----------------------------|             |          |       Registery          (model v1)               HTTPS    ------|
     |   Endpoint Configuration   |             |          |                                                   Endpoint <-|   |
     | -------------------------  | ----------> |----------|                               |                      |       |   |
     | | Production Variant 1  |  |                                                        |                      |       |   |
     | |-----------------------|  |                                                        |                      |       |   |
     |----------------------------|                                CloudWatch  <-- logs ---|      CloudTrail <----|       |   |
                                                                                                                          |   |
                                                                                               |-----------------|        |   |
                                                                                               |Application      |  HTTPS |   |
                                                                                               | SageMaker API   |   Post |   |
                                                                                               | InvokeEndpoint()|  ------|   |
                                                                                               |-----------------|  <---------|

  Other ML Deployment Options: Model Creation


     MxNet   Jupyter
             Notebook  ----------->
                                       SageMaker  ------>  Model ------- Model Artifacts
     Training Data   (S3) -------->                                           S3



  Other ML Deployment Options: Deploying with ECS (with Fargate or EC2)
    - you use the proper inference image from the container repository and then you would deploy using Amazon ECS or EC2.
    - could either manually deploy on an EC2 instance, or fleet of instances that you manage, or you could use AWS Fargate
      to automatically provision resources for our ML Containers.


            Inference                                                            ---------------------------|
            Container  ----------->    Elastic                                   | ML Containers ...        |
                                       Container  ------>  AWS Fargate  ---->    |--------------------------|
          Model Artifacts -------->    Service                                   | Container    Container   |
              on S3                                                              |  Host         Host       |
                                                                                 |--------------------------|
     Deploying with EC2
       - If deploy using EC2, you would select one of the Deep Learning AMIs from the AMI catalog and use that AMI
       - you would spin up our own EC2 Instance, using that Deep Learning AMI and select the Instance type that is appropriate
         for the algorithm that we're using, be it CPU, GPU, maybe a memory heavy instance type. You manage it like any other
         EC2 instance.
       - You could create an API gateway front end to serve up as a REST API for our interface model

                                    |------------|
            Deep Learning           |  VPC       |
            AMI        -----------> |            |                    HTTP       |--------------|
                                    |  EC2 P3    | <-----  API     <---------    |  Application |
          Model Artifacts --------> |  Instance  |         Gateway    REST       |--------------|
              on S3                 |            |
                                    |------------|

         P3: Include up to 8 NVidia V100 Tensor Core GPUs



     Submitting User Applications with spark-submit BLOG
       https://aws.amazon.com/blogs/big-data/submitting-user-applications-with-spark-submit/

     Deploying with EMR (Spark)
       - deploy to an Elastic Map Reduce Cluster using Spark

                                    |----------------------------------------------------|
                                    |                                              Spark |
                                    | spark-submit                                       |
                                    |  --packages con.amazonaws:aws-java-sdk:1.11.238 \  |
                                    |  --deploy-mode cluster \                           |         Elastic
                                    |  --conf spark.driver.userClassPathFirst=true \     | ----->  Map
                    Model     ----> |  --conf spark.executor.userClassPathFirst=true \   |         Reduce
        Model ---> Artifacts        |  --jars SageMakerSparkApplicationJary.jar,...      |
                    on S3           |  ....                                              |
                                    |                                                    |
                                    |----------------------------------------------------|

     SageMaker with Spark
       - using the SageMaker Spark SDK, we can make use of an existing Spark landscape to pre-process data, or maybe
         source data from an existing Hadoop data warehouse, either on-prem, or running on EMR.
          - use existing Spark pipelines for pre-processing data
          - potentially more cost-efficient for non-GPU workloads
          - leverage existing Hadoop landscape and resources


     Training Data   <------------------------------------->   Elastic
                                    |-------------|            Map
     SageMaker        ----------->  |             | ------->   Reduce
      |                             |  Spark      |
      v                             |             |
     Model           <------------  |-------------|


  Other ML Deployment Options: Deploying Locally
     - Finally, we can deploy the model locally

    Model Creation for Local Deployment
      - starts like any other model building process.
      - In this case we're either using mxnet, or TensorFlow as a framework and then we would go through the training
        process and eventually it would spit out a model onto S3.
      - We would then download those model artifacts that are contained in a file called, model.tar.gz



     MxNet       Jupyter
     TensorFlow  Notebook  -------->                                                        Download        Local Data Center
                                       SageMaker  ------>  Model ------- Model Artifacts  ------------>
     Training Data   (S3)  -------->                                           S3            Artifacts       onprem
                                                                                             model.tar.gz    TensorFlow or MxNet



------------------------------------------------------
9.6 Security



  Security
    Visibility
      - VPC endpoints, Network Access Control Lists, security Groups
      - Using these things, we can make sure that we only allow certain people, or certain callers, to see
        our endpoints, or our servers.
    Authentication
      - Identity and Access Management (IAM)
      - provides services so that we can try to determine if the person trying to get into our application is indeed
        who they say they are
    Access Control
      - Identity and Access Management (IAM)
      - once we've authenticated them, we can then grant them access to do different things with our service, or our
        application, or our instance, and we can do this through roles or policies or groups.
    Encryption
      - Key Management Service (KMS)
      - create encryption keys, and there are many AWS services that work with KMS natively, so that it makes it
        very easy for us to encrypt stuff while using those services.


  Typical Security Archictecture
    - how you might secure our instances.
      - In the innermost layer, we have security groups, which are like little firewalls around our instances.
      - network access control lists, which govern which traffic we allow in and out of our subnets,
      - internet gateway, or a NAT instance, that provides us access to the internet.
      - VPC endpoint talking to S3.


                         |-------------------------------------------------------------|
                         |AWS                                                          |
                         |    |--------------------------------------------|           |
                         |    |VPC                                         |           |
                         |    |     |-----------------------------------|  |           |
                         |    |     |Network ACL                        |  |           |
                         |    |     |   |---------------------------|   |  |           |
                         |    |     |   |Security Group             |   |  |           |
                         |    |     |   |   |--------------------|  |   |  |           |
                         |    |     |   |   |EC2 Instance        |  |   |  |           |
              Internet <-----IG---NAT  LCK------->          <-------------VPC-----> S3 |
                         |    |     |   |   |                    |  |   | Endpoint     |
                         |    |     |   |   |--------------------|  |   |  |           |
                         |    |     |   |                           |   |  |           |
                         |    |     |   |---------------------------|   |  |           |
                         |    |     |                                   |  |           |
                         |    |     |-----------------------------------|  |           |
                         |    |                                            |           |
                         |    |--------------------------------------------|           |
                         |                                                             |
                         |-------------------------------------------------------------|

                         IG: Internet Gateway
                         LCK: Lock




  VPC Endpoints
    - a way to access other AWS services using AWS network without having to send traffic over the public internet
    - VPC Endpoints increase security and allow AWS services to communicate privately and reliably

    - VPC endpoint as a sort of virtual wormhole that allows our VPC to communicate with other AWS services,
      without having to exit to the public internet.


    VPC Endpoint types
      https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html
      Interface Endpoints
        - use a private DNS entry
        - enable connectivity to services over AWS PrivateLink.
        - uses DNS redirection and trickery to route traffic to internal addresses of the AWS services,
          versus the public addresses.
      Gateway Endpoints
        - only available for S3 and DynamoDB
        - use a route table entry versus a private DNS entry

        - targets specific IP routes in an Amazon VPC route table, in the form of a prefix-list, used for
          traffic destined to DynamoDB or S3. Gateway endpoints do not enable AWS PrivateLink.
        - uses something called a prefix list, which is really just a list of IP addresses for that AWS Service

      PrivateLink
        - AWS service that powers VPC Interface Endpoints
        - also used to commmunicate between AWs resources (e.g. SageMaker API <-----> Notebook instance)

   Securing ML Resources
      Specify subnets and security groups in CreateModel Call
        - when creating a model for the first time, you can tell SageMaker which subnets and security groups
          for the SageMaker training job to use, and then SageMaker would create an Elastic Network Interface
          (using and Elastic IP Address (EIP)) linking the Notebook subnets to the training container subnet.

          VpcConfig: {
            "Subnets": [
                     "subnet-0123456789abdcedf0",
                     "subnet-0123456789abdcedf1",
                     "subnet-0123456789abdcedf2"
              ],
              "SecurityGroupIds": [
                 sg-0123456789abcdef0"
               ]
            }


  Jupyter Notebook Instance Security info
    Internet-enabled by Default
      - by default, SageMaker notebook instance are internet-enabled
      - this allows for download and access to popular packages and libraries
    Can Disable Internet Access
      - internet access may represent a security concern to some, so you can disable direct internet access
      - for training and hosting models, you will need a NAT gateway (or NAT instance), with routes and security
        groups that permit internet access  or  you need to set up an interface endpoint
    Designed for Single User
      - Notebook instances provide the user with root access for installing packages
      - Best practices is one user per notebook instance
      - You can restrict access to users or roles via IAM Policy


  IAM Policy types
    Identity-based Policy
      - attached to identities, like users and groups
    Resource-based Policy
      - assigned to resources like S3, or KMS, or SQS

            Identity-based policy                    Resource-based Policy

   What     A permission policy attached             A permissions policy attached
              to IAM Identity                          to a resource

   Why      Allow or deny access to role             Allow or deny access at the
              or user                                  resource

   When     Only allow user 'mary' to access         Restrict R/W access to an S3 bucket by
            the notebook instance 'Mary_Notebook'      by specific AWS accounts


   Example SageMaker IAM Policy:
     Note: Some SageMaker actions, like create model, and create training job, require the user to pass an IAM role
           to SageMaker so that the service can assume those permissions to attempt the action,
     - in this example, we allow the action iam:PassRole, so that the user of this policy has the ability to
       hand off their role to the SageMaker service.


         {
           "Version": "2012-10-17",
           "Statement":[{
                 "Sid":"AllowCreate-Describe-Delete-Models",
                 "Effect":"Allow",
                 "Action":[
                    "sagemaker:CreateModel",
                    "sagemaker:DescribeModel",
                    "sagemaker:DeleteModel" ],
                 "Resource":"*"
              }
           ],
           "Statement":[{
                 "Sid":"AdditionalIAMPermissions",
                 "Effect":"Allow",
                 "Action":[
                    "iam:PassRole"]
                 "Resource":"arn:aws:iam:account-d:role/role-name"
              }
           ]
         }


  Encryption

                        What              Generic Example                 AWS Example

     Encryption      Stored Data in       Use GPG to encrypt a file       Configure S3 bucket to use S3-KMS encryption
       At Rest        encrypted format

     Encryption      Encrypt Data Strem   Using the TLS/SSL to encrypt    Use Certificate Manager with CloudFront to
       At Transit     carry the data       an HTTP connection (HTTPS)      provide TLS support for a custom domain


  SageMaker Encryption At Rest
    - AWS has done a really good job of incorporating encryption into the various SageMaker products,
    - when you create a notebook instance, create a training job, create endpoints, or create batch transform jobs,
      you can select an encryption key.
       - these are examples of encryption at rest

  SageMaker Encryption in Transit
    - by default, everything is already using HTTPS, which means it's encrypted,
    - you'd have to work really, really hard to figure out a way to not use HTTPS when you're using SageMaker.
    - Even the connection between the SageMaker API in our application and the HTTPS endpoint is encrypted using TLS.

    - when you build our hosted model, we're gonna be given an endpoint name that's just a computer-generated name,
      which we would plug into our SageMaker API inside our application,


  SageMaker Model with external access
    - what if we wanted to provide external access to our super-special model, then:
        - could set up an API Gateway using a custom domain, with a TLS certificate provided by AWS Certificate Manager.
        - could then use a lambda function to pass in the requests received from the API Gateway, and we would have access
          to all the features of API Gateway, like client certificates, throttling, and AWS Marketplace integration for
          creating a SaaS business model
                                                     |---------------------------------------|
       AWS Console    ---------------> |-------|     |Behind the Scenes                      |
          or           CreateEndpoint  |       |     |                                       |
         SDK          ---------------> |       |     |                                       |                               https://
                                       | Sage  | --->|Container  ---> Inference              |                               api.mydomain.com
     |-----------------|               | Maker |     | Registery      Container  <---  HTTPS | <---- Lambda       <----> API   <------------> Application
     |   Endpoint      | ----------->  |       |     |                (model v1)     Endpoint|     InvokeEndpoint()    GateWay                 REST Call
     |   Configuration |               |-------|     |                       |           |   |                            ^
     | |------------|  |                             | CloudWatch <-- logs --|           |   |                            |
     | | Production |  |                             |                                   |   |                            |
     | | Variant 1  |  |                             |                  CloudTrail <------   |                       AWS Certificate
     | |------------|  |                             |---------------------------------------|                           Manager
     |-----------------|                                                                                             (api.mydomain.com)




------------------------------------------------------
9.7 Monitor and Evaluate


  Resources
    Monitor Amazon SageMaker with Amazon CloudWatch
      https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html
      include info  on SageMaker Metrics and Dimensions:
         - SageMaker endpoint invocation metrics
         - SageMaker inference component metrics
         - SageMaker multi-model endpoint metrics
         - SageMaker jobs and endpoint metrics
         - SageMaker Inference Recommender jobs metrics
         - SageMaker Ground Truth metrics
         - Amazon SageMaker Feature Store metrics
         - SageMaker pipelines metrics


  DEFCON
    - respect people's privacy (no pics or videos)
    - practice reasonable safety with connected devices
    - 99.9% of attendees are curious, friendly, and love to share
    - you'll learn stuff that will increase your paranoia

  CloudWatch with SageMaker
    Wide Variety of Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html
      - Endpoint Invocation, Endpoint Instance, Training, Transform and Ground Truth metrics available
        in addition to algorithm specific metrics
    Near Real-Time
      - Most SageMaker-specific metrics are available at 1-minute frequency for quick reactions
    Metrics Available for 15 months
      - Statistics are kept 15 months to provide plenty of historical information on model performance
        and trend
    2 Week Limit on Console
      - on the console only the last two weeks of metrics are visible, but you can access that 15 months
        using the CLI or an SDK.
      - Metric are collected and sent every five minutes for training and prediction jobs


  Using CloudWatch with SageMaker Log Group / Log Stream
    - any stdout or stderr messages from notebooks instances, algorithm containers, or model containers
      is sent to CloudWatch Logs

    Log Group Name                        Log Stream Name

    /aws/sagemaker/TraingingJobs          [training-job-name]/algo-[instance-number-in-cluster]-[epoch-timestamp]

    /aws/sagemaker/Endpoints/             [production-variant-name]/[instance-id]
    [EndpointName]

    /aws/sagemaker/NotebockInstances      [notebook-instance-name]/[LifecycleConfigHook]

    /aws/sagemaker/TransformJobs          [transform-job-name]/[instance-id]-[epoch-timestamp]
                                          [transform-job-name]/[instance-id]-[epoch-timestamp]/data-log

  Re-Training
    - Over time our model may become less and less accurate.
    - What we might do is set up some CloudWatch alarm that monitors some metric, some accuracy metric,
      and once it falls below a certain threshold we might allow that alarm to trigger a Lambda function
      which could initiate another training job.
    - might use the new ground truth data that we've collected as the model has run in production so far,
      and that might yield an improved v2 model.
    - could then test that model in production using some AB testing scenario to see if indeed it is better.

                                                            New Ground  -----
                                                             Truth Data     |
                                                                            |
                                                                            v
         Accuracy  ---->  CloudWatch   -------> Lambda      -------------> SageMaker    --------> Model V2
          metric            Alarm               CreateTrainingJobs       New Training Job



  CloudTrail
    Log API Access
      - Captures any API calls made by or on behalf of SageMaker within your account
    Last 90 Day's Events
      - by default, the last 90 days of events are visible in CloudTrail Event History
    Can be Kept Indefinitely
      - you can setup a CloudTrail to store logs on S3 to reference for as log as you'd like, as well as
        practice lifecycle processes like archive to Glacier
    Query with Athena
      - once on S3, for advanced queries, you can use Athena to query and analyzed data directly from S3


------------------------------------------------------
9.8 Exam Tips

  Resources:

    AWS Docs: Best Practices for Deploying Models  <--- supposed to be, but instead was:
     Overview of machine learning with Amazon SageMaker
       https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-mlconcepts.html

      Maybe related ???:
      Deploy models for inference
        https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

    AWS Well-Architected Framework - Machine Learning Lens
      download PDF (in resources/resources_9_9/wellarchitected-machine-learning-lens.pdf)
      HTML version: https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html

    AWS Whitepaper: AWS Cloud Adoption Framework for Artificial Intelligence, Machine Learning, and Generative AI
      download PDF (in resources/resources_9_9/aws-caf-for-ai.pdf)
      HTML version: https://docs.aws.amazon.com/whitepapers/latest/aws-caf-for-ai/aws-caf-for-ai.html

    AWS Whitepaper: SageMaker Studio Administration Best Practices
      download PDF (in resources/resources_9_9/sagemaker-studio-admin-best-practices.pdf)
      HTML version: https://docs.aws.amazon.com/whitepapers/latest/sagemaker-studio-admin-best-practices/sagemaker-studio-admin-best-practices.html

     Augmented AI: The Power of Human and Machine
      download PDF (in resources/resources_9_9/augmented-ai-the-power-of-human-and-machine.pdf)



  Concepts:
    - understand the difference between offline and online usage for models and when you might use each
    - have a high-level understanding of the types of deployments and respective pros and cons
    - Understand A/B testing and how it might be used to introduce a newly updated model
    - understand the concepts behind continous integration, continuous delivery, and continuous deployment

  AI Developer Services
    - the AI developers services provided by AWS are scalable, fault-tolerant, and ready to use
    - Know each services, its purpose, and in what use-cases they might be used
      -------------------
     AI Services
       - App Developers, no ML experience required
       Amazon Comprehend
          - natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text
       Amazon forecast
          - combines time-series data with variables to deliver highly accurate forecasts
       Amazon Lex
         - Build conversational interfaces that can understand the intent and content of natural speech
         - provides response
       Amazon Personalize
         - recommendation engine as a service based on demographic and behavioral data
       Amazon Polly
         - text-to-speech service supporting multiple languages, accents, and voices
       Amazon Rekognition
         - image and video analysis to parse and recognize objects, people, activities, and facial expressions,
       Amazon Textrac
         - Extract text, content, and metadata from scanned documents
       Amazon Translate
          - translate text to and from many different languages
       Amazon Transcribe
          - Speech-to-text as a service
      -------------------
    - Experiment with each service via the console or CLI to get a feel for what it does and how it works


  SageMaker Deployments
    - know the three main steps in creating a deployment using SageMaker Hosting Services
      ----------------------------

        SageMaker Hosting Services - 3 main steps
          Create a [production deployable] Model
            - this is the inference enging that will provide predictions for your endpoint
          Create an Endpoint Configuration
            - define the model to use, inference instance type, instance count, variant name and weight
            - Also called a Production Variant
          Create an Endpoint
            - publish the model via the endpoint configuration to be called by SageMaker API InvokeEndpoint() method

      ----------------------------
    - Understand ProducitonVariants and how they can be used to introduct new evolutions of your models
    - Know how to calculate the percentage of traffic given weights for each production variant
    - Understand the purpose and limitations of Interface Pipelines
      SageMaker Neo, Elastic Inference (EI is no longer supported), Auto-Scaling, and Cool Down
      ----------------------------

        Inference Pipeline
          - A SageMaker model composed of a sequence of two to five containers which can process data as a flow
          - these can be built-in algorithms or your own custom algorithms in docker containers
          - can be used for both real-time inference and Batch transforms
          - all containers deployed to the same EC2 instance for local speed

          https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html
          Define a pipeline
          - To create an inference pipeline, You can use either the SageMaker Pipelines Python SDK or the drag-and-drop visual
            designer in Amazon SageMaker Studio to author, view, edit, execute, and monitor your ML workflows.

        SageMaker Neo
          - Enables a simplified way to optimize machine learning models for a variety of computing architectures such as
            ARM, Intel, and nVidia processors
          - consists of a compiler to convert the machine learning model into an optimized binary and runtime to execute
            the model on the target architecture

          - for example, if you have created a model using MXNet, you can use Neo to optimize that single model for running
            on an IOT device or similarly an Nvidia CUDA platform without having to change anything about our model.
          - Neo takes care of porting it and optimizing it to whatever target architecture we're deploying to.

        Elastic Inference (EI)  - EI service is no longer supported
          - speeds up throughput and decreases latency of real-time inferences deployed on SageMaker Hosted Services using
            only CPU-based instances but much more cost-effective than a full GPU instance
          - must be configured when you create a deployable model and EI is not available for all algorithms yet

          - If you have periods of heavy demand, using elastic interfaces might be a good option over deploying
            a lot of resources and just having them hanging out there waiting for it demand spike


        Automatic Scaling
          - dynamically add and remove instances to a production variant based on changes in workload
          - you define and apply a scaling policy that uses a CloudWatch metric and target value such as InvocationsPerInstance


           Cool Down period
             - provide our landscape with a chance to stabilize before adding or removing instances
             - add a delay period between CloudWatch alarm and the scale in or scale out auto scaling events
             - scale out cool down provids a buffer to allow for the newly-introduced instances to spin up and take on
               some load before we start launching another instance
      ----------------------------

    - Recall best practice suggestions for high availability with SageMaker
      ----------------------------

        High Availability With SageMaker
          - AWS strongly recommends that you use multiple instances for mission critical endpoints.
          - AWS will automatically distribute those multiple instances over separate AZs to guard against an AZ failure
          - If AZ running an instance goes down, AWS will automatically spin-up another instance in a different AZ

      ----------------------------

  Other ML Deployment Options
    - know the four other options for ML Deployment aside from SageMaker Hosting Services and when you might chose
      them instead of SageMaker

      ----------------------------
         Other Deployment Options
           - Elastic Container Service (ECS)
           - EC2
           - Elastic Map Reduce (EMR)
           - On-premise


        Other ML Deployment Options: Deploying with ECS (with Fargate or EC2)
          - you use the proper inference image from the container repository and then you would deploy using Amazon ECS or EC2.
          - could either manually deploy on an EC2 instance, or fleet of instances that you manage, or you could use AWS Fargate
            to automatically provision resources for our ML Containers.


                  Inference                                                            ---------------------------|
                  Container  ----------->    Elastic                                   | ML Containers ...        |
                                             Container  ------>  AWS Fargate  ---->    |--------------------------|
                Model Artifacts -------->    Service                                   | Container    Container   |
                    on S3                                                              |  Host         Host       |
                                                                                       |--------------------------|
           Deploying with EC2
             - If deploy using EC2, you would select one of the Deep Learning AMIs from the AMI catalog and use that AMI
             - you would spin up our own EC2 Instance, using that Deep Learning AMI and select the Instance type that is appropriate
               for the algorithm that we're using, be it CPU, GPU, maybe a memory heavy instance type. You manage it like any other
               EC2 instance.
             - You could create an API gateway front end to serve up as a REST API for our interface model
                  Deep Learning           |  VPC       |
                  AMI        -----------> |            |                    HTTP       |--------------|
                                          |  EC2 P3    | <-----  API     <---------    |  Application |
                Model Artifacts --------> |  Instance  |         Gateway    REST       |--------------|
                    on S3                 |            |
                                          |------------|

               P3: Include up to 8 NVidia V100 Tensor Core GPUs

           Deploying with EMR (Spark)
             - deploy to an Elastic Map Reduce Cluster using Spark

                                          |----------------------------------------------------|
                                          |                                              Spark |
                                          | spark-submit                                       |
                                          |  --packages con.amazonaws:aws-java-sdk:1.11.238 \  |
                                          |  --deploy-mode cluster \                           |         Elastic
                                          |  --conf spark.driver.userClassPathFirst=true \     | ----->  Map
                          Model     ----> |  --conf spark.executor.userClassPathFirst=true \   |         Reduce
              Model ---> Artifacts        |  --jars SageMakerSparkApplicationJary.jar,...      |
                          on S3           |  ....                                              |
                                          |                                                    |
                                          |----------------------------------------------------|

           SageMaker with Spark
             - using the SageMaker Spark SDK, we can make use of an existing Spark landscape to pre-process data, or maybe
               source data from an existing Hadoop data warehouse, either on-prem, or running on EMR.
                - use existing Spark pipelines for pre-processing data
                - potentially more cost-efficient for non-GPU workloads
                - leverage existing Hadoop landscape and resources


           Training Data   <------------------------------------->   Elastic
                                          |-------------|            Map
           SageMaker        ----------->  |             | ------->   Reduce
            |                             |  Spark      |
            v                             |             |
           Model           <------------  |-------------|


        Other ML Deployment Options: Deploying Locally
           - Finally, we can deploy the model locally

          Model Creation for Local Deployment
            - starts like any other model building process.
            - In this case we're either using mxnet, or TensorFlow as a framework and then we would go through the training
              process and eventually it would spit out a model onto S3.
            - We would then download those model artifacts that are contained in a file called, model.tar.gz



           MxNet       Jupyter
           TensorFlow  Notebook  -------->                                                        Download        Local Data Center
                                             SageMaker  ------>  Model ------- Model Artifacts  ------------>
           Training Data   (S3)  -------->                                           S3            Artifacts       onprem
                                                                                                   model.tar.gz    TensorFlow or MxNet
      ----------------------------



  Security
    - Be familiar with using VPCs, NACLs, Security Groups, IAM, and KMS to secure Amazon ML and SageMaker resources
    - Understand a VPC Endpoint and why it increases security
      ----------------------------
        VPC Endpoints
          - a way to access other AWS services using AWS network without having to send traffic over the public internet
          - VPC Endpoints increase security and allow AWS services to communicate privately and reliably

          VPC Endpoint types
            Interface Endpoints
              - use a private DNS entry
              - enable connectivity to services over AWS PrivateLink.
              - uses DNS redirection and trickery to route traffic to internal addresses of the AWS services,
                versus the public addresses.
            Gateway Endpoints
              - only available for S3 and DynamoDB
              - use a route table entry versus a private DNS entry

      ----------------------------
    - Know that Notebook instances are internet-enabled by default but this can be disabled giver certain conditions
      and introducing limitations
      ----------------------------
        Jupyter Notebook Instance Security info
          Internet-enabled by Default
            - by default, SageMaker notebook instancea are internet-enabled
            - this allows for download and access to popular packages and libraries
          Can Disable Internet Access
            - internet access may represent a security concern to some, so you can disable direct internet access
            - for training and hosting models, you will need a NAT gateway (or NAT instance), with routes and security
              groups that permit internet access  or  you need to set up an interface endpoint
          Designed for Single User
            - Notebook instances provide the user with root access for installing packages
            - Best practices is one user per notebook instance
            - You can restrict access to users or roles via IAM Policy
      ----------------------------

  Monitor and Evaluate
    - Understand the difference between CloudWatch and CloudTrail
    - know that CloudWatch has a limited storage while CloudTrail can be have unlimited storage if you log to an S3 bucket
    - Understand how you might use metrics to trigger events like re-training


------------------------------------------------------
9.9 Demo: Implementation and Operations Lab

  Resources:
    Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda
      https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/

    Note: Downloaded demo files to:
      C:\pat-cloud-ml-repo\machine-learning-training\acloudguru-machine-learning-aws-certified-course\demos\9_10_implementation_and_operations_demo

    Jupyter Notebook: ufo-implementation-operations-lab.ipynb
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter9/ufo-implementation-operations-lab.ipynb

    Lambda Function: lambda_function.py
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter9/lambda_function.py

    Sample Request JSON: sample_request.json
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter9/sample_request.json

    UFO Full Dataset: ufo_fullset.csv
      https://github.com/ACloudGuru-Resources/Course_AWS_Certified_Machine_Learning/blob/master/Chapter9/ufo_fullset.csv


  Deploy Model into Production
    - Mr K has give us the go to deploy the optimized Linear Learner model in production
    - Once this ic complete, Mr K's team can use it to investigate any newly reported UFO sightings

    - our goal is deploy the model into production and give Mr K's team some way to interact with the deployed model

  Deploy Model into Production
    - deploy our Linear Learner model using SageMaker hosting
    - create a way to interact with the SageMaker endpoint created for the deployed model
    - How will Mr K's team interact with the model? Will there be some type of user interface?
    - What does the input and output of a request look like? Will it be in batches of immediate response?

  Final Results
    - Make a request to the SageMaker hosted model
        {
         "data": "45.0, 10.0,38.5816667,-121.49333329999999,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0"
        }
          Note: input data had to be transformed to numeric format for the Linear learner model
    - view the output response from the trained model:
          Explained, Unexplained, or Probable

  Deploy to SageMaker Hosting
    - Simply call the deploy() method after training our model to deploy it to SageMaker hosting

    Notes:
        - reason I suggest using the .deploy method() is because when we use it within a Jupyter notebook, it makes
          it really easy and only one line of code to deploy our model.
        - if you're using the user interface, you could simply point it to the model artifact that you already trained,
          or you could point it to a training job that you had pre created.

  Create Lambda and API GateWay Endpoint
    - Create an API Gateway endpoint that invokes a Lambda function
    - This Lambda function calls '.invoke_endpoint()' method with an new UFO sightings (calls the sagemaker trained model)
    - return the results from the models prediction

   SageMaker Hosting, Lambda, API Gateway
      - create a new Jupyter notebook and retrain our model.
      - when retraining our model, so we can hold out a portion of our data set to test it once we deploy our model.
      - after retraining, going to then deploy it to a SageMaker model endpoint and then use our test data set to test
        against that newly created endpoint
      -  Next, create a Lambda function which calls the .invoke_endpoint() method and sends a request of a new UFO sighting.
      - Once we have our Lambda function set up, we can then create an API Gateway endpoint, which creates an HTTP endpoint
        that we can call from API Gateway, or we can call it from any other application.

                                                                        |----------------------------------|
          UFO         HTTPS Endpoint         Lambda Function            |SageMaker Model Endpoint          |
        Sighting ---> API Gateway     ---->  .invoke_endpoint() ------> |                                  |
          input       Endpoint                                          |             Notebook             |
                                                                        |             .deploy()            |
                                                                        | Train                       Model|
                                                                        |----------------------------------|

   ------------------------------------------------------
   AWS Console -> SageMaker -> Notebook -> Notebook Instances -> select "my-notebook-inst" -> start
   # when running:
   -> Open Jupyter
     -> Upload "ufo-implementation-operations-lab.ipynb"
        -> start uploaded notebook



    code:  UFO Implementation and Operations Lab

         >>> # First let's go ahead and import all the needed libraries.
         >>> import pandas as pd
         >>> import numpy as np
         >>> from datetime import datetime
         >>>
         >>> # import module in terms of dealing with various types of I/O
         >>> import io
         >>>
         >>> # import sagemaker common library
         >>> import sagemaker.amazon.common as smac
         >>>
         >>> import boto3
         >>> from sagemaker import get_execution_role
         >>> import sagemaker
         >>>
         >>> import matplotlib.pyplot as plt
         >>> import seaborn as sns

         >>> # Step 1: Loading the data from Amazon S3
         >>> # Let's get the UFO sightings data that is stored in S3 and load it into memory.

         >>> role = get_execution_role()
         >>> bucket='modeling-ufo-lab1'
         >>> sub_folder = 'ufo_dataset'
         >>> data_key = 'ufo_fullset.csv'
         >>> data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)

             df = pd.read_csv(data_location, low_memory=False)
             df.head()

              	reportedTimestamp 	eventDate 	eventTime 	shape 	duration 	witnesses 	weather 	firstName 	lastName 	latitude 	longitude 	sighting 	physicalEvidence 	contact 	researchOutcome
             0 	1977-04-04T04:02:23.340Z 	1977-03-31 	23:46 	circle 	4 	1 	rain 	Ila 	Bashirian 	47.329444 	-122.578889 	Y 	N 	N 	explained
             1 	1982-11-22T02:06:32.019Z 	1982-11-15 	22:04 	disk 	4 	1 	partly cloudy 	Eriberto 	Runolfsson 	52.664913 	-1.034894 	Y 	Y 	N 	explained
             .  . .

             Step 2: Cleaning, transforming and preparing the dataset

             This step is so important. It's crucial that we clean and prepare our data before we do anything else.

             Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types.
             Here is what we are going to take care of.

              1. Convert the reportedTimestamp and eventDate to a datetime data types.
              2. Convert the shape and weather to a category data type.
              3. Map the physicalEvidence and contact from 'Y', 'N' to 0, 1.
              4. Convert the researchOutcome to a category data type (target attribute).

             Let's also drop the columns that are not important.

              1. We can drop sighting becuase it is always 'Y' or Yes.
              2. Let's drop the firstName and lastName becuase they are not important in determining the researchOutcome.
              3. Let's drop the reportedTimestamp becuase when the sighting was reporting isn't going to help us determine
                 the legitimacy of the sighting.
              4. We would need to create some sort of buckets for the eventDate and eventTime, like seasons for example, but
                 since the distribution of dates is pretty even, let's go ahead and drop them.

             Finally, let's apply one-hot encoding

              1. We need to one-hot both the weather attribute and the shape attribute.
              2. We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the
                alogrithm is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2.


         >>> # Replace the missing values with the most common shape (circle)
         >>> df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])
         >>>
         >>> # Convert the reportedTimestamp and eventDate to a datetime data types.
         >>> df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])
         >>> df['eventDate'] = pd.to_datetime(df['eventDate'])
         >>>
         >>> # Convert the shape and weather to a category data type.
         >>> df['shape'] = df['shape'].astype('category')
         >>> df['weather'] = df['weather'].astype('category')
         >>>
         >>> # Map the physicalEvidence and contact from 'Y', 'N' to 0, 1
         >>> df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})
         >>> df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})
         >>>
         >>> # Convert the researchOutcome to a category data type (target attribute).
         >>> df['researchOutcome'] = df['researchOutcome'].astype('category')
         >>>
         >>> # We can drop sighting becuase it is always 'Y' or Yes.
         >>> # Let's drop the firstName and lastName becuase they are not important in determining
         >>> #    the researchOutcome.
         >>> # Let's drop the reportedTimestamp becuase when the sighting was reporting isn't going
         >>> #   to help us determine the legitimacy of the sighting.
         >>> df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)
         >>>
         >>> # Let's one-hot the weather and shape attribute
         >>> # Note: FIXED! needed to add 'dtype='int' because get_dummies dtype default is now 'bool'
         >>> df = pd.get_dummies(df, columns=['weather', 'shape'], dtype='int')
         >>>
         >>> # Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable
         >>> df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})

         >>> display(df.head())
         >>> display(df.shape)

              	duration 	witnesses 	latitude 	longitude 	physicalEvidence 	contact 	researchOutcome 	weather_clear 	weather_fog 	weather_mostly cloudy 	... 	weather_stormy 	shape_box 	shape_circle 	shape_disk 	shape_light 	shape_oval 	shape_pyramid 	shape_sphere 	shape_square 	shape_triangle
             0 	4 	1 	47.329444 	-122.578889 	0 	0 	1 	0 	0 	0 	... 	0 	0 	1 	0 	0 	0 	0 	0 	0 	0
             1 	4 	1 	52.664913 	-1.034894 	1 	0 	1 	0 	0 	0 	... 	0 	0 	0 	1 	0 	0 	0 	0 	0 	0
             . . .

             5 rows  23 columns

             (18000, 23)


             Step 3: Creating and training our model (Linear Learner)

             Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready
             for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects
             the first column to be the target attribute)


         >>> # random and split data (80% train, 10% valication, 10% test)
         >>> np.random.seed(0)
         >>> rand_split = np.random.rand(len(df))
         >>> train_list = rand_split < 0.8
         >>> val_list = (rand_split >= 0.8) & (rand_split < 0.9)
         >>> test_list = rand_split >= 0.9
         >>>
         >>>  # This dataset will be used to train the model.
         >>> data_train = df[train_list]
         >>>
         >>> # This dataset will be used to validate the model.
         >>> data_val = df[val_list]
         >>>
         >>> # This dataset will be used to test the model.
         >>> data_test = df[test_list]
         >>>
         >>> # Breaks the datasets into attribute numpy.ndarray and the same for target attribute.
         >>> train_X = data_train.drop(columns='researchOutcome').values
         >>> train_y = data_train['researchOutcome'].values
         >>>
         >>> val_X = data_val.drop(columns='researchOutcome').values
         >>> val_y = data_val['researchOutcome'].values
         >>>
         >>> test_X = data_test.drop(columns='researchOutcome').values
         >>> test_y = data_test['researchOutcome'].values

             Next, Let's create recordIO file for the training data and upload it to S3.


         >>> train_file = 'ufo_sightings_train_recordIO_protobuf.data'
         >>>
         >>> # converts the data in numpy array format to RecordIO format (using SageMaker common library)
         >>> f = io.BytesIO()
         >>> smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))
         >>> f.seek(0)
         >>>
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)
         >>> training_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_train/{}'.format(bucket, train_file)
         >>> print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))
             The Pipe mode recordIO protobuf training data: s3://modeling-ufo-lab1/implementation_operations_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data

             Let's create recordIO file for the validation data and upload it to S3

         >>> validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'
         >>>
         >>> f = io.BytesIO()
         >>> smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))
         >>> f.seek(0)
         >>>
         >>> boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)
         >>> validate_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_validation/{}'.format(bucket, validation_file)
         >>> print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))
             The Pipe mode recordIO protobuf validation data: s3://modeling-ufo-lab1/implementation_operations_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data


             Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call the Linear Learner algorithm.


         >>> from sagemaker import image_uris
         >>> container = image_uris.retrieve('linear-learner', boto3.Session().region_name, '1')

         >>> # Create a training job name
         >>> job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime("%Y%m%d%H%M%S"))
         >>> print('Here is the job name {}'.format(job_name))
         >>>
         >>> # Here is where the model-artifact will be stored
         >>> output_location = 's3://{}/implementation_operations_lab/linearlearner_output'.format(bucket)
             Here is the job name ufo-linear-learner-job-20240717192831


             Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create
               a Linear Learner model.

             First I like to always create a specific job name. Next, we'll need to specify training parameters.

             Finally, after everything is included and ready, then we can call the .fit() function which specifies the S3 location for
               training and validation data.

         >>> print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))
             The feature_dim hyperparameter needs to be set to 22.

         >>> sess = sagemaker.Session()
         >>>
         >>> # Setup the LinearLeaner algorithm from the ECR container
         >>> linear = sagemaker.estimator.Estimator(container,
         >>>                                        role,
         >>>                                        instance_count=1,
         >>>                                        instance_type='ml.c4.xlarge',
         >>>                                        output_path=output_location,
         >>>                                        sagemaker_session=sess,
         >>>                                        input_mode='Pipe')
         >>> # Setup the hyperparameters
         >>> linear.set_hyperparameters(feature_dim=22,
         >>>                            predictor_type='multiclass_classifier',
         >>>                            num_classes=3,
         >>>                            # based on video hyperparameter tunning results
         >>>                            early_stopping_patience=3,
         >>>                            early_stopping_tolerance=0.001,
         >>>                            epochs=15, l1=0.0647741539306635,
         >>>                            learning_rate=0.09329042024421902,
         >>>                            loss='auto', mini_batch_size=744,
         >>>                            num_models='auto',
         >>>                            optimizer='auto',
         >>>                            unbias_data='auto',
         >>>                            unbias_label='auto',
         >>>                            use_bias='true',
         >>>                            wd=0.000212481391205101
         >>>                           )
         >>>
         >>> # Launch a training job. This method calls the CreateTrainingJob API call
         >>> data_channels = {
         >>>     'train': training_recordIO_protobuf_location,
         >>>     'validation': validate_recordIO_protobuf_location
         >>> }
         >>> linear.fit(data_channels, job_name=job_name)

             INFO:sagemaker:Creating training-job with name: ufo-linear-learner-job-20240717193437

             2024-07-17 19:34:50 Starting - Starting the training job...
             2024-07-17 19:35:05 Starting - Preparing the instances for training...
             2024-07-17 19:35:30 Downloading - Downloading input data...
             2024-07-17 19:36:04 Downloading - Downloading the training image.........
             2024-07-17 19:37:41 Training - Training image download completed. Training in progress..Docker entrypoint called with argument(s): train
             . . .
             [07/17/2024 19:38:23 INFO 139933004003136] #quality_metric: host=algo-1, validation multiclass_accuracy <score>=0.9396503102086858
             . . .
             [07/17/2024 19:38:23 INFO 139933004003136] Test data is not provided.
             #metrics {"StartTime": 1721245071.1513948, "EndTime": 1721245103.5236073, "Dimensions": {"Algorithm": "Linear Learner", "Host": "algo-1", "Operation": "training"}, "Metrics": {"initialize.time": {"sum": 1321.202039718628, "count": 1, "min": 1321.202039718628, "max": 1321.202039718628}, "epochs": {"sum": 15.0, "count": 1, "min": 15, "max": 15}, "check_early_stopping.time": {"sum": 6.572246551513672, "count": 13, "min": 0.1647472381591797, "max": 0.8738040924072266}, "update.time": {"sum": 27831.69913291931, "count": 12, "min": 2293.8809394836426, "max": 2408.231735229492}, "finalize.time": {"sum": 2153.674840927124, "count": 1, "min": 2153.674840927124, "max": 2153.674840927124}, "setuptime": {"sum": 1.8579959869384766, "count": 1, "min": 1.8579959869384766, "max": 1.8579959869384766}, "totaltime": {"sum": 32466.008186340332, "count": 1, "min": 32466.008186340332, "max": 32466.008186340332}}}


             2024-07-17 19:38:39 Uploading - Uploading generated training model
             2024-07-17 19:38:39 Completed - Training job completed
             Training seconds: 190
             Billable seconds: 190


         >>> print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))
             Here is the location of the trained Linear Learner model: s3://modeling-ufo-lab1/implementation_operations_lab/linearlearner_output/ufo-linear-learner-job-20240717193437/output/model.tar.gz


             Step 4: Deploying the model into SageMaker hosting

             Next, let's deploy the model into SageMaker hosting onto a single m4 instance. We can then use this instance to test the model with the
               test data that we help out at the beginning of the notebook. We can then evaluate things like accuracy, precision, recall, and f1 score.

             We can use some fancy libraries to build out a confusion matrix/heatmap to see how accurate our model is.


         NOTE: linear.deploy() method does
                - spins up the specified type and number the instances (e.g. 1 ml.m4.xlarge instance) that our model is going to be deployed to,
                - creates the endpoint configuration
                - creates an endpoint for us on SageMaker hosting,


         >>> multiclass_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')
             INFO:sagemaker:Creating model with name: linear-learner-2024-07-17-20-51-48-106
             INFO:sagemaker:Creating endpoint-config with name linear-learner-2024-07-17-20-51-48-106
             INFO:sagemaker:Creating endpoint with name linear-learner-2024-07-17-20-51-48-106


            This next code is just setup code to allow us to draw out nice and pretty confusion matrix/heatmap.


         >>> from sklearn.metrics import confusion_matrix
         >>> from sklearn.utils.multiclass import unique_labels
         >>>
         >>> def plot_confusion_matrix(y_true, y_pred, classes,
         >>>                           normalize=False,
         >>>                           title=None,
         >>>                           cmap=None):
         >>>     """
         >>>     This function prints and plots the confusion matrix.
         >>>     Normalization can be applied by setting `normalize=True`.
         >>>     """
         >>>     if not title:
         >>>         if normalize:
         >>>             title = 'Normalized confusion matrix'
         >>>             plt.cm.Greens
         >>>         else:
         >>>             title = 'Confusion matrix, without normalization'
         >>>
         >>>     # Compute confusion matrix
         >>>     cm = confusion_matrix(y_true, y_pred)
         >>>     # Only use the labels that appear in the data
         >>>     classes = classes[unique_labels(y_true, y_pred)]
         >>>     if normalize:
         >>>         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
         >>> #         print("Normalized confusion matrix")
         >>> #     else:
         >>> #         print('Confusion matrix, without normalization')
         >>>
         >>> #     print(cm)
         >>>
         >>>     fig, ax = plt.subplots()
         >>>     im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
         >>>     ax.figure.colorbar(im, ax=ax)
         >>>     # We want to show all ticks...
         >>>     ax.set(xticks=np.arange(cm.shape[1]),
         >>>            yticks=np.arange(cm.shape[0]),
         >>>            # ... and label them with the respective list entries
         >>>            xticklabels=classes, yticklabels=classes,
         >>>            title=title,
         >>>            ylabel='Actual',
         >>>            xlabel='Predicted')
         >>>
         >>>     # Rotate the tick labels and set their alignment.
         >>>     plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         >>>              rotation_mode="anchor")
         >>>
         >>>     # Loop over data dimensions and create text annotations.
         >>>     fmt = '.2f' if normalize else 'd'
         >>>     thresh = cm.max() / 2.
         >>>     for i in range(cm.shape[0]):
         >>>         for j in range(cm.shape[1]):
         >>>             ax.text(j, i, format(cm[i, j], fmt),
         >>>                     ha="center", va="center",
         >>>                     color="white" if cm[i, j] > thresh else "black")
         >>>     fig.tight_layout()
         >>>     return ax
         >>>
         >>>
         >>> np.set_printoptions(precision=2)


         >>> # from sagemaker.predictor import json_deserializer, csv_serializer
         >>>
         >>> # multiclass_predictor.content_type = 'text/csv'
         >>> multiclass_predictor.serializer = sagemaker.serializers.CSVSerializer()
         >>> multiclass_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()
         >>>
         >>> predictions = []
         >>> results = multiclass_predictor.predict(test_X)
         >>> predictions += [r['predicted_label'] for r in results['predictions']]
         >>> predictions = np.array(predictions)


         >>> %matplotlib inline
         >>> sns.set_context("paper", font_scale=1.4)
         >>>
         >>> y_test = test_y
         >>> y_pred = predictions
         >>>
         >>> class_names = np.array(['Unexplained', 'Explained', 'Probable'])
         >>>
         >>> # Plot non-normalized confusion matrix
         >>> plot_confusion_matrix(y_test, y_pred, classes=class_names,
         >>>                       title='Confusion matrix',
         >>>                       cmap=plt.cm.Blues)
         >>> plt.grid(False)
         >>> plt.show()


         >>> from sklearn.metrics import precision_recall_fscore_support
         >>> from sklearn.metrics import accuracy_score
         >>>
         >>> y_test = data_test['researchOutcome']
         >>> y_pred = predictions
         >>> scores = precision_recall_fscore_support(y_test, y_pred, average='macro', labels=np.unique(y_pred))
         >>> acc = accuracy_score(y_test, y_pred)
         >>> print('Accuracy is: {}'.format(acc))
         >>> print('Precision is: {}'.format(scores[0]))
         >>> print('Recall is: {}'.format(scores[1]))
         >>> print('F1 score is: {}'.format(scores[2]))
             Accuracy is: 0.9499165275459098
             Precision is: 0.9127438555359534
             Recall is: 0.9365168251900394
             F1 score is: 0.9241429254888969

   ------------------------------------------------------
   lambda function

   AWS Console -> Lambda Function -> Create Function -> select "author from scratch",
      Function Name: invoke-sagemaker-endpoint, Runtime: python: 3.8;
      select: "Create a new role with basic Lambda permissions"
      -> Create function

   role created: invoke-sagemaker-endpoint-role-7t49csfu

   AWS Console -> IAM -> Role -> select "invoke-sagemaker-endpoint-role-7t49csfu",
      -> Add permissions -> Attach Policy -> AmasonSageMakerFullAccess -> Add permissions


   insert "lambda_function.py" code

   -> Configuration <tab> -> Environment Variables <left tab> -> Edit -> Add environment variable ->
     Key: ENDPOINT_NAME  value: linear-learner-2024-07-17-20-51-48-106
     -> Save
     -> Deploy
     -> Test <center middle> -> Request Body:
         {
          "data": "45.0, 10.0,38.5816667,-121.49333329999999,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0"
        }
        -> Test

          / - POST method test results
          Request /
          Status 200
          Response body "Unexplained"

     -> Test <center middle> -> Request Body:
         {
          "data": "45.0,1.0,36.5816667,-121.49333329999999,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0
        }
        -> Test

          / - POST method test results
          Request /
          Status 200
          Response body "Explained"

          -> Deploy -> Deployment Staget: [New Stage], Stage name: production  -> Deploy

   --------------l----------------------------------------
   AWS console -> API Gateway -> Create API Gateway -> REST API -> Build
      "New API", API Name: ufo-inference-api, API endpoint type: Regional -> Create API

       Resources <left tab> -> Methods -> Create method -> Method Type: POST, Integration Type: Lambda,
         Lambda Function: *invoke-sagemaker-endpoint ->  create method

         Returns a Invoke URL


   ------------------------------------------------------


------------------------------------------------------
9.10 Demo: Deconstruction
   -> Clean-up UFO related labs


   AWS console -> API Gateway ->  select "ufo-inference-api -> Delete

   AWS console -> SageMaker -> Inference -> Endpoints ->   select "linear-learner*" -> Action -> Delete
                                         -> Endpoint Configuration -> select "linear-learner*" -> Action Delete

   AWS console -> SageMaker -> Notebooks -> select "my-notebook-inst -> Action -> Stop
                                                                     -> Action -> Delete

   AWS Console -> S3 -> select bucket -> Empty
                     -> select bucket -> Delete

   AWS Console -> Lambda -> select function: invoke-sagemaker-endpoint -> Action -> Delete

------------------------------------------------------
9.12 Quiz AWS Certified Machine Learning - Specialist 2020 - Implementation and Operations

Question 2

  To make use of your published model in a custom application, what must you do?

Choices:

  - Use a Lambda function to perform the inferences for your application.

  - Create an entry in Route 53 to point your desired DNS name to the endpoint.

  - Instruct SageMaker to generate a unique endpoint URL for your application.

  - Use the CloudTrail API to monitor for inference requests and trigger the SageMaker model endpoint.

  - Use the SageMaker API InvokeEndpoint() method via SDK.       -> correct answere
Sorry!
Correct Answer

To use your endpoint for inferences, you can use the SageMaker API InvokeEndpoint() method with the automatically generated HTTPS URL specified.


Question 3

  You need to increase the performance of your Image Classification inference endpoint and want to do so in the most
  cost-effective manner. What should you choose?

choices:
  - Create an additional production variant that is the same as the original variant, and direct 50% of the traffic to that variant.

  - Create a new production variant that uses a multi-GPU instance.

  - Deploy your endpoint on Amazon EC2 Inf1 instances powered by AWS Inferentia accelerators.   -> Correct answer

  - Offload some traffic to a less costly AWS Region.

  - Create a new endpoint deployment that uses a single-CPU instance given the algorithm being used.
Sorry!

  Opting for a single-CPU instance for image classification, especially if the algorithm is resource-intensive, can often result
  in suboptimal performance. CPUs are generally not as efficient as GPUs for parallel processing tasks inherent in deep learning.
  This choice may slow down the inference and fail to harness the full potential of the algorithm, leading to bottlenecks.

Correct Answer

  Amazon EC2 Inf1 instances are designed to be powered by AWS Inferentia accelerators, which optimize deep learning (DL) inference
  applications. These accelerators provide a significant boost in performance, delivering up to 2.3x higher throughput for inference
  tasks and offer up to 70% lower cost per inference compared to other Amazon EC2 instances. In the recent past, we would have to
  redeploy the endpoint using Elastic Inference added to the production variant, but Elastic Inference (EI) is no longer recommended
  by AWS and is not available anymore for new customers.


Question 7

  You want to deploy an XGBoost-backed model to a fleet of traffic sensors using Raspberry Pis as the local compute component.
  Will this work?

choices:
  - No, XGBoost cannot be compiled to run on an ARM processor. It can only run on x86 architectures.

  - No, best practice says that you should not deploy ML models into the field but rather use a centralized inference landscape.

  - Yes, you can use SageMaker Neo to compile the model into a format that is optimized for the ARM processor on the Raspberry Pi.  <- correct answer

  - No, a Raspberry Pi is not powerful enough to run an ML model using XGBoost.

  - Yes, you can deploy the model using Amazon Robomaker using the native ARM support.
Good work!

  SageMaker Neo provides a way to compile XGBoost models, which are optimized for the ARM processor in the Raspberry Pi.


Question 9

  You are helping a digital asset media company create a system which can automatically extract metadata from photographs submitted
  by freelance photographers. They want a solution that is highly scalable and only charges for the photographs you analyze and the
  metadata you store. What would you recommend?

Choices:
  - Make use of Amazon Comprehend to extract metadata from the images.

  - Build a model using the Semantic Segmentation algorithm and host it using SageMaker Hosting Services.

  - Build a model using Image Analysis to extract metadata from images and host it using Lambda and the API Gateway.

  - Build a model using Object Detection to extract metadata from images and host it using EC2.

  - Make use of Amazon Rekognition for metadata extraction.      <- correct answer
Good work!

  Amazon Rekognition is a highly scalable service that makes it easy to add powerful visual analysis to your applications.
  With Rekognition Image, you only pay for the images you analyze and the face metadata you store.


Question 11

  Your company has just discovered a security breach occurred in a division separate from yours but has ordered a full review of all
  access logs. You have been asked to provide the last 180 days of access to the three SageMaker Hosted Service models that you manage.
  When you set up these deployments, you left everything default. How will you be able to respond?

Choices:
  - Use CloudWatch along with IPInsights to analyse the logs for suspicious activity from the past 180 days then download these records.

  - Use CloudWatch to pull a list of all access records for the ML models. Make use of a Python library to parse out only the access records.

  - Use CloudTrail to pull a list of all access to the ML models for the last 180 days.

  - Use CloudTrail to pull a list of all access to the models for the last 90 days. Any data beyond 90 days is unavailable. <- correct answer

  - Use SageMaker Detailed Logging to produce a CSV file of access from the past 180 days.
Good work!

  CloudTrail is the proper service if you want to see who has sent API calls to your SageMaker Hosted model but, by default, it will
  only store the last 90 days of events. You can configure CloudTrail to store an unlimited amount of logs on S3 but this is not turned
  on by default. Whilst CloudTrail is not necessarily an Access Log, it performs the same auditing functions you might expect; and an
  auditor may not necessarily be familiar with the nuances of AWS

Question 2 (retry 1)

  You are preparing to release an updated version of your latest machine learning model. It is provided to about 3,000 customers
  who use it in a SaaS capacity. You want to minimize customer disruption, minimize risk, and be sure the new model is stable before
  full deployment. What is the best course of action?

Choices:
  Use a canary deployment to evaluate the new model, then use a phased rollout.                                <--- Correct Answer

  Conduct an A/B test to evaluate the new model and then distribute the new production URL to your customers.

  Perform offline validation, then release the new version all at once to minimize risk.

  Use a continuous integration process to preserve the stability of the new model and deploy in a "Big Bang" manner.
Good work!

  Of the options provided, using a canary deployment to first evaluate the stability of the update with a small group, then using
  a phased rollout, seems to fulfill the objectives. A phased rollout could be done as an extension of the deployment until 100% of
  the traffic is moved to the new version.

------------------------------------------------------

Chapter 10 Wrap-Up

------------------------------------------------------
10.1 Wrap-Up
------------------------------------------------------
10.2 AWS Certified Machine Learning - Specialist 2020 - Practice Exam
------------------------------------------------------
Other Resources that I found:


  Resources (I found):

    KD nuggets: Understanding Bias-Variance Trade-Off in 3 Minutes
      https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html

      Bias
        - the simplifying assumptions made by the model to make the target function easier to approximate.
      Variance
        - the amount that the estimate of the target function will change, given different training data.

      Errors due to Bias (results in underfitting)
        - the distance between the predictions of a model and the true values.
        - In this type of error, the model pays little attention to training data and oversimplifies the model and
          doesn't learn the patterns. The model learns the wrong relations by not taking in account all the features
      Errors due to Variance (result ins overfitting)
       - Variability of model prediction for a given data point or a value that tells us the spread of our data.
       - In this type of error, the model pays an lot of attention in training data, to the point to memorize it instead
         of learning from it.
       - A model with a high error of variance is not flexible to generalize on the data which it hasnt seen before.

  Amazon SageMaker Example Notebooks
    https://sagemaker-examples.readthedocs.io/en/latest/

  Atlassian: Essential chart types for data visualization
    https://www.atlassian.com/data/charts/essential-chart-types-for-data-visualization


    video resources for Machine Learning Algorithms, Sagemaker, etc, by Prof Ryan Ahmed
       K Nearest Neighbors Algorithm (KNN)
          https://www.youtube.com/watch?v=v5CcxPiYSlA

       Overview of AWS SageMaker Built-in Algorithms
         https://www.youtube.com/watch?v=79y4WtA-zqA




  If you like this video, check out this full course on Udemy:
    https://www.udemy.com/course/become-a...


   Available SageMaker Algorithms:
    - BlazingText Word2Vec: BlazingText implementation of the Word2Vec algorithm for scaling and accelerating the generation of word embeddings from a large number of documents.

    - DeepAR: An algorithm that generates accurate forecasts by learning patterns from many related time-series using recurrent neural networks (RNN).

    - Factorization Machines: A model with the ability to estimate all of the interactions between features even with a very small amount of data.

    - Gradient Boosted Trees (XGBoost): Short for Extreme Gradient Boosting, XGBoost is an optimized distributed gradient boosting library.

    - Image Classification (ResNet): A popular neural network for developing image classification systems.

    - IP Insights: An algorithm to detect malicious users or learn to usage patterns of IP addresses.

    - K-Means Clustering: One of the simplest ML algorithms. Its used to find groups within unlabeled data.

    - K-Nearest Neighbor (k-NN): An index based algorithm to address classification and regression based problems.

    - Latent Dirichlet Allocation (LDA): A model that is well suited to automatically discovering the main topics present in a set of text files.

    - Linear Learner (Classification): Linear classification uses an objects characteristics to identify the appropriate group that it belongs to.

    - Linear Learner (Regression): Linear regression is used to predict the linear relationship between two variables.

    - Neural Topic Modelling (NTM): A neural network based approach for learning topics from text and image datasets.

    - Object2Vec: A neural-embedding algorithm to compute nearest neighbors and to visualize natural clusters.

    - Object Detection: Detects, classifies, and places bounding boxes around multiple objects in an image.

    - Principal Component Analysis (PCA): Often used in data pre-processing, this algorithm takes a table or matrix of many features and reduces it to a smaller number of representative features.

    - Random Cut Forest: An unsupervised machine learning algorithm for anomaly detection.

    - Semantic Segmentation: Partitions an image to identify places of interest by assigning a label to the individual pixels of the image.

    - Seqence2Sequence: A general-purpose encoder-decoder for text that is often used for machine translation, text summarization, etc.
------------------------------------------------------

  For Cloud Job info:
      Check out Kesha Williams pluralSight blogs
      https://www.pluralsight.com/resources/blog
         https://www.pluralsight.com/resources/blog?q=kesha+williams&unified-tags=cloud

         How to get a cloud Engineer job talk:
           https://www.youtube.com/live/p6fsxpQiYJA

------------------------------------------------------
10.2 AWS Certified Machine Learning - Specialty - Practice Exam


Question 13

  You are working for a hot new startup that calculates different metrics about their customers depending on how much
  money they spend on a weekly, quarterly, and yearly basis. These metrics are classified as elite, novice, and beginner.
  Depending on their ranking they get more/less discounts and placed in higher/lower priority for customer support. The
  algorithm you have chosen expects all numerical inputs. What can be done to handle these classification values so that
  ordering is taken into account for the required numerical inputs?

Choices:
  Apply random numbers to each classification value and apply gradient descent until the values converge to expect results

  Use one-hot encoding techniques to map values for each classification dropping the original classification feature

  Experiment with mapping different values for each status and see which works best               <--- Correct Answer

  Use one-hot encoding techniques to map values for each classification

Answer Info
  Since these classification values are ordinal (order does matter) we cannot use one-hot encoding techniques. We either
  need to map these values to a scale, or we train our model with different encodings and seeing which encoding works best.
  Ordinal data - Wikipedia



Question 21
  You are a machine learning specialist evaluating a current model that has been deployed into production. It has been
  deployed for a few weeks now and the results are not accurate and sometimes the inference data is missing values. What
  are some techniques you can review to help solve this problem?

Choose 3:
  Ensure the target variable used as the predictor during training represents the actual outcome that the machine learning
  model is trying to predict.                                                   <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are the
    extraction methods used to generate these datasets the same as for the production data? How similar is the sample data
    to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or treatments
    the same? Managing Machine Learning Projects

  Ensure the inference data has placeholder values for any of the missing values   <--- Incorrect Answer

    Putting in random placeholders would negatively impact the accuracy of your inferences.

  Ensure the training datasets are large, representative samples of the populations that the model needs to make
  predictions.                                                                  <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are
    the extraction methods used to generate these datasets the same as for the production data? How similar is the sample
    data to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or
    treatments the same? Managing Machine Learning Projects

  Ensure the inference data is exactly the same for the training and testing data.

  Ensure the extraction methods used to generate the training datasets are the same as for the production
  inference data.                                                                <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are
    the extraction methods used to generate these datasets the same as for the production data? How similar is the sample
    data to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or
    treatments the same? Managing Machine Learning Projects



Question 31

  You work for a manufacturing company who has hundreds of conveyor belts with built-in IoT sensors. These sensors stream
  data into AWS using Amazon Kinesis Data Streams. The features associated with the data is belt_id, building_number,
  belt_temp, outside_temp, and power_consumption. During the processing of the data, you need to transform the data and
  store it in a data store. Which combination of services can you use to achieve this?

Choose 3:
  Set up Amazon Kinesis Data Firehose to ingest data from Amazon Kinesis Data Streams, and then send data to AWS Lambda.
  Transform the data in AWS Lambda and write the transformed data into Amazon S3.                 <--- Correct Answer

    Amazon Kinesis Data Firehose can be used as a destination for Amazon Kinesis Data Streams. You can also use an AWS
    Lambda function to prepare and transform incoming data in your delivery stream before loading it to destinations,
    such as Amazon S3.

    AWS Documentation: Amazon Kinesis Data Firehose FAQs > Data Transformation and Format Conversion

  Use Amazon Kinesis Data Streams to immediately write your data into Amazon S3. Next, set up an AWS Lambda function that
  fires whenever an object is PUT into Amazon S3. Transform the data from the AWS Lambda function, and then write the
  transformed data into Amazon S3.

  Capture, query, and analyze your streaming data using Amazon Kinesis Data Analytics. Run real-time SQL queries to
  transform and load your data into Amazon S3.                                                      <--- Incorrect Answer

    Amazon Kinesis Data Analytics can transform and deliver data to Amazon S3. However, Amazon Kinesis Data Streams should
    be used as a data source to capture streaming data. This solution would be valid if Amazon Kinesis Data Streams was
    described as a data source for Amazon Kinesis Data Analytics.

    Note: Misleading answer since question states Kinesis Data Streams is being used

  Use Amazon Kinesis Data Streams as a source for Amazon Kinesis Data Analytics. Run real-time SQL queries on the data
  to transform it. Ingest the transformed data into Amazon Kinesis Data Firehose and load it into Amazon S3.  <--- Correct Answer

    You can create an Amazon Kinesis Data Analytics application that leverages Amazon Kinesis Data Streams as a source
    and Amazon Kinesis Data Firehose as a delivery stream into an Amazon S3 bucket.

    AWS Documentation: Example: Writing to Kinesis Data Firehose

  Immediately send the data to AWS Lambda from Amazon Kinesis Data Streams. Transform the data in AWS Lambda and write
  the transformed data into Amazon S3.                                                                <--- Correct Answer

    Amazon Kinesis Data Streams can be quickly paired with AWS Lambda to respond to or adjust immediate occurrences
    within event-driven applications in your environment, at any scale. You can also use AWS Lambda to transform and
    write data into Amazon S3.

    AWS Documentation: Using AWS Lambda with Amazon Kinesis


Question 8

  What needs to be done to the following phrase before using it in your machine learning process?
  The quk BROWN FOX jumped over the lazy dog.

Choose 3:
  Replace each word with a respective n-gram vector

  One-hot encode values

  Remove stop words                                                                                     <-- Correct Answer

  Replace each word with a respective tf-idf vector

  Fix the "quk" to "quick"

    We should leave words that are misspelled in place as these need to be part of the analysis.

  Create tokens from each value                                                                          <-- Correct Answer

  Lowercase transformation                                                                               <-- Correct Answer


Answers Info:
  Before we use corpus data in some Machine Learning processes like language translation, sentiment analysis, or spam
  filtering it is important we properly apply text processing to the data. Some of the important text processing that
  needs to be done is tokenization.
  This includes - removing stop words  frequent words such as the, is, etc. that do not have specific meaning.
  This includes - lowercase transformation.



Question 24

  Your organization has the need to set up a petabyte scaled BI and dashboard analysis tool that will query millions
  of rows of data spread across thousands of files stored in S3. Your organization wants to save as much money as possible.
  Which solution will allows developers to run dozens if not hundreds or thousands of queries per day, and possibly scanning
  many TBs of data each, while still being cost effective?

Choice:
  EC2 Spot instances and Presto                                                    <--- Correct Answer

    You pay a constant fee for the compute instances you are running (EC2 instances cost). The more machines you run
    and the bigger they are - the higher the fee, yes. But Presto is very efficient and if your data is correctly stored,
    a few commodity machines will do a great job if you are running your Presto cluster on the same region as your S3
    bucket, and within one AZ, as there is no network or data transfer costs at all. The compute costs can be further
    optimized by using spot instances for worker nodes, and completely shutting them down off-hours (where applicable).
    Presto can deal with a lost worker node - which might slow down some queries but spot instances come at a great
    discount. Presto - Amazon EMR Presto | Distributed SQL Query Engine for Big Data

  AWS Glue Data Catalog and Amazon Athena                                            <--- Incorrect Answer

    While AWS Glue Data Catalog and Amazon Athena are capable of meeting this use case, consider that the cost of
    data processing with Athena is a set price per terabyte. When dealing with full petabyte-scale data warehouses,
    this may cost far more than building your own solution using EC2 Spot Instances

  Data Pipeline and RDS

  Lambda Functions with extremely long timeouts


Notes:
      Elastic Map Reduced (EMR)
        . . .
        Apache Spark:
          - Spark is an open-source, distributed computing system that provides a unified analytics engine for big data processing.
          - It offers support for various programming languages, including Java, Scala, Python, and R
          - a multi-language engine processing framework and programming model for executing data engineering, data science, and
            machine learning on single-node machines or clusters.
        Apache Presto
          - SQL Query Engine designed for interactive analytics at scale
          - An open source, distributed SQL query engine optimized for low-latency, ad-hoc analysis of data.
          - supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions.
          - can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.

        Apache Spark vs Presto
          Spark
            - Spark is known for its exceptional performance and scalability, making it ideal for big data processing and analytics.
            - It supports batch processing, real-time stream processing, as well as machine learning and graph processing
          Presto
            - shines in interactive analytics and ad-hoc queries, allowing users to explore and analyze data in real-time.
            - It also offers federated querying capabilities, enabling users to query data from multiple sources seamlessly.




Question 30

  You are preparing plain text corpus data to use in a NLP process. Which of the following is/are one of the important step(s)
  to pre-process the text in NLP based projects?

Choose 3:
  One-hot encode ordinal n-gram values

  Stemming                                                                <--- Correct Answer

  Word standardization                                                    <--- Correct Answer

  Add random text noise

  Congregate all of the plain text corpus data into a single document     <--- Incorrect Answer

    The data would be processed identically whether the data is in a single document or several

  Stop word removal                                                       <--- Correct Answer


Answer Info:
  Stemming is a rudimentary rule-based process of stripping the suffixes (ing, ly, es, s etc) from a word.

  Stop words are those words which will have no relevance to the context of the data for example is/am/are.

  Object Standardization is also one of the good ways to pre-process the text by removing things like acronyms, hashtags
  with attached words, and colloquial slang that typically are not recognized by search engines and models.

Notes:
  Natural language processing - Wikipedia  https://en.wikipedia.org/wiki/Natural_language_processing

  Stemming
    The process of reducing inflected (or sometimes derived) words to a base form (e.g., "close" will be the root for
    "closed", "closing", "close", "closer" etc.). Stemming yields similar results as lemmatization, but does so on grounds
    of rules, not a dictionary.

  Lemmatization
    The task of removing inflectional endings only and to return the base dictionary form of a word which is also known
    as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the
    transformation actually uses a dictionary to map words to their actual form



Question 53

  You have been tasked with creating a labeled dataset by classifying text data into different categories depending on the
  summary of the corpus. You plan to use this data with a particular machine learning algorithm within AWS. Your goal is
  to make this as streamlined as possible with minimal amount of setup from you and your team. What tool can be used to help
  label your dataset with the minimum amount of setup?

Choices:
  AWS SageMaker GroundTruth text classification job                   <--- Correct Answer

    You can use SageMaker Ground Truth to create ground truth datasets by creating labeling jobs. When you create a text
    classification job, workers group text into the categories that you define. You can define multiple categories but the worker
    can apply only one category to the text. Use the instructions to guide your workers to make the correct choice. Always
    define a generic class in addition to your specific classes. Giving your workers a generic option helps to minimize
    inaccurately classified text. Amazon SageMaker Ground Truth - Amazon SageMaker

  Marketplace AMI for NLP problems

  AWS Comprehend entity detection

  Amazon Neural Topic Modeling (NTM) built-in algorithm

  AWS Comprehend sentiment analysis

  Amazon Latent Dirichlet Allocation (LDA) algorithm                   <--- Incorrect Answer


Question 36

  When evaluating a model after the training and testing process, you notice that the error rate during training is high but
  the error rate during testing is low. Which of the following could be the reason for obtaining these error rates?

Choose 2:
  You need to re-evaluate the selection of your algorithm.               <--- Incorrect Answer
                                                                              (note: should also be a correct answer)
  Your model is underfitting the testing data.

  Your model is overfitting the training data.

  You have a programmatic issue with your algorithm.                      <--- Correct Answer

  You should train for a longer period of time.

  You have a data issue with both your training and testing datasets.     <--- Correct Answer

Answer Info:
  When training error is high and testing error is low, this is highly unusual as it infers that the model is somehow
  predicting better than the data which was used to train the model. This is usually an indicator of a data issue or some
  systemic problem in the algorithm. Overfitting - Wikipedia


Question 40

  You are trying to classify a number of items based on different features into one of 6 groups (books, electronics, movies, etc.)
  based on features. Which algorithm would be best suited for this type of problem?

Chooses:
  Use K-Means algorithm with k set to the number of classes                 <--- Incorrect Answer

    While K-Means is a clustering algorithm that can be used to group data, it's not typically used for multiclass classification.
    It is an unsupervised learning algorithm, primarily used when we do not have any labeled data.

  Use Linear Learner with predictor set to regressor

  Use a stochastic approach when choosing target parameters and recommended ranges

  Use regression forest with the number of trees set to the number of categories

  Use XGBoost with objective set to multi:softmax                          <--- Correct Answer

    The XGBoost algorithm with the objective set to 'multi:softmax' is a good choice for this multiclass classification problem.
    XGBoost is an implementation of gradient boosted decision trees. The 'multi:softmax' option allows it to handle multiple classes.

Note:

   Question should have indicated a labeled training set.


Question 54

  Assuming equal importance of all misclassification types and a need to evaluate the model performance across all possible
  classification thresholds, what is a good target metric to use when comparing different binary classification models?

Chooses:
  F1 score                                              <--- Incorrect Answer

  Area under the curve (AUC)                            <--- Correct Answer


  Mean squared root error (MSRE)

  Recall

Answer Info:

  The F1 score is a weighted average of precision and recall, hence it is more useful than accuracy, especially if you have an
  uneven class distribution. However, it does not consider all possible classification thresholds, as it is calculated from a
  confusion matrix at a specific threshold, usually 0.5. It is not as comprehensive as AUC for this task.

  AUC-ROC (Area Under the Receiver Operating Characteristic curve) is a performance measurement for classification problem
  at various thresholds settings. It tells how much model is capable of distinguishing between classes, taking into account
  both the true positive rate and the false positive rate, and is not sensitive to imbalanced classes. It is suitable when
  equal importance is given to all misclassification types and you need to evaluate the model performance across all possible
  classification thresholds.


Question 1

  After several weeks of working on a model for genome mapping, you believe you have perfected it and now want to deploy it to a
  platform that will provide the highest performance. Which of the following AWS platforms will provide the highest
  cost-to-performance for this compute-intensive model?

Choose 2:
  EC2 F1 instance

  EC2 G3 Instance                    <--- Correct Answer

  EC2 X1 Instance                    <--- Incorrect Answer

  EC2 P2 Instance                    <--- Correct Answer

  EC2 M5 Instance

Answer Info:

  EC2 G3 instances are designed for high-performance computing (HPC) and machine learning (ML) workloads. They feature
  powerful NVIDIA GPUs and Intel CPUs, making them ideal for genome mapping workloads. EC2 G3 instances are relatively
  inexpensive, making them a good choice for cost-sensitive projects.

  X1 instances are optimized for large-scale, enterprise-class and in-memory applications, and offer one of the lowest price
  per GiB of RAM among Amazon EC2 instance types. Use cases: In-memory databases (e.g. SAP HANA), big data processing engines
  (e.g. Apache Spark or Presto), high performance computing (HPC). Certified by SAP to run Business Warehouse on HANA (BW),
  Data Mart Solutions on HANA, Business Suite on HANA (SoH), Business Suite S/4HANA.

  P2 instances are intended for general-purpose GPU compute applications. Use cases: Machine learning, high performance
  databases, computational fluid dynamics, computational finance, seismic analysis, molecular modeling, genomics, rendering,
  and other server-side GPU compute workloads. Amazon EC2 Instance Types.

  P5, P4, P3, & P2
     -> GPU based instances with:
       P2: P2 instances are intended for general-purpose GPU compute applications - High-performance NVIDIA K80 GPUs.
       P3: Intel Xeon Scalable Processor  high performance compute in the cloud with up to 8 NVIDIA V100 Tensor Core GPUs
       P4: 3.0 GHz 2nd Generation Intel Xeon Scalable processors (Cascade Lake P-8275CL) and Up to 8 NVIDIA A100 Tensor Core GPU
       P5: 3rd Gen AMD EPYC processors (AMD EPYC 7R13), and Up to 8 NVIDIA H100 (in P5) or H200 (in P5e) Tensor Core GPUs


Notes:
    G* series: some use cases state ML workloads and inferencing, but some like G3 state Graphic workloads3

    X1 pick was due to confusing Inf1/Inf2 with X1

    M5 General Purpose Instances powered by Intel Xeon Platinum 8175M or 8259CL processors. These instances provide a balance
       of compute, memory, and network resources, and is a good choice for many applications.


Question 48

  You have been tasked with using Polly to translate text to speech in the company announcements that launch weekly. The
  problem you are encountering is how Polly is incorrectly translating the companies acronyms. What can be done for future
  tasks to help prevent this?

Choose 2:
  Use speech marks for input text documents                                                <--- Incorrect Answer

  Use Amazon Comprehend to pull parts of speech and use to help pronounce acronyms

  Create dictionary lexicon                                                                <--- Correct Answer

  Use Amazon Transcribe to first map the acronyms to pronunciations then include them in the Amazon polly pipeline

  Use SSML tags in documents                                                               <--- Correct Answer

Answer Info:
  Using SSML-enhanced input text gives you additional control over how Amazon Polly generates speech from the text you provide.
  Using these tags allows you to substitute a different word (or pronunciation) for selected text such as an acronym or abbreviation.

  You can also create a dictionary lexicon to apply to any future tasks instead of apply SSML to each individual document.
  Amazon Polly


Notes:

 Amazon Polly
   - Amazon Polly is a cloud service that converts text into lifelike speech.
   Speech Synthesis Markup Language (SSML)
     - You can use Polly to generate speech from either plain text or from documents marked up with SSML.
     - Using SSML-enhanced text gives you additional control over how Amazon Polly generates speech from the text you provide.
     - The <speak> tag is the root element of all Amazon Polly SSML text. All SSML-enhanced text must be enclosed within a
        pair of <speak> tags.
         <speak>Mary had a little lamb.</speak>
     - With SSML tags, you can include a long pause within your text, or change the speech rate or pitch. Other options
       include: emphasizing specific words or phrases; using phonetic pronunciation; including breathing sounds; whispering;
       use a different language; etc.
   Speech Marks
     - Speech marks are metadata that describe the speech that you synthesize, such as where a sentence or word starts and
       ends in the audio stream.
     - When you request speech marks for your text, Amazon Polly returns this metadata instead of synthesized speech.
     - By using speech marks in conjunction with the synthesized speech audio stream, you can provide your applications with
       an enhanced visual experience.
     - For example, combining the metadata with the audio stream from your text can enable you to synchronize speech with
       facial animation (lip-syncing) or to highlight written words as they're spoken.
   Pronounciation Lexicons:
     - Pronunciation lexicons enable you to customize the pronunciation of words.
     - Amazon Polly provides API operations that you can use to store lexicons in an AWS region.
     - Those lexicons are then specific to that particular region. You can use one or more of the lexicons from that region
       when synthesizing the text by using the SynthesizeSpeech operation.
     - lexicon examples:
       - Your text might include an acronym, such as W3C. You can use a lexicon to define an alias for the word W3C so that
         it is read in the full, expanded form (World Wide Web Consortium).
       - Lexicons give you additional control over how Amazon Polly pronounces words uncommon to the selected language
       - Common words are sometimes stylized with numbers taking the place of letters, as with "g3t sm4rt" (get smart). In this
         example, you can specify an alias (get smart) for the word "g3t sm4rt" in the lexicon.


Question 11

  You have been tasked with transforming highly sensitive data using AWS Glue. Which of the following AWS Glue settings
  allowing you to control encryption for your transformation process?

Choose 3:
  Encryption of your Data Catalog at its components using symmetric keys                          <--- Correct Answer

  The security configurations that you create (S3 encryption, CloudWatch logs encryption, and Job
  bookmark encryption)                                                                             <--- Correct Answer

  Encryption of the classifier used during the transformation job

  The server-side encryption setting (SSE-S3 or SSE-KMS) that is passed as a parameter to your
  AWS Glue ETL job.                                                                                 <--- Correct Answer

  Encryption of your Data Catalog at its components using asymmetric keys

  Encrypting the managed EBS volumes used to run Apache Spark environment running PySpark code


Answer Info:

  You can encrypt metadata objects in your AWS Glue Data Catalog in addition to the data written to S3 and CloudWatch
  Logs by jobs, crawlers, and development endpoints.  You can enable encryption of the entire Data Catalog in your account.

  When you create jobs, crawlers, and development endpoints in AWS Glue, you can provide encryption settings, such as a
  'security configuration', to configure encryption for that process. With AWS Glue, you can encrypt data using keys that
  you manage with AWS Key Management Service (AWS KMS).

  With encryption enabled, when you add Data Catalog objects, run crawlers, run jobs, or start development endpoints,
  AWS KMS keys are used to write data at rest.

  You can also configure AWS Glue to only access Java Database Connectivity (JDBC) data stores through a trusted Secure
  Sockets Layer (SSL) protocol. Encrypting Glue Resources - AWS Glue Encrypting Data Written by Crawlers, Jobs, and
  Development Endpoints - AWS Glue


Note:
  Key Type:
     Symmetric:  A single key used for encrypting and decrypting data or generating and verifying HMAC codes.
                 - only users with valid AWS KMS credentials can encrypt and decrypt using synmetric key
                 - for course, focus is on symmetic because it is covered in exam
                 - symmetric can also be used for generationg HASH based message authentication code (HMAC code).
                   Used when you want to determine authenticity of a message, e.g. JWT Token, JSON Web Token,
                   Tokenized credit card info
     Asymmetric: A public and private key pair used for encrypting and decrypting data or signing and verifying messages.
                 - if encryption requirement is outside KMS, you need to use asymmetric keys
                 - public key used to encrypt data, and private key is used to decrypt data


     AWS KMS with S3:
       - When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key.
       - Amazon S3 supports only symmetric encryption KMS keys.
     AWS KMS with AWS Glue:
       - AWS Glue supports only symmetric customer managed keys.
       - The KMS key list displays only symmetric keys. However, if you select Choose a KMS key ARN, the console lets
         you enter an ARN for any key type. Ensure that you enter only ARNs for symmetric keys.


  AWS Glue Security Configuration:
    - A security configuration in AWS Glue contains the properties that are needed when you write encrypted data.
    - You create security configurations on the AWS Glue console to provide the encryption properties that are used by
      crawlers, jobs, and development endpoints.
    Encryption settings
      Enable S3 encryption
        - Enable at-rest encryption for data stored on S3.
      Enable CloudWatch logs encryption
        - Enable at-rest encryption when writing logs to Amazon CloudWatch.
      Enable job bookmark encryption
        - Enable at-rest encryption of job bookmark.
      Enable data quality encryption
        - Enable at-rest encryption of data quality.


Question 33

  You are preparing plain text corpus data to build a model for Amazon's Neural Topic Model (NTM) algorithm. What are
  the steps you need to take before the data is ready for training?

Chooses:
  First normalize the corpus data. Then, count the occurrence of each of the value produced, creating word
  count vectors. Use these vectors as training data.

  First tokenize the corpus data. Then, count the occurrence of each token and form bag-of-words vectors.
  Use these vectors as training data.                                                                     <--- Correct Answer

  First perform tf-idf to remove words that are not important. Use the number of unique n-grams to create
  vectors and respective word counts. Use these vectors as training data.

  First create bigrams of the corpus data. Then, count the occurrence of each bigram produced, creating
  word count vectors. Use these vectors as training data.

Answer Info:

  Both in training and inference, need to be vectors of integers representing word counts. This is so-called bag-of-words
  (BOW) representation. To convert plain text to BOW, we need to first tokenize our documents, that is, identify words
  and assign an integer ID to each of them. Then, we count the occurrence of each of the tokens in each document and
  form BOW vectors.

  Introduction to the Amazon SageMaker Neural Topic Model | AWS Machine Learning Blog
    https://aws.amazon.com/blogs/machine-learning/introduction-to-the-amazon-sagemaker-neural-topic-model/

Notes:
  NTM input - From plain text to bag-of-words (BOW)
    - The input documents to the NTM algorithm, both in training and inference, need to be vectors of integers representing
      word counts. This is so-called bag-of-words (BOW) representation.
    - To convert plain text to BOW, we need to first tokenize our documents, that is, identify words and assign an
      integer ID to each of them. Then, we count the occurrence of each of the tokens in each document and form BOW vectors.
    - We will only keep the most frequent 2,000 tokens (words) because rarely used words have a much smaller impact on the
      model and thus can be ignored.

Notes:
 Bag-of-words model
   - The bag-of-words model (BoW) is a model of text which uses a representation of text that is based on an unordered collection
   (a "bag") of words.
   - It is used in natural language processing and information retrieval (IR).
   - It disregards word order (and thus most of syntax or grammar) but captures multiplicity.


Question 35

  You are a data scientist working on a model that predicts fraudulent and non-fraudulent transactions. You notice
  that 90% of the samples are non-fraudulent, which makes up the majority of the dataset. What are some methods you
  can use to address this issue in the data?

Choose 3:
  Drop all fraudulent transaction before training the model.

  Use K-means cluster to find outliers for non-fraudulent transactions, and use those as fraudulent samples.

  Apply SMOTE to the dataset to oversample the minority class.                             <--- Correct Answer

  Collect more data samples to increase the accuracy of the dataset.                       <--- Correct Answer


  Apply Principal Component Analysis (PCA) to under-sample non-fraudulent transactions.

  Combine the dataset with a public dataset that has a majority of fraudulent transactions.

  Resample the dataset to correct imbalances of each transaction type.                     <--- Correct Answer

Answer Info:

  You can use techniques like SMOTE (Synthetic Minority Over-Sample Technique) to create more samples of the fraudulent transactions.

  Since there is not enough data present in the current dataset to be sampled effectively with sufficient fraudulent
  transactions for the model to use, you may opt to collect additional data for the dataset, so your sampling can contain
  a greater ratio of fraudulent transactions.

  There are many different ways to handle imbalanced data. What is important is that you have a keen sense for what damage
  an imbalanced dataset can cause and how to balance it. Reference Documentation: Handling Imbalanced Datasets - a review

Notes:

  Synthetic Minority Over-sampling TEchnique (SMOTE):
    https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c
    - a preprocessing technique used to address a class imbalance in a dataset.
    At a high level, the SMOTE algorithm can be described as follows:
      - Take difference between a sample and its nearest neighbour
      - Multiply the difference by a random number between 0 and 1
      - Add this difference to the sample to generate a new synthetic example in feature space
      - Continue on with next nearest neighbour up to user-defined number



Question 43

  During the data analysis portion of your machine learning process you have several hundred compressed JSON files stored
  in Amazon S3 around 200 MB in size. These files are categorised as semi-structured data and have already been crawled
  by AWS Glue to determine the schema associated with it. You have been using Amazon Athena to query your Amazon S3 data
  but finding it extremely expensive scanning 10 or more GBs of data each query. What are some techniques you can perform
  to cut down query execution costs?

Choose 3:
  Convert files to CSV

  Convert files to Apache Parquet or Apache ORC                <--- Correct Answer

  Partition your data                                          <--- Correct Answer

  Decompress and split files

  Break files into smaller files

  Only include columns in the queries being run that you need  <--- Correct Answer

Answer Info:

  Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently
  by employing column-wise compression, different encoding, compression based on data type, and predicate pushdown.
  They are also splittable. Generally, better compression ratios or skipping blocks of data means reading fewer bytes
  from Amazon S3, leading to better query performance.

  When running your queries, limit the final SELECT statement to only the columns that you need instead of selecting all columns.
  Trimming the number of columns reduces the amount of data that needs to be processed through the entire query execution
  pipeline. This especially helps when you are querying tables that have large numbers of columns that are string-based, and
  when you perform multiple joins or aggregations.

  Top 10 Performance Tuning Tips for Amazon Athena | AWS Big Data Blog
    https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/

    Storage
     - This section discusses how to structure your data so that you can get the most out of Athena. You can apply
       the same practices to Amazon EMR data processing applications such as Spark, Trino, Presto, and Hive when your
       data is stored in Amazon S3. We discuss the following best practices:
         1. Partition your data
         2. Bucket your data
            - Another way to reduce the amount of data a query has to read is to bucket the data within each partition.
            - Bucketing is a technique for distributing records into separate files based on the value of one of the columns.
         3. Use compression
         4. Optimize file size
         5. Use columnar file formats

    Query tuning
      - The Athena SQL engine is built on the open source distributed query engines Trino and Presto. Understanding
      how it works provides insight into how you can optimize queries when running them. This section details the following
      best practices:
       1.  Optimize ORDER BY
       2. Optimize joins
       3. Optimize GROUP BY
       4. Use approximate functions
       5. Only include the columns that you need



------------------------------------------------------
AWS Certified Machine LearningSpecialty (MLS-C01) Final Practice Exam

Question 58

The Colorado Pets and More is looking to settle, once and for all, whether more people are happier with dogs or cats as pets.
Dr. Cantrill, leading veterinarian, is looking to conduct a sentiment analysis of social media textual data. As the data is
ingested, real-time metrics will be calculated. The data will be stored in S3 for model processing. What are the best alternatives
for implementing this solution?

  Chambers Streaming Analytics

  Amazon Kinesis Firehose               <-- Incorrect answer

  Amazon Kinesis Data Streams           <-- correct answer

  Amazon Kinesis Video Streams


  Amazon Kinesis Firehose does have support for data streams and transformations but is not designed for custom data stream
    processing and real-time metrics.

  Amazon Kinesis Data Streams can be used to collect and process large streams of data records in real time. You can create Kinesis
    Data Streams applications to process data from streams.


Question 55

A large online clothing retailer receives products from many different brands. The data for these products comes with an image of
the product and a wide variety of categories and labels not consistently used across the brands. To mitigate this problem, the retailer
is going to use image classification to automatically identify categories consistently for the different brands of products. What
machine learning method is optimal for solving this problem?

  Image classification algorithm             <--- Correct Answer

  Sequence-to-sequence algorithm

  Ryker Labelpro

  Neural Topic Model (NTM) algorithm         <--- Incorrect Answer


  The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification.
    It takes an image as input and outputs one or more labels assigned to that image.

  Amazon SageMaker NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain
    word groupings based on their statistical distribution. Documents that contain frequent occurrences of words such as "bike," "car,"
    "train," "mileage," and "speed" are likely to share a topic on "transportation," for example. Topic modeling can be used to classify
    or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities.



Question 2

A Machine Learning Specialist is looking to track the number of requests sent to a model endpoint in Amazon SageMaker. What
tool can be used to collect this information?

  AWS DeepLens

  Amazon CloudWatch      <--- Correct Answer

  Amazon CloudTrail      <--- Incorrect Answer

  AWS Config


  You can monitor Amazon SageMaker using Amazon CloudWatch. Amazon CloudWatch collects endpoint invocation metrics, including
    the number of requests sent to a model endpoint (invocations), the number of requests where the model returned a 4xx HTTP
    code (Invocation4XXErrors), and more.

  Amazon CloudTrail captures API calls and related events, not specific metrics.


Question 21

A Machine Learning Engineer is deploying a model using Amazon SageMaker hosting services. Which of the following steps are required
in this process? (Choice 3)

  Create a model in Amazon SageMaker.                         <--- Correct Answer

  Create an endpoint configuration for an HTTPS endpoint.     <--- Correct Answer

  Set up ECS services for each model variant.                 <--- Incorrect Answer

  Create an HTTPS endpoint.                                   <--- Correct Answer


  To create the model, you tell Amazon SageMaker where it can find the model components, including the S3 path where the model
    artifacts are stored and the Docker registry path for the image that contains the inference code. In subsequent deployment steps,
    the model is specified by name.

  [endpoint configuration]
  You specify the name of one or more models and the compute instances you want Amazon SageMaker to launch to host each model.

  Configuring an ECS service is not a step in deploying a model to Amazon Sagemaker hosting services.

  [create an HTTPS endpoint]
  The endpoint configuration is provided to Amazon SageMaker, the appropriate machine learning compute instances are launched,
    and model(s) are deployed as specified in the configuration.


Question 31

The Starfleet Engineering Research and Development Department is working on deep learning models for the next generation of Computer
Vision systems used for space navigation and exploration. Workloads vary significantly throughout the day, so automatic scaling is
being set up for Amazon SageMaker models. What must Chief Engineer Chambers do to set up a permissions policy for automatic scaling?

  Use the SageMakerAutomaticScalingPolicy.        <--- Incorrect Answer

    info: There is currently no policy of this name.

  Use the SageMakerFullAccessPolicy.              <--- Correct Answer

    info: The SagemakerFullAccessPolicy IAM policy has all of the permissions required to perform automatic scaling actions. AWS Managed
      Policies for Amazon SageMaker
      https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html

  Use the HillaryNorgayAutoScalingPolicy.

  Use the AmazonSageMakerReadOnlyPolicy to limit permissions to autoscaling feature


AWS -> Documentation -> Amazon SageMaker -> Developer Guide -> AWS managed policies for Amazon SageMaker AI
  https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html

    Important: We recommend that you use the most restricted policy that allows you to perform your use case.

  The following AWS managed policies, which you can attach to users in your account, are specific to Amazon SageMaker AI:

    AmazonSageMakerFullAccess
       Grants full access to Amazon SageMaker AI and SageMaker geospatial resources and the supported operations. This does not
        provide unrestricted Amazon S3 access, but supports buckets and objects with specific sagemaker tags. This policy allows all
        IAM roles to be passed to Amazon SageMaker AI, but only allows IAM roles with "AmazonSageMaker" in them to be passed to the
        AWS Glue, AWS Step Functions, and AWS RoboMaker services.

    AmazonSageMakerReadOnly
       Grants read-only access to Amazon SageMaker AI resources.

  The following AWS managed policies can be attached to users in your account but are not recommended:

    AdministratorAccess
       Grants all actions for all AWS services and for all resources in the account.

    DataScientist
       Grants a wide range of permissions to cover most of the use cases (primarily for analytics and business intelligence)
        encountered by data scientists.


Question 49

Director Tzavelas is the Engineering Manager for a new bowtie manufacturing company. His team is looking to use AWS Glue for ETL
jobs moving data between AWS RDS, S3, and Lake Formation. He is looking to hire an engineer to work on AWS Glue ETL jobs. What
development languages should the engineer have experience with? (Choice 2)

  Java

  Python                <--- Correct Answer
    info: Python is a programming language used for AWS Glue.

  Scala                <--- Correct Answer
    info: Scala is a programming language used for AWS Glue.

  .NET

4.9 Helper Tools
...
  AWS Glue
    - fully managed ETL service that allo
    - allows you to run Python or Scala code on your datasets to transform them
    - It allows you to create jobs that transforms your data and gives you the ability to run them on demand or
      schedule them or when another service is triggered.
    - When a transformation process needs to happen it automatically spins up the necessary resources, does the
      transformation and outputs the data into your desired output target.
    Scala:
      - Scala is a general-purpose programming language built on the Java virtual machine.


Question 1

A marketing company is looking to determine if a particular email campaign will generate a response for a particular market segment
(yes or no) based on customer responses. Which of the following Amazon SageMaker built-in algorithms might be considered for a
binary classification problem?

  XGBoost algorithm               <--- Correct Answer

    info: This is a supervised machine learning problem with a label that fits into discrete categories (binary). The XGBoost algorithm
      can be used for binary classification.SageMaker supports the following built-in binary classification algorithms: XGBoost, Linear
      Learner Algorithm, Factorization Machines Algorithm, and the K-Nearest Neighbors (k-NN) Algorithm.

  Principal component analysis (PCA)

  Linear learner algorithm               <--- Correct Answer

  info: This is a supervised machine learning problem with a label that fits into discrete categories (binary). SageMaker supports the
    following built-in binary classification algorithms: Linear Learner Algorithm, Factorization Machines Algorithm, K-Nearest
    Neighbors (k-NN) Algorithm, and the XGBoost Algorithm.

  Object Detection algorithm


Question 10

What actions do you take when mitigating model underfit? (Choice 2)

  Improve performance due to insufficient data by:                       <--- Correct Answer
    - Increasing the amount of training data examples
    - Increasing the number of passes on the existing training data

    info:  If the learning algorithm did not have enough data to learn from, these changes are valid.

  choice 2:                                                             <--- Correct Answer
    - Add new domain-specific features
    - Add more Cartesian products
    - Change the types of feature processing (e.g., increase n-grams size)
    - Decrease the amount of regularization used

    Info:  These adjustments are for when the model is underfitting.

  Use the Regression Fit algorithm.

  Reduce model flexibility by:
    - Using fewer feature combinations
    - Increasing the amount of regularization used

Note:
  Regularization
  https://c3.ai/introduction-what-is-machine-learning/regularization/
  - More regularization prevents overfitting, while less regularization prevents underfitting.
  - Balancing the regularization parameter helps find a good tradeoff between bias and variance.

Question 34

Linda Smiley owns a local chain of ice cream stores. Recently, she started working with a data scientist consultant to predict
the number of ice cream cones sold per store as it relates to temperature. The training data performed well with 92.3% accuracy.
Unfortunately, the test data showed only a 77.6% accuracy. The data scientist advised her the model is overfitting the training
data. What corrective actions can be taken when a model is overfitting the data for this problem or other types of models? (Choice 2)

  Improve performance due to insufficient data by:                           <--- Correct Answer
    - Increasing the amount of training data examples
    - Increasing the number of passes on the existing training data

    Info: If the learning algorithm did not have enough data to learn from, these changes are valid.

  Reduce model flexibility by:                                               <--- Correct Answer
    - Using fewer feature combinations
    - Decreasing n-grams size
    - Decreasing the number of numeric attribute bins
    - Increasing the amount of regularization used

    Info: If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility by
      making these types of changes.

   Choice 3:
    - Add new domain-specific features
    - Add more Cartesian products
    - Change the types of feature processing (e.g., increase n-grams size)
    - Decrease the amount of regularization used

  Use Brokoff Fitting algorithm

Notes:

  Model Fit: Underfitting vs. Overfitting
    https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html

   Underfitting
   - Your model is underfitting the training data when the model performs poorly on the training data. This is because the model
     is unable to capture the relationship between the input examples (often called X) and the target values (often called Y).

   - Poor performance on the training data could be because the model is too simple (the input features are not expressive enough)
     to describe the target well. Performance can be improved by increasing model flexibility. To increase model flexibility,
     try the following:
      - Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used
        (e.g., increasing n-grams size)
      - Decrease the amount of regularization used

   Overfitting
   - Your model is 'overfitting' your training data when you see that the model performs well on the training data but does not
     perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize
     to unseen examples.

   - If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model
     flexibility, try the following:
      - Feature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric
        attribute bins.
      - Increase the amount of regularization used.

   Insufficient Data
   - Accuracy on training and test data could be poor because the learning algorithm did not have enough data to learn from.
     You could improve performance by doing the following:
      - Increase the amount of training data examples.
      - Increase the number of passes on the existing training data.

Question 28

Hailey MacDonald, the Document Management Director at a large financial institution, is looking to improve the quality of
searches for their library of documents. Documents have been uploaded in PDF, Rich Text Format, or ASCII text. Hailey is looking
to use machine learning to automate the identification of key topics for each of the documents. What machine learning resources
are best suited for this problem? (Choice 2)

  BlazingText algorithm

  Topic Finder (TF) algorithm

  Latent Dirichlet Allocation (LDA) algorithm                         <--- Correct Answer

    info: The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts
      to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified
      number of topics shared by documents within a text corpus.

  Neural Topic Model (NTM) algorithm                                  <--- Correct Answer

    info: Amazon SageMaker NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics
      that contain word groupings based on their statistical distribution. Documents that contain frequent occurrences of words
      such as "bike," "car," "train," "mileage," and "speed" are likely to share a topic on "transportation," for example. Topic
      modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend
      content based on topic similarities.


Question 40

A data scientist is looking to tune hyperparameters for a model. What automatic model tuning (also known as hyperparameter tuning)
strategies are available?

  Fibonacci search

  Bayesian search                                   <--- Correct Answer

    Info: Bayesian search treats hyperparameter tuning like a regression problem. Given a set of input features (the
      hyperparameters), hyperparameter tuning optimizes a model for the metric you choose. To solve a regression problem,
      hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results and runs
      training jobs to test these values. After testing the first set of hyperparameter values, hyperparameter tuning uses
      regression to choose the next set of hyperparameter values to test.

  Degrees of Freedom

  Random search                                   <--- Correct Answer

    Info: In a random search, hyperparameter tuning chooses a random combination of values from within the ranges you
      specify for hyperparameters for each training job it launches. Because the choice of hyperparameter values doesn't depend
      on the results of previous training jobs, you can run the maximum number of concurrent training jobs without affecting the
      performance of the search.

Automatic model tuning [AMT] with SageMaker AI
  https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html

  - Amazon SageMaker AI automatic model tuning (AMT) finds the best version of a model by running many training jobs on your dataset.
  - Amazon SageMaker AI automatic model tuning (AMT) is also known as hyperparameter tuning.
  - To do this, AMT uses the algorithm and ranges of hyperparameters that you specify.
  - It then chooses the hyperparameter values that creates a model that performs the best, as measured by a metric that you choose.


Understand the hyperparameter tuning strategies available in Amazon SageMaker AI
  https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html

  You can use the tuning strategies described on this page with the 'HyperParameterTuningJobConfig' and 'HyperbandStrategyConfig' APIs.

  Grid search
   - When using grid search, hyperparameter tuning chooses combinations of values from the range of categorical values that you
     specify when you create the job.
   - Only categorical parameters are supported when using the grid search strategy.
   - You do not need to specify the MaxNumberOfTrainingJobs.
     - The number of training jobs created by the tuning job is automatically calculated to be the total number of distinct
       categorical combinations possible.
     - If specified, the value of MaxNumberOfTrainingJobs should equal the total number of distinct categorical combinations possible.

  Random search
   - When using random search, hyperparameter tuning chooses a random combination of hyperparameter values in the ranges that you
     specify for each training job it launches.
   - The choice of hyperparameter values doesn't depend on the results of previous training jobs.
   - As a result, you can run the maximum number of concurrent training jobs without changing the performance of the tuning.

  Bayesian optimization
    - Bayesian optimization treats hyperparameter tuning like a regression problem.
    - Given a set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the metric that you choose.
    - To solve a regression problem, hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get
      the best results. It then runs training jobs to test these values.
    - After testing a set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter
      values to test.

  Hyperband
    - Hyperband is a multi-fidelity based tuning strategy that dynamically reallocates resources.
    - Hyperband uses both intermediate and final results of training jobs to re-allocate epochs to well-utilized hyperparameter
      configurations and automatically stops those that underperform.
    - It also seamlessly scales to using many parallel training jobs.
    - These features can significantly speed up hyperparameter tuning over random search and Bayesian optimization strategies.

    - Hyperband should only be used to tune iterative algorithms that publish results at different resource levels.
    - For example, Hyperband can be used to tune a neural network for image classification which publishes accuracy metrics
      after every epoch.

Question 60

Rihanna, a Trekkie, wants to know which Star Trek television series had the "angriest" Klingons. She was able to locate and download
all of the scripts from each series. She plans to do a sentiment analysis from the speech of all Klingon characters. Which modeling
techniques are most appropriate for Rihanna to use to develop a baseline sentiment model? (Choice 2)

  K-means

  Principal component analysis (PCA)

  BlazingText                                           <--- Correct Answer

    Info: The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text
      classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks,
      such as sentiment analysis, named entity recognition, machine translation, etc.

  Recurrent neural network (RNN)                        <--- Correct Answer

    Info: Recurrent neural networks (RNNs) can be used for sentiment analysis.

Question 25

Captain Williams has a team of volcanologists using deep learning models to study volcanic activities on Jupiter's Io moon. Some of
the team's confidential research has been stolen by the Ferengi Alliance. How can Captain Williams monitor which actions are taken
by her team when using Amazon SageMaker to aid in identifying any unauthorized or suspicious activities? (Choice 3)

  Review SageMaker API calls in AWS CloudTrail.                  <--- Correct Answer

    Info: AWS CloudTrail provides a record of actions taken by a user, role, or AWS service in Amazon SageMaker. CloudTrail captures
      all API calls for Amazon SageMaker with the exception of InvokeEndpoint as events.

  Use AWS CloudWatch Logs.                                      <--- Correct Answer

    Info: Amazon CloudWatch Logs enables you to monitor, store, and access your log files from EC2 instances, AWS CloudTrail, and
      other sources. CloudWatch Logs can monitor information in the log files and notify you when certain thresholds are met.

  Use AWS Config.

  Use AWS CloudWatch Events.                                      <--- Correct Answer

    Info: CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. Captain
      Williams can create CloudWatch Events rules to react to a status change in an Amazon SageMaker training, hyperparameter
      tuning, or batch transform job to send notifications on suspicious activity.


Question 44

A Machine Learning Specialist is looking to build a custom algorithm for Amazon SageMaker. What library is used to implement the
functionality necessary to run scripts, train algorithms, or deploy models that are compatible with Amazon SageMaker?

  SageMaker ML Boostrap

  Amazon Neural Spark

  Amazon SageMaker Containers                                <--- Correct Anwser

    Info: Amazon SageMaker Containers is a library that implements the functionality you need to create containers to run
    scripts, train algorithms, or deploy models that are compatible with Amazon SageMaker.

  Jassy SageMaker Framework 2.0

  To make use of your published model in a custom application, what must you do?

Choices:

  - Use a Lambda function to perform the inferences for your application.

  - Create an entry in Route 53 to point your desired DNS name to the endpoint.

  - Instruct SageMaker to generate a unique endpoint URL for your application.

  - Use the CloudTrail API to monitor for inference requests and trigger the SageMaker model endpoint.

  - Use the SageMaker API InvokeEndpoint() method via SDK.       -> correct answere
Sorry!
Correct Answer

To use your endpoint for inferences, you can use the SageMaker API InvokeEndpoint() method with the automatically generated HTTPS URL specified.


Question 3

  You need to increase the performance of your Image Classification inference endpoint and want to do so in the most
  cost-effective manner. What should you choose?

choices:
  - Create an additional production variant that is the same as the original variant, and direct 50% of the traffic to that variant.

  - Create a new production variant that uses a multi-GPU instance.

  - Deploy your endpoint on Amazon EC2 Inf1 instances powered by AWS Inferentia accelerators.   -> Correct answer

  - Offload some traffic to a less costly AWS Region.

  - Create a new endpoint deployment that uses a single-CPU instance given the algorithm being used.
Sorry!

  Opting for a single-CPU instance for image classification, especially if the algorithm is resource-intensive, can often result
  in suboptimal performance. CPUs are generally not as efficient as GPUs for parallel processing tasks inherent in deep learning.
  This choice may slow down the inference and fail to harness the full potential of the algorithm, leading to bottlenecks.

Correct Answer

  Amazon EC2 Inf1 instances are designed to be powered by AWS Inferentia accelerators, which optimize deep learning (DL) inference
  applications. These accelerators provide a significant boost in performance, delivering up to 2.3x higher throughput for inference
  tasks and offer up to 70% lower cost per inference compared to other Amazon EC2 instances. In the recent past, we would have to
  redeploy the endpoint using Elastic Inference added to the production variant, but Elastic Inference (EI) is no longer recommended
  by AWS and is not available anymore for new customers.


Question 7

  You want to deploy an XGBoost-backed model to a fleet of traffic sensors using Raspberry Pis as the local compute component.
  Will this work?

choices:
  - No, XGBoost cannot be compiled to run on an ARM processor. It can only run on x86 architectures.

  - No, best practice says that you should not deploy ML models into the field but rather use a centralized inference landscape.

  - Yes, you can use SageMaker Neo to compile the model into a format that is optimized for the ARM processor on the Raspberry Pi.  <- correct answer

  - No, a Raspberry Pi is not powerful enough to run an ML model using XGBoost.

  - Yes, you can deploy the model using Amazon Robomaker using the native ARM support.
Good work!

  SageMaker Neo provides a way to compile XGBoost models, which are optimized for the ARM processor in the Raspberry Pi.


Question 9

  You are helping a digital asset media company create a system which can automatically extract metadata from photographs submitted
  by freelance photographers. They want a solution that is highly scalable and only charges for the photographs you analyze and the
  metadata you store. What would you recommend?

Choices:
  - Make use of Amazon Comprehend to extract metadata from the images.

  - Build a model using the Semantic Segmentation algorithm and host it using SageMaker Hosting Services.

  - Build a model using Image Analysis to extract metadata from images and host it using Lambda and the API Gateway.

  - Build a model using Object Detection to extract metadata from images and host it using EC2.

  - Make use of Amazon Rekognition for metadata extraction.      <- correct answer
Good work!

  Amazon Rekognition is a highly scalable service that makes it easy to add powerful visual analysis to your applications.
  With Rekognition Image, you only pay for the images you analyze and the face metadata you store.


Question 11

  Your company has just discovered a security breach occurred in a division separate from yours but has ordered a full review of all
  access logs. You have been asked to provide the last 180 days of access to the three SageMaker Hosted Service models that you manage.
  When you set up these deployments, you left everything default. How will you be able to respond?

Choices:
  - Use CloudWatch along with IPInsights to analyse the logs for suspicious activity from the past 180 days then download these records.

  - Use CloudWatch to pull a list of all access records for the ML models. Make use of a Python library to parse out only the access records.

  - Use CloudTrail to pull a list of all access to the ML models for the last 180 days.

  - Use CloudTrail to pull a list of all access to the models for the last 90 days. Any data beyond 90 days is unavailable. <- correct answer

  - Use SageMaker Detailed Logging to produce a CSV file of access from the past 180 days.
Good work!

  CloudTrail is the proper service if you want to see who has sent API calls to your SageMaker Hosted model but, by default, it will
  only store the last 90 days of events. You can configure CloudTrail to store an unlimited amount of logs on S3 but this is not turned
  on by default. Whilst CloudTrail is not necessarily an Access Log, it performs the same auditing functions you might expect; and an
  auditor may not necessarily be familiar with the nuances of AWS

Question 2 (retry 1)

  You are preparing to release an updated version of your latest machine learning model. It is provided to about 3,000 customers
  who use it in a SaaS capacity. You want to minimize customer disruption, minimize risk, and be sure the new model is stable before
  full deployment. What is the best course of action?

Choices:
  Use a canary deployment to evaluate the new model, then use a phased rollout.                                <--- Correct Answer

  Conduct an A/B test to evaluate the new model and then distribute the new production URL to your customers.

  Perform offline validation, then release the new version all at once to minimize risk.

  Use a continuous integration process to preserve the stability of the new model and deploy in a "Big Bang" manner.
Good work!

  Of the options provided, using a canary deployment to first evaluate the stability of the update with a small group, then using
  a phased rollout, seems to fulfill the objectives. A phased rollout could be done as an extension of the deployment until 100% of
  the traffic is moved to the new version.

------------------------------------------------------

Chapter 10 Wrap-Up

------------------------------------------------------
10.1 Wrap-Up
------------------------------------------------------
10.2 AWS Certified Machine Learning - Specialist 2020 - Practice Exam
------------------------------------------------------
Other Resources that I found:


  Resources (I found):

    KD nuggets: Understanding Bias-Variance Trade-Off in 3 Minutes
      https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html

      Bias
        - the simplifying assumptions made by the model to make the target function easier to approximate.
      Variance
        - the amount that the estimate of the target function will change, given different training data.

      Errors due to Bias (results in underfitting)
        - the distance between the predictions of a model and the true values.
        - In this type of error, the model pays little attention to training data and oversimplifies the model and
          doesn't learn the patterns. The model learns the wrong relations by not taking in account all the features
      Errors due to Variance (result ins overfitting)
       - Variability of model prediction for a given data point or a value that tells us the spread of our data.
       - In this type of error, the model pays an lot of attention in training data, to the point to memorize it instead
         of learning from it.
       - A model with a high error of variance is not flexible to generalize on the data which it hasnt seen before.

  Amazon SageMaker Example Notebooks
    https://sagemaker-examples.readthedocs.io/en/latest/

  Atlassian: Essential chart types for data visualization
    https://www.atlassian.com/data/charts/essential-chart-types-for-data-visualization


    video resources for Machine Learning Algorithms, Sagemaker, etc, by Prof Ryan Ahmed
       K Nearest Neighbors Algorithm (KNN)
          https://www.youtube.com/watch?v=v5CcxPiYSlA

       Overview of AWS SageMaker Built-in Algorithms
         https://www.youtube.com/watch?v=79y4WtA-zqA




  If you like this video, check out this full course on Udemy:
    https://www.udemy.com/course/become-a...


   Available SageMaker Algorithms:
    - BlazingText Word2Vec: BlazingText implementation of the Word2Vec algorithm for scaling and accelerating the generation of word embeddings from a large number of documents.

    - DeepAR: An algorithm that generates accurate forecasts by learning patterns from many related time-series using recurrent neural networks (RNN).

    - Factorization Machines: A model with the ability to estimate all of the interactions between features even with a very small amount of data.

    - Gradient Boosted Trees (XGBoost): Short for Extreme Gradient Boosting, XGBoost is an optimized distributed gradient boosting library.

    - Image Classification (ResNet): A popular neural network for developing image classification systems.

    - IP Insights: An algorithm to detect malicious users or learn to usage patterns of IP addresses.

    - K-Means Clustering: One of the simplest ML algorithms. Its used to find groups within unlabeled data.

    - K-Nearest Neighbor (k-NN): An index based algorithm to address classification and regression based problems.

    - Latent Dirichlet Allocation (LDA): A model that is well suited to automatically discovering the main topics present in a set of text files.

    - Linear Learner (Classification): Linear classification uses an objects characteristics to identify the appropriate group that it belongs to.

    - Linear Learner (Regression): Linear regression is used to predict the linear relationship between two variables.

    - Neural Topic Modelling (NTM): A neural network based approach for learning topics from text and image datasets.

    - Object2Vec: A neural-embedding algorithm to compute nearest neighbors and to visualize natural clusters.

    - Object Detection: Detects, classifies, and places bounding boxes around multiple objects in an image.

    - Principal Component Analysis (PCA): Often used in data pre-processing, this algorithm takes a table or matrix of many features and reduces it to a smaller number of representative features.

    - Random Cut Forest: An unsupervised machine learning algorithm for anomaly detection.

    - Semantic Segmentation: Partitions an image to identify places of interest by assigning a label to the individual pixels of the image.

    - Seqence2Sequence: A general-purpose encoder-decoder for text that is often used for machine translation, text summarization, etc.
------------------------------------------------------

  For Cloud Job info:
      Check out Kesha Williams pluralSight blogs
      https://www.pluralsight.com/resources/blog
         https://www.pluralsight.com/resources/blog?q=kesha+williams&unified-tags=cloud

         How to get a cloud Engineer job talk:
           https://www.youtube.com/live/p6fsxpQiYJA

------------------------------------------------------
10.2 AWS Certified Machine Learning - Specialty - Practice Exam


Question 13

  You are working for a hot new startup that calculates different metrics about their customers depending on how much
  money they spend on a weekly, quarterly, and yearly basis. These metrics are classified as elite, novice, and beginner.
  Depending on their ranking they get more/less discounts and placed in higher/lower priority for customer support. The
  algorithm you have chosen expects all numerical inputs. What can be done to handle these classification values so that
  ordering is taken into account for the required numerical inputs?

Choices:
  Apply random numbers to each classification value and apply gradient descent until the values converge to expect results

  Use one-hot encoding techniques to map values for each classification dropping the original classification feature

  Experiment with mapping different values for each status and see which works best               <--- Correct Answer

  Use one-hot encoding techniques to map values for each classification

Answer Info
  Since these classification values are ordinal (order does matter) we cannot use one-hot encoding techniques. We either
  need to map these values to a scale, or we train our model with different encodings and seeing which encoding works best.
  Ordinal data - Wikipedia



Question 21
  You are a machine learning specialist evaluating a current model that has been deployed into production. It has been
  deployed for a few weeks now and the results are not accurate and sometimes the inference data is missing values. What
  are some techniques you can review to help solve this problem?

Choose 3:
  Ensure the target variable used as the predictor during training represents the actual outcome that the machine learning
  model is trying to predict.                                                   <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are the
    extraction methods used to generate these datasets the same as for the production data? How similar is the sample data
    to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or treatments
    the same? Managing Machine Learning Projects

  Ensure the inference data has placeholder values for any of the missing values   <--- Incorrect Answer

    Putting in random placeholders would negatively impact the accuracy of your inferences.

  Ensure the training datasets are large, representative samples of the populations that the model needs to make
  predictions.                                                                  <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are
    the extraction methods used to generate these datasets the same as for the production data? How similar is the sample
    data to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or
    treatments the same? Managing Machine Learning Projects

  Ensure the inference data is exactly the same for the training and testing data.

  Ensure the extraction methods used to generate the training datasets are the same as for the production
  inference data.                                                                <--- Correct Answer

    We must ensure that training, validation, and test sets adequately represent reality. Are the sets large, representative
    samples of the populations that the model needs to make predictions about? Does the target variable that the model
    predicts represent the actual outcome that the ML model is trying to predict, or is it a proxy for that outcome? Are
    the extraction methods used to generate these datasets the same as for the production data? How similar is the sample
    data to the real data, and is the belief about the similarity supported by testable facts? Are the error sources or
    treatments the same? Managing Machine Learning Projects



Question 31

  You work for a manufacturing company who has hundreds of conveyor belts with built-in IoT sensors. These sensors stream
  data into AWS using Amazon Kinesis Data Streams. The features associated with the data is belt_id, building_number,
  belt_temp, outside_temp, and power_consumption. During the processing of the data, you need to transform the data and
  store it in a data store. Which combination of services can you use to achieve this?

Choose 3:
  Set up Amazon Kinesis Data Firehose to ingest data from Amazon Kinesis Data Streams, and then send data to AWS Lambda.
  Transform the data in AWS Lambda and write the transformed data into Amazon S3.                 <--- Correct Answer

    Amazon Kinesis Data Firehose can be used as a destination for Amazon Kinesis Data Streams. You can also use an AWS
    Lambda function to prepare and transform incoming data in your delivery stream before loading it to destinations,
    such as Amazon S3.

    AWS Documentation: Amazon Kinesis Data Firehose FAQs > Data Transformation and Format Conversion

  Use Amazon Kinesis Data Streams to immediately write your data into Amazon S3. Next, set up an AWS Lambda function that
  fires whenever an object is PUT into Amazon S3. Transform the data from the AWS Lambda function, and then write the
  transformed data into Amazon S3.

  Capture, query, and analyze your streaming data using Amazon Kinesis Data Analytics. Run real-time SQL queries to
  transform and load your data into Amazon S3.                                                      <--- Incorrect Answer

    Amazon Kinesis Data Analytics can transform and deliver data to Amazon S3. However, Amazon Kinesis Data Streams should
    be used as a data source to capture streaming data. This solution would be valid if Amazon Kinesis Data Streams was
    described as a data source for Amazon Kinesis Data Analytics.

    Note: Misleading answer since question states Kinesis Data Streams is being used

  Use Amazon Kinesis Data Streams as a source for Amazon Kinesis Data Analytics. Run real-time SQL queries on the data
  to transform it. Ingest the transformed data into Amazon Kinesis Data Firehose and load it into Amazon S3.  <--- Correct Answer

    You can create an Amazon Kinesis Data Analytics application that leverages Amazon Kinesis Data Streams as a source
    and Amazon Kinesis Data Firehose as a delivery stream into an Amazon S3 bucket.

    AWS Documentation: Example: Writing to Kinesis Data Firehose

  Immediately send the data to AWS Lambda from Amazon Kinesis Data Streams. Transform the data in AWS Lambda and write
  the transformed data into Amazon S3.                                                                <--- Correct Answer

    Amazon Kinesis Data Streams can be quickly paired with AWS Lambda to respond to or adjust immediate occurrences
    within event-driven applications in your environment, at any scale. You can also use AWS Lambda to transform and
    write data into Amazon S3.

    AWS Documentation: Using AWS Lambda with Amazon Kinesis


Question 8

  What needs to be done to the following phrase before using it in your machine learning process?
  The quk BROWN FOX jumped over the lazy dog.

Choose 3:
  Replace each word with a respective n-gram vector

  One-hot encode values

  Remove stop words

  Replace each word with a respective tf-idf vector

  Fix the "quk" to "quick"

    We should leave words that are misspelled in place as these need to be part of the analysis.

  Create tokens from each value

  Lowercase transformation


Answers Info:
  Before we use corpus data in some Machine Learning processes like language translation, sentiment analysis, or spam
  filtering it is important we properly apply text processing to the data. Some of the important text processing that
  needs to be done is tokenization.
  This includes - removing stop words  frequent words such as the, is, etc. that do not have specific meaning.
  This includes - lowercase transformation.



Question 24

  Your organization has the need to set up a petabyte scaled BI and dashboard analysis tool that will query millions
  of rows of data spread across thousands of files stored in S3. Your organization wants to save as much money as possible.
  Which solution will allows developers to run dozens if not hundreds or thousands of queries per day, and possibly scanning
  many TBs of data each, while still being cost effective?

Choice:
  EC2 Spot instances and Presto                                                    <--- Correct Answer

    You pay a constant fee for the compute instances you are running (EC2 instances cost). The more machines you run
    and the bigger they are - the higher the fee, yes. But Presto is very efficient and if your data is correctly stored,
    a few commodity machines will do a great job if you are running your Presto cluster on the same region as your S3
    bucket, and within one AZ, as there is no network or data transfer costs at all. The compute costs can be further
    optimized by using spot instances for worker nodes, and completely shutting them down off-hours (where applicable).
    Presto can deal with a lost worker node - which might slow down some queries but spot instances come at a great
    discount. Presto - Amazon EMR Presto | Distributed SQL Query Engine for Big Data

  AWS Glue Data Catalog and Amazon Athena                                            <--- Incorrect Answer

    While AWS Glue Data Catalog and Amazon Athena are capable of meeting this use case, consider that the cost of
    data processing with Athena is a set price per terabyte. When dealing with full petabyte-scale data warehouses,
    this may cost far more than building your own solution using EC2 Spot Instances

  Data Pipeline and RDS

  Lambda Functions with extremely long timeouts


Notes:
      Elastic Map Reduced (EMR)
        . . .
        Apache Spark:
          - Spark is an open-source, distributed computing system that provides a unified analytics engine for big data processing.
          - It offers support for various programming languages, including Java, Scala, Python, and R
          - a multi-language engine processing framework and programming model for executing data engineering, data science, and
            machine learning on single-node machines or clusters.
        Apache Presto
          - SQL Query Engine designed for interactive analytics at scale
          - An open source, distributed SQL query engine optimized for low-latency, ad-hoc analysis of data.
          - supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions.
          - can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.

        Apache Spark vs Presto
          Spark
            - Spark is known for its exceptional performance and scalability, making it ideal for big data processing and analytics.
            - It supports batch processing, real-time stream processing, as well as machine learning and graph processing
          Presto
            - shines in interactive analytics and ad-hoc queries, allowing users to explore and analyze data in real-time.
            - It also offers federated querying capabilities, enabling users to query data from multiple sources seamlessly.




Question 30

  You are preparing plain text corpus data to use in a NLP process. Which of the following is/are one of the important step(s)
  to pre-process the text in NLP based projects?

Choose 3:
  One-hot encode ordinal n-gram values

  Stemming                                                                <--- Correct Answer

  Word standardization                                                    <--- Correct Answer

  Add random text noise

  Congregate all of the plain text corpus data into a single document     <--- Incorrect Answer

    The data would be processed identically whether the data is in a single document or several

  Stop word removal                                                       <--- Correct Answer


Answer Info:
  Stemming is a rudimentary rule-based process of stripping the suffixes (ing, ly, es, s etc) from a word.

  Stop words are those words which will have no relevance to the context of the data for example is/am/are.

  Object Standardization is also one of the good ways to pre-process the text by removing things like acronyms, hashtags
  with attached words, and colloquial slang that typically are not recognized by search engines and models.

Notes:
  Natural language processing - Wikipedia  https://en.wikipedia.org/wiki/Natural_language_processing

  Stemming
    The process of reducing inflected (or sometimes derived) words to a base form (e.g., "close" will be the root for
    "closed", "closing", "close", "closer" etc.). Stemming yields similar results as lemmatization, but does so on grounds
    of rules, not a dictionary.

  Lemmatization
    The task of removing inflectional endings only and to return the base dictionary form of a word which is also known
    as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the
    transformation actually uses a dictionary to map words to their actual form



Question 53

  You have been tasked with creating a labeled dataset by classifying text data into different categories depending on the
  summary of the corpus. You plan to use this data with a particular machine learning algorithm within AWS. Your goal is
  to make this as streamlined as possible with minimal amount of setup from you and your team. What tool can be used to help
  label your dataset with the minimum amount of setup?

Choices:
  AWS SageMaker GroundTruth text classification job                   <--- Correct Answer

    You can use SageMaker Ground Truth to create ground truth datasets by creating labeling jobs. When you create a text
    classification job, workers group text into the categories that you define. You can define multiple categories but the worker
    can apply only one category to the text. Use the instructions to guide your workers to make the correct choice. Always
    define a generic class in addition to your specific classes. Giving your workers a generic option helps to minimize
    inaccurately classified text. Amazon SageMaker Ground Truth - Amazon SageMaker

  Marketplace AMI for NLP problems

  AWS Comprehend entity detection

  Amazon Neural Topic Modeling (NTM) built-in algorithm

  AWS Comprehend sentiment analysis

  Amazon Latent Dirichlet Allocation (LDA) algorithm                   <--- Incorrect Answer


Question 36

  When evaluating a model after the training and testing process, you notice that the error rate during training is high but
  the error rate during testing is low. Which of the following could be the reason for obtaining these error rates?

Choose 2:
  You need to re-evaluate the selection of your algorithm.               <--- Incorrect Answer
                                                                              (note: should also be a correct answer)
  Your model is underfitting the testing data.

  Your model is overfitting the training data.

  You have a programmatic issue with your algorithm.                      <--- Correct Answer

  You should train for a longer period of time.

  You have a data issue with both your training and testing datasets.     <--- Correct Answer

Answer Info:
  When training error is high and testing error is low, this is highly unusual as it infers that the model is somehow
  predicting better than the data which was used to train the model. This is usually an indicator of a data issue or some
  systemic problem in the algorithm. Overfitting - Wikipedia


Question 40

  You are trying to classify a number of items based on different features into one of 6 groups (books, electronics, movies, etc.)
  based on features. Which algorithm would be best suited for this type of problem?

Chooses:
  Use K-Means algorithm with k set to the number of classes                 <--- Incorrect Answer

    While K-Means is a clustering algorithm that can be used to group data, it's not typically used for multiclass classification.
    It is an unsupervised learning algorithm, primarily used when we do not have any labeled data.

  Use Linear Learner with predictor set to regressor

  Use a stochastic approach when choosing target parameters and recommended ranges

  Use regression forest with the number of trees set to the number of categories

  Use XGBoost with objective set to multi:softmax                          <--- Correct Answer

    The XGBoost algorithm with the objective set to 'multi:softmax' is a good choice for this multiclass classification problem.
    XGBoost is an implementation of gradient boosted decision trees. The 'multi:softmax' option allows it to handle multiple classes.

Note:

   Question should have indicated a labeled training set.


Question 54

  Assuming equal importance of all misclassification types and a need to evaluate the model performance across all possible
  classification thresholds, what is a good target metric to use when comparing different binary classification models?

Chooses:
  F1 score                                              <--- Incorrect Answer

  Area under the curve (AUC)                            <--- Correct Answer


  Mean squared root error (MSRE)

  Recall

Answer Info:

  The F1 score is a weighted average of precision and recall, hence it is more useful than accuracy, especially if you have an
  uneven class distribution. However, it does not consider all possible classification thresholds, as it is calculated from a
  confusion matrix at a specific threshold, usually 0.5. It is not as comprehensive as AUC for this task.

  AUC-ROC (Area Under the Receiver Operating Characteristic curve) is a performance measurement for classification problem
  at various thresholds settings. It tells how much model is capable of distinguishing between classes, taking into account
  both the true positive rate and the false positive rate, and is not sensitive to imbalanced classes. It is suitable when
  equal importance is given to all misclassification types and you need to evaluate the model performance across all possible
  classification thresholds.


Question 1

  After several weeks of working on a model for genome mapping, you believe you have perfected it and now want to deploy it to a
  platform that will provide the highest performance. Which of the following AWS platforms will provide the highest
  cost-to-performance for this compute-intensive model?

Choose 2:
  EC2 F1 instance

  EC2 G3 Instance                    <--- Correct Answer

  EC2 X1 Instance                    <--- Incorrect Answer

  EC2 P2 Instance                    <--- Correct Answer

  EC2 M5 Instance

Answer Info:

  EC2 G3 instances are designed for high-performance computing (HPC) and machine learning (ML) workloads. They feature
  powerful NVIDIA GPUs and Intel CPUs, making them ideal for genome mapping workloads. EC2 G3 instances are relatively
  inexpensive, making them a good choice for cost-sensitive projects.

  X1 instances are optimized for large-scale, enterprise-class and in-memory applications, and offer one of the lowest price
  per GiB of RAM among Amazon EC2 instance types. Use cases: In-memory databases (e.g. SAP HANA), big data processing engines
  (e.g. Apache Spark or Presto), high performance computing (HPC). Certified by SAP to run Business Warehouse on HANA (BW),
  Data Mart Solutions on HANA, Business Suite on HANA (SoH), Business Suite S/4HANA.

  P2 instances are intended for general-purpose GPU compute applications. Use cases: Machine learning, high performance
  databases, computational fluid dynamics, computational finance, seismic analysis, molecular modeling, genomics, rendering,
  and other server-side GPU compute workloads. Amazon EC2 Instance Types.

  P5, P4, P3, & P2
     -> GPU based instances with:
       P2: P2 instances are intended for general-purpose GPU compute applications - High-performance NVIDIA K80 GPUs.
       P3: Intel Xeon Scalable Processor  high performance compute in the cloud with up to 8 NVIDIA V100 Tensor Core GPUs
       P4: 3.0 GHz 2nd Generation Intel Xeon Scalable processors (Cascade Lake P-8275CL) and Up to 8 NVIDIA A100 Tensor Core GPU
       P5: 3rd Gen AMD EPYC processors (AMD EPYC 7R13), and Up to 8 NVIDIA H100 (in P5) or H200 (in P5e) Tensor Core GPUs


Notes:
    G* series: some use cases state ML workloads and inferencing, but some like G3 state Graphic workloads3

    X1 pick was due to confusing Inf1/Inf2 with X1

    M5 General Purpose Instances powered by Intel Xeon Platinum 8175M or 8259CL processors. These instances provide a balance
       of compute, memory, and network resources, and is a good choice for many applications.


Question 48

  You have been tasked with using Polly to translate text to speech in the company announcements that launch weekly. The
  problem you are encountering is how Polly is incorrectly translating the companies acronyms. What can be done for future
  tasks to help prevent this?

Choose 2:
  Use speech marks for input text documents                                                <--- Incorrect Answer

  Use Amazon Comprehend to pull parts of speech and use to help pronounce acronyms

  Create dictionary lexicon                                                                <--- Correct Answer

  Use Amazon Transcribe to first map the acronyms to pronunciations then include them in the Amazon polly pipeline

  Use SSML tags in documents                                                               <--- Correct Answer

Answer Info:
  Using SSML-enhanced input text gives you additional control over how Amazon Polly generates speech from the text you provide.
  Using these tags allows you to substitute a different word (or pronunciation) for selected text such as an acronym or abbreviation.

  You can also create a dictionary lexicon to apply to any future tasks instead of apply SSML to each individual document.
  Amazon Polly


Notes:

 Amazon Polly
   - Amazon Polly is a cloud service that converts text into lifelike speech.
   Speech Synthesis Markup Language (SSML)
     - You can use Polly to generate speech from either plain text or from documents marked up with SSML.
     - Using SSML-enhanced text gives you additional control over how Amazon Polly generates speech from the text you provide.
     - The <speak> tag is the root element of all Amazon Polly SSML text. All SSML-enhanced text must be enclosed within a
        pair of <speak> tags.
         <speak>Mary had a little lamb.</speak>
     - With SSML tags, you can include a long pause within your text, or change the speech rate or pitch. Other options
       include: emphasizing specific words or phrases; using phonetic pronunciation; including breathing sounds; whispering;
       use a different language; etc.
   Speech Marks
     - Speech marks are metadata that describe the speech that you synthesize, such as where a sentence or word starts and
       ends in the audio stream.
     - When you request speech marks for your text, Amazon Polly returns this metadata instead of synthesized speech.
     - By using speech marks in conjunction with the synthesized speech audio stream, you can provide your applications with
       an enhanced visual experience.
     - For example, combining the metadata with the audio stream from your text can enable you to synchronize speech with
       facial animation (lip-syncing) or to highlight written words as they're spoken.
   Pronounciation Lexicons:
     - Pronunciation lexicons enable you to customize the pronunciation of words.
     - Amazon Polly provides API operations that you can use to store lexicons in an AWS region.
     - Those lexicons are then specific to that particular region. You can use one or more of the lexicons from that region
       when synthesizing the text by using the SynthesizeSpeech operation.
     - lexicon examples:
       - Your text might include an acronym, such as W3C. You can use a lexicon to define an alias for the word W3C so that
         it is read in the full, expanded form (World Wide Web Consortium).
       - Lexicons give you additional control over how Amazon Polly pronounces words uncommon to the selected language
       - Common words are sometimes stylized with numbers taking the place of letters, as with "g3t sm4rt" (get smart). In this
         example, you can specify an alias (get smart) for the word "g3t sm4rt" in the lexicon.


Question 11

  You have been tasked with transforming highly sensitive data using AWS Glue. Which of the following AWS Glue settings
  allowing you to control encryption for your transformation process?

Choose 3:
  Encryption of your Data Catalog at its components using symmetric keys                          <--- Correct Answer

  The security configurations that you create (S3 encryption, CloudWatch logs encryption, and Job
  bookmark encryption)                                                                             <--- Correct Answer

  Encryption of the classifier used during the transformation job

  The server-side encryption setting (SSE-S3 or SSE-KMS) that is passed as a parameter to your
  AWS Glue ETL job.                                                                                 <--- Correct Answer

  Encryption of your Data Catalog at its components using asymmetric keys

  Encrypting the managed EBS volumes used to run Apache Spark environment running PySpark code


Answer Info:

  You can encrypt metadata objects in your AWS Glue Data Catalog in addition to the data written to S3 and CloudWatch
  Logs by jobs, crawlers, and development endpoints.  You can enable encryption of the entire Data Catalog in your account.

  When you create jobs, crawlers, and development endpoints in AWS Glue, you can provide encryption settings, such as a
  'security configuration', to configure encryption for that process. With AWS Glue, you can encrypt data using keys that
  you manage with AWS Key Management Service (AWS KMS).

  With encryption enabled, when you add Data Catalog objects, run crawlers, run jobs, or start development endpoints,
  AWS KMS keys are used to write data at rest.

  You can also configure AWS Glue to only access Java Database Connectivity (JDBC) data stores through a trusted Secure
  Sockets Layer (SSL) protocol. Encrypting Glue Resources - AWS Glue Encrypting Data Written by Crawlers, Jobs, and
  Development Endpoints - AWS Glue


Note:
  Key Type:
     Symmetric:  A single key used for encrypting and decrypting data or generating and verifying HMAC codes.
                 - only users with valid AWS KMS credentials can encrypt and decrypt using synmetric key
                 - for course, focus is on symmetic because it is covered in exam
                 - symmetric can also be used for generationg HASH based message authentication code (HMAC code).
                   Used when you want to determine authenticity of a message, e.g. JWT Token, JSON Web Token,
                   Tokenized credit card info
     Asymmetric: A public and private key pair used for encrypting and decrypting data or signing and verifying messages.
                 - if encryption requirement is outside KMS, you need to use asymmetric keys
                 - public key used to encrypt data, and private key is used to decrypt data


     AWS KMS with S3:
       - When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key.
       - Amazon S3 supports only symmetric encryption KMS keys.
     AWS KMS with AWS Glue:
       - AWS Glue supports only symmetric customer managed keys.
       - The KMS key list displays only symmetric keys. However, if you select Choose a KMS key ARN, the console lets
         you enter an ARN for any key type. Ensure that you enter only ARNs for symmetric keys.


  AWS Glue Security Configuration:
    - A security configuration in AWS Glue contains the properties that are needed when you write encrypted data.
    - You create security configurations on the AWS Glue console to provide the encryption properties that are used by
      crawlers, jobs, and development endpoints.
    Encryption settings
      Enable S3 encryption
        - Enable at-rest encryption for data stored on S3.
      Enable CloudWatch logs encryption
        - Enable at-rest encryption when writing logs to Amazon CloudWatch.
      Enable job bookmark encryption
        - Enable at-rest encryption of job bookmark.
      Enable data quality encryption
        - Enable at-rest encryption of data quality.


Question 33

  You are preparing plain text corpus data to build a model for Amazon's Neural Topic Model (NTM) algorithm. What are
  the steps you need to take before the data is ready for training?

Chooses:
  First normalize the corpus data. Then, count the occurrence of each of the value produced, creating word
  count vectors. Use these vectors as training data.

  First tokenize the corpus data. Then, count the occurrence of each token and form bag-of-words vectors.
  Use these vectors as training data.                                                                     <--- Correct Answer

  First perform tf-idf to remove words that are not important. Use the number of unique n-grams to create
  vectors and respective word counts. Use these vectors as training data.

  First create bigrams of the corpus data. Then, count the occurrence of each bigram produced, creating
  word count vectors. Use these vectors as training data.

Answer Info:

  Both in training and inference, need to be vectors of integers representing word counts. This is so-called bag-of-words
  (BOW) representation. To convert plain text to BOW, we need to first tokenize our documents, that is, identify words
  and assign an integer ID to each of them. Then, we count the occurrence of each of the tokens in each document and
  form BOW vectors.

  Introduction to the Amazon SageMaker Neural Topic Model | AWS Machine Learning Blog
    https://aws.amazon.com/blogs/machine-learning/introduction-to-the-amazon-sagemaker-neural-topic-model/

Notes:
  NTM input - From plain text to bag-of-words (BOW)
    - The input documents to the NTM algorithm, both in training and inference, need to be vectors of integers representing
      word counts. This is so-called bag-of-words (BOW) representation.
    - To convert plain text to BOW, we need to first tokenize our documents, that is, identify words and assign an
      integer ID to each of them. Then, we count the occurrence of each of the tokens in each document and form BOW vectors.
    - We will only keep the most frequent 2,000 tokens (words) because rarely used words have a much smaller impact on the
      model and thus can be ignored.

Notes:
 Bag-of-words model
   - The bag-of-words model (BoW) is a model of text which uses a representation of text that is based on an unordered collection
   (a "bag") of words.
   - It is used in natural language processing and information retrieval (IR).
   - It disregards word order (and thus most of syntax or grammar) but captures multiplicity.


Question 35

  You are a data scientist working on a model that predicts fraudulent and non-fraudulent transactions. You notice
  that 90% of the samples are non-fraudulent, which makes up the majority of the dataset. What are some methods you
  can use to address this issue in the data?

Choose 3:
  Drop all fraudulent transaction before training the model.

  Use K-means cluster to find outliers for non-fraudulent transactions, and use those as fraudulent samples.

  Apply SMOTE to the dataset to oversample the minority class.                             <--- Correct Answer

  Collect more data samples to increase the accuracy of the dataset.                       <--- Correct Answer


  Apply Principal Component Analysis (PCA) to under-sample non-fraudulent transactions.

  Combine the dataset with a public dataset that has a majority of fraudulent transactions.

  Resample the dataset to correct imbalances of each transaction type.                     <--- Correct Answer

Answer Info:

  You can use techniques like SMOTE (Synthetic Minority Over-Sample Technique) to create more samples of the fraudulent transactions.

  Since there is not enough data present in the current dataset to be sampled effectively with sufficient fraudulent
  transactions for the model to use, you may opt to collect additional data for the dataset, so your sampling can contain
  a greater ratio of fraudulent transactions.

  There are many different ways to handle imbalanced data. What is important is that you have a keen sense for what damage
  an imbalanced dataset can cause and how to balance it. Reference Documentation: Handling Imbalanced Datasets - a review

Notes:

  Synthetic Minority Over-sampling TEchnique (SMOTE):
    https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c
    - a preprocessing technique used to address a class imbalance in a dataset.
    At a high level, the SMOTE algorithm can be described as follows:
      - Take difference between a sample and its nearest neighbour
      - Multiply the difference by a random number between 0 and 1
      - Add this difference to the sample to generate a new synthetic example in feature space
      - Continue on with next nearest neighbour up to user-defined number



Question 43

  During the data analysis portion of your machine learning process you have several hundred compressed JSON files stored
  in Amazon S3 around 200 MB in size. These files are categorised as semi-structured data and have already been crawled
  by AWS Glue to determine the schema associated with it. You have been using Amazon Athena to query your Amazon S3 data
  but finding it extremely expensive scanning 10 or more GBs of data each query. What are some techniques you can perform
  to cut down query execution costs?

Choose 3:
  Convert files to CSV

  Convert files to Apache Parquet or Apache ORC                <--- Correct Answer

  Partition your data                                          <--- Correct Answer

  Decompress and split files

  Break files into smaller files

  Only include columns in the queries being run that you need  <--- Correct Answer

Answer Info:

  Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently
  by employing column-wise compression, different encoding, compression based on data type, and predicate pushdown.
  They are also splittable. Generally, better compression ratios or skipping blocks of data means reading fewer bytes
  from Amazon S3, leading to better query performance.

  When running your queries, limit the final SELECT statement to only the columns that you need instead of selecting all columns.
  Trimming the number of columns reduces the amount of data that needs to be processed through the entire query execution
  pipeline. This especially helps when you are querying tables that have large numbers of columns that are string-based, and
  when you perform multiple joins or aggregations.

  Top 10 Performance Tuning Tips for Amazon Athena | AWS Big Data Blog
    https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/

    Storage
     - This section discusses how to structure your data so that you can get the most out of Athena. You can apply
       the same practices to Amazon EMR data processing applications such as Spark, Trino, Presto, and Hive when your
       data is stored in Amazon S3. We discuss the following best practices:
         1. Partition your data
         2. Bucket your data
            - Another way to reduce the amount of data a query has to read is to bucket the data within each partition.
            - Bucketing is a technique for distributing records into separate files based on the value of one of the columns.
         3. Use compression
         4. Optimize file size
         5. Use columnar file formats

    Query tuning
      - The Athena SQL engine is built on the open source distributed query engines Trino and Presto. Understanding
      how it works provides insight into how you can optimize queries when running them. This section details the following
      best practices:
       1.  Optimize ORDER BY
       2. Optimize joins
       3. Optimize GROUP BY
       4. Use approximate functions
       5. Only include the columns that you need



------------------------------------------------------
AWS Certified Machine LearningSpecialty (MLS-C01) Final Practice Exam (retry 1)

Question 7

  A machine learning model is being created using Amazon's Factorization Machines algorithm to help make click
  predictions and item recommendations for new customers. Which of the following would be candidates during the training process?

Choose 3:

  Making inferences to the model in application/csv format.

  Using sparse data in CSV format as training data.

  Creating a regression model where the testing dataset is scored using Root Mean Square Error (RMSE).                <- Correct Answer

  Creating a binary classification model where the testing dataset is scored using Binary Cross Entropy (Log Loss),   <- Correct Answer
    Accuracy, and F1 Score.

  Creating a multi-classification model where the testing dataset is scored using Area Under The Curve (AUC).

  Using sparse data in recordIO-protobuf format with Float32 tensors as training data.                                <- Correct Answer


Answer Info:
  The factorization machine algorithm can be run in either in binary classification mode or regression mode. In regression
  mode, the testing dataset is scored using Root Mean Square Error (RMSE). In binary classification mode, the test dataset
  is scored using Binary Cross Entropy (Log Loss), Accuracy (at threshold=0.5) and F1 Score (at threshold =0.5). For training,

  the factorization machines algorithm currently supports only the recordIO-protobuf format with Float32 tensors. CSV format
  is not a good candidate. For inference, factorization machines support the application/json and x-recordio-protobuf formats.
  Factorization Machines Algorithm - Amazon SageMaker


Question 23

  You are working for an online shopping platform that records actions made by its users. This information is captured
  in multiple JSON files stored in S3. You have been tasked with moving this data into Amazon Redshift database tables
  as part of a data lake migration process. Which of the following needs to occur to achieve this in the most efficient way?

Choose 3:
  Setup DynamoDB table and use Data Pipeline to load the S3 data into DynamoDB table.

  Use the INSERT command to load the tables from the data files on Amazon S3.

  Use COPY commands to load the tables from the data files on DynamoDB.

  Use COPY commands to load the tables from the data files on Amazon S3.                         <- Correct Answer

  Launch an Amazon Redshift cluster and create database tables.                                  <- Correct Answer

  Use multiple concurrent COPY commands to load the table from each JSON file.

  Troubleshoot load errors and modify your COPY commands to correct the errors.                  <- Correct Answer

Answer info:

  You can add data to your Amazon Redshift tables either by using an INSERT command or by using a COPY command. At the
  scale and speed of an Amazon Redshift data warehouse, the COPY command is many times faster and more efficient than
  INSERT commands.

  You can load data from an Amazon DynamoDB table, or from files on Amazon S3, Amazon EMR, or any remote host through
  a Secure Shell (SSH) connection. When loading data from S3, you can load table data from a single file, or you can
  split the data for each table into multiple files. The COPY command can load data from multiple files in parallel.
  Using a COPY Command to Load Data - Amazon Redshift

notes:
  Tutorial: Loading data from Amazon S3
    https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html

    In this tutorial, you do the following:

      Download data files that use comma-separated value (CSV), character-delimited, and fixed width formats.

      Create an Amazon S3 bucket and then upload the data files to the bucket.

      Launch an Amazon Redshift cluster and create database tables.

      Use COPY commands to load the tables from the data files on Amazon S3.

      Troubleshoot load errors and modify your COPY commands to correct the errors.


    Overview
      - You can add data to your Amazon Redshift tables either by using an INSERT command or by using a COPY command.
      - At the scale and speed of an Amazon Redshift data warehouse, the COPY command is many times faster and more
       efficient than INSERT commands.

      - The COPY command uses the Amazon Redshift massively parallel processing (MPP) architecture to read and load data in
        parallel from multiple data sources.
      - You can load from data files on Amazon S3, Amazon EMR, or any remote host accessible through a Secure Shell (SSH)
        connection. Or you can load directly from an Amazon DynamoDB table.

     Steps:
       Step 1: Create a [Redshift] cluster
       Step 3: Upload the files to an Amazon S3 bucket
       Step 4: Create the [Redshift] sample tables
         - For this tutorial, you use a set of tables based on the Star Schema Benchmark (SSB) schema
         - Run the following CREATE TABLE commands in your SQL client.
       Step 5: Run the COPY commands
         - You run COPY commands to load each of the tables in the SSB schema.
         - The COPY command examples demonstrate loading from different file formats, using several COPY command options,
           and troubleshooting load errors.
         COPY command syntax
          - The basic COPY command syntax is as follows.
            COPY table_name [ column_list ] FROM data_source CREDENTIALS access_credentials [options]
            CREATE TABLE part
            (
            ...
            )
        Step ...


Question 42

  You are in charge of training a deep learning (DL) model at scale using massively large datasets. These datasets are too
  large to load into memory on your Notebook instances. What are some best practices to use to solve this problem and still
  have fast training times?

Choose 2:

  Use a fleet of RAM intensive ml.m5 EC2 instances with MapReduce and Hadoop installed onto them. Load the           <- Incorrect Answer
    data in parallel to the cluster to distribute across multiple machines.

  Use a fleet of GPU intensive ml.p2 EC2 instances with MapReduce and Hadoop installed onto them. Load the data
    data in parallel to the cluster to distribute across multiple machines.

  Once the data is split into a small number of files and partitioned, the preparation job can be parallelized       <- Correct Answer
    and thus run faster.

  Pack the data in parallel, distributed across multiple machines and split the data into a small number             <- Correct Answer
    of files with a uniform number of partitions.

  Once the data is loaded onto the instances, split the data into a small number of files and partitioned,           <- Incorrect Answer
    then the preparation job can be parallelized and thus run faster.


Answer Info:
  When you perform deep learning (DL) at scale, for example, datasets are commonly too large to fit into memory and
  therefore require pre-processing steps to partition the datasets. In general, a best practice is to pack the data
  in parallel, distributed across multiple machines. You should do this in a single run, and split the data into a
  small number of files with a uniform number of partitions.

  When the data is partitioned, it is readily accessible and easily fed in as batches across multiple machines. When
  the data is split into a small number of files, the preparation job can be parallelized and thus run faster. You can
  do all of this using frameworks such as MapReduce and Apache Spark. Running an Apache Spark cluster on Amazon EMR
  provides a managed framework that can process massive quantities of data. Power Machine Learning at Scale




Question 1

  A term frequencyinverse document frequency (tfidf) matrix using trigrams is built from a text corpus consisting
  of the following three documents: { Hold please }, { Please try again }, { Please call us back }. What are the
  dimensions of the tfidf vector/matrix?

Choose:
  (3, 9)

  (3, 3)

  (3, 2)

  (3, 16)                                           <- Correct Answer


  (9, 3)

Answer Info:

  You can determine the tf-idf vectorized format by using the following: (number of documents, number of unique n-grams).
  There are 3 documents (or corpus data we are vectorizing) with 3 unique trigrams ['call us back', 'please call us',
  'please try again'], 6 unique bigrams ['call us', 'hold please', 'please call', 'please try', 'try again', 'us back'],
  and 7 unique unigrams ['again', 'back', 'call', 'hold', 'please', 'try', 'us']. So, the vectorized matrix is (3, 16). TfidfVectorizer.

  Note:
     looks like tf-idf matrix (No of documents, No of trigams+bigrams+unigrams)
       where trigrams/bigrams/unigrams are identified for each document, but only count the unique ones between all documents



Question 35

  You are a machine learning specialist finding ways to detect anomalous data points within a given labeled data set.
  You've been tasked with creating a model to achieve this and also determine how accurate the model is along with other
  metrics like precision, recall, and F1-score metrics on the labeled data. How can this easily be achieved?

Choose:
  Create a model using the XGBoost algorithm with both a train and the optional test data channels.
  Use application/x-recordio-protobuf for training and validation data. Train the model on an ml.m4
  or ml.g4 instance type.

  Create a model using the Random Cut Forest (RCF) algorithm with both a train and the optional test              <- Incorrect Answer
  data channels. Use application/json for training and validation data. Train the model on an ml.m4
  or ml.c4 instance type.

  Create a model using the XGBoost algorithm with both a train and optional validation channels. Use
  application/x-recordio-protobuf for training and validation data. Train the model on an ml.c4 or
  ml.g4 instance type.

  Create a model using the Random Cut Forest (RCF) algorithm with a single train channel. Use
  application/x-recordio-protobuf for training and validation data. Train the model on an ml.m4
  or ml.c4 instance type.

  Create a model using the Random Cut Forest (RCF) algorithm with both a train and the optional                    <- Correct Answer
  test data channels. Use text/csv for training and validation data. Train the model on an ml.m4
  or ml.c4 instance type.

Answer Info:

  Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within
  a data set. When using RCF the optional test channel is used to compute accuracy, precision, recall, and F1-score
  metrics on labeled data. Train and test data content types can be either application/x-recordio-protobuf or
  text/csv and AWS recommends using ml.m4, ml.c4, and ml.c5 instance families for training.
  Random Cut Forest (RCF) Algorithm - Amazon SageMaker


Question 21

  A financial institution is seeking a way to improve security by implementing two-factor authentication (2FA). However,
  management is concerned about customer satisfaction by being forced to authenticate via 2FA for every login. The company
  is seeking your advice. What is your recommendation?

Choose:
  Recommend that the company invests in customer education on why 2FA is important to their well-being.
  Train customer support staff on properly handling customer complaints.

  Recommend that the company create a custom login page for their website where customers can login by              <- Incorrect Answer
  simply enabling their webcam. Use Amazon Rekognition to detect whether the face is of the customer and
  authenticate them into their account.

  Create a ML model that uses IP Insights to detect anomalies in client activity. Only if anomalies are              <- correct Answer
  detected, force a 2FA step.


  Create an ML model using Linear Learner that can evaluate whether a customer is truly a human or some
  scripted bot typical of hacking attempts. Hold off on implementing 2FA until there is sufficient data
  to support its need.

  Create a binary classifier model using Object2Vec to detect unusual activity for customer logins. If unusual
  activity is detected, trigger an SNS notification to the Fraud Department.

Answer Info:

  IP Insights is a built-in SageMaker algorithm that can detect anomalies as it relates to IP addresses. In this case,
  only enforcing 2FA where unusual activity is detected might be a good compromise between security and ease-of-use.

  While using facial recognition might be a tempting alternative, it can easily be bypassed by holding up a picture
  of some customer and it would not be true multi-factor authentication. IP Insights Algorithm - Amazon SageMaker

Question 25

  A machine learning specialist is running a training job on a single EC2 instance using their own Tensorflow code on
  a Deep Learning AMI. The specialist wants to run distributed training and inference using SageMaker. What should
  the machine learning specialist do?

Choose:
  It is not possible to run custom Tensorflow code in SageMaker

  Use Tensorflow in SageMaker and modify the AWS Deep Learning Docker containers

  Use Tensorflow in SageMaker and edit your code to run using the SageMaker Python SDK                     <- Correct Answer

  Ensure both the SageMaker Notebook instance and EC2 instance have the same role assigned to them.
  Use Notebook peering to gain access to run scripts from SageMaker on the EC2 instance

  Use Tensorflow in SageMaker and run your code as a script                                                 <- Incorrect Answer


Anwser Info:

  When using custom TensorFlow code, the Amazon SageMaker Python SDK supports script mode training scripts.
  Script mode has the following advantages: Script mode training scripts are more similar to training scripts
  you write for TensorFlow in general, so it is easier to modify your existing TensorFlow training scripts to
  work with Amazon SageMaker. Script mode supports both Python 2.7- and Python 3.6-compatible source files.
  Script mode supports Horovod for distributed training. Use TensorFlow with Amazon SageMaker - Amazon SageMaker

Notes:

    Working with Frameworks in SageMaker:
       - Script mode requires an estimator that points to a python script
       - Python script contains custom training and inference code
       - with script-mode, SageMaker handles the container for you


WS Certified Machine Learning - Specialty (MLS-C01): Modeling

4.5 Training s SageMaker Model Using a Training Script


  Using Scikit-learn with the SageMaker Python SDK
    https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html

  Algorithm Implementation options
    built-in algorithm.
      - no code required
      - requires algorithm with training data, hyperparameters, and computing resources.
    script mode in supported framework
      - Develop a custom Python script
      - in a supported framework, like Psyche learn, TensorFlow, pyarch, or MXNet.
      - leverage the additional Python libraries that are preloaded with these frameworks for
        training an algorithm.
    custom docker image,
      - requires docker expertise
      - if your use case is not addressed by previous two options.
      - the Docker image must be uploaded to Amazon ECR before you can start training the model.

   Code:  Train a SKLearn Model using Script Mode
      >>> .  .  .

      >>> # ## Train model
      >>> # The model is trained using the SageMaker SDK's Estimator class. Firstly, get the execution role for training.
      >>> # This role allows us to access the S3 bucket in the last step, where the train and test data set is located.

      >>> # Use the current execution role for training. It needs access to S3
      >>> role = sagemaker.get_execution_role()
      >>> print(role)


      >>> # Then, it is time to define the SageMaker SDK Estimator class. We use an Estimator class specifically desgined to train
      >>> # scikit-learn models called `SKLearn`. In this estimator, we define the following parameters:
      >>> # 1. The script that we want to use to train the model (i.e. `entry_point`). This is the heart of the Script Mode method.
      >>> # Additionally, set the `script_mode` parameter to `True`.
      >>> # 2. The role which allows us access to the S3 bucket containing the train and test data set (i.e. `role`)
      >>> # 3. How many instances we want to use in training (i.e. `instance_count`) and what type of instance we want to use in
      >>> # training (i.e. `instance_type`)
      >>> # 4. Which version of scikit-learn to use (i.e. `framework_version`)
      >>> # 5. Training hyperparameters (i.e. `hyperparameters`)
      >>> #
      >>> # After setting these parameters, the `fit` function is invoked to train the model.

      >>> # Docs: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html

      >>> from sagemaker.sklearn import SKLearn

      >>> sk_estimator = SKLearn(
      >>>     entry_point="train.py",
      >>>     role=role,
      >>>     instance_count=1,
      >>>     instance_type="ml.c5.xlarge",
      >>>     py_version="py3",
      >>>     framework_version="1.2-1",
      >>>     script_mode=True,
      >>>     hyperparameters={"estimators": 20},
      >>> )

      >>> # Train the estimator
      >>> sk_estimator.fit({"train": training_input_path})

      >>> .  .  .

    Code:  Entrypoint script 'train.py' called by above SageMaker script mode script

      >>> import argparse, os
      >>> import boto3
      >>> import json
      >>> import pandas as pd
      >>> import numpy as np
      >>> from sklearn.model_selection import train_test_split
      >>> from sklearn.preprocessing import StandardScaler
      >>> from sklearn.ensemble import RandomForestRegressor
      >>> from sklearn import metrics
      >>> import joblib

      >>> if __name__ == "__main__":

      >>>     # Pass in environment variables and hyperparameters
      >>>     parser = argparse.ArgumentParser()

      >>>     # Hyperparameters
      >>>     parser.add_argument("--estimators", type=int, default=15)

      >>>     # sm_model_dir: model artifacts stored here after training
      >>>     parser.add_argument("--sm-model-dir", type=str, default=os.environ.get("SM_MODEL_DIR"))
      >>>     parser.add_argument("--model_dir", type=str)
      >>>     parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN"))

      >>>     args, _ = parser.parse_known_args()
      >>>     estimators = args.estimators
      >>>     model_dir = args.model_dir
      >>>     sm_model_dir = args.sm_model_dir
      >>>     training_dir = args.train

      >>>     # Read in data
      >>>     df = pd.read_csv(training_dir + "/train.csv", sep=",")

      >>>     # Preprocess data
      >>>     X = df.drop(["class", "class_cat"], axis=1)
      >>>     y = df["class_cat"]
      >>>     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
      >>>     sc = StandardScaler()
      >>>     X_train = sc.fit_transform(X_train)
      >>>     X_test = sc.transform(X_test)

      >>>     # Build model
      >>>     regressor = RandomForestRegressor(n_estimators=estimators)
      >>>     regressor.fit(X_train, y_train)
      >>>     y_pred = regressor.predict(X_test)

      >>>     # Save model
      >>>     joblib.dump(regressor, os.path.join(args.sm_model_dir, "model.joblib"))

      >>> # Model serving
      >>> # INFERENCE
      >>> # SageMaker uses four functions to load the model and use it for inference: model_fn, input_fn, output_fn, and predict_fn

      >>> """
      >>> Deserialize fitted model
      >>> """
      >>> def model_fn(model_dir):
      >>>     model = joblib.load(os.path.join(model_dir, "model.joblib"))
      >>>     return model

      >>> """
      >>> input_fn
      >>>     request_body: The body of the request sent to the model.
      >>>     request_content_type: (string) specifies the format/variable type of the request
      >>> """
      >>> def input_fn(request_body, request_content_type):
      >>>     if request_content_type == "application/json":
      >>>         request_body = json.loads(request_body)
      >>>         inpVar = request_body["Input"]
      >>>         return inpVar
      >>>     else:
      >>>         raise ValueError("This model only supports application/json input")

      >>> """
      >>> predict_fn
      >>>     input_data: returned array from input_fn above
      >>>     model (sklearn model) returned model loaded from model_fn above
      >>> """
      >>> def predict_fn(input_data, model):
      >>>     return model.predict(input_data)

      >>> """
      >>> output_fn
      >>>     prediction: the returned value from predict_fn above
      >>>     content_type: the content type the endpoint expects to be returned. Ex: JSON, string
      >>> """
      >>> def output_fn(prediction, content_type):
      >>>     res = int(prediction[0])
      >>>     respJSON = {"Output": res}
      >>>     return respJSON



Question 33

  To satisfy an external security auditor, you need to demonstrate that you can monitor all traffic going in and
  out of your VPC containing your deployed SageMaker model. What would you show the auditor to satisfy this audit requirement?

Choose:
  CloudTrail Logs
Selected

  CloudWatch Alerts

  SageMaker Logs

  CloudWatch Events

  VPC Flow Logs

Answer Info:
  VPC Flow Logs allow you to allows you to monitor all network traffic in and out of your model containers within a VPC.
  VPC Flow Logs - Amazon Virtual Private Cloud

Notes:

11.6 Work AWS VPC Flow Logs for Network Monitoring

  Learning Objectives:
   - Create a CloudWatch Log Group and a VPC Flow Log to CloudWatch
   - Create CloudWatch Filters and Alerts
   - Use CloudWatch Logs Insights
   - Analyze VPC Flow Logs Data in Athena

  ------------------------------|
  | VPC                         |
  |                             |
  ||----------------|           |
  || EC2 instance   |           | /--> S3 ---> Athena
  ||           ENI----> VPC Flow /    bucket
  ||----------------|    Logs    \
  | security Group              | \--> CloudWatch --> CloudWatch --> SNS --> Email --> user
  --------------------------------                       Alarm

  Note: VPC flow logs can be configured at the VPC, network interface (ENI), or subnet level.
        For lab, VPC Flow will be at the VPC level.


Exam Tips:

    VPC Flow Logs:
     - a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.
     - Flow log data can be published to the following locations: Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data
       Firehose. After you create a flow log, you can retrieve and view the flow log records in the log group, bucket, or
       delivery stream that you configured.

    VPC Flow logs can help you with a number of tasks, such as:
     - Diagnosing overly restrictive security group rules
     - Monitoring the traffic that is reaching your instance
     - Determining the direction of the traffic to and from the network interfaces



Question 52

  You work for a company that builds custom python libraries for transforming and preprocessing data sets before
  using them in BI tools and machine learning pipelines. One of your customers is using your libraries and has the
  need to include them within their AWS Glue pipeline. What suggestions can you make to allow your customers to use
  your libraries within their AWS Glue pipelines?

Choose:
  Upload the custom library as a .zip archive onto S3. Before your customers create an ETL job, include the     <- Correct Answer
  S3 link as a script library and job parameter

Answer Info:
  When you are creating a new Job on the console, you can specify one or more library .zip files by choosing Script
  Libraries and job parameters (optional) and entering the full Amazon S3 library path(s) in the same way you would
  when creating a development endpoint: Using Python Libraries with AWS Glue - AWS Glue


Question 13

  You are applying normalization techniques to a column in your dataset. The column has the following values {1, 5, 7}.
  When we apply normalization what will the respective output results be?


  {0.00, 0.66, 1.00}                           <- Correct Answer

Answer Info:
   Applying normalization translates each feature individually such that it is in the given range on the training set
   between 0 and 1. In this case {1, 5, 7} maps to {0.00, 0.66, 1.00} respectively. sklearn.preprocessing.MinMaxScaler
    scikit-learn 0.21.2 documentation

Notes:
        The Normalization transformation formula:
             X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
             X_scaled = X_std * (max - min) + min
             where min, max = feature_range.

Question 41

  You are apply standardization techniques to a feature in your dataset. The column has the following values {5, 20, 15}.
  The standard deviation is 6.23 and the mean of the feature 13.33. When we apply standardization what will the respective
  output results be?

{-1.33, 1.06, 0.26}                                   <- Correct Answer

Answer Info:

  Let's take the value 5. To calculate the standardization value we use the following formula z = (x - u) / s where 'z' is the
  standardized value, where 'x' is our observed value, where 'u' is the mean value of the feature, and 's' is the standard
  deviation. For 5, -1.33 = (5 - 13.33) / 6.23. For 15, 0.26 = (15 - 13.33) / 6.23. For 20, 1.06 = (20 - 13.33) / 6.23.
  Since 5 is the only value that produces a negative value, {-1.33, 1.06, 0.26} is the only acceptable answer.
  sklearn.preprocessing.StandardScaler  scikit-learn 0.21.2 documentation

   std deviation formula:

        =  [(1/N)(Xi)**2]

          = standard deviation symbol
          = mean   = (1/N)Ni=1(Xi)
         N = number of observations
         Xi = ith observation
           = SUM of i=1 to i=N


Question 30

  You are applying text transformation on corpus data before using it in Amazon SageMaker BlazingText algorithm.
  The corpus data consists of 3 attributes (label index, title, and abstract) with the labeled index mapping to
  either Film, Music, or Art. What does the BlazingText algorithm expect as training data when applying supervised
  training with File Mode?

  A single preprocessed text file with space-separated tokens where the training file should contain a training        <- Correct Answer
  sentence per line along with the labels. Labels are words that are prefixed by the string __label__

Answer Info:

  The BlazingText algorithm expects a single preprocessed text file with space-separated tokens.
  Each line in the file should contain a single sentence. If you need to train on multiple text files,
  concatenate them into one file and upload the file in the respective channel.

  If using File Mode the training/validation file should contain a training sentence per line along with the labels.
  Labels are words that are prefixed by the string __label__

  If using Pipe mode the manifest file format should be in JSON Lines format in which each line represents one sample.
  The sentences are specified using the source tag and the label can be specified using the__label__ tag. Both source
  and label tags should be provided under the AttributeNames parameter value as specified in the request.
  BlazingText Algorithm - Amazon SageMaker


Question 6

  You are designing a binary classification model using the XGBoost algorithm. Which of the following would you most
  likely use as an objective for evaluating the model?

Choose:

  Macro F1 score approaches 0.

  Area Under the Curve approaches 0.

  Root Square Mean Error approaches 0.

  Root Square Mean Error approaches 1.

  Area Under the Curve approaches 1.                                    <- Correct Answer

Answer Info:

  For binary classification problems, the AUC or Area Under the Curve is an industry-standard metric to evaluate the
  quality of a binary classification machine learning model. AUC measures the ability of the model to predict a higher
  score for positive examples, those that are correct, than for negative examples, those that are incorrect. The AUC
  metric returns a decimal value from 0 to 1. AUC values near 1 indicate an ML model that is highly accurate.
  AWS Glossary - Amazon Web Services

Notes:
  From ORiely Hands ON Machine Learning (chapter 3):

    The ROC Curve:

      The receiver operating characteristic (ROC) curve plots the True Positive rate (TPR) (aka recall) (y-axis) against
        the False Positive Rate (FPR) (x-axis).
      The FPR (aka fall-out) is the ratio of negative instances incorrectly classified as positive.
      The TNR (true negative rate) (aka specificity) is the ration of negative instances that are correctly classified as negative.

               ROC Curve: plots  TPR (aka recall)  versus   FPR
                          -> equivalent: sensitivity (recall) versus 1 - specificity

               TPR or Recall or sensitivity:       TP  / (TP + FN)

               TNR or specificity:                 TN / (FP + TN)

               FPR (or fall-out):                  FP  / (FP + TN) = 1 - TNR

               FNR              :                  FN  / (TP + FN)



  From ORiely Hands ON Machine Learning (chapter 3):

      AUC (Area Under the Curve):
      - one way to compare classifiers is to measure the AUC.
      - a perfect Classifier will have and ROC AUC equal to 1.
      - a purely random Classifier will have and ROC AUC equal to 0.5.

------------------------------------------------------
AWS Certified Machine LearningSpecialty (MLS-C01) Final Practice Exam
  https://practice-exam.acloud.guru/f87ac9a1-2d47-44f1-8e10-2a8e43959ef5?_ga=2.255201096.1764985697.1738024440-277800900.1727277947
  01/24/25 retry 2

Question 2

  A new modeler has joined the team at a medical research laboratory. During the first week, the modeler was very focused
  on minimizing costs for running instances and chose the smallest instance sizes available. One of the original notebook
  instances created in Amazon SageMaker is becoming resource constrained. What is the best approach in increasing resources
  without losing any files or data associated with the notebook instance?

Choose 1:
  Launch a new notebook Instance with the larger size, and use AWS Glue to transfer the files and data from the original
    instance to the new instance.

  Launch a new notebook instance with the larger size, copy the files from the original notebook             <- Incorrect Answer
    instance to an S3 bucket, and then copy the files from the S3 bucket to the new instance.

  Unfortunately, you are unable to change notebook instance types after they have been launched.

  Stop the notebook instance. Change the notebook instance type to a larger size. Then restart                <- Correct Answer
    the notebook instance.


Answer Info:
  Launching an entirely new notebook instance and copying the files to/from an S3 bucket is unnecessary.
  This is not the appropriate solution for this scenario.

  Notebook instance sizes can be stopped, the configuration changed to a larger size, and then restarted.


Question 37

  A Machine Learning Engineer has been using a TensorFlow with Amazon SageMaker using a pre-built container image and
  needs to add new functional requirements for an algorithm. How can this be accomplished with the least amount of effort?

Choose 1:
  Adjust the hyperparameters.

  Modify the Amazon SageMaker Image.                                                                        <- Incorrect Answer


  Add the new functionality by loading Python code in S3 buckets read by the EC2 instances at               <- Incorrect Answer
    launch time via custom code in the user data.

  Add additional code into new Lambda layers and include references to the code in the core
    algorithm Lambda functions.

Answer Info:
  If you have additional functional requirements for an algorithm or model you developed in a framework for a pre-built
  Amazon SageMaker Docker image, you can modify an Amazon SageMaker image.

  Amazon SageMaker uses Docker containers for scripts, algorithms, and models. Code is not deployed directly to EC2 instances.



Question 50

  A research and development department for a large university is using machine learning to aid in designing the
  next generation of cancer treatment solutions. Terabytes of data are collected from government agencies, medical
  facilities, other universities, and patient volunteers across the globe. Models are run in batch. Resources are
  scarce, and time is critical. How can the batch transform jobs be sped up without increasing instance sizes?

Choose 1:
  Deploy the models using inference pipelines.                                                               <- Incorrect Answer

  Enable AWS Batch Processing Acceleration (BPA).

  Use different parameter values such as max payload size (MB), max concurrent transforms,                     <- Correct Answer
    and batch strategy.

  Set the batch cache size and enable batch cache flushing.


Answer Info:

  This [inference pipeline] does not address optimizing the batch transformation jobs and is not the appropriate
    solution for this scenario.

  You can reduce the time it takes to complete batch transform jobs by using different parameter values, such
    as max payload size (MB), max concurrent transforms, and batch strategy. Amazon SageMaker automatically finds
    the optimal parameter settings for built-in algorithms. For custom algorithms, you can provide these values
    through an execution-parameters endpoint.

Notes:

  Batch transform for inference with Amazon SageMaker AI
    https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html

    Speed up a batch transform job
      - If you are using the CreateTransformJob API, you can reduce the time it takes to complete batch transform jobs
        by using optimal values for parameters.
      - This includes parameters such as 'MaxPayloadInMB', 'MaxConcurrentTransforms', or 'BatchStrategy'.
      - The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.


  CreateTransformJob
    https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#sagemaker-CreateTransformJob-request-BatchStrategy
    CreateTransformJob
      - Starts a transform job.
      - A transform job uses a trained model to get inferences on a dataset and saves these results to an Amazon S3
        location that you specify.

      In the request body, you provide the following:
        TransformJobName
          - Identifies the transform job. The name must be unique within an AWS Region in an AWS account.
        ModelName
          - Identifies the model to use. ModelName must be the name of an existing Amazon SageMaker model in the same
            AWS Region and AWS account. For information on creating a model, see CreateModel.
        TransformInput
          - Describes the dataset to be transformed and the Amazon S3 location where it is stored.
        TransformOutput
          - Identifies the Amazon S3 location where you want Amazon SageMaker to save the results from the transform job.
        TransformResources
          - Identifies the ML compute instances for the transform job.
   Request Parameters
     - The request accepts the following data in JSON format.

     BatchStrategy
       - Specifies the number of records to include in a mini-batch for an HTTP inference request.
       - To enable the batch strategy, you must set the 'SplitType' property to 'Line', 'RecordIO', or 'TFRecord'.

       - To use only one record when making an HTTP invocation request to a container, set 'BatchStrategy' to 'SingleRecord'
         and 'SplitType' to 'Line'.

       - To fit as many records in a mini-batch as can fit within the 'MaxPayloadInMB' limit, set 'BatchStrategy' to
         'MultiRecord' and 'SplitType' to 'Line'.

       - Type: String;  Valid Values: MultiRecord | SingleRecord;  Required: No

     MaxConcurrentTransforms
       - The maximum number of parallel requests that can be sent to each instance in a transform job.
       - If 'MaxConcurrentTransforms' is set to 0 or left unset, SageMaker checks the optional execution-parameters to
         determine the settings for your chosen algorithm. If the execution-parameters endpoint is not enabled, the default
         value is 1.
       - Type: Integer:  Valid Range: Minimum value of 0.; Required: No

     MaxPayloadInMB
       - The maximum allowed size of the payload, in MB. A payload is the data portion of a record (without metadata).
       - The value in 'MaxPayloadInMB' must be greater than, or equal to, the size of a single record.
       - To estimate the size of a record in MB, divide the size of your dataset by the number of records.
       - To ensure that the records fit within the maximum payload size, we recommend using a slightly larger value.
       - The default value is 6 MB.

       - The value of MaxPayloadInMB cannot be greater than 100 MB. If you specify the MaxConcurrentTransforms parameter,
         the value of (MaxConcurrentTransforms * MaxPayloadInMB) also cannot exceed 100 MB.

       - For cases where the payload might be arbitrarily large and is transmitted using HTTP chunked encoding, set the
         value to 0. This feature works only in supported algorithms. Currently, Amazon SageMaker built-in algorithms
         do not support HTTP chunked encoding.
       - Type: Integer;   Valid Range: Minimum value of 0.;   Required: No



  AWS Certified Machine Learning - Specialty (MLS-C01): Machine Learning Implementation and Operations

    2.7 DEMO: Obtaining Inferences for an Entire Dataset Using SageMaker Batch Transform

      >>> # #### 1. Create a transform job
      >>> #
      >>> %%time

      >>> sm_transformer = sm_estimator.transformer(1, "ml.m5.large")

      >>> # start a transform job
      >>> input_location = "s3://{}/{}/batch/{}".format(
      >>>     bucket, prefix, batch_file_noID
      >>> )  # use input data without ID column
      >>> sm_transformer.transform(input_location, content_type="text/csv", split_type="Line")
      >>> sm_transformer.wait()


Question 30

A Machine Learning Specialist is looking to optimize hyperparameters for a model. What are the key steps for automatic model tuning (also known as hyperparameter tuning)?

Choose 3:
  Select the SageMaker AMT launch configuration.                                    <- Incorrect Answer

  Define hyperparameter ranges.                                                     <- Correct Answer

  Define metrics.                                                                   <- Correct Answer

  Configure the training jobs.                                                      <- Correct Answer

Answer Info:

  This [SageMaker AMT launch configuration] is a made-up option.

  You specify the hyperparameters and range of values over which to search by defining hyperparameter ranges for your tuning job.
    Choosing hyperparameters and ranges significantly affects the performance of your tuning job.

  To optimize hyperparameters, a tuning job evaluates the training jobs it launches by using a metric that the training
    algorithm writes to logs. Amazon SageMaker hyperparameter tuning parses your algorithms stdout and stderr streams to
    find algorithm metrics, such as loss or validation-accuracy, that show how well the model is performing on the data set.

  This [training job] is required to specify the container image for the training algorithm, input configuration for
    training and test data, storage location for the algorithm's output, type of instance to use for the training jobs,
    values for hyperparameters not tuned in the tuning job, stopping condition for the training job, and optionally the
    metrics that the training jobs emit.

Notes:

Automatic model tuning [AMT] with SageMaker AI
  https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html

  - Amazon SageMaker AI automatic model tuning (AMT) finds the best version of a model by running many training jobs on your dataset.
  - Amazon SageMaker AI automatic model tuning (AMT) is also known as hyperparameter tuning.
  - To do this, AMT uses the algorithm and ranges of hyperparameters that you specify.
  - It then chooses the hyperparameter values that creates a model that performs the best, as measured by a metric that you choose.

  - Before you start using hyperparameter tuning, you should have a well-defined machine learning problem, including the
    following:
      - A dataset
      - An understanding of the type of algorithm that you need to train
      - A clear understanding of how you measure success

  - Prepare your dataset and algorithm so that they work in SageMaker AI and successfully run a 'training job' at least once.
  Topics
    - Understand the hyperparameter tuning strategies available in Amazon SageMaker AI
    - Define metrics and environment variables
    - Define Hyperparameter Ranges
    - Track and set completion criteria for your tuning [training] job



Question 55

  MDB Bank is looking to minimize fraud on their credit card products. What is the best metric when evaluating machine
  learning for this task?

Choose 1:
  Recall                                                                                     <-- Correct Answer

  Precision                                                                                  <-- Incorrect Answer

  Accuracy

  Fraud detection

Answer Info:

  Recall measures the fraction of actual positives predicted as positive. What proportion of actual positives was identified correctly?
  In cases where it is better to err on the side of predicting too many positives (such as predicting fraud for a bank), high recall
  is important. In this case, we would prefer to err on the side of calling more things fraud than to miss a fraud classification.

  Precision measures the fraction of actual positives among those that are predicted as positive. What proportion of positive
  identifications was actually correct? An application that needs to be extremely sure about positive predictions is highly precise.


Question 51

  A Machine Learning Specialist has loaded several massive data sets into S3. What is the best method for minimizing cost and effort
  for querying this data?

Choose 1:
  Load the data into DynamoDB, and run SQL queries on the data.

  Load the data into RDS, and run SQL queries on the data.

  Load the data into Amazon Elastic MapReduce (EMR), and run Hive queries on the data.                   <-- Incorrect Answer

  Use AWS Glue and Amazon Athena.                                                                        <-- Correct Answer

Answer Info

  This [EMR] would result in greater expense and effort than some other alternatives.

  AWS Glue is a fully managed ETL (extract, transform, and load) service that can categorize your data, clean it, enrich it,
  and move it reliably between various data stores. AWS Glue crawlers automatically infer database and table schema from your
  source data, storing the associated metadata in the AWS Glue Data Catalog. When you create a table in Athena, you can choose
  to create it using an AWS Glue crawler.

Note:
  If question asks about handling petabytes of data, then select EMR. Since this just stated 'massive data sets' then select
  AWS Glue and Athena.


Question 43

  What are some of the key components of AWS Glue?

Choose 4:
  AWS Glue Cluster Manager

  AWS Glue ETL operations                                                                  <-- Correct Answer

  AWS Glue Jobs system                                                                     <-- Correct Answer

  AWS Glue crawlers and classifiers                                                        <-- Correct Answer

  AWS Glue Data Catalog                                                                    <-- Correct Answer

Answer Info:

  Using the metadata in the Data Catalog, AWS Glue can autogenerate Scala or PySpark (the Python API for Apache Spark) scripts
    with AWS Glue extensions that can be used and modified for ETL operations.

  The AWS Glue Jobs system provides managed infrastructure to orchestrate ETL workflows. Jobs can be created to automate ETL
    scripts and transfer data to different locations. Jobs can be scheduled and chained, or they can be triggered by events such
    as the arrival of new data.

  AWS Glue crawlers can scan data in supported repositories, classify it, extract schema information from it, and store the metadata
   in the AWS Glue Data Catalog.

  The AWS Glue Data Catalog is a managed service used to store, annotate, and share metadata (in the same way as an Apache Hive
    metastore).


Question 34

  What is the folder used to store the model for SageMaker containers?

Choose 1:
  /sagemaker/model/

  /home/ec2-user/model/

  /var/model/

  /opt/ml/model/                                                                      <-- Correct Answer

Answer Info:

  The /opt/ml/model/ directory contains the model generated by your algorithm in a single file or an entire directory tree in
  any format.


Notes:

  Train and host Scikit-Learn models in Amazon SageMaker by building a Scikit Docker container
    https://aws.amazon.com/blogs/machine-learning/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container/


Anatomy of an Amazon SageMaker container

                           input data             Inference
                           (request)              (response)
                              |                      ^
                              V                      |
                            |--------------------------|
                            |         Endpoints        |
                            |--------------------------|
                              |                      ^
                              V                      |
                            |--------------------------|
                            |         Models           |
                            |--------------------------|
                              |                      ^
                              V                      |
                          |------------------------------|
                          | opt/ml/        WORKDIR/      |
         S3 Bucket    --> |   model/         ngnix.conf  |            |----------------------|
      model artifacts     |                  predictor.py| <----------|  WORKDIR/            |
             ^            |                  serve       |            |    ngnix.conf        |
             |            |                  wsgi.py     |            |    predictor.py      |
             |            |                              |            |    serve             |
             |            |    Deployment / Hosting      |            |    train             |
             |            |   on ML Compute Instances    |            |    wsgi.py           |
             |            |------------------------------|            |                      |
             |                                                        |                      |
             |              |--------------------------|              |                      |
             |------------- |         Jobs             |              |        Docker Model  |
                            |--------------------------|              |                      |
                                         ^                            | Elastic Container    |
                                         |                            |    Registry (ECR)    |
                          |------------------------------|            |                      |
                          | opt/ml/          WORKDIR/    |<---------- |----------------------|
                          |   input/           train     |
        S3 Bucket   ----> |   model/                     |
      Training Data       |   output/                    |
                          |                              |
                          |                              |
                          |        Model Training        |
                          |   on ML Compute Instances    |
                          |------------------------------|


    Example Container Folder
      The key files are shown in the following container.

        container/
            decision_trees/
                nginx.conf
                predictor.py
                serve
                train
                wsgi.py
            Dockerfile

      DockerFile
      - The Dockerfile is used to build the container.
      - In the Dockerfile, you can specify requirements and dependencies to be installed in the container (such as
        Python, NGINX, and Scikit). Then the installed container executes programs included in the program folder.
      Program Folder (e.g. decision_trees)
      - To do that, you need to name the program folder in your container folder.
      - The program folder in our example container folder is named after the Scikit model, that is decision_trees.
      - Next, you need to have a line in the Dockerfile to copy the program folder to containers WORKDIR, which is also
        defined in the Dockerfile. This allows the container to know where to look for the installed programs.

    Container startup program
     - When you run a Docker image, the container either starts to execute the program defined in 'ENTRYPOINT', or it looks
       for a program whose name is passed in as an argument.
     - If an 'ENTRYPOINT' is defined, the following command will run the default startup program:
        docker run <image>

     - Alternatively, a specified program can be executed with the following command:
        docker run <image> <startup program>

     Train Only image
       - if you are building an image for training only, you only need to have a 'train' file in the program folder.
       - The 'train' can also be set as the 'ENTRYPOINT', and the image starts with the train program by default for training service.
     Hosting Only image
       - if you are building an image for hosting only, files to be included in the program folder are 'nginx.conf', 'predictor.py',
         'serve', and 'wsgi.py'. The 'serve' can be set as the 'ENTRYPOINT', and the image would start the serve program by default.
     Train and Hosting image
       - If you are building a single image for both training and hosting purpose, you dont define a default startup program
         as an 'ENTRYPOINT'.
       - Instead, Amazon SageMaker runs the image by using one of the two following commands. Then the container starts programs
         for training or hosting service accordingly.

            docker run <image> train
              - This means that your Docker image should have an executable file in it that is called 'train'.
              - You will modify this program to implement your training algorithm.

            docker run <image> serve
              - This means that your Docker image should have an executable file in it that is called 'serve'.
              - This launches a RESTful API to serve HTTP requests for inference
              - In most Amazon SageMaker containers, serve is simply a wrapper that starts the inference server.

              Using Flask
              - If you opt to run Flask for your RESTful microservice, you can use the following files from the Amazon SageMaker
                sample notebooks as-is:
                 - serve
                 - wsgi.py
                 - nginx.conf
              - The only file that you need to consider modifying for hosting service is the 'predictor.py'.
              - The inference code is in 'predictor.py', and it implements the 'Flask app' for serving predictions.


    /opt/ml/
      - At runtime, Amazon SageMaker injects external model artifacts, training data, and other configuration information
        available to Docker containers in /opt/ml/.
      - This is also where Amazon SageMaker processes model artifacts, and where program output you wish to access outside
        of the Docker container is located. Knowing this will be helpful when you write your training and inferencing code.

          /opt/ml/
              input/
                  config/
                  data/
              model/
              output/
                  failure/

      - It is especially important to know the correct training input location (/opt/ml/input/data/) and model location
        (/opt/ml/model/) for your program because it aids in local testing and debugging.

dockerfile CMD vs ENTRYPOINT

  AI Overview
    - Both CMD and ENTRYPOINT instructions in a Dockerfile are used to define the command that gets executed when a container
      starts, but they have distinct differences:
  CMD:
    Purpose: Sets the default command to run when the container starts.
    Overridable: Yes, you can override the CMD command by providing a different command when running the container using docker run.
    Example:
       CMD ["echo", "Hello World"]

     - If you run 'docker run image_name ls -l', the echo command will be overridden, and the container will execute 'ls -l' instead.

  ENTRYPOINT:
    Purpose: Sets the main command for the container, which can be thought of as the container's executable.
    Overridable: Not directly, but you can pass arguments to the ENTRYPOINT command using docker run.
    Example:
      ENTRYPOINT ["/bin/bash"]

      - If you run 'docker run image_name -c "echo Hello World"', the container will execute '/bin/bash -c "echo Hello World"'.

------------------------------------------------------
AWS Certified Machine LearningSpecialty (MLS-C01) Practice Exam
  https://practice-exam.acloud.guru/aws-certified-machine-learning-specialty?_ga=2.209185174.522091785.1737764093-277800900.1727277947
  01/27/25 retry 2

  -> need to review Question 1
  -> need to review Question 8
  -> need to review Question 18
  -> need to review Question 21
  -> need to review Question 23
  -> need to review Question 33


Question 8

  You work for a team that has a model being used in production, for which the data it is sent to perform inferences on is coming
  from a different source. The model was built to work well for cleaned data inputs. How do you ensure that the models performance
  in production will be similar?

Choose 2:
  Never allow input data for a production model come from another data source.

  Ensuring that the data is accurate for data inputs and training data.                                          <-- Correct Answer

  Review counts, data durations, and the precision of the data inputs compared to training data.                 <-- Correct Answer

  Ensure bias is introduced to the data being used in production since it is from another data source.

  Create a Lambda function that replaces missing values with the mean value on the data source before it
    is used in production.

  Use Data Pipeline workflows to compare the data source and the data used to train the model.                   <-- Incorrect Answer

Answer Info:

  Comparing counts lets you identify, track, and highlight data loss, and test against what seems reasonable.
  Reviewing data duration lets you determine what time period each dataset is for.
  Quantify precision by comparing the mean, median and standard deviation of the data source and the data used to train the model.
  Calculate the number or percentage of outliers.
  For lower dimensional data or key variables, boxplots can provide a quick visual assessment of reasonableness.


Notes:

  AI Overview

    - To ensure the performance of a machine learning model on AWS, you can monitor key metrics like review counts, data durations,
      and the similarity between inference data inputs and the training data, primarily using services like Amazon SageMaker Model
      Monitor, which helps detect data drift and potential degradation in model accuracy over time by comparing new data to the
      original training data.

  Key points about these monitoring aspects:

    Review Counts:
      - Regularly reviewing predictions generated by the model, especially for edge cases or high-impact scenarios, can identify
        areas where the model might be performing poorly and require further tuning or data analysis.

    Data Durations:
      - Analyzing how long data is being used for training and inference can indicate potential issues like concept drift, where
        the underlying patterns in the data may have changed over time, requiring model retraining with newer data.

    Precision of Inference Data:
      - Comparing the data used for inference to the training data is crucial to ensure consistency in data format and
        processing, which helps maintain model accuracy.

  How AWS facilitates this monitoring:

    Amazon SageMaker Model Monitor:
      - This service automatically tracks key metrics and alerts you when significant deviations from the expected behavior
        are detected, including data drift analysis, which compares the distribution of new data to the training data.

    Data Quality Checks:
      - Implementing data quality checks before feeding data into the model, both during training and inference, helps
        identify inconsistencies and potential problems.

    Feature Engineering and Preprocessing:
      - Ensuring consistent feature engineering and data preprocessing pipelines for both training and inference data is
        critical for maintaining model accuracy.

   Benefits of monitoring these aspects:
     Early Detection of Issues:
       - By monitoring model performance with these metrics, you can quickly identify potential problems and take corrective
         actions before they significantly impact the model's accuracy.
     Improved Model Reliability:
       - Regularly reviewing and updating models based on data drift and performance monitoring helps maintain trust in the
         model's predictions.
     Optimized Resource Utilization:
       - By understanding how data is being used, you can optimize your model training and inference processes, potentially
         reducing costs.

Question 21

  You have been tasked with determining whether a given dataset has anomalous data associated with it. Which algorithm is
    a good fit and how can you ensure incorrectly detected anomalies are minimized?

Choose 2:
  Use QuickSight and the ML-Powered Anomaly Detection built-in feature                                             <-- Incorrect Answer

  Random Cut Forest (RCF) algorithm and increase/decrease the num_samples_per_tree hyperparameter                  <-- Correct Answer

  Random Cut Forest (RCF) algorithm and increase/decrease the num_trees hyperparameter                             <-- Correct Answer

  Principal Component Analysis (PCA) algorithm and decrease the mini_batch_size hyperparameter

  Principal Component Analysis (PCA) algorithm and increase the mini_batch_size hyperparameter

Answer Info:

  The Random Cut Forest algorithm is an unsupervised learning algorithm for detecting anomalous data points.
  The hyperparameter, num_trees, sets the number of trees used in the RCF model.

Notes:

  Quicksight ML-Powered Anomaly Detection built-in feature does NOT have hyperparameters to tune nor other tuning options


  AI Overview

  Key strategies to minimize false positives with SageMaker RCF:

    Hyperparameter tuning:
      Number of trees:
        - Experiment with different values for "num_trees" to find the optimal balance between accuracy and computation time.
        - A larger number of trees can improve detection accuracy but might also lead to overfitting.
        Samples per tree:
          - Adjusting "num_samples_per_tree" controls how much data is used to build each tree, impacting the model's sensitivity
            to outliers.
        Feature engineering:
          - Select relevant features that are most likely to contain anomalies and consider feature scaling to normalize the
            data distribution.

   RCF Model Tuning

   Metrics Computed by the RCF Algorithm
     - The RCF algorithm computes the following metric during training.
     - When tuning the model, choose this metric as the objective metric.

     test:f1
      - F1-score on the test dataset, based on the difference between calculated labels and actual labels.
      - Optimization Direction: Maximize

   Tunable RCF Hyperparameters

         Parameter Name 	Parameter Type 	             Recommended Ranges
         ---------------------  ----------------------       -----------------
         num_samples_per_tree 	IntegerParameterRanges       MinValue: 1, MaxValue:2048
         num_trees 	        IntegerParameterRanges       MinValue: 50, MaxValue:1000

Question 41

  Your model consists of a linear series of steps using LDA, Random Cut Forest and a scikit-learn step. What is the
  most efficient way to deploy this model?

Choose 1:
  Use AWS Glue to chain the transform steps together.

  Use AWS Batch to chain the steps together.

  Use inference pipelines to chain the steps together.                                             <-- Correct Answer


  Use AWS Step Functions to chain the steps together.

  Use Data Pipeline to chain the steps together.                                                   <-- Incorrect Answer

Answer Info:

  For this situation, the most efficient way of creating a chained series of algorithm steps is to use SageMaker Inference Pipeline.
  It handles transitioning the data to the next operation in the chain and deploys the containers on the same EC2 instance for
  efficiency. Deploy an Inference Pipeline - Amazon SageMaker

Note:

  Inference pipelines in Amazon SageMaker AI
    https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html

    - An inference pipeline is a Amazon SageMaker AI model that is composed of a linear sequence of two to fifteen containers
      that process requests for inferences on data.
    - You use an inference pipeline to define and deploy any combination of pretrained SageMaker AI built-in algorithms and your
      own custom algorithms packaged in Docker containers.
    - You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks.
    - Inference pipelines are fully managed.

    - You can add SageMaker AI Spark ML Serving and scikit-learn containers that reuse the data transformers developed for
      training models.
    - The entire assembled inference pipeline can be considered as a SageMaker AI model that you can use to make either
      real-time predictions or to process batch transforms directly without any external preprocessing.

    - Within an inference pipeline model, SageMaker AI handles invocations as a sequence of HTTP requests.
    - The first container in the pipeline handles the initial request, then the intermediate response is sent as a request
      to the second container, and so on, for each container in the pipeline. SageMaker AI returns the final response to the client.


Question 57

  You are designing a security approach for ensuring only members of certain projects can access notebook instances for their
  own projects. DevTeam1 should only access Project1 notebooks while DevTeam2 should only access Project2 notebooks.
  What is a valid way to implement this restriction?

Choose 1:
  Use the ResourceTag condition and add a Project tag to each notebook.                                     <-- Correct Answer

  Created shared accounts for the DEV teams and record all activities using CloudTrail.
    Use CloudTrail alarms to notify if a team access a notebook they are not authorized to access.

  Implement Federation using LDAP and SAML over SSH. Ensure that all DEV team members use MFA               <-- Incorrect Answer
    upon each attempt to access their respective project notebooks.

  Create a VPC Gateway Endpoint and route all traffic from each team member to only the S3 buckets
    containing their respective project models.

Answer Info:

  Of the given options, the one that makes the most sense is using a ResourceTag condition attached to the respective
  Dev team groups.
  Then you would add a Project tag to the notebook instance indicating it's project designation.
  These two items together would allow you to restrict notebook instances to only those in the respective DEV team group.
  Using Identity-based Policies (IAM Policies) for Amazon SageMaker - Amazon SageMaker

Note:

  Controlling access to AWS resources using tags
    https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html

    You can then create an IAM policy that allows or denies access to a resource based on that resource's tag.
      In that policy, you can use tag condition keys to control access to any of the following:

      Resource
         Control access to AWS service resources based on the tags on those resources.
        - To do this, use the aws:ResourceTag/key-name condition key to determine whether to allow access to the resource
          based on the tags that are attached to the resource.

      Request
         Control what tags can be passed in a request.
        - To do this, use the aws:RequestTag/key-name condition key to specify what tag key-value pairs can be passed
          in a request to tag an AWS resource.

      Any part of the authorization process
         Use the aws:TagKeys condition key to control whether specific tag keys can be in a request.


Question 1

  In AWS SageMaker automatic hyperparameter tuning, which of the following methods are supported?

Choose 4:
  Stochastic search

  Hyperband                                                                                         <-- Correct Answer

  Grid search                                                                                       <-- Correct Answer

  Bayesian optimization                                                                             <-- Correct Answer

  Random search                                                                                     <-- Correct Answer

Answer Info:

  'Hyperband' is a dynamic tuning strategy in SageMaker that reallocates resources based on the performance of training jobs.
  It combines elements of other methods, using both intermediate and final results to prioritize promising hyperparameter configurations.
  This approach aims to achieve a balance between computational efficiency and the accuracy of the tuned model.

  'Grid search' in Amazon SageMaker methodically explores combinations of hyperparameter values from specified categorical ranges.
  It's thorough and evaluates every possible combination within the given categorical parameters.
  This exhaustive approach can be computationally intensive, especially when dealing with large hyperparameter spaces.

  'Bayesian optimization' in SageMaker treats hyperparameter tuning as a regression problem, predicting which combinations
  might yield the best results. Using a statistical model, it intelligently selects hyperparameter values based on previous
  evaluations, aiming for efficiency and accuracy. It's a sophisticated method that balances exploration and exploitation
  of the hyperparameter space.

  'Random search' in SageMaker samples hyperparameter values randomly from specified ranges. This method is less computationally
  demanding than grid search, offering a quicker but potentially less optimal exploration of the hyperparameter space.
  It's a balance between computational efficiency and the chance of finding the best values.



Question 23

  You have setup autoscaling for your deployed model using SageMaker Hosting Services. You notice that in times of heavy
  load spikes, it takes a long time for the hosted model to scale out in response to the load. How might you speed up the
  autoscaling process?

Choose 1:
  Change the timeout in the auto-scaling Lambda function.

  Change the scale metric from InvocationsPerInstance to MemoryUtilization.

  Reduce the cooldown period for automatic scaling.                                                     <-- Correct Answer

  Disable CloudWatch advanced tracking metrics.

  Create a new target metric based on time since last scale event.

Anwser Info:

  When scaling responsiveness is not as fast as you would like, you should look at the cooldown period.
  The cooldown period is a duration when scale events will be ignored, allowing the new instances to become established and take on load.
  Decreasing this value will launch new variant instance faster.
  Automatically Scale Amazon SageMaker Models - Amazon SageMaker

#############################################################################################
Other Practice Exams
#############################################################################################
------------------------------------------------------
Official Practice Question Set: AWS Certified Machine Learning - Specialty (MLS-C01 - English) v2
01/28/25


Question 1

A company has been summarizing insights from customer reviews for years. A machine learning (ML) specialist must use ML to automate this task. The ML specialist grouped the reviews by product. The company wants to use the output of the ML specialist's initial model to prioritize the order of products and features to change.

Which algorithm should the ML specialist use to train the model?

seq2seq


Correct. The SageMaker seq2seq algorithm is a supervised learning algorithm that transforms a sequence of elements into another sequence. The seq2seq algorithm works well for summarizing the text in the reviews. This algorithm is commonly used for text summarization.

For more information about the seq2seq algorithm, see Sequence-to-Sequence Algorithm.

Question 2

A machine learning (ML) specialist has a large number of voice recordings that are stored in Amazon S3. The voice recordings are in English and need to be grouped by their conversation topics.

What should the ML specialist do to meet this requirement with the LEAST amount of effort?

Transcribe -> Comprehend


Question 3:

A company is setting up a system to manage all of the datasets that it stores in Amazon S3. The company wants to automate running transformation jobs on the data and maintaining a catalog of the metadata about the datasets.

Which solution will meet these requirements with the LEAST operational overhead?

AWS Glue Crawler -> AWS Glue ETL

Question 4:



A real estate company wants to create a machine learning (ML) model to predict housing prices based on a historical dataset. The dataset contains 32 features.

Which algorithm will meet these requirements?

Linear Regression


Question 5



A company is building a website that offers a variety of comedy content for adults and children. The company intends to automate the process of ingesting the content and tagging the content as safe for viewing by children as the positive class. The company's top priority is to avoid showing inappropriate content to children.

What is the MOST relevant metric for the company to use to evaluate the machine learning (ML) model for this task?

Precision

Correct. The precision metric is the ratio of true positives to the sum of true positives and false positives. Accordingly, it is the most relevant metric when the goal is to minimize false positives.

Question 6:



A machine learning (ML) specialist is training a model by using a supervised learning algorithm. The ML specialist split the dataset to use 80% of the data for training and 20% of the data for testing. While evaluating the model, the ML specialist discovers that the model is 97% accurate for the training dataset and 75% accurate for the test dataset.

Which action should the ML specialist take?

Change the hyperparameters to reduce the overfitting of the model. Retrain the model

Question 7


A company operates a fleet of vehicles. A business analyst wants to improve maintenance plans for the vehicles. The business analyst has very little experience with machine learning (ML). The business analyst has gathered a dataset with 500,000 measurements of various vehicle sensors during normal operations and during failures. The sensor data is stored as a set event in a time series. In the dataset, 98% of the samples represent normal operations and 2% of the samples represent failures.

Which action should the business analyst take to generate accurate maintenance predictions?


Use Sagemaker Canvas to prepare the data and train the model


Correct. SageMaker Canvas is a service that you can use to create ML models without having to write code. You can use SageMaker Canvas to build a custom model trained with your data. SageMaker Canvas can perform no-code data preparation, feature engineering, algorithm selection, training and tuning, inference, continuous model monitoring, and other tasks. SageMaker Canvas can generate models that are accurate, even when datasets are highly imbalanced. Because this scenario requires time series forecasting to generate predictions, SageMaker Canvas is a suitable option to build the custom model.

Question 8


A machine learning (ML) specialist is optimizing a solution to define whether online payment transactions are fraudulent. The historical data of manually classified transactions includes the following data:

    customer name (string)
    customer type (integer)
    transaction amount (float)
    customer tenure (integer)
    transaction type (string) with values "normal" or "abnormal"

Which action should the ML specialist take to meet the requirements?

Drop customer name and change the transaction type from string to numeric category. Launch training phase


Question 9

A machine learning (ML) specialist is setting up an ML environment that multiple data scientists will access. The ML specialist is deploying one Amazon SageMaker notebook instance for each data scientist. The ML specialist must ensure that each data scientist has access to only their personal notebook instance.

What should the ML specialist do to meet this requirement?


Correct. You can define an IAM policy to allow specific access to SageMaker resources.

Question 10

A company is building a fraud-detection model. Currently, the company does not have a sufficient amount of information because of a low number of fraud cases.

Which method will improve the accuracy of the model?

Oversampling by using SMOTE

Correct. SMOTE is a method of oversampling that creates synthetic samples for the minority class. If you have a dataset that is not fully populated, you can use SMOTE to add new information by adding synthetic data points to the minority class. SMOTE adds more diversity to the dataset. Therefore, this method helps to reduce overfitting and enhances the model's accuracy.


Note: need to understand 'oversampling by using bootstrapping

Incorrect. Oversampling by using bootstrapping can create additional samples by resampling with replacements from the existing minority class. The existing minority class in this scenario are the fraud cases. This method can increase the number of fraud cases and can solve the imbalance issue. However, this method does not add new data points. Therefore, this method does not provide enrichment. Oversampling by using bootstrapping only repeats some minority class members without adding any new information.



Question 11

A machine learning (ML) specialist is setting up an ML pipeline. The objective is to enable the ETL part of the pipeline to activate ML training jobs in Amazon SageMaker. Specifically, the ML specialist intends to use batch jobs for ETL. The solution must integrate with SageMaker without the use of additional services.

Which solution will meet these requirements?

Use AWS Glue for the ETL

Correct. AWS Glue integrates directly with SageMaker. With AWS Glue, completed ETL jobs can start ML jobs in SageMaker.




Question 12


A machine learning (ML) specialist is retraining a new version of a model that is already in production. The model is deployed as an endpoint in Amazon SageMaker. When the retraining is complete, the ML specialist will test the new version of the model before removing the existing production model. The ML specialist's objective is to test the new model at the same time that the existing model is handling the majority of the requests. The deployment must result in minimum disruption to the users of the endpoint.

Which deployment will meet these requirements with the LEAST operational overhead?

Update the endpoint  configuraiton of the existing endpoint to include the new model as a ProductionVariant API call.
Set the InitialVariantWeight for the new model to be a small percentage of the original ProductionVariant VariantWeight


Question 13


A digital newspaper owns a large collection of articles and human-written summaries that are associated with these articles. The summaries are used as headers for each article that is posted online. The newspaper editors want a solution that produces summaries automatically. A machine learning (ML) specialist needs to automate the process of summary generation.

Which solution will meet this requirement?

D. Apply a seq2seq recurrent neural network (RNN)

Correct. The seq2seq algorithm decodes and encodes sequences of tokens, such as words. The seq2seq algorithm is appropriate for article summarization.

Incorrect. Word2vec is a text classification algorithm. Word2vec is useful for sentiment analysis, entity recognition, and translation. Word2vec is not useful for summarization.

Incorrect. An NTM will create topics, not natural language summaries.

Incorrect. Latent Dirichlet Allocation (LDA) is a categorization algorithm. LDA is not appropriate for summarization.

-----------

Seq2seq:
   -> Think translation, e.g. language translation, article summation (full articule -> translated to -> summary),
      translate speech to text
AWS Certified Machine Learning - Specialty (MLS-C01): Modeling
3.15 Examining SageMaker's Sequence-to-Sequence (Seq2Seq) Algorithm


  SageMaker sequence-to-sequence (Seq2Seq) algorithm Overview:
    - a supervised learning algorithm that uses a neural network architecture where a sequence of input
      tokens is transformed to another sequence of tokens as output.
    - Example applications include: machine translation (input a sentence from one language and predict what that
      sentence would be in another language), text summarization (input a longer string of words and predict a shorter
      string of words that is a summary), speech-to-text (audio clips converted into output sentences in tokens).

  Sequence-to-sequence algorithm's layers
     embedding layer
       - the encoded input tokens are mapped to a dense feature layer
       - It is a standard practice to initialize this embedding layer with pre-trained word vector,
         like FastText, and learn the parameters during the training process.
     encoder layer
       - compresses the input into a fixed-length feature vector.
       - Typically, an encoder is made of RNN network, like LSTM or GRU.
     decoder layer
       - converts the encoder feature to an output sequence of tokens.
       - This layer also is typically built with RNN architecture.


  Sagemaker Sequence-to-Sequence (Seq2Seq) algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html
    Learning Type:
      - supervised learning algorithm that specializes in language processing.
    File/Data Types:
      - training, test, and validation states expects data in RecordIO-Protobuf format
      - protobuf and JSON supported during the inference stage.
    Instance Type:
      - single GPU instances

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html
      No required hyperparameters

    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-tuning.html
      validation:accuracy
         - Accuracy computed on the validation dataset.
         - goal: Maximize
      validation:bleu
        - BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has
          been machine-translated from one natural language to another.
        - BLEU score computed on the validation dataset.
        - Because BLEU computation is expensive, you can choose to compute BLEU on a random subsample of the validation
          dataset to speed up the overall training process.
        - Use the bleu_sample_size parameter to specify the subsample.
        - goal: Maximize
      validation:perplexity
        - perplexity is a loss function computed on the validation dataset.
        - Perplexity measures the cross-entropy between an empirical sample and the distribution predicted by a model
          and so provides a measure of how well a model predicts the sample values,
        - Models that are good at predicting a sample have a low perplexity.
        - goal: Minimize

  Sequence-to-Sequence (Seq2Seq) Business use cases
    language translations
      - to translate it in sequence of words from one language to another language.
    speech-to-text conversion.
      - given an audio vocabulary, you can then predict the textual representation of those spoken words.
    code generation and auto completion
      - assist developers by generating code snippets or completing the code based on content.


-----------

Question 14


A machine learning (ML) specialist is building a new recommendation engine. The ML specialist wants to test multiple models by using live data in a beta environment where customers will interact with the model. Based on these interactions, the ML specialist will compare models by using A/B testing. The ML specialist then will select and deploy the best model.

What is the MOST operationally efficient way to test the multiple model variants?

Correct. SageMaker supports the deployment of multiple models, known as production variants, to a single SageMaker endpoint. You configure the production variants so that a small portion of the live traffic goes to the model that you want to validate. You can collect statistics about model effectiveness and change weightings with simple calls to the SageMaker service.


Question 15

A manufacturing company wants to deploy a solution that can identify the size of packages as they are passed down manufacturing lines in its factories. When a package is identified, the manufacturing line will route the package down one of five separate secondary lines for quality assurance and shipping.

The company needs to train and deploy a machine learning (ML) model to accomplish these goals. The company has created and made available thousands of hours of training videos that can be analyzed anywhere and at any time. Many of the factories are in remote areas around the world where internet connectivity is not always guaranteed.

Which solution will meet these requirements?

Correct. The Object Detection algorithm is appropriate for training the recognition. AWS IoT Greengrass that uses Lambda will run within the factory to solve any connectivity issues.


Question 16


A machine learning (ML) specialist wants to create a data preparation job that uses a PySpark script to prepare data for training and testing. The script includes complex window aggregation operations. The ML specialist needs to evaluate the effects of the number of features and the sample count on model performance.

Which approach should the ML specialist use to determine the ideal data transformations for the model?


Add Sagemaker Experiments tracker to the script to capture parameters. Run the script as a Sagemaker job

Correct. SageMaker Experiments is a feature of SageMaker that you can use to organize, track, compare, and evaluate ML experiments. SageMaker Experiments is able to capture artifacts, parameters, and metrics. SageMaker Experiments can quickly revisit the origins of a model when you are troubleshooting issues in production or auditing your models for compliance verifications. Additionally, SageMaker processing jobs support PySpark, which is required in this scenario.

For more information about SageMaker Experiments, see
https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html


Question 17

A company has 1,000 sentences with sentiments categorized as positive, neutral, or negative.

Which algorithm should a machine learning (ML) specialist select for training a baseline sentiment model?

SageMaker K-NN

Correct. The SageMaker k-NN algorithm is an index-based algorithm that is used for classification and regression. The scenario asks you to categorize sentences into sentiments, which requires a classification method. Additionally, k-NN can handle multi-class scenarios, which is required to categorize sentiments as positive, neutral, or negative.


-----------
A Cloud Guru: Machine Learning on AWS Deep Dive

4.6 Using SageMaker Experiments To Manage Training Jobs

  Cost: $1 USD

  Machine Learning is an Iterative Process

         Training  -------------> Evaluation ---------------> Prediction
             |                        |
             |            Do we need to change the algorithm?
             |            Do we need to do more feature engineering?
             |            Do we need new or different data?
             |                        |
             |------------------------|

    - over time, you may have thousands of similar training jobs

  Why SageMaker Experiments
    - a capability of SageMaker thats lets you organize, track, compare, and evaluate your
      ML experiments
    - Created through the SageMaker Python SDK
    - track and visualize [the experiments] through the SageMaker UI

  The Experiment Hierarchy
     - an Experiment is a collection of 1 or more Runs
     - a "Run" consists of all the inputs, parameters, configurations for one iteration of training of the model

                      Experiment
                            |
           |-------------|---------|-|-|-------|
           |             |         | | |       |
          Run 1         Run 2      . . .    Run N


  Creating an Experiment through the Python SDK
    - reviewing code in the SageMaker Studio Notebook
         - Dataset using MNIST Dataset with Keras
         - The MNIST database (Modified National Institute of Standards and Technology database[1]) is a large
           database of handwritten digits that is commonly used for training various image processing systems
    - creating multiple runs

-----------

Amazon SageMaker Experiments in Studio Classic
   https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html

   - Experiment tracking using the SageMaker Experiments Python SDK is only available in Studio Classic.
   - We recommend using the new Studio experience and creating experiments using the latest SageMaker AI integrations with MLflow.

   - Amazon SageMaker Experiments Classic is a capability of Amazon SageMaker AI that lets you create, manage, analyze,
     and compare your machine learning experiments in Studio Classic.
   - Use SageMaker Experiments to view, manage, analyze, and compare both custom experiments that you programmatically
     create and experiments automatically created from SageMaker AI jobs.

-----------
Machine learning experiments using Amazon SageMaker AI with MLflow
  https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html

  - Amazon SageMaker AI with MLflow is a capability of Amazon SageMaker AI that lets you create, manage, analyze, and
    compare your machine learning experiments.
  - Use MLflow with Amazon SageMaker AI to track, organize, view, analyze, and compare iterative ML experimentation to gain
    comparative insights and register and deploy your best performing models.

  MLflow integrations
    - Use MLflow while training and evaluating models to find the best candidates for your use case.
    - You can compare model performance, parameters, and metrics across experiments in the MLflow UI, keep track of your
      best models in the MLflow Model Registry, automatically register them as a SageMaker AI model, and deploy registered
      models to SageMaker AI endpoints.

    Amazon SageMaker AI with MLflow
      - Use MLflow to track and manage the experimentation phase of the machine learning (ML) lifecycle with AWS integrations
        for model development, management, deployment, and tracking.
    Amazon SageMaker Studio
      - Create and manage tracking servers, run notebooks to create experiments, and access the MLflow UI to view and
        compare experiment runs all through Studio.
    SageMaker Model Registry
      - Manage model versions and catalog models for production by automatically registering models from MLflow Model
        Registry to SageMaker Model Registry.
    SageMaker AI Inference
      - Prepare your best models for deployment on a SageMaker AI endpoint using ModelBuilder.
-----------


Question 18


An insurance company needs to automate claim compliance reviews because human reviews are expensive and error-prone. The company has a large set of claims and a compliance label for each. Each claim consists of a few sentences in English, many of which contain complex related information.

The company wants to use Amazon SageMaker built-in algorithms to design a machine learning (ML) supervised model. The ML model must be trained to read each claim and predict if the claim is compliant or not. The solution must extract features from the claims to be used as inputs for the downstream supervised task.

Which solution will meet these requirements?

D. Apply Object2Vec to the claims in the train set. Send the derived feature space as inputs for the downstream supervised task.

Correct. SageMaker Object2Vec generalizes the Word2Vec embedding technique for words to more complex objects, such as sentences and paragraphs. The supervised learning task is at the level of whole claims, for which there are labels. However, no labels are available at the word level. Therefore, you need to use Object2Vec instead of Word2Vec.

--------

AWS Certified Machine Learning - Specialty (MLS-C01): Modeling

3.11 Discovering SageMaker's Object2Vec Algorithm `


  Object2Vec algorithm Overview
    - It is a highly customizable neural embedding algorithm that can be used to create vector representations of objects.
    - The objects can be anything such as words, sentences, or abstract entities like users or products.

  Object2Vec algorithm analogy
    - Imagine you are a librarian and your task is to organize a large number of books so that the readers
      can easily find them.
    - Each book has many attributes like genre, author, publication year, and so on.
    - Similarly, the readers have favorite genre, favorite authors, and preferences based on past reading habits.

    - The books are like the objects that this algorithm deals with.  Initially, the books may be placed in
      random order, but eventually you decide to create groups of books that are similar in any of the choose and attribute.
    - You mentally map out where each book should go, so similar books are placed together on the shelves.
    - This mental map is similar to creating an embedding space to organize the objects.
    - As readers start borrowing books, you start observing a pattern that some readers often borrow books from
      specific clusters, and based on this observation, you update the mental map, which makes it even easier for them.
    - This is like iterative adjustment of embeddings to better reflect the relationship between the objects.

  SageMaker Object2Vec algorithm analogy
    - During the training process, the algorithm accepts pairs of objects and the relationship labels as inputs.
      - For example, in a typical recommendation system, these pairs could be user and item.
      - These pairs are associated with labels indicating the nature of the relationships, whether the user
        liked or disliked the item.
    - Each object is initially presented as a random vector.
    - The goal is to adjust these vectors such that the objects with similar relationships are closer together.
    - Object2Vec uses a neural network to understand and learn embeddings.
    - For each object pair the neural network process their embeddings and predicts their relationships.
    - The predictor relationship is compared to the actual relationship using the loss function.
    - The error is then propagated back through the network, and the embeddings are updated to minimize the loss.
    - This process iteratively refines the embeddings to better capture the relationships between the objects.


  Sagemaker Object2Vec algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html
    Learning Type:
      - general-purpose neural embedding algorithm
    File/Data Types:
      - This algorithm expects the data to be provided in a sentence -sentence pair, label-sentence pair.
        and other pairs
          Sentence-sentence pairs
	    "A soccer game with multiple males playing." and "Some men are playing a sport."
          Labels- sequence pairs
	    The genre tags of the movie "Titanic", such as "Romance" and "Drama", and its short description:
            "James Cameron's Titanic is an epic, action-packed romance set against the ... of April 15, 1912."
          Customer-customer pairs
            The customer ID of Jane and customer ID of Jackie.
          Product-product pairs
            The product ID of football and product ID of basketball.

      - the data must be pre-processed and transformed into supported formats.
      - For training, the data must be in jsonlines format,
      - for inference, the data format must be in JSON or jsonlines format.
    Instance Type:
      - Amazon recommends CPU and GPU instances for training purposes.

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html
      required hyperparameters
        enc0_max_seq_len
          - The maximum sequence length for the enc0 encoder.
          - Valid values: 1  integer  5000
        enc0_vocab_size
          - The vocabulary size of enc0 tokens.
          - Valid values: 2  integer  3000000
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-tuning.html
      - This algorithm reports means square error (mse) for any regression tasks.
      - For classification tasks, it reports accuracy and cross entropy.

  Object2Vec Business use cases
     user behavior analysis
       - for creating detailed user profiles based on their interactions and behavior, which can be used
         for personalized marketing.
     natural language processing (NLP)
        - to detect spam and perform sentiment analysis.
     social network analysis
        - to identify groups of users with similar interests or behavior, which can be used for targeted advertising.


--------

Question 19

A city government wants to track cars in many parking lots across the city. The parking lots are all equipped with video cameras that stream video over an isolated camera network by using real-time streaming protocol (RTSP). Because of government regulations, no access to the internet is allowed from the camera network. The city wants to use machine learning (ML) to identify license plates to automate the parking payment process. The city also wants to integrate license plate information with its line of business application.

Which solution will meet these requirements with the LEAST operational overhead?

Connect AWS Panorama Appliance to the camera network to process RTSP video streams. Train Sagemaker Computer
Vision model to identify license plate information. Deploy the SageMaker model to AWS Panorama Appliance.

Correct. You can use AWS Panorama to add computer vision to an on-premises camera network. You can use SageMaker to manage ML training jobs and provision training resources with minimal operational overhead. An AWS Panorama Appliance integrates with existing camera networks and can read license plate information. You can also deploy applications and models that are trained with SageMaker to AWS Panorama Appliance. Then, the appliance can identify license plate information locally without the need to connect to the internet.

For more information about AWS Panorama Appliance, see Managing AWS Panorama Appliance.
https://docs.aws.amazon.com/panorama/latest/dev/panorama-appliance.html

--------

What is AWS Panorama?
  https://docs.aws.amazon.com/panorama/latest/dev/panorama-welcome.html

  - AWS Panorama is a service that brings computer vision to your on-premises camera network.
  - You install the AWS Panorama Appliance or another compatible device in your datacenter, register it with AWS Panorama,
    and deploy computer vision applications from the cloud.
  - AWS Panorama works with your existing real time streaming protocol (RTSP) network cameras.
  - The appliance runs secure computer vision applications from AWS Partners, or applications that you build yourself
    with the AWS Panorama Application SDK.

  Panaroma Appliance
    - The AWS Panorama Appliance is a compact edge appliance that uses a powerful system-on-module (SOM) that is optimized
      for machine learning workloads.
    - The appliance can run multiple computer vision models against multiple video streams in parallel and output the
      results in real time.
    - It is designed for use in commercial and industrial settings and is rated for dust and liquid protection (IP-62).

    - The AWS Panorama Appliance enables you to run self-contained computer vision applications at the edge, without
      sending images to the AWS Cloud.
    - By using the AWS SDK, you can integrate with other AWS services and use them to track data from the application over time.

   SageMaker AI
      You can use SageMaker AI to collect training data from cameras or sensors, build a machine learning model, and
       train it for computer vision.
     - AWS Panorama uses SageMaker AI Neo to optimize models to run on the AWS Panorama Appliance.

--------

Question 20

A machine learning (ML) specialist has more than 1 TB of objects that are stored in an Amazon S3 bucket. The objects are named with a subpath under a common S3 path. The ML specialist wants to group the objects for batch loading into an Amazon EMR cluster for processing.

Which solution will meet these requirements with the LEAST amount of effort?


use recursive partitionining in AWS Glue

Correct. AWS Glue is more than a standard ETL tool. AWS Glue automatically classifies data, creates schemas, and catalogs metadata.

For more information about partitioning input files in AWS Glue, see Reading Input Files in Larger Groups.
https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html

----------
[AWS Glue] Reading input files in larger groups
  https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html

  - You can set properties of your tables to enable an AWS Glue ETL job to group files when they are read from an Amazon S3 data store.
  - These properties enable each ETL task to read a group of input files into a single in-memory partition, this is
    especially useful when there is a large number of small files in your Amazon S3 data store.
  - When you set certain properties, you instruct AWS Glue to group files within an Amazon S3 data partition and set the size
    of the groups to be read.
  - You can also set these options when reading from an Amazon S3 data store with the create_dynamic_frame.from_options method.

  - To enable grouping files for a table, you set key-value pairs in the parameters field of your table structure.
  - Use JSON notation to set a value for the parameter field of your table.

  - You can use this method to enable grouping for tables in the Data Catalog with Amazon S3 data stores.

  groupFiles
    - Set 'groupFiles' to 'inPartition' to enable the grouping of files within an Amazon S3 data partition.
    - AWS Glue automatically enables grouping if there are more than 50,000 input files, as in the following example.

      'groupFiles': 'inPartition'

  groupSize
    - Set groupSize to the target size of groups in bytes.
    - The groupSize property is optional, if not provided, AWS Glue calculates a size to use all the CPU cores in the
      cluster while still reducing the overall number of ETL tasks and in-memory partitions.
    - For example, the following sets the group size to 1 MB.

      'groupSize': '1048576'

  recurse
    - Set 'recurse' to 'True' to recursively read files in all subdirectories when specifying paths as an array of paths.
    - You do not need to set recurse if paths is an array of object keys in Amazon S3, or if the input format is parquet/orc,
      as in the following example.

      'recurse':True

    - If you are reading from Amazon S3 directly using the create_dynamic_frame.from_options method, add these connection options.
    - For example, the following attempts to group files into 1 MB groups.

      df = glueContext.create_dynamic_frame.from_options("s3", {'paths': ["s3://s3path/"], 'recurse':True,
          'groupFiles': 'inPartition', 'groupSize': '1048576'}, format="json")

----------
------------------------------------------------------
Dojo AWS Certified Machine Learning Specialty MLS-C01 Sample Exam Questions
  https://tutorialsdojo.com/aws-certified-machine-learning-specialty-mls-c01-sample-exam-questions/


Question 1

A trucking company wants to improve situational awareness for its operations team. Each truck has GPS devices installed to monitor their locations.

The company requires to have the data stored in Amazon Redshift to conduct near real-time analytics, which will then be used to generate updated dashboard reports.

Which workflow offers the quickest processing time from ingestion to storage?

  1 Use Amazon Kinesis Data Stream to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.
  2 Use Amazon Managed Streaming for Apache Kafka (MSK) to ingest the location data. Use Amazon Redshift Spectrum to deliver the data in the cluster.
  3 Use Amazon Data Firehose to ingest the location data and set the Amazon Redshift cluster as the destination.
  4 Use Amazon Data Firehose to ingest the location data. Load the streaming data into the cluster using Amazon Redshift Streaming ingestion.


Answer info:

Correct Answer: 1

The Amazon Redshift Streaming ingestion feature makes it easier to access and analyze data coming from real-time data sources. It simplifies the streaming architecture by providing native integration between Amazon Redshift and the streaming engines in AWS, which are Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (Amazon MSK). Streaming data sources like system logs, social media feeds, and IoT streams can continue to push events to the streaming engines, and Amazon Redshift simply becomes just another consumer.


Before, loading data from a stream into Amazon Redshift included several steps. These included connecting the stream to Amazon Data Firehose and waiting for Data Firehose to stage the data in Amazon S3, using various-sized batches at varying-length buffer intervals. After this, Data Firehose initiated a COPY command to load the data from Amazon S3 to a table in Redshift.

Amazon Redshift Streaming ingestion eliminates all of these extra steps, resulting in faster performance and improved latency.


--------
[Redshift] Streaming ingestion to a materialized view
  https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html

  - Streaming ingestion provides low-latency, high-speed data ingestion from Amazon Kinesis Data Streams or Amazon Managed Streaming
    for Apache Kafka to an Amazon Redshift provisioned or Amazon Redshift Serverless database.
  - The data lands in a Redshift materialized view that's configured for the purpose.
  - This results in fast access to external data. Streaming ingestion lowers data-access time and reduces storage cost.
  - You can configure it for your Amazon Redshift cluster or for your Amazon Redshift Serverless workgroup, using a small
     collection of SQL commands.
  - After it's set up, each materialized-view refresh can ingest hundreds of megabytes of data per second.

--------

  Amazon Redshift Cheat Sheet
    https://tutorialsdojo.com/amazon-redshift/

    - A fully managed, petabyte-scale data warehouse service.
    - Redshift extends data warehouse queries to your data lake. You can run analytic queries against petabytes of data stored locally
      in Redshift, and directly against exabytes of data stored in S3.
    - RedShift is an OLAP (OnLine Analytics Processing) type of DB.

    - Currently, Redshift only supports Single-AZ deployments.

    Features
      - Redshift uses columnar storage, data compression, and zone maps to reduce the amount of I/O needed to perform queries.
      - It uses a massively parallel processing data warehouse architecture to parallelize and distribute SQL operations.
      - Redshift uses machine learning to deliver high throughput based on your workloads.
      - Redshift uses result caching to deliver sub-second response times for repeat queries.
      - Redshift automatically and continuously backs up your data to S3. It can asynchronously replicate your snapshots to S3 in another region for disaster recovery.

    RedShift Spectrum
      - Enables you to run queries against exabytes of data in S3 without having to load or transform any data.
      - Redshift Spectrum supports Enhanced VPC Routing.
      - If you store data in a columnar format, Redshift Spectrum scans only the columns needed by your query, rather than processing entire rows.
      - If you compress your data using one of Redshift Spectrums supported compression algorithms, less data is scanned.

    RedShift Streaming Ingestion
      - Allows you to consume and process data directly from a streaming source to a Redshift cluster using SQL.
      - Streaming ingestion eliminates the need for staging data in Amazon S3, which gives you a low-latency, high-speed ingestion.
      - Valid data source:
              - Amazon Kinesis Data Streams
              - Amazon Managed Streaming for Apache Kafka (MSK)
    Redshift ML
      - Allows you to train and deploy machine learning models using the data stored in your Amazon Redshift cluster through a simple
        CREATE MODEL SQL statement.

      - You can make in-database local inferences using SQL, eliminating the need to move data between Redshift and other storage services
        like Amazon S3.
      - Redshift ML uses Amazon SageMaker Autopilot behind the scenes to find the best model based on your input data.

--------

A Cloud Guru AWS Certified Solutions Architect Associate 2020 Class

5.6 Redshift

Resources: None

 - What is Redshift
   - a fast, fully, managed, petabyte-scale data warehouse service
   - AWS's data warehousing solution
   - can start for just $0.25/hour with no commitments or upfront costs,
   - can scale to petabytes or more for $1K per terabyte per year, less than 1/10th of most other
     data warehousing solutions
   - used for business intelligence and data warehousing in the cloud

 - OLTP (OnLine Transaction Processing) vs OLA (OnLine Analytics Processing)
   - OLAP example:
     - Net profit for EMEA and Pacific for the Digital Radio Product
     - OLAP pulls in large number of records to perform
   - Data warehousing databases use different type of architecture both from a database perspective
     and infrasture layer

 - Redshift configurations
    - can be configured as follows:
      - Single Node (160 GB)
      - Multi-node
        - Leader Node: manages client connections and receives queries
        - Compute Node: store data and perform queries and computations
        - Up to 128 Compute Nodes

 - Redshift Advanced compression:
  - columnar data stores can be compressed much more than row-based data because similar data is store sequentially on disk
  - employs multiple compression techniques
  - doesn't require indexes or materialized views, and s uses less space than traditional relation database systems
  - when loading data into an empty table, it automatically samples your data and selects the most appropriate
    compression scheme

 - Redshift Parallel Processing (MPP)
   - automatically distributes data and query load across all nodes
   - makes it easy to add nodes to your data warehouse allowing to maintain fast query as your data warehouse grows

--------

Question 2

A Machine Learning Specialist is training an XGBoost-based model for detecting fraudulent transactions using Amazon SageMaker AI. The training data contains 5,000 fraudulent behaviors and 500,000 non-fraudulent behaviors. The model reaches an accuracy of 99.5% during training.

When tested on the validation dataset, the model shows an accuracy of 99.1% but delivers a high false-negative rate of 87.7%. The Specialist needs to bring down the number of false-negative predictions for the model to be acceptable in production.

Which combination of actions must be taken to meet the requirement? (Select TWO.)

  1 Increase the model complexity by specifying a larger value for the max_depth hyperparameter.
  2 Increase the value of the rate_drop hyperparameter to reduce the overfitting of the model.
  3 Adjust the balance of positive and negative weights by configuring the scale_pos_weight hyperparameter.
  3 Alter the value of the eval_metric hyperparameter to MAP (Mean Average Precision).
  4 Alter the value of the eval_metric hyperparameter to Area Under The Curve (AUC).

Correct Answer: 3,5

Since the fraud detection model is a binary classifier, we should evaluate it using the Area Under the Curve metric. The AUC metric examines the ability of a binary classification model as its discrimination threshold is varied.

The scale_pos_weight hyperparameter allows you to fine-tune the threshold that matches your business need. In the scenario, the model has a high chance of outputting a high FNR (false-negative rate) due to a largely imbalanced dataset. You can fix that to reduce the predicted false-negatives by adjusting the scale_pos_weight.


Question 3

A manufacturing company wants to aggregate data in Amazon S3 and analyze it using Amazon Athena. The company needs a solution that can both ingest and transform streaming data into Apache Parquet format.

Which AWS Service meets the requirements?

  1 Amazon Data Streams
  2 AWS Batch
  3 Amazon Data Firehose
  4 AWS Database Migration Service


Correct Answer: 3

Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, MongoDB, and New Relic.

Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Data Firehose data transformation when you create your delivery stream. When you enable Data Firehose data transformation, Data Firehose buffers incoming data up to 3 MB by default. (To adjust the buffering size, use the  ProcessingConfiguration API with the ProcessorParameter called BufferSizeInMBs.)

Data Firehose then invokes the specified Lambda function asynchronously with each buffered batch using the AWS Lambda synchronous invocation model. The transformed data is sent from Lambda to Data Firehose. Data Firehose then sends it to the destination when the specified destination buffering size or buffering interval is reached, whichever happens first.


Question 4

A Data Scientist uses an Amazon Data Firehose stream to ingest data records produced from an on-premises application. These records are compressed using GZIP compression. The Scientist wants to perform SQL queries against the data stream to gain real-time insights.

Which configuration will enable querying with the LEAST latency?

  1 Transform the data with Amazon Kinesis Client Library and deliver the results to an Amazon OpenSearch cluster.
  2 Use a Kinesis Data Analytics application configured with AWS Lambda to transform the data.
  3 Use a streaming ETL job in AWS Glue to transform the data coming from the Firehose stream.
  4 Store the data records in an Amazon S3 bucket and use Amazon Athena to run queries.

Correct Answer: 2

You can configure your Amazon Kinesis Analytics applications to transform data before it is processed by your SQL code. This new feature allows you to use AWS Lambda to convert formats, enrich data, filter data, and more. Once the data is transformed by your function, Kinesis Analytics sends the data to your applications SQL code for real-time analytics.

Question 5

A financial company is receiving hundreds of credit card applications daily and is looking for ways to streamline its manual review process. The companys machine learning (ML) specialist has been given a CSV dataset with a highly imbalanced class.

The specialist must train a prototype classifier that predicts whether to approve or reject an application. The company wants the model to be delivered as soon as possible.

How can the ML specialist meet the requirement with the LEAST operational overhead?

  1 Upload the dataset to an Amazon S3 bucket. Create an Amazon SageMaker AutoPilot job and specify the bucket location as the source for the job. Choose the best version of the model.
  2 Upload the dataset to an Amazon S3 bucket. Use the built-in XGBoost algorithm in Amazon SageMaker to train the model. Run an automatic model tuning job with early stopping enabled. Select the best version of the model.
  3 Upload the dataset to an Amazon S3 bucket. Perform feature engineering on the data using Amazon SageMaker Data Wrangler. Train the model using the built-in XGBoost algorithm in Amazon SageMaker.
  4 Upload the dataset to an Amazon S3 bucket. Create an Amazon SageMaker Ground Truth labeling job. Select Text Classification (Single Label) as the task selection. Add the companys credit officers as workers.

Correct Answer: 1

Amazon SageMaker Autopilot is a feature of Amazon SageMaker that allows you to automatically train and tune machine learning models with minimal setup and no machine learning expertise required.

Autopilot automatically generates pipelines, trains, and tunes the best ML models for classification or regression tasks on tabular data while allowing you to maintain full control and visibility. The autopilot automatically analyzes the dataset, processes the data into features, and trains multiple optimized ML models.
-------

SageMaker Autopilot
  https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html

  Note: As of November 30, 2023, Autopilot's UI is migrating to Amazon SageMaker Canvas as part of the updated SageMaker Studio experience.

  - Amazon SageMaker Autopilot is a feature set that simplifies and accelerates various stages of the machine learning workflow
    by automating the process of building and deploying machine learning models (AutoML).

  - Autopilot performs the following key tasks that you can use on autopilot or with various degrees of human guidance:

      Data analysis and preprocessing:
        - Autopilot identifies your specific problem type, handles missing values, normalizes your data, selects features, and
          overall prepares the data for model training.

      Model selection:
        - Autopilot explores a variety of algorithms and uses a cross-validation resampling technique to generate metrics that
          evaluate the predictive quality of the algorithms based on predefined objective metrics.

      Hyperparameter optimization:
        - Autopilot automates the search for optimal hyperparameter configurations.

      Model training and evaluation:
        - Autopilot automates the process of training and evaluating various model candidates.
        - It splits the data into training and validation sets, trains the selected model candidates using the training data,
          and evaluates their performance on the unseen data of the validation set.
        - Lastly, it ranks the optimized model candidates based on their performance and identifies the best performing model.

      Model deployment:
        - Once Autopilot has identified the best performing model, it provides the option to deploy the model automatically by
          generating the model artifacts and the endpoint exposing an API.
        - External applications can send data to the endpoint and receive the corresponding predictions or inferences.


-------

A Machine Learning Specialist has graphed the results of a K-means model fitted through a range of k-values. The Specialist needs to select the optimal k parameter.

Based on the graph, which k-value is the best choice?

Note: Graph is y-axis: Distortion Score; x-axis: K-values; Elbow is graph is  between k-values of 4 and 5


  1 1.4
  2 2.9
  3 3.3
  4 4.6


Correct Answer: 4

The elbow method runs k-means clustering on the dataset for a range of values for k (e.g., 1-10), and then for each value of k computes an average score for all clusters. In this scenario, the distortion score is computed  the sum of square distances from each point to its assigned center.

When the resulting metrics for each model are plotted, it is possible to determine the best value for k visually. If the line chart looks like an armm, then the elbow (the point of inflection on the curve) is the best value of k. The arm can be either up or down, but if there is a strong inflection point, it is a good indication that the underlying model fits best at that point.

------------------------------------------------------

Specialty (MLS-C01) Sample Exam Questions - awsstatic.com
https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Sample-Questions.pdf&ved=2ahUKEwjL49Ph1JuLAxXoKkQIHcAoJ2oQFnoECBkQAQ&usg=AOvVaw0HndT83C7NvNpCVq__HWJK
  Machine Learning  Specialty (MLS-C01) Sample Exam Questions
     AWS-Certified-Machine-Learning-Specialty_Sample-Questions.pdf


4) A data scientist is working on optimizing a model during the training process by varying multiple
   parameters. The data scientist observes that, during multiple runs with identical parameters, the loss
   function converges to different, yet stable, values.
   What should the data scientist do to improve the training process?
   A) Increase the learning rate. Keep the batch size the same.
   B) Decrease the learning rate. Reduce the batch size.
   C) Decrease the learning rate. Keep the batch size the same.
   D) Do not change the learning rate. Increase the batch size.


answer info:
  4) B  It is most likely that the loss function is very curvy and has multiple local minima where the training is
     getting stuck. Decreasing the batch size would help the data scientist stochastically get out of the local minima
     saddles. Decreasing the learning rate would prevent overshooting the global loss function minimum.

5) A data scientist is evaluating different binary classification models. A false positive result is 5 times
more expensive (from a business perspective) than a false negative result.
The models should be evaluated based on the following criteria:
    1) Must have a recall rate of at least 80%
    2) Must have a false positive rate of 10% or less
    3) Must minimize business costs
After creating each binary classification model, the data scientist generates the corresponding confusion matrix.
Which confusion matrix represents the model that satisfies the requirements?
    A) TN = 91, FP = 9
       FN = 22, TP = 78
    B) TN = 99, FP = 1
       FN = 21, TP = 79
    C) TN = 96, FP = 4
       FN = 10, TP = 90
    D) TN = 98, FP = 2
       FN = 18, TP = 82

answer info:

  Recall = TP / (TP + FN)
  False Positive Rate (FPR) = FP / (FP + TN)
  Cost = 5 * FP + FN
                             A                       B                          C                         D
  Recall                78 / (78 + 22) = 0.78    79 / (79 + 21) = 0.79      90 / (90 + 10) = 0.9      82 / (82 + 18) = 0.82
  False Positive Rate    9 / (9 + 91) = 0.09      1 / (1 + 99) = 0.01        4 / (4 + 96) = 0.04       2 / (2 + 98) = 0.02
  Costs                  5 * 9 + 22 = 67         5 * 1 + 21 = 26             5 * 4 + 10 = 30            5 * 2 + 18 = 28

  Options C and D have a recall greater than 80% and an FPR less than 10%, but D is the most cost effective


6) A data scientist uses logistic regression to build a fraud detection model. While the model accuracy is
   99%, 90% of the fraud cases are not detected by the model.
   What action will definitively help the model detect more than 10% of fraud cases?
   A) Using undersampling to balance the dataset
   B) Decreasing the class probability threshold
   C) Using regularization to reduce overfitting
   D) Using oversampling to balance the dataset


answer info:
  6) B  Decreasing the class probability threshold makes the model more sensitive and, therefore, marks more
    cases as the positive class, which is fraud in this case. This will increase the likelihood of fraud detection.
    However, it comes at the price of lowering precision


7) A company is interested in building a fraud detection model. Currently, the data scientist does not have
   a sufficient amount of information due to the low number of fraud cases.
   Which method is MOST likely to detect the GREATEST number of valid fraud cases?
   A) Oversampling using bootstrapping
   B) Undersampling
   C) Oversampling using SMOTE
   D) Class weight adjustment

answer info:
  7) C  With datasets that are not fully populated, the Synthetic Minority Over-sampling Technique (SMOTE) adds
    new information by adding synthetic data points to the minority class. This technique would be the most effective
    in this scenario. Refer to Section 4.2 at this link for supporting information.


   Note: SMOTE is the choice because it says "does not have a sufficient amount of information" (vs class weight adjustment)

8) A machine learning engineer is preparing a data frame for a supervised learning task with the Amazon
   SageMaker Linear Learner algorithm. The ML engineer notices the target label classes are highly
   imbalanced and multiple feature columns contain missing values. The proportion of missing values
   across the entire data frame is less than 5%.
   What should the ML engineer do to minimize bias due to missing values?
   A) Replace each missing value by the mean or median across non-missing values in same row.
   B) Delete observations that contain missing values because these represent less than 5% of the data.
   C) Replace each missing value by the mean or median across non-missing values in the same column.
   D) For each feature, approximate the missing values using supervised learning based on other features.


answer info:
  8) D  Use supervised learning to predict missing values based on the values of other features. Different
    supervised learning approaches might have different performances, but any properly implemented supervised
    learning approach should provide the same or better approximation than mean or median approximation, as
    proposed in responses A and C. Supervised learning applied to the imputation of missing values is an active field
    of research.

    Note: since no budget criteria, imputation via supervised learning is the choice.

9) A company has collected customer comments on its products, rating them as safe or unsafe, using
   decision trees. The training dataset has the following features: id, date, full review, full review summary,
   and a binary safe/unsafe tag. During training, any data sample with missing features was dropped. In a
   few instances, the test set was found to be missing the full review text field.
   For this use case, which is the most effective course of action to address test data samples with missing
   features?
   A) Drop the test samples with missing full review text fields, and then run through the test set.
   B) Copy the summary text fields and use them to fill in the missing full review text fields, and then run
      through the test set.
   C) Use an algorithm that handles missing data better than decision trees.
   D) Generate synthetic data to fill in the fields that are missing data, and then run through the test set.


answer info:
  9) B  In this case, a full review summary usually contains the most descriptive phrases of the entire review and is
    a valid stand-in for the missing full review text field.


------------------------------------------------------
https://www.whizlabs.com/aws-certified-machine-learning-specialty/

WHIZLABS - AWS Certified Machine Learning - Specialty (MLS-C01)
           Practice Exams, Video Lectures, Hands-on Labs and Cloud Sandboxes

-----------------------------------
15 question Exam Quiz

Question 8: improving accuracy of imbalance dataset

  From O'Reilly:

    Generative Adversarial Networks (GAN)
     - A GAN is a generative model that is trained using two neural network models by treating the unsupervised problem
       as supervised and using both a generative and a discriminative model.
     - The generators role is to create synthetic outputs that closely resemble authentic data, often to the point of being
       indistinguishable from real data.
     - The discriminators purpose is to determine which of the presented outputs are the result of artificial generation.
       It is a binary classifier that assigns a probability score upon each data sample
          GAN Architecure:
              Real Image  ----> Sample   ------------------\
                                                            ------> Discrimator  --------> Real or Fake
              Random Input ----> Generator ----> Sample ---/


  GANs vs SMOTE:
    - GANs and SMOTE both involve the generation of data, they serve different purposes and operate in fundamentally distinct ways.
    - GANs are advanced generative models that produce entirely new data samples across various domains, leveraging complex neural
      networks and adversarial training.
    - SMOTE, in contrast, is a targeted oversampling technique designed to balance class distribution in datasets through interpolation
      of existing data points.

   Note: Where costs/time are not limited, GANs produces unique observations of your minority class so would produce better results
         than additional observations than SMOTE

question 10:

   Hyperparameter Tuning job JSON fields and API

CreateHyperParameterTuningJob
https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html


https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-tuning-job.html


        tuning_job_config = {
            "ParameterRanges": {
              "CategoricalParameterRanges": [],
              "ContinuousParameterRanges": [ { "MaxValue": "1", "MinValue": "0", "Name": "eta" },
                { "MaxValue": "2", "MinValue": "0", "Name": "alpha" },
                { "MaxValue": "10", "MinValue": "1", "Name": "min_child_weight" }
              ],
              "IntegerParameterRanges": [
                { "MaxValue": "10", "MinValue": "1", "Name": "max_depth" }
              ]
            },
            "ResourceLimits": {
              "MaxNumberOfTrainingJobs": 20, "MaxParallelTrainingJobs": 3
            },
            "Strategy": "Bayesian",
            "HyperParameterTuningJobObjective": { "MetricName": "validation:auc", "Type": "Maximize" },
            "RandomSeed" : 123
          }

  - After you configure the hyperparameter tuning job, you can launch it by calling the 'CreateHyperParameterTuningJob' API.
  - The following code example uses 'tuning_job_config' and 'training_job_definition'.

    tuning_job_name = "MyTuningJob"
    smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,
                                               HyperParameterTuningJobConfig = tuning_job_config,
                                               TrainingJobDefinition = training_job_definition)

Question 13:
    Understand XGBoost 'alpha' and possibly 'gamma' parameter values impact on model

  Xgboost Hyperparameters:
    alpha
      - L1 regularization term on weights. Increasing this value makes models more conservative.
      - Optional;  Valid values: Float.;  Default value: 0

    gamma
      - Minimum loss reduction required to make a further partition on a leaf node of the tree.
      - The larger, the more conservative the algorithm is.
      - Optional;  Valid values: Float. Range: [0,).;  Default value: 0

Question 15:

  Spark ML job with AWS Glue - which machine learning packages / engines are best chooses:

     MLeap:
       - MLeap is a common serialization format and execution engine for machine learning pipelines.
       - It supports Spark, Scikit-learn and Tensorflow for training pipelines and exporting them to an MLeap Bundle.
       - Serialized pipelines (bundles) can be deserialized back into Spark for batch-mode scoring or the MLeap runtime to
         power realtime API services.
     MLlib
       - Apache Spark comes with a Machine Learning library called MLlib which lets you build ML pipelines using most of the
         standard feature transformers & algorithms.
     [SG] SparkML Serving Container
       - The SageMaker Python SDK SparkML Serving model and predictor and the Amazon SageMaker AI open-source SparkML Serving
         container support deploying Apache Spark ML pipelines serialized with MLeap in SageMaker AI to get inferences
       - SparkML Serving includes
         class sagemaker.sparkml.model.SparkMLModel
           - Model data and S3 location holder for MLeap serialized SparkML model.
           - Calling 'deploy()' creates an Endpoint and return a Predictor to performs predictions against an MLeap serialized
             SparkML model .
           - Initialize a SparkMLModel.
          class sagemaker.sparkml.model.SparkMLPredictor(
           - used to perform predictions against SparkML models serialized via MLeap.
           - The implementation of predict() in this Predictor requires a json as input
           - The response is returned in text/csv format which is the default response format for SparkML Serving container.
       - SageMaker SparkML Serving Container leverages an open source library called MLeap.
       - You need to pass a schema specifying the structure of input columns and output column.
       - The web server will return you the contents of the output column in a specific format depending on content-type and Accept.


-----------------------------------
------------------------------------------------------
WhizLabs Quiz 1

Question 3:

Which linear learner hyperparameters to use to produce discrete results:

answer: predictor_type="binary_classifier"
Note: multiclass_classifier was option was not an provided choice

incorrect answer:
  A. set the objective hyperparameter to reg:logistic
   -> The objective hyperparameter is set to reg:logistic when using the XGBoost algorithm


Notes:
   - linear learner uses 'predictor_type' and 'loss' function hyperparameters while XGBoost combines specifying the target type and loss function/objective
     with the 'objective' hyperparameter (ie.. Specifies the learning task and the corresponding learning objective. Examples: reg:logistic, multi:softmax,
     reg:squarederror)

  Amazon SageMaker -> Developer Guide -> Linear learner hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html
    num_classes
      - The number of classes for the response variable. The algorithm assumes that classes are labeled 0, ...,
        num_classes - 1.
      - Required when predictor_type is multiclass_classifier. Otherwise, the algorithm ignores it.
      - Valid values: Integers from 3 to 1,000,000
    predictor_type
      - Specifies the type of target variable as a binary classification, multiclass classification, or regression.
      - Required
      - Valid values: binary_classifier, multiclass_classifier, or regressor
    loss
      - Specifies the loss function.
      - The available loss functions and their default values depend on the value of predictor_type:

        - If the predictor_type is set to regressor:
            - the available options are auto, squared_loss, absolute_loss, eps_insensitive_squared_loss, eps_insensitive_absolute_loss,
              quantile_loss, and huber_loss. The default value for auto is squared_loss.

        - If the predictor_type is set to binary_classifier:
            - the available options are auto,logistic, and hinge_loss. The default value for auto is logistic.

        - If the predictor_type is set to multiclass_classifier:
          - the available options are auto and softmax_loss. The default value for auto is softmax_loss.

        - Valid values: auto, logistic, squared_loss, absolute_loss, hinge_loss, eps_insensitive_squared_loss, eps_insensitive_absolute_loss,
          quantile_loss, or huber_loss

        - Optional;  Default value: auto

Question 6:
    Recognition model fails to recognize visitors - What could be the problem?

    answer: Confidence threshold tolerance is set to the default
       -> determines the min confidence level required for a fact to be recognized as a known individual. If set to high,
          the model might fail to recognize visitors even if they have a strong resembelence to someone in the database


Question 9:

  using a regression decision tree to predict housing prices. It overfits the data and doesn't generalize well.
  How to improve the results most efficiently

  answer:  Use a random forest by building a multiple randomized decision trees and averaging their outputs to get the
           housing prices predictions

  answer info:
    random forest is well known to increase the prediction accuracy and prevent overfitting that occurs with a single decision tree

  Random forest:
    A tree-based algorithm that uses several decision trees on random sub-samples of the data with replacement.
    The trees are split into optimal nodes at each level. The decisions of each tree are averaged together to prevent
    overfitting and improve predictions.


Question 10:

  how to visualize: show how likely it is for a customer to recommend your products, how much profit a customer brings to
   the business net acquistion and retention costs. What types of charts to create these visualizations (select 2)

   answer:
   1. Use a Net Promoter Score KPI chart to graph a customer recommendations
     -> show how likely a current customer would recommend your company's products
   2. Use a Customer Profitability Score KPI chart to graph a customer profitability
     -> show how much profit a customer contributes to your company's profits after expenses

   incorrect answers include
   1. Use a Conversion Rate KPI chart to show the conversion rate of customers
      -> show how many leads were converted to customers
   2. Use a Relative Market Share KPI chart to show competitive market share
      -> shows how much market share your company owns versus your competitors


  Analyzing & Visualizing your Data for Business Analytics
    https://aws.amazon.com/data-visualization/

    Key Performance Indicator
      - A KPI is usually a single value that relates to a particular area or function and is a reflection of how well you
        are doing in that area or function.
      - This varies from business to business and function to function.
      - KPIs are best represented using KPI charts.
      - Here are some popular KPIs that companies like to track:

      Net Promoter Score (NPS):
        - How likely is it for a customer to recommend your product or service to a friend?

      Customer Profitability Score (CPS):
        - How much profit does a customer bring to your business after deducting customer acquisition and customer retention costs?

      Conversion Rate:
        - How many leads get converted to customers?

      Relative Market Share:
        - How big is your slice of the pie compared to your competitors in the market?

      Net Profit Margin:
        - The percent of your revenue which is net profit.



Question 14:

  metric to evaluation cancer screening model (very negatively skewed.

  answer: PR AUC
     -> best metric to evaluate models on data sets where most of the cases are negative

     PR curve
      - It is the curve between precision (y-axis) and recall (x-axis) for various threshold values.

   incorrect answer: Recall
     -> it only takes into account the percentage of positive cases out of the total actual cases

   incorrect answer: RoC Curve
     -> It is best used when both outcomes have equal importance. Due to the true negative in this equation, it will not
        differentiate models will for the cancer screening problem, since this data is skewed to the true negatives. The
        true negatives cases are heavily weighted in this equation, thus amplifying the impact of the imbalance

Notes:


   PR AUC | Average Precision
     - It is a curve that combines precision (PPV) and recall (TPR) in a single visualization.
     - For every threshold, you calculate PPV and TPR and plot them. The higher the y-axis on your curve, the better your models performance.

           Precision  (Positive Predictive Value - PPV): (TP)  / (TP + FP)
              - accuracy of positive predictions
              - percentage of positive predictions that were correct:
              - Use when the cost of false positives is high
                 - example: an email is flagged and deleted as spam when it really isn't

           Recall: (TP)  / (TP + FN)
              - also called sensitivity or true positive rate (TPR)
              - percentage of actual positive predictions that were correctly identified:
              - Use when the cost of false negatives is high
                 - example: someone has cancer, but screening does not find it


               ROC Curve: plots  TPR (aka recall)  versus   FPR
                          -> equivalent: sensitivity (recall) versus 1 - specificity

               TPR or Recall or sensitivity:       TP  / (TP + FN)

               TNR or specificity:                 TN / (FP + TN)

               FPR (or fall-out):                  FP  / (FP + TN) = 1 - TNR
                 - the probability that a false alarm will be raised: that a positive result will be given when the true value is negative

               FNR              :                  FN  / (TP + FN)
                 - the probability that a true positive will be missed by the test.

Question 15:


   You created Glue Crawler to crawl S3 data, and wrote a custom classifier. Crawler failed to create a schema.
   What might be the cause?

   correct answer:  All classifiers return a certainity of 0.0
     -> data from the market data provider did not match with certainity any of the built-in classifiers that are
        part of Glue or your custom classifier. Therefore Glue returned a default classification string of UNKNOWN.

   incorrect answer:  The IAM role using assigned to the crawler has the AWSGlueServiceRole managed policy attached
     plus and inline policy htat allows your read access to the your S3 bucket
     -> The IAM role assigned to your crawler needs is exactly this managed Policy, AWSGlueServiceRole, and S3 bucket access.


  Defining and managing [Glue] classifiers
    - A classifier reads the data in a data store.
    - If it recognizes the format of the data, it generates a schema.
    - The classifier also returns a certainty number to indicate how certain the format recognition was.

    - If no classifier returns certainty=1.0, AWS Glue uses the output of the classifier that has the highest certainty.
    - If no classifier returns a certainty greater than 0.0, AWS Glue returns the default classification string of UNKNOWN.

   [Glue] Custom classifiers
     - The output of a classifier includes a string that indicates the file's classification or format (for example, json)
       and the schema of the file. For custom classifiers, you define the logic for creating the schema based on the
       type of classifier. Classifier types include defining schemas based on grok patterns, XML tags, and JSON paths.

Question 18:

  have well-functioning ensemble model deployed in production, and you notice a recent increase in false positives.
  You find it is due to a shift in spending patterns. Which ensemble adjustments has the most potential to mitigate this issue?

  Answer: Periodically update the weights of the base learners based on recent performance
    -> the adaptive solution. It allows the ensemble to continuously learn and adjust to shfits in the data patterns.

  incorrect answer:
    Completely retrain the ensemble and all it base learners on a dataset that includes newly observed spending patterns
    -> Necessary in the long run to fully incorporate the new patterns. However, it can be resource-intensive and introduce
       a delay responding to the immediate issue.

  ensemble model update base learners weight
    - In an ensemble model, "updating base learner weights" means adjusting the relative importance of each individual model
      (also called a "base learner") within the ensemble, based on their performance, so that better performing models contribute
      more significantly to the final prediction when combining their outputs; this is often done by assigning higher weights
      to more accurate models

Question 19:

  answer: Kinesis Video Streams to stream the video to a set of processing workers in ECS Fargate. The workers send the video data
          to your SageMaker machine learning model which identifies an alert situations. These alerts are processed by Kinesis
          data Streams which uses a lambda function to trigger the alert system


  Build a video processing pipeline by using Amazon Kinesis Video Streams and AWS Fargate
    https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-a-video-processing-pipeline-by-using-amazon-kinesis-video-streams-and-aws-fargate.html

    Summary
      - This pattern demonstrates how to use Amazon Kinesis Video Streams and AWS Fargate to extract frames from a video stream
        and store them as image files for further processing in Amazon Simple Storage Service (Amazon S3).

      - The user creates a Kinesis video stream, uploads a video, and sends a JSON message that contains details about the
        input Kinesis video stream and the output S3 bucket to an SQS queue.
      - AWS Fargate, which is running the main application in a container, pulls the message from the SQS queue and starts
        extracting frames.
      - Each frame is saved in an image file and stored in the target S3 bucket.

Question 21:
  Need to convert JSON data files to data format most efficient to use with Hive.

   answer: D: orc
     -> From Apache Hive Language Manual: "The Optimized Row Columnar (ORC) file format provides a highly efficient way to store
        Hive data. It is design to overcome the limitations of the other file formats. Using ORC files improves performance when
        Hive is reading, writing, and processing data". Also, AWS Glue shupports ORC for output.


Question 25:

   Set up data pipeline delivery stream using Kinesis Data Firehose as your data streaming service and Redshift as your data
   warehouse. The S3 bucket is in your Researchers own account.
   Your streaming data does not load into your redshift data warehouse. What could cause this (Select 2):

    answers:
    1. did not create an IAM role for Firehose to access the s3 bucket
      -> Firehose uses the specified Redshift user name and password to access your cluster and uses an IAM role to access the
         specified bucket, key, CloudWatch log group, and streams. You are required to have an IAM Role when creating a delivery
         system.
    2. The access policy associated with your Firehose does not have S3:PutObjectAcl specified to allow action section of the
       s3 action
      -> Since you are not the owner of the S3 bucket used by Firehose, you need to specify the S3:PutObjectAcl in the S3 actions
         of the access policy.


  Grant Firehose access to an Amazon Redshift destination
    https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-rs

    IAM role and access policy
     - When you're using an Amazon Redshift destination, Amazon Data Firehose delivers data to your S3 bucket as an intermediate location.
     - It can optionally use an AWS KMS key you own for data encryption.
     - Amazon Data Firehose then loads the data from the S3 bucket to your Amazon Redshift provisioned cluster or Amazon Redshift
       Serverless workgroup.

     - Use the following access policy to enable Amazon Data Firehose to access your S3 bucket and AWS KMS key.
     s3:PutObjectAcl
     - If you don't own the S3 bucket, add 's3:PutObjectAcl' to the list of Amazon S3 actions, which grants the bucket
       owner full access to the objects delivered by Amazon Data Firehose.


Question 26:


  You have limited time to build a text summarization tool as a proof of concenpt that will summarize the combined information
  from the media and internal company information.
  Which of the following options is must suitable for this use case?

  correct answer:
    B. Use Sagemaker built-in text summization algorithm
    -> Sagemaker is a powerful and versatile platofrm that can be used for a variety of text summarization tasks. Sagemaker
       built-in text summarization algorithm is an easy-to-use, scalable, fast, and cost-effective solution for text
       summarization.
    Note: Referenced link uses SageMaker with Hugging Face pre-trained models for text summarization
      https://aws.amazon.com/blogs/machine-learning/text-summarization-with-amazon-sagemaker-and-hugging-face/

  incorrect answer:
    D. Build a model by fine-tuning an open-source model on your data



Question 27:

  SimpleImputer
  https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html

     class sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, copy=True, add_indicator=False,
                                        keep_empty_features=False)

        - Univariate imputer for completing missing values with simple strategies.

        - Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column,
          or using a constant value.

Question 32:

   Using Xgboost on a binary classification problem. Need to find optimal hyperparmaters running many tuning jobs.
   Which of the following hyperparameter must you use in your the tuning jobs if objective="multi:softprob" (select 2)

   Answers:
     1. num_round
       -> used to set the number of rounds to run in your hyperparameter tuning jobs. This term is required.
     2. num_class
       -> used to set the number of classes. Required if object is multi:softmax or mutl:softprob

   Xgboost Required Hyperparameters
     num_class
       - The number of classes.
       - Required if objective is set to multi:softmax or multi:softprob.
       - Valid values: Integer.
     num_round
       - The number of rounds to run the training.
       - Required;  Valid values: Integer.

   Xgboost some optional Hyperparameters
     base_score
       - The initial prediction score of all instances, global bias.
       - Optional;  Valid values: Float.; Default value: 0.5
     alpha
       - L1 regularization term on weights. Increasing this value makes models more conservative.
       - Optional;  Valid values: Float.;  Default value: 0

     gamma
       - Minimum loss reduction required to make a further partition on a leaf node of the tree.
       - The larger, the more conservative the algorithm is.
       - Optional;  Valid values: Float. Range: [0,).;  Default value: 0

Question 39:

    ... Considering the need to understand complex inter-variable relationships in high-dimensional data from diverse environment
    surveillance feeds, feed technique should the data analysts employ to effectively unravel the intricate data structure and
    enhance the predictive capabilitys of their machine learning model?

    answer: Manifold Learning with t-distributed Stochastic Neighbor Embedding (t-SNE)
      -> t-SNE is specifically designed to handle high-dimensional data and uncover hidden patterns by reducing the data to a
         lower-dimensional space while preserving the local relationships

    Manifold Learning with t-distributed Stochastic Neighbor Embedding
      "Manifold Learning with t-distributed Stochastic Neighbor Embedding (t-SNE)" refers to a technique in machine learning where
       the t-SNE algorithm is used to visualize and explore high-dimensional data by projecting it onto a lower-dimensional space,
       effectively revealing the underlying manifold structure of the data, allowing for better understanding of complex relationships
       between data points, especially when the data lies on a non-linear manifold

    Note:  TSNE is in the sklearn manifold package
        from sklearn.manifold import TSNE

Question 40:

  Need to refine sentiment analysis model. Model hosted on SageMaker shows discrepancies in performance between training and real-world.
  To understand, you plan to conduct exploratory data analysis (EDA) on the incoming real-time data and the model's predictions
  within SageMaker. Aim to diagnose potential issues with data quality, model assumptions, and feature behavior.
  Which SageMaker feature should you primarily untilize to perform EDA on streaming data to diagnose the model's performance issues?

  answer: Data Wrangler for preparing and visualizing streaming data
    -> SageMaker is the most suitable choice for conducting EDA, particularly for streaming data. It provides extensive tools for data
       preparation, visualization, and analysis.
       -> It is the most suitable choice for exploratory data analysis within Sagemaker, particularly for for streaming data. it helps
          in identifying inconsistencies, anamolies, or patterns in data that may affect the model performance, which is crucial for
          diagnosing issues observied in the sentiment analysis model.

   incorrect answer: Model Monitor
     -> used to detect an alert any deviations in the model performance over time, such as data drift of model degradation.
   incorrect answer: Sagemaker Debbuger for realtime monitoriing of themodel parameters
     -> SG Debugger is primarily designed to monitor and debug ML models by capturing real-time metrics about the model's training
        parameters such as gradients and weights.



Question 41:
   Linear model is being used to predict region with highest quarterly sales. The predictor_type is set to "binary_classifier".
   Which loss function hyperparameter setting is NOT one of your options

   Answer: softmax_loss
     -> the 3 hyperparameters values that you can set for the loss function are auto, logistic, and hinge_loss. The default for
        auto is logistic. The softmax_loss setting is an option if your predictor is set to multiclass_classifier.

        Note: logistic and hinge_loss are appropriate binary classification loss functions

  Amazon SageMaker -> Developer Guide -> Linear learner hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html
    Linear Learner Hyperparameters
    num_classes
      - The number of classes for the response variable. The algorithm assumes that classes are labeled 0, ...,
        num_classes - 1.
      - Required when predictor_type is multiclass_classifier. Otherwise, the algorithm ignores it.
      - Valid values: Integers from 3 to 1,000,000
    predictor_type
      - Specifies the type of target variable as a binary classification, multiclass classification, or regression.
      - Required
      - Valid values: binary_classifier, multiclass_classifier, or regressor
    loss
      - Specifies the loss function.
      - The available loss functions and their default values depend on the value of predictor_type:
      - If the predictor_type is set to regressor, the available options are auto, squared_loss, absolute_loss, eps_insensitive_squared_loss,
        eps_insensitive_absolute_loss, quantile_loss, and huber_loss. The default value for auto is squared_loss.
      - If the predictor_type is set to binary_classifier, the available options are auto,logistic, and hinge_loss. The default
        value for auto is logistic.
      - If the predictor_type is set to multiclass_classifier, the available options are auto and softmax_loss. The default value for
        auto is softmax_loss.
      - Valid values: auto, logistic, squared_loss, absolute_loss, hinge_loss, eps_insensitive_squared_loss,
        eps_insensitive_absolute_loss, quantile_loss, or huber_loss
      - Optional;  Default value: auto


    Hinge Loss
      - Hinge Loss is a loss function utilized within machine learning to train classifiers that optimize to increase
        the margin between data points and the decision boundary. Hence, it is mainly used for maximum margin classifications.

        L(y, f(x)) = max(0, 1 - y * f(x))

            Where:
                L    : represents the Hinge Loss
                y    : the true label or target value (-1 or 1)
                f(x) : the predicted value or decision function output

  Factors to consider when selecting a loss function

    Classification vs Regression
      Classification:
        - cross-entropy loss function
      Regression:
        - mean squared error (MSE) or mean absolute error (MAE)

    Binary vs Multiclass Classification
      Binary Classification
        - binary cross-entropy loss function is best
      Multiclass Classification
        - categorical cross-entropy should be utilized.

    Sensitivity to Outliers
      penalize Outliers
        - mean squared error (MSE)
      less sensitive to Outliers
        - mean absolute error (MAE)

Question 42:

  Using K-means model in SageMaker and trying to select the hyperparameters.
  You set feature_dim to equal the number of input features, set k = 10, and epoch =1.
  You need to report a score for your model. Which k-means hyperparameters allows you to select the metric types to report
  this scoring, and what are the available metric options?

  Answer: eval_metrics with msd, ssd, or [msd,ssd] as the available metric type values
    -> 'eval_metric' is k-means hyperparameter to report a score for your model. The 'eval_metric' allowed values of msd
       (mean square deviation, ssd (sun of square distance), and option of both msd and ssd.


  K-Means Hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html

    feature_dim
      - The number of features in the input data.
      - Required;  Valid values: Positive integer
    k
      - The number of required clusters.
      - Required;   Valid values: Positive integer
    epochs
      - The number of passes done over the training data.
      - Optional;  Valid values: Positive integer;  Default value: 1
    eval_metrics
       - A JSON list of metric types used to report a score for the model.
       - Allowed values are msd for Mean Square Deviation and ssd for Sum of Square Distance.
       - If test data is provided, the score is reported for each of the metrics requested.
       - Optional;  Valid values: Either [\"msd\"] or [\"ssd\"] or [\"msd\",\"ssd\"] .;  Default value: [\"msd\"]


    "mean square deviation"
      - refers to the Mean Squared Error (MSE), a metric used to evaluate the performance of a regression model by
        calculating the average of the squared differences between the predicted values and the actual target values;
      - essentially, it measures how far off, on average, a model's predictions are from the true values, with larger
        errors being penalized more heavily due to the squaring operation.

    Mean Square Error (MSE) / L2 Loss
       MSE = (1/n) * (y_i_predicted - y_i_target)

          where:
             n              the number of samples in the dataset
             y_i_predicted: the predicted value for the i-th sample
             y_i_target:    the target value for the i-th sample

     When to use MSE
       - MSE is a standard loss function utilized in most regression tasks
       - when it is conducive to penalize significantly the presence of outliers.

    sum of square distance
      - refers to a metric that calculates the total squared difference between two points or sets of points
      - essentially summing up the squares of the individual coordinate differences between them, often used in the
        context of Euclidean distance to measure how far apart data points are in a given space;
      - it's a key component of calculating metrics like Mean Squared Error (MSE)


       sum of Square Distance = (y_i_predicted - y_i_target)

Question 43:


   Take text given in the form of a document and use a histogram to measure the occurrence of the individual words
   in the document for use in document classification.
   Which text feature engineering technique is best solution for this task?

   answer: Bag-of-words
     -> BoW creates tokens of the input document test and outputs a statistical depiction of the text. The statistical
        depiction, such as a histogram, shows the count of each word in the document

    incorrect answers:
      Orthogonal Sparse Bigram
        -> OSB creates groups of words and outputs the pairs of words that include the first word. OSB is trying to
          measure the the occurrence of individual words
      term frequency - inverse document frequency (tf-idf)
        -> tf-idf determines how important a word is in a document by giving weights to words that are common and less common
           in the document. You are not trying to determine the importance of the words in your document, just the count of
           individual words
      N-Gram
         -> N-gram is used to find multi-word phrases in the text of a document. You are not trying ot find multi-word phrases.
            You are just trying to find the count of individual words


Question 44:

  You need to analyze the streamed text to find important or relevant repeated common words and phrases and correlate this data to
  client products. You'll then include these topics in your client product marketing material.
  Which of the following text feature engineering techniques is the best solution for this task?

  Correct answer:
    B. Term Frequency-Inverse document frequency (tf-idf)
      -> tf-idf determines how important a word is in a document by giving weights to words that are common and less common in the
         document. You can use this information to select the most important repeated phrases in the user's tweets in your client
         marketing material

  incorrect answer:
    D. N-Gram
      -> The N-Gram natural language processing algorithm is used to find multi-word phrases in the text of a document. However,
        it does not weigh common words of phrases. You need the weighting aspec of the rf-idf algorihtm to find the relevant
        import repeated phrases used in the tweets.


Question 46:

  Using trained XGBoost model to categories contact lenses as malformed or correct formed. Using CSV as your Traning ContentType.
  Assuming you used the default configuration settings, which of the following are true statements about your hosted model (Select 3).


  Correct Answers:
    1. The training instance class is a multiple instance GPU.
      -> supports a single-instance or multiple instance GPU for training
    2. The training data target value should be the first column of the CSV with no header.
    3. The inference CSV data has no label column

   Incorrect Answers:
    4. The algorith is not parallelizable for distributed training
      -> XGBoost algorithm is parallelizable and therefore can be deployed on multiple CpU isntances for disttributed training
    5. The training target value should be in the last column of the CSV with no header
    6. The inference data target value should be in the first column of the CSV with no header
      -> For CSV inference, CSV input does not have a label column


  Training
   - The SageMaker AI XGBoost algorithm supports CPU and GPU training.
  [XGBoost] Distributed training
    - SageMaker AI XGBoost supports CPU and GPU instances for distributed training.
   Distributed CPU training
    - To run CPU training on multiple instances, set the instance_count parameter for the estimator to a value greater than one.
    - The input data must be divided between the total number of instances.
    Distributed GPU training
      - You can use distributed training with either single-GPU or multi-GPU instances.

  [XGBoost] Inference
   - SageMaker AI XGBoost supports CPU and GPU instances for inference


Question 47:

  Using SageMaker Image Classification to read and classify license plates by state and then identify the actual license
  plate number. Very rarely, cars pass through with plates from foreign countries. These outliers must not adversely affect
  your models predictions.
  Which hyperparameter should you set, and to what value, to ensure these outliers do not adversely impact your model?

  answer: learning_rate set to 0.1
    -> learning rate governs how quickly the model adapts to new or changing data. Valid values are from 0.0 to 1.0.
       Setting to a low value, such as 0.1, will make the model learn more slowly and be less sensitive to outliers.
       Note: default: 0.1

   incorrect answers include setting feature_dim and sample_size to values
     -> feature_dim and sample_size are K-NN hyperparameters, and not Image Classification hyperparameters



  Image Classification [required] Hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html
    num_classes
      - Number of output classes. This parameter defines the dimensions of the network output and is typically set to the number
        of classes in the dataset.
      - Besides multi-class classification, multi-label classification is supported too.
      - Required;  Valid values: positive integer;
    num_training_samples
      - Number of training examples in the input dataset.
      - If there is a mismatch between this value and the number of samples in the training set, then the behavior of the
        lr_scheduler_step parameter is undefined and distributed training accuracy might be affected.
       - Required;  Valid values: positive integer


  k-NN [required] Hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_hyperparameters.html
    feature_dim
      - The number of features in the input data.
      - Required;  Valid values: positive integer.
    k
      - The number of nearest neighbors.
      - Required;  Valid values: positive integer
    predictor_type
      - The type of inference to use on the data labels.
      - Required;  Valid values: classifier for classification or regressor for regression.
     sample_size
       - The number of data points to be sampled from the training data set.
       - Required;  Valid values: positive integer
     dimension_reduction_target
       - The target dimension to reduce to.
       - Required when you specify the dimension_reduction_type parameter.
       - Valid values: positive integer greater than 0 and less than feature_dim.


Question 48:

 Using SageMaker built-in algorithm. You have created the model using CreateModel, and you have create your HTTPS endpoint.
 You docker container running your model is now ready to receive inference requests for real-time inferences. When inference
 results are returned, which are true (Select 3).

 Correct Answers.
   1. To receive inference requests your inference container must have a web server running on port 8080
     -> to receive inference requests, the container must have a web server listing on port 8080
   2. SageMaker strips all POST headers except those supported by InvokeEndpoint. SageMaker might add additional headers.
      Your inference container must be able to ignore these additional headers safely.
   3. Your inference container must accept POST requests to the /invocation endpoint

  Incorrect answers:
    4. Your inference container must accept GET requests to the /invocation endpoint
    5. Your inference container must accept PUT requests to the /inference endpoint
    6. Your inference container must accept POST requests to the /inference endpoint


    How Your Container Should Respond to Inference Requests
     - To obtain inferences, the client application sends a POST request to the SageMaker AI endpoint.
     - SageMaker AI passes the request to the container, and returns the inference result from the container to the client.


  Custom Inference Code with Hosting Services
    https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html

    Requirements for inference containers
      To respond to inference requests, your container must meet the following requirements:
         - SageMaker AI strips all POST headers except those supported by InvokeEndpoint. SageMaker AI might add additional headers.
           Inference containers must be able to safely ignore these additional headers.
         - To receive inference requests, the container must have a web server listening on port 8080 and must accept POST requests
           to the /invocations and /ping endpoints.
         - A customer's model containers must accept socket connection requests within 250 ms.
         - A customer's model containers must respond to requests within 60 seconds. The model itself can have a maximum processing
           time of 60 seconds before responding to the /invocations. If your model is going to take 50-60 seconds of processing time,
           the SDK socket timeout should be set to be 70 seconds.

  A Cloud Guru AWS Certified Developer - Associate (DVA-C02)
  4.13 Understanding CloudFront AllowedMethods

    CloudFront AllowedMethods
      - When you create a CloudFront distribution, you need to specify which HTTP methods
        you distibution suports. Options:
          - GET, HEAD  <read only method options>
          - GET, HEAD. OPTIONS;   <read only method options> or
          - GET, HEAD. OPTIONS, PUT, POST, PATCH, DELETE  <read and write method options>

    Supported HTTTP Methods
         HTTP Method      Definition                                        Example
         ------------     ------------------------------------------        --------------
            GET           Read data; often the default method used by        Read a web page
                            HTTP clients
            HEAD          Inspect resource headers; similar to GET,          Read a web page's header
                            expect without the resouse body
            PUT           Send data to create a raws resource, or replace    Update data or change the status
                           an existing resource. Idempotent                     of a resource
            PATCH         Partially modify a resource                        Modify the contents of a shopping
                                                                               cart
            POST          Insert data; used to create or update a            Comment on a blog post
                            resource. Not Idempotent
            DELETE        Delete data                                        Remove you email address from a
                                                                               mailing list
            OPTIONS       Used to find out what other HTTP methods are       Receive a list of supported HTTP
                            supported by the given URL                          methods


  [SG API Reference] InvokeEndpoint
    - After you deploy a model into production using Amazon SageMaker AI hosting services, your client applications use
      this API to get inferences from the model hosted at the specified endpoint.
    - Amazon SageMaker AI strips all POST headers except those supported by the API.
    - Amazon SageMaker AI might add additional headers.
    - A customer's model containers must respond to requests within 60 seconds

    [InvokeEndpoint] Request Syntax
      POST /endpoints/EndpointName/invocations HTTP/1.1
      Content-Type: ContentType
      Accept: Accept
      X-Amzn-SageMaker-Custom-Attributes: CustomAttributes
      X-Amzn-SageMaker-Target-Model: TargetModel
      X-Amzn-SageMaker-Target-Variant: TargetVariant
      X-Amzn-SageMaker-Target-Container-Hostname: TargetContainerHostname
      X-Amzn-SageMaker-Inference-Id: InferenceId
      X-Amzn-SageMaker-Enable-Explanations: EnableExplanations
      X-Amzn-SageMaker-Inference-Component: InferenceComponentName
      X-Amzn-SageMaker-Session-Id: SessionId

      Body

Question 51:

  To enhance query efficiency on complex nested structures in Apache Hive on Amazon EMR, which configuration should be specifically
  optimized to handle large joins efficiently without excessive computation overhead?

  Correct Answer: set hive.cbo.enable to true to utilize cost-based optimization for complex queries
    -> enable cost-based optimization (CBO) through the setting of hive.cbo.enable. CBO allows Hive to create more efficient
       query plans by understanding the cost of different query paths, especially beneficial in scenarios dealing with complex joins
       and nested data structures.

  Incorrect answers include:
    1. implement hive.vectorization.execution.enabled to true for leveraging columnar processing
      -> enabling vectorization execution does improve performance of queries by processing data in batches, it's mainly effective
         for simple operations like scans and aggregations on flat data.
    2. Configure hive.optimization.index.filter to exploit automatic indexing on joins
      -> Indexing can impove query performance by reducing the amount of data scanned, but Hive's indexing capabilities are limited
         and not always effective for large-scale more complex joins.
    3. Adjust hive.exec.parallel.thread.number for improved parallelism during execution
      -> increasing the number of parallel thread may improve performance in some cases, but it is primarily impacts the parallel
         execution tasks.

  search: apache hive vs spark
  AI Overview
    - Apache Hive and Apache Spark are both popular tools for big data analytics.
    - Hive is best for analyzing large data sets using SQL
    - Spark is better for running big data analytics in real-time

Question 54:

  developing a multi-lingual interactive chatbot application using Polly. You need to generate speech to sound convincing human,
  especially for longer-form conversational content. Which combination of actions is most likely to maximize realism and engagement?

  Correct Answer: Custom Neural Voice and & SSML
      -> A custom NTTS voice ensures a consistent and unique character. SSML allows for nuanced expressions (pauses, emphasis, etc)
         crucial in longer conversations.

   Incorrect Answers:
     1. SSML Tags & Standard Voices
       -> SSML helps but standard voices might sound generic
     2. Pronunciation Lexicons and Standard Voices
       -> address specific pronounciation issues, but won't elevate the overall naturalness of standard voices
     3. Standard Voices and Voice Modulation
       -> Modulation adds some life, but diverse standard voices can sound disjointed, and real-time modulation
          migh be harder to maintain for long interactions

 Amazon Polly
   - Amazon Polly is a cloud service that converts text into lifelike speech.
   Standard voices vs Custom Neural voices
     - Amazon Polly's standard voices use traditional synthesis, while its neural voices use neural networks to create more
       natural-sounding speech.
     - Neural voices are more expressive and can improve the user experience
     - Neural voices cost  ~4x more than standard voices
   Speech Synthesis Markup Language (SSML)
     - You can use Polly to generate speech from either plain text or from documents marked up with SSML.
     - Using SSML-enhanced text gives you additional control over how Amazon Polly generates speech from the text you provide.
     - The <speak> tag is the root element of all Amazon Polly SSML text. All SSML-enhanced text must be enclosed within a
        pair of <speak> tags.
         <speak>Mary had a little lamb.</speak>
     - With SSML tags, you can include a long pause within your text, or change the speech rate or pitch. Other options
       include: emphasizing specific words or phrases; using phonetic pronunciation; including breathing sounds; whispering;
       use a different language; etc.
   Speech Marks
     - Speech marks are metadata that describe the speech that you synthesize, such as where a sentence or word starts and
       ends in the audio stream.
     - When you request speech marks for your text, Amazon Polly returns this metadata instead of synthesized speech.
     - By using speech marks in conjunction with the synthesized speech audio stream, you can provide your applications with
       an enhanced visual experience.
     - For example, combining the metadata with the audio stream from your text can enable you to synchronize speech with
       facial animation (lip-syncing) or to highlight written words as they're spoken.
   Pronounciation Lexicons:
     - Pronunciation lexicons enable you to customize the pronunciation of words.
     - Amazon Polly provides API operations that you can use to store lexicons in an AWS region.
     - Those lexicons are then specific to that particular region. You can use one or more of the lexicons from that region
       when synthesizing the text by using the SynthesizeSpeech operation.
     - lexicon examples:
       - Your text might include an acronym, such as W3C. You can use a lexicon to define an alias for the word W3C so that
         it is read in the full, expanded form (World Wide Web Consortium).
       - Lexicons give you additional control over how Amazon Polly pronounces words uncommon to the selected language
       - Common words are sometimes stylized with numbers taking the place of letters, as with "g3t sm4rt" (get smart). In this
         example, you can specify an alias (get smart) for the word "g3t sm4rt" in the lexicon.

Question 57:

  Building a serverless application that will analyze customer reviews to generate summaries. Using Polly to create audio versions
  of the summaries for accessibility purposes. Which architecture is most likely to be provide cost-efficiency and scalability?

  Correcct Answer: Decouple Polly calls with an SQS queue. A Lambda function fetches summaries, triggers Polly, and stores
      results in S3.

Question 60:

  Building home pricing model using Kaggle housing price dataset. You want make sure your model is not overly influenced
  by outliers.

  answer:  Visualize your data using scatter plots and/or box plots
    -> with large dataset, the quickest way to find outliers is to visualize your data. The best plots for this task are
       scatter plots and box plots.
  incorrect answer: Calculate the z-score for you data points
    -> the z-score of a data point shows how many std deviations the data point is from the mean. This would help find
       your outliers. But it will involve more effort, and therefor more time than visualizing your data.


  Common Methods for Detecting Outliers
    https://fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers/
    Box-Plot
      - The quickest and easiest way to identify outliers is by visualizing them using plots.
      - If your dataset is not huge (approx. up to 10k observations & 100 features), I would highly recommend you build
        scatter plots & box-plots of variables.
      - this method is not recommended for high dimensional data where the power of visualization fails.
    Cook's Distance
      - This method is used only for linear regression and therefore has a limited application.
      - Cooks distance measures the effect of deleting a given observation.
      - Its represents the sum of all the changes in the regression model when observation i is removed from it.
       Cook's distance =  (1/[p * mse**2]) * (y_i_predicted - y_i_target)
          where p is the number of predictors
    Z-Score
      - This method assumes that the variable has a Gaussian distribution.
      - It represents the number of standard deviations an observation is away from the mean:
              z-score = (x - mean) / std_dev
      - normally define outliers as points whose modulus of z-score is greater than a threshold value.
      - This threshold value is usually greater than 2 (3 is a common value).

Question 62:

   To build an election prediction model that uses multiple independent variables such as the age, region, sex,
   registered afflication, etc. to predict the candidatae for each observed voter. Which algorithm is not a good
   choice to use for your prediction (Select 4).

   Correct answers:
     1. Ordinal Least Square Regressions (OLSR)
       -> OLSR is a regression technique that predicts a dependent variable using one or more independent variables.
          You are trying to solve a classification problem
     2. Local Outlier Factor (LOF)
       -> LOF is used to discover outliers data points.
     3. Least-Angle Regression (LARS)
       -> LARS is also a regression technique that predicts a dependent variable using one or more independent variables.
          You are trying to solve a classification problem
     4. K-means
       -> K-means is a clustering algorithm, not classification algorithm

   Incorrect answers:
     5. Naive Bayes
        -> Naive Bayes algorithm can be used as a classifier.

  The most popular regression algorithms are:
      Ordinary Least Squares Regression (OLSR)
      Linear Regression
      Logistic Regression
      Stepwise Regression
      Multivariate Adaptive Regression Splines (MARS)
      Locally Estimated Scatterplot Smoothing (LOESS)

  The most popular regularization algorithms are:
      Ridge Regression
      Least Absolute Shrinkage and Selection Operator (LASSO)
      Elastic Net
      Least-Angle Regression (LARS)


  AWS Certified Machine Learning - Specialty (MLS-C01): Modeling
  2.2 Exploring Supervised Learning Classification
    ...
    Naive Bayes algorithm
       - based on Bayes theorem.
       - Assumes that each feature is independent of each other.
       -    P(X|Y) = [P(Y|X) * P(X)]  /  P(Y)
           where P(X): probability of X
                 P(X|Y): probability of X when Y is true
       Businese Cases:
         - commonly used in text classification such as spam filtering and sentiment analysis.
         Pros
           - it is very fast
           - can handle missing data without requiring imputation.
         Cons
           - the assumption of feature independence as most real world features are somewhat related.


Question 64:

  investigating ML to enhance gaming platform. Team decided to use the built-in SageMaker data transformations. Specifically,
  decided to use built-in OneHotEncoder transformer to transform your categorical data. You have decided to drop one of the
  categories per feature because you suspect you may have perfectly collinear features. Which of the following is NOT
  a drop methodology used in the OneHotEncoder tranformer?

  Correct answer: Last
    -> OneHotEncoder transformer has the following methodologies you can use to drop one of the categories per feature:
       None, first, array, None is the default.

  Incorrect answer: Array

  SageMaker built-in transformer info:
    - preprocessing with the Amazon SageMaker built in Scikit-learn library.
    - includes the SimpleImputer , StandardScaler, and OneHotEncoder transformers
    - these are commonly-used data transformers included in Scikit-learns preprocessing library that process the data
      into a format required by ML models.

  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
  class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, ...)

    categoriesauto or a list of array-like, default=auto
      - Categories (unique values) per feature:
      auto :
         - Determine categories automatically from the training data.
      list :
         - categories[i] holds the categories expected in the ith column.
         - The passed categories should not mix strings and numeric values within a single feature, and should
           be sorted in case of numeric values.
         - The used categories can be found in the categories_ attribute.

    drop{first, if_binary} or an array-like of shape (n_features,), default=None
      - Specifies a methodology to use to drop one of the categories per feature.
      - This is useful in situations where perfectly collinear features cause problems, such as when feeding the
         resulting data into an unregularized linear regression model.

      - However, dropping one category breaks the symmetry of the original representation and can therefore induce
        a bias in downstream models, for instance for penalized linear classification or regression models.
      None :
        - retain all features (the default).

      first :
        - drop the first category in each feature.
        - If only one category is present, the feature will be dropped entirely.

      if_binary :
        - drop the first category in each feature with two categories.
        - Features with 1 or more than 2 categories are left intact.

      array : drop[i] is the category in feature X[:, i] that should be dropped.
        - When max_categories or min_frequency is configured to group infrequent categories, the dropping behavior
          is handled after the grouping.


------------------------------------------------------
WhizLabs Quiz 2

Question 2:

  The ground truth automated labeling job initially follows this set of steps:
    - select a random sample of data
    - send random sample data to human workers
    - uses the human-labeled data as validation
    - runs a SageMaker batch transform using the validation set, which generates a quality metric used to estimate
      the potential of auto-labeling the rest of the unlabeled data
    - runs a Sagemaker batch transform on the unlabeled data
    - data, where the expected quality of the automatically labeling the data is above level of accuracy is labeled

  After performing the above steps, what does Ground Truth do next to complete the labeling ALL of your data?

  correct answer: Selects a new sample of the most hard to identify unlabeled data and sends it to human workers;
     it uses the existing labeled data and the new human-labeled data to train a new model; repeats this later steps
     until all the data in the dataset is labeled.

  [SageMaker Ground Truth] Automate data labeling
  https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html
    -> provides details on the automate labeling flow

Question 6:

  you have millions of brain scans data to use in your model to detect brain tumors. You have incoming stream of new
  scans every day, so your volume is very high. Your reseach team requires a model to perform at scale with a high degree
  of accuracy to the nature of the consequences of false negative predictions.
  Which algorighm best fits your probem?

  correct answer: Image classification
    -> Sagemake built-in Image classification algorithm uses a CNN to classify images. It breaks up each image into
       a series of tiles and then predicts what each tile contains. This is the optimal way to find a tumor with a
       large brain scan image.

  incorrect answer: Object Detection
    -> Object detection is used to identify all instances of an object within an image. You are trying to classify a
       high-resolution image as either containing a tumor or not. You are not trying to identify and surrounding elements
       in an image with a bounding box.

Question 8:

  News organization want to used online comments from users to gain insights into what interest them the most. Editors
  would like to use ML to find the underlying intent of comments.
  You will using Amazon Comprehend. Which Comprend APIs would give the information your editors requested (select 3).

  correct answers:
    1. DetectSentiment
      -> gives you the underlying sentiment (positive, neutral, mixed, or negative) of a string, such as a comment
    2. DetectEntities
      -> finds the named entities in text. This would help find entities such as news organizations, politicians,
         celebrities, companies,etc. This info will help you identify the subject matter of the comments
    3. DetectKeyPhrases
      -> find key noun phrases in the text. This will also help you identify the subject matter for the comment.


  incorrect answer:
    4. CreateDocumentClassifier
      -> Creates a new document classifier that you can use to categorize documents. Your editors want you to find
         the underlying intent of the comments


Question 9:


 You company has decided to segment its customers by their purchase history. You have all the online retailer purchase histories
 for the last 5 years that you can use for your ML model.
 Which type of ML algorithm would  give segmentation based on purchase history in the expeditious manner?

 correct answer:
   B. K-means
   -> K-means is used to find groups within data where the group members are similar to each other but ddifferent for members
      of other groups. This is exactly what you are trying to solve: find groups of customers with similar purchase history

  incorrect answer:
   A. K-NN
   -> K-NN algorithm is used to find items that are similar to each other. This may find purchases that are similar to each other,
      but not customers that have similar purchases. You would have to do additional modeling using the K-NN algorithm.
      Note: You don't have labeled similar customers, so it K-means.




Question 15:

  Using Sagemaker Groundtruth to label your user's photos and videos. Sometimes Ground Truth human workers mislabel
  images and /or videos. Which Ground Truth feature helps you continue to get high-quality labelling in automated way
  even when your workers occasionally mislabel?

  correct answer: Annotation consolidation
    -> this feature allows you to combine annotations of multiple workers to produce an automated probabilistic estimate
       of what the correct label should be

    -> Annotation consolidation uses probabilistic approach to estimate the correct label which estimate each labels
       skill level along with the votes for each image.

  incorrect answers:
    chain labeling jobs
      -> allows you to reuse dataset from previous labelling jobs. This would not help you address mislabeled images or video
    label verification and adjustment
      -> allows you to have workers verify and correct mislabeled labels. This would help correct mislabeled items, but it
         is not automated process. It is manual.

Question 16:

  Using Ground Truth to label images of x-rays, MRIs, and other medical imagery.
  Which ground truth annotation consolidation functions should you use to ensure the accuracy of your labelling tasks (select 2).

  correct answers:
    A bounding Box
      -> the bounding box finds the most similar bounding boxes from workers and averages them, thus using the power of
         multiple workers to annotate your images more accurately
    Semantic segmentation
      -> semantic segmentation feature fuses the pixel annotations of multiple works and applying a smoothing function
         to the image, thus using the power of of multiple workers to annotate your images more accurately

  Incorrect answers:
     C. Named entity
       -> The named entity feature is used with test annotation work, not image classification.


  Annotation consolidation
    https://docs.aws.amazon.com/sagemaker/latest/dg/sms-annotation-consolidation.html

    - Annotation consolidation combines the annotations of two or more workers into a single label for your data objects.
    - A label, which is assigned to each object in the dataset, is a probabilistic estimate of what the true label should be.
    - Each object in the dataset typically has multiple annotations, but only one label or set of labels
    - You decide how many workers annotate each object in your dataset.

    - If you use the Amazon SageMaker AI console to create a labeling job, the following are the defaults for the number
      of workers who can annotate objects:
        - Text classification3 workers
        - Image classification3 workers
        - Bounding boxes5 workers
        - Semantic segmentation3 workers
        - Named entity recognition3 workers

     - When you use the 'CreateLabelingJob' operation, you set the number of workers to annotate each data object with
       the 'NumberOfHumanWorkersPerDataObject' parameter.
     - You can override the default number of workers that annotate a data object using the console or the 'CreateLabelingJob'
       operation.

     - Ground Truth provides an annotation consolidation function for each of its predefined labeling tasks:
       bounding box, image classification, name entity recognition, semantic segmentation, and text classification


Question 18:

  Using a Deep Learning model that estimates the value of various art products put up for auction. Your dataset has many
  features. Some features have outliers. In training your have realized that you have an overfitting problem.
  You need to address the overfitting problem. You decided to use regularization to address the overfiting problem.
  Which regularization technique best fits your situation.

  Correct answer: Lasso Regularization
    -> Lasso regularization (L1) is the best choice here because you have outliers in your feature data. L1 regularization
       handles outliers well.

  incorrect answers:
    Ridge Regularization
      -> Ridge Regularization (L2) does not handle outliers as well as L1 regularization
    Dropout
      -> using dropout with outliers is more effort that using L1.
    Early Stopping
      -> will not address outlier problem

  Fighting Overfitting With L1 or L2 Regularization: Which One Is Better?
    https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization
    - A major snag to consider when using L2 regularization is that its not robust to outliers.
    - The squared terms will blow up the differences in the error of the outliers.
    - The regularization would then attempt to fix this by penalizing the weights.

    The differences between L1 and L2 regularization:
      - L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the
        sum of squares of the weights.
      - The L1 regularization solution is sparse. The L2 regularization solution is non-sparse.
      - L2 regularization doesnt perform feature selection, since weights are only reduced to values near 0 instead of 0.
        L1 regularization has built-in feature selection.
      - L1 regularization is robust to outliers, L2 regularization is not.


Question 21:

  The ML models that you will build for this solution (using in-ground soil sensors together with enrichment from geolocation,
  rainfall, and other weather info) will analyze growing conditions so farming corp can schedule wahtering appropriately.

  What collection of AWS Services would you use to implement a solution that first trains your model, then gathers the
  information from the in-ground sensors, then enriches the sensor data, and finally deploys the mode to run inference
  on the connected devices in the field?

  correct answer:  Sagemaker, IoT Core, IoT Analytics, and IoT Greengrass
    -> SageMaker is used to create your model and train it initially. IoT Core sends sensor data to IoT Analytics for
       enrichment and analysis. The pre-trained model is deployed in IoT Greengrass so you can perform ML inference
       using enriched data on the farm local devices in the field.


  using IoT Core, IoT Analytics, and IoT Greengrass
  AI Overview
    - When using AWS IoT Core, IoT Analytics, and IoT Greengrass together, you essentially create an IoT system where
      devices collect data at the edge using Greengrass, perform initial processing locally on the device, then send relevant
      information to AWS IoT Core for further analysis and insights through IoT Analytics, enabling real-time decision-making
      and efficient data management even with intermittent connectivity.

    Breakdown of the roles:

      AWS IoT Core:
        - Acts as the central hub for managing device connections, securely receiving data from IoT devices (via Greengrass),
          and routing it to appropriate AWS services like IoT Analytics.

      AWS IoT Greengrass:
        - Deployed on edge devices, it enables local data processing, filtering, and aggregation before sending the most
          important information to the cloud via IoT Core, allowing for faster response times and reduced bandwidth usage.

      AWS IoT Analytics:
        - Receives data streamed from IoT Core and performs advanced analytics, including data transformation, filtering,
          and machine learning based analysis to extract valuable insights from the collected IoT data

    How it works:
      1. Data collection:
        - IoT devices collect sensor data and send it to the local Greengrass core running on the device.
      2. Edge processing:
        - Greengrass performs initial data filtering, aggregation, and potentially basic analysis on the local device.
      3. Data transmission:
        - Greengrass sends the processed data to AWS IoT Core using the MQTT protocol.
      4. Cloud analytics:
        - IoT Core routes the data to IoT Analytics, where complex analysis, data transformation, and insights generation occur.

Question 25:

  Using K-means built-in Sagemaker algorithm to help decide where to place exploratory oil drilling teams. The tean is tasked
  you with providing metric visualization charts for the training runs of your team's model.
  How would you go about visualizing the training metrics? (select 2)

  correct answer:
    1. In your SageMaker Jupyter Notebook, using SageMaker python module call sagemaker.analytics, import TrainingJobAnalytics.
      use this python module to gain access to the python methods that allow you to visualize your metrics in a chart.
    2. Set model of the metrics names to "test:msd"
      to set the metric name you wish to visualization, you need to give a valid metric for the algorithm that your are training.
      The 'test:msd' metric is one of the two valid for K-Means training run. The other valid metric for K-means is "test:ssd"


  Easily monitor and visualize metrics while training models on Amazon SageMaker
    https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/

    Step 3: Using Amazon SageMaker Python SDK APIs to visualize metrics
      - You can also visualize the metrics inline in your Amazon SageMaker Jupyter notebooks using the Amazon SageMaker Python
        SDK APIs. Here is a sample code snippet.

          %matplotlib inline
          from sagemaker.analytics import TrainingJobAnalytics

          training_job_name = '<insert job name>'
          metric_name = 'validation:cross_entropy'

          metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[metric_name]).dataframe()
          plt = metrics_dataframe.plot(kind='line', figsize=(12,5), x='timestamp', y='value', style='b.', legend=False)
          plt.set_ylabel(metric_name);

Question 28:

  Building a ML model to track each scooter (in a fleet of electric scooters deployed in different cities) and decide when
  they are ready for maintenance. You are tracking many features in addition to mile accumulated. Since you have so many
  features, you need to find the most predictive features in your model to avoid low model performance due to collinearity.

  You have built your model in sagemaker using built-in XGBoost algorithm. Using the XGBoost Python API package, which type of
  booster and which API call would you use to select the most predictive features based on the total gain across all splits
  in the feature is used?

  Correct answer:  booster = gbtree using the get_score with importance_type parameter set to total_gain
    -> To get the features based on the total gain across all splits in which the  feature is used, you need to use 'gbtree'
       booster and call 'get_score' passing the parameter 'importance_type' set to 'total_gain'

   XGBoost hyperparameter
     booster
       - Which booster to use.
       - The 'gbtree' and 'dart' values use a tree-based model, while 'gblinear' uses a linear function.
       - Optional;  Valid values: String. One of "gbtree", "gblinear", or "dart".;  Default value: "gbtree"


    class xgboost.Booster(params=None, cache=None, model_file=None)
      get_score(fmap='', importance_type='weight')
        - Get feature importance of each feature. For tree model Importance type can be defined as:

          weight: the number of times a feature is used to split the data across all trees.

          gain: the average gain across all splits the feature is used in.

          cover: the average coverage across all splits the feature is used in.

          total_gain: the total gain across all splits the feature is used in.

          total_cover: the total coverage across all splits the feature is used in.


  search: xgboost booster gbtree dart gblinear
  AI Overview
    - In XGBoost, "booster" refers to the type of base model used in the boosting process, with "gbtree" representing
      a decision tree based model (the default), "dart" using a dropout-based tree approach to potentially reduce overfitting,
      and "gblinear" utilizing a linear model, best suited for sparse data with mostly linear relationships between features;
      essentially choosing which type of model to build within each boosting iteration.

  Breakdown:

    gbtree (Gradient Boosting Tree):
      - The most common choice, using decision trees as base learners, ideal for complex non-linear relationships in data.
    dart (Dropouts meet Multiple Additive Regression Trees):
      - Incorporates a "dropout" mechanism where some trees are randomly dropped during training, helping to prevent overfitting.
    gblinear (Gradient Boosting Linear):
      - Employs linear models as base learners, performing well on datasets with many features and sparse data where linear
        relationships are expected.

  Key points to remember:

    Default booster: "gbtree" is the default booster in XGBoost.

   When to use gbtree:
     - For most datasets, especially when dealing with complex non-linear relationships.
   When to use dart:
     - When overfitting is a concern, especially with large datasets.
   When to use gblinear:
     - When dealing with sparse data or suspecting primarily linear relationships between features.

Question 29:

  Two retail chained merged, and they are in the process of merging the 2 chain's systems.

  You are assigned to create the new customer data source for the presently merged retail chain. Instead of trying to find
  duplicate customer data manually through traditional programming techniques, you have decided to use ML techniques.

  You will use AWS Glue ML 'FindMatches' Transform to find duplicate customers. Knowing that incorrectly linking what appears
  to be duplicate customers must be avoided at all costs, how should you configure the AWS Glue 'FindMatches' ML transform
  parameters to achieve the most efficient and accurate duplicate detecton process?

  correct answer: Set the 'FindMatches' 'precision-recall' parameter to 'precision' and the 'accuracy-cost' parameter to 'accuracy'
    -> setting the "FindMatches' precision-recall paramter to 'precision' minimizes false positives. That is what you want.
       Setting the "FindMatches' 'accuracy-cost' parameter to 'accuracy' maximizes the transform accuracy of finding matching
       records as duplicates. This is also what you want.


  [AWS Glue FindMatches Transform] Deciding Between accuracy and cost
    https://docs.aws.amazon.com/glue/latest/dg/machine-learning-accuracy-cost-tradeoff.html

    Each FindMatches transform contains an 'accuracy-cost' parameter. You can use this parameter to specify one of the following:
       accuracy
         - If you are more concerned with the transform accurately reporting that two records match, then you should
           emphasize accuracy.
       lower cost
         - If you are more concerned about the cost or speed of running the transform, then you should emphasize lower cost.

    You can make this trade-off on the AWS Glue console or by using the AWS Glue machine learning API operations.

  [AWS Glue FindMatches Transform] Deciding between precision and recall
    https://docs.aws.amazon.com/glue/latest/dg/machine-learning-precision-recall-tradeoff.html

    Each FindMatches transform contains a 'precision-recall' parameter. You use this parameter to specify one of the following:

      precision
        - If you are more concerned about the transform falsely reporting that two records match when they actually don't
          match, then you should emphasize precision.

       recall
         - If you are more concerned about the transform failing to detect records that really do match, then you should
           emphasize recall.

      You can make this trade-off on the AWS Glue console or by using the AWS Glue machine learning API operations.


Question 30:

  You are using Kinesis Firehose transformation to pre-process your IoT data before storing it in S3. You have written
  your lambda function that pre-processes the data, and you are now testing your data transformation process flow. When
  running your tests, you see tha the Kinesis Firehose rejects every record as a data transformation failure. What could
  be the reason for the failure?

  correct answer: The transformed records from your lambda function consist of 'recordid' and 'result' parameters
    -> Transformed records received by Firehose from lambda must contain the 'recordid', 'result', and 'data parameters'.
       Your tranformed records only contain the 'recordid' and the 'result' parameters.

  incorrect answer: In your lambda function, you have set the result to 'OK' or "Dropped' for each record processed.
    ->  The status of your transformed record produced by your lambda function can be 'Ok' (the record was transformed
       successfully, Dropped (the recored was dropped intentially by your transform logic), or Processing Failed (the
       record could not be tranformed). A status of 'ok' or 'Dropped' indicates to Firehose that the record was successfully
       processed. A status 'ProcessingFailed' indicates a failed transformation. You lambda function has set each record's
       status to either 'ok' or "Dropped', so this option is incorrect.

  Required parameters for data transformation
    https://docs.aws.amazon.com/firehose/latest/dev/data-transformation-status-model.html

  The following parameters are required for all transformed records from Lambda.
    recordId
       The record ID is passed from Amazon Data Firehose to Lambda during the invocation.
      - The transformed record must contain the same record ID.
      - Any mismatch between the ID of the original record and the ID of the transformed record is treated as a data
        transformation failure.

    result
       The status of the data transformation of the record.
      - The possible values are: Ok (the record was transformed successfully), Dropped (the record was dropped intentionally
        by your processing logic), and ProcessingFailed (the record could not be transformed).
      - If a record has a status of Ok or Dropped, Amazon Data Firehose considers it successfully processed. Otherwise,
        Amazon Data Firehose considers it unsuccessfully processed.

    data
       The transformed data payload, after base64-encoding.

    Following is a sample Lambda result output:

          {
              "recordId": "<recordId from the Lambda input>",
              "result": "Ok",
              "data": "<Base64 encoded Transformed data>"
          }


Question 35:

  You have massive amounts of customer and company operation data on legacy mainframe systems and their associate datastores,
  such as aging relational databases.

  You are building an ML model to streaming data from the company's in-house routers, functioning as IoT devices, and use
  that data to help the company sell additional services to its customer bases. The IoT data is unstructured, so you need to
  transform it to CSV format before ingesting it into S3 buckets you use to house your datasets for SageMaker model. You
  also need to enrich the IoT data with real-time data from your legacy mainframe systesm as the data streams into your
  AWS Cloud enviroment.
  Which set of AWS Services would you use to set up this data transformation and ingestion pipeline?

  Correct answer: Have your legacy mainframe  write to AWS StorageGateway using the File Gateway Configuration via an
     NFS (Network File System) connection. Use Kinesis Firehose to receive the stream data from the IoT devices. Use the
     Firehose lambda integration capability to enrich the IoT data from your legacy mainfram systesm data and convert it
     to CSV before writing it to the S3 bucket used by your Sagemaker model.

Question 36:

   Due to a large number of observations, you management team anticipates that training this model could get costly, so they
   have asked you to keep the costs of your project as low as possible.

   You have written the following python code using SageMaker Python SDK in your SageMaker jupyter notebook:


         >>> s3_train = sagemaker.s3_inputs(s3_data='s3://{}/{}/'.format(bucket,path_train), content_type='csv', distribution='SharedByS3Key')
         >>> sess = sagemaker.Session()
         >>>
         >>> xgb = sagemaker.estimator.Estimator(container, role, train_instance_count=5, train_instance_type='ml.m4.xlarge',
         >>>                                     output_path=output_location, sagemaker_session=sess)
         >>>
         >>> xgb.set_hyperparameters(objective='multi:softmax', num_class=3, num_round=100)
         >>>
         >>> xgb.fit('train':s3_train, 'validation':s3_input_validation, job_name=job_name)

   Using this code, how does sagemaker replicate your dataset to your ML instances for training;

   Correct Answer: SageMaker replicates a subset of yourdataset on each 5 ML instances that launched for training


     S3DataDistributionType
      FullyReplicated
        - If you want SageMaker to replicate the entire dataset on each ML compute instance that is launched for model training,
          specify FullyReplicated.
       ShardedByS3Key
        - If you want SageMaker to replicate a subset of data on each ML compute instance that is launched for model training,
          specify ShardedByS3Key.

      S3DataType
       S3Prefix
         - S3Uri identifies a key name prefix. SageMaker uses all objects that match the specified key name
           prefix for model training.

       ManifestFile
          - S3Uri identifies an object that is a manifest file containing a list of object keys that you want SageMaker to use for model training.

       AugmentedManifestFile
         - S3Uri identifies an object that is an augmented manifest file in JSON lines format.
         - This file contains the data you want to use for model training.
         - AugmentedManifestFile can only be used if the Channel's input mode is Pipe.
         Type: String
         Valid Values: ManifestFile | S3Prefix | AugmentedManifestFile

Question 40:

   Creating an ML model to identify similar products for a product comparison chart on many of the product pages on your website.
   You want to show a grid of a product compared to similar products. The grid will show the prices, review summary (stars), and key
   features of each product. You are the stage where you are gathering, cleaning, and tranforming your data and training your model.

   Using ML techniques, how can determine similar product data for use in this gride in the most efficient manner?

   correct answer: Use AWS LakeFormation FindMatches ML Transform
     -> LakeFormation FindMatches transformation can be used to find similar products in yur data stores and even external data sources
        such as those of competitor products

   incorrect answer: Use AWS Glue FindMatches ML Transform and set it precision-recall parameter to recall.
      -> Glue FindMatches ML Transform uses ML capabilities to find matching records in your database, even when the records don't have
         exactly matching fields. Setting the the FindMatches ML Transform precision-recall parameter to 'recall' is incorrect since
         this setting is used when you want to minimize false negatives. This is not optimal result, but is a better outcome that
         incorrectly identifying two items as similar when they are really aren't (false positives).


  Record matching with AWS Lake Formation FindMatches
    https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html

     - The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do
       not have a common unique identifier and no fields match exactly.
     - This will not require writing any code or knowing how machine learning works. FindMatches can be useful in many different problems,
       such as:
       Matching customers:
         - Linking customer records across different customer databases, even when many customer fields do not match exactly across
           the databases (e.g. different name spelling, address differences, missing or inaccurate data, etc).

       Matching products:
         - Matching products in your catalog against other product sources, such as product catalog against a competitor's catalog,
           where entries are structured differently.

       Improving fraud detection:
         - Identifying duplicate customer accounts, determining when a newly created account is (or might be) a match for a
           previously known fraudulent user.

    Other matching problems: Match addresses, movies, parts lists, etc etc. In general, if a human being could look at your database
    rows and determine that they were a match, there is a really good chance that the FindMatches transform can help you


Question 44:

  The IoT data that the electric bikes produces is unstructured, and somtimes, depending on the manufacturer of the IoT part,
  the data has a different schema structure. You need to clean and classify the IoT data before using it your ML model. How
  can you build an ETL script to perform the necessary cleaning and classification knowing that you have message data with
  differing schema structures?

  correct answer:  Use AWS Glue to build a series of transforms that use DynamicFrames to pass the data from transform to transform.
       Each tranform performs a different cleaning and/or transforming task.
       -> Glue DynamicFrame allows each record to be self-describing to handle unknown or changing schemas

  incorrect answer:  Use AWS Glue to build a series of transforms that use DynamicRecord to pass the data from transform to transform.
       Each tranform performs a different cleaning and/or transforming task.
       -> DynamicRecord represents a logical record withina DynamicFrame. It is a rwo in a DynamicFrame. So you wouldn't pass
          individual DynamicRecords from transform to transform. You pass a DynamicFrame.

   [AWS Glue] DynamicFrame class
     - AWS Glue introduces the DynamicFrame.
     - A DynamicFrame is similar to a DataFrame, except that each record is self-describing, so no schema is required initially.
     - Instead, AWS Glue computes a schema on-the-fly when required, and explicitly encodes schema inconsistencies using a choice
       (or union) type. You can resolve these inconsistencies to make your datasets compatible with data stores that require a fixed schema.

     - Similarly, a DynamicRecord represents a logical record within a DynamicFrame.
     - It is like a row in a Spark DataFrame, except that it is self-describing and can be used for data that does not conform
       to a fixed schema.
     - When using AWS Glue with PySpark, you do not typically manipulate independent DynamicRecords. Rather, you will transform
       the dataset together through its DynamicFrame.

      - You can convert DynamicFrames to and from DataFrames after you resolve any schema inconsistencies.

Question 45:

  You have created your data source file as a CSV, and you have also created your labeling file used to train your FindMatches
  to transform. When you run your AWS Glue transform job, if fails. Which of the following could be the root of the problem?

  Correct answer: The labeling file is not encoded in UTF-8 without BOM (byte order mark).
    -> When using AWS Glue FindMatches ML Transform, the labeling file must be encoded as UTF-8 without BOM.


   UTF-8 without BOM
     - The UTF-8 encoding without a BOM has the property that a document which contains only characters from the US-ASCII
       range is encoded byte-for-byte the same way as the same document encoded using the US-ASCII encoding.
     - Such a document can be processed and understood when encoded either as UTF-8 or as US-ASCII.

   Using the FindMatches transform
    - You can use the FindMatches transform to find duplicate records in the source data.
    - A labeling file is generated or provided to help teach the transform.



Question 46:

  A part of the book publishing companies merger activity is joing the two publisher book databases. Your team has been given the
  assignment to build a data lake source from the two company's relational data stores.
  How would un construct on ETL pipeline to achieve this goal? (Select four)?

  correct answers:
    1. Use an AWS Glue Crawler to build your AWS Glue catalog.
      -> Once your data has been ingested from your databases, you need to catalog the data using an AWS Glue crawler
    2. Have a lambda function triggered by an S3 trigger to start your AWS Glue crawler
      -> The AWS Glue Crawler can be started by a lambda function that is triggered by an S3 object create event.
    3. Use a lambda function triggered by a CloudWatch event trigger to start your AWS Glue ETL job that processes/transforms you
       data and places it into your S3 Data Lake
       -> You can have your AWS Glue job started by a lambda function that is triggered by a CloudWatch event trigger
    4. Use AWS Database Migration Service to ingest the relational data from your book data stores and store it in S3
       -> You can used AWS Database Migration Services to ingest data from your relational databases and then store the data in s3 bucket

  incorrect answer:
    5. Use AWS DataSync to ingest the relational data from book data stores and store it in S3
    -> DataSync is used to ingest data from a Network Files System (NFS), not relational database



    A Cloud Guru AWS Certified Developer - Associate (DVA-C02)

      Eventbridge (formally CloudWatch Events)
        - is a serverless event bus that was built on top of the existing AWS CloudWatch Events API
        - enables developers to integrate many of the AWS services through events.
           - For example, if there is a change in the state of an EC2 instance, a CloudWatch Event can trigger an event to
             AWS Lambda to take an action.
        - EventBridge extends this functionality beyond the AWS ecosystem, letting you brings together your own (legacy)
          applications, SaaS (Software-as-a-Service), and AWS Services.
        - It can stream real-time data from various event sources like PagerDuty, Symantec, and routes to various targets
          (AWS services) like SQS, Lambda, and others.



Question 47:

   You have trained your XGBoost model and deployed it to Sagemaker hosting services where you are now ready to send
   inference requests to your model.
   You are sending request to your inference point, but you are seeing the inferences failing. Which of these would not be
   the source of the problem (select 2)?

   correct answers:
     a . you have serialized your inference request in the text/csv format.
     c . you have serialized your inference request in the text/libsvm format.

   incorrect answers:
     c . you have serialized your inference request in the application/json format.
     -> Inference endpoints built using XGBoost algorithm only support text/csv, recordio-protobfu, and text/libsvm request formats.
        Your inference request will fail if you serailize your inference request using application/json format


Question 51:


  These farm product containers have IoT sensors built into  them that transmit information such as fill rate, capacity usage, etc.
  The IoT devices transmit their data back to your cloud environment via the MQTT protocol. You want to used a ML model to predict
  container usage by region and product store. You have created, trained and deployed to SM Hosting services your model based
  on linear learner.

  What AWS services would  you used to create your pipeline to feaed your inference request to your model (Select three)?

  correct answers:
    B. IoT Core to receive the IoT device MQTT messages.
    -> IoT Core is designed to allow IoT devices to interact with other AWS Services, such as Kinesis Data streams
    D. Kinesis Data streams to stream the Iot Messages
    -> Kinesis Data Streams can receive IoT messages from IoT core and stream them to your SM inference endpoint via a Lambda function.
    E. A Lambda function to transform the IoT message data to the inference request serialization format
    -> You can write a Lambda function that is triggered by Kinesis Data streams that transforms your IoT messages into
       the inference serialization format required by your inference point

  incorrect answer:
    A. IoT Greengrass to receive the IoT device MQTT messages
    -> AWS IoT Greengrass is used to exted the AWS to edge devicess, such as your sensors in your farming container. Greengrass
       us used to perform prediction directly on the devices themselves. IoT Core is a better option for receiving your MQTT IoT
       messages for processing via your ML inference running in SM hosting services.




Question 54:


  You have chosen to use visualization techniques to decide which of your 7 features are important or most relevant, in other
  words, which of the 7 features are needed to train your model propery.
  Which visualization techniques are best to use for this purpose (choose two).

  correct answers:
    c. pairs plot
      -> a pairs plot is used to show the relationship between pairs of features and the distribution of one of the variables
         in relation to the other. This is what you need to analyze. You want to see which training features correlate with your
         target feature, police officers.

    d. covariance matrix
      -> a covariance matrix shows the degree of correlation between two features. This visualization gives you a numerical
         representation of the correlation, where the pairs plot gives you a visual representation as points plotted in
         two-dimensional space.

  incorrect answers
    e. entropy matrix
      -> entropy represents the measure of randomness in your features. This would not help you find the correlation between
         your target feature, police officers, and potential training features.

  Pairs Plot
    - A pairs plot, also known as a scatterplot matrix, is a grid of scatterplots that displays pairwise relationships between
      multiple variables in a dataset.
    - Each cell in the grid represents the relationship between two variables, and the diagonal cells display histograms or
      kernel density plots of individual variables

  Covarinace Matrix:
    - A covariance matrix is used to describe the relationships between multiple variables in the dataset.
    - The element in the Nth row and Mth column represents the covariance between Nth and Mth variable in
      the dataset.

  Numpy Convariance:
    numpy.cov(m, y=None, rowvar=True, bias=False, ...)
      - Estimate a covariance matrix, given data and weights.
      - Covariance indicates the level to which two variables vary together.
      - If we examine N-dimensional samples, X = [x_1, x_2, ...x_n]**T, then the covariance matrix
        element C_ij is the covariance of x_i and x_j. The element C_ii is the variance of x_i

  Eigenvectors
    - The eigenvectors represents the direction of maximum variance
  EigenValues
    - eigenvalue indicates a magnitude of the variance.


  Key Elements of a Pair Plot
    - At its core, a pair plot consists of:
      Histograms:
        - Diagonal plots showing the distribution of a single variable.
      Scatter plots:
        - Off-diagonal plots showing the relationship between two variables. These can reveal patterns, trends, and correlations.

  Identifying Patterns: Highlighting Trends, Clusters, Outliers, and Potential Correlations
    - Pair plots are instrumental in uncovering:
      Trends:
        - Linear or nonlinear relationships that suggest predictability.
      Clusters:
        - Groups of data points that share similar characteristics, hinting at subpopulations within the dataset.
      Out liers:
        - Data points that deviate significantly from other observations, which could be indicative of data entry errors or
          novel discoveries.
      Correlations:
        - The strength and direction of relationships between variables.


Question 57:


  Your company has IoT devices that send usage and diagnostic MQTT messages to your AWS IoT Core service. Will use IoT messages
  data to run inference through your ML inference endpoint. IoT data is unstructured, so you need to preprocess the data by
  performing feature engineering on the observation before feeding it the to inference endpoint.

  Using SageMaker Inference Pipeline to construct the ML solution. As you define the containers for your pipeline, one for feature
  engineering pre-processing and one for inference predictions, which SageMaker CLI command and which parameers on the command do
  you need to run using SageMaker CLI to build your inference pipeline?

  correct answer:  CreateModel command with Containers request parameter
    -> The SageMaker CLI 'CreateModel' command  is correct command, and the 'Containers' request parameter is the correct parameter.
       The 'Containers' request parameter is used to set the containers that make up your pipeline.

  incorrect answer:  CreateModel command with PrimaryContainer request parameter
    -> The SageMaker CLI CreateModel command  is correct command. But you use PrimaryContainer request parameter when you want to
       create a single container, not when you want to create an inference pipeline.

  Inference pipelines in Amazon SageMaker AI
    https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html
    - An inference pipeline is a Amazon SageMaker AI model that is composed of a linear sequence of two to fifteen containers that
      process requests for inferences on data.
    - You use an inference pipeline to define and deploy any combination of pretrained SageMaker AI built-in algorithms and your own
      custom algorithms packaged in Docker containers.
    - You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks.
    - Inference pipelines are fully managed.

    - You can add SageMaker AI Spark ML Serving and scikit-learn containers that reuse the data transformers developed for training models.

    - You define the containers for a pipeline model using the 'CreateModel' operation or from the console.
    - Instead of setting one 'PrimaryContainer', you use the 'Containers' parameter to set the containers that make up the pipeline.
    - You also specify the order in which the containers are executed

    - A pipeline model is immutable, but you can update an inference pipeline by deploying a new one using the 'UpdateEndpoint' operation.



  Inference Pipeline with Scikit-learn and Linear Learner
    https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb
          Set up the inference pipeline

          Setting up a Machine Learning pipeline can be done with the Pipeline Model. This sets up a list of models in a single endpoint;
          in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model.
          Deploying the model follows the same deploy pattern in the SDK.

      >>> from sagemaker.model import Model
      >>> from sagemaker.pipeline import PipelineModel
      >>> import boto3
      >>> from time import gmtime, strftime
      >>>
      >>> timestamp_prefix = strftime("%Y-%m-%d-%H-%M-%S", gmtime())
      >>>
      >>> scikit_learn_inferencee_model = sklearn_preprocessor.create_model()
      >>> linear_learner_model = ll_estimator.create_model()
      >>>
      >>> model_name = "inference-pipeline-" + timestamp_prefix
      >>> endpoint_name = "inference-pipeline-ep-" + timestamp_prefix
      >>> sm_model = PipelineModel(
      >>>     name=model_name, role=role, models=[scikit_learn_inferencee_model, linear_learner_model]
      >>> )
      >>>
      >>> sm_model.deploy(initial_instance_count=1, instance_type="ml.c4.xlarge", endpoint_name=endpoint_name)
      >>>
           Make a request to our pipeline endpoint
      >>>
      >>> from sagemaker.predictor import Predictor
      >>> from sagemaker.serializers import CSVSerializer
      >>>
      >>> payload = "M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155"
      >>> actual_rings = 10
      >>> predictor = Predictor(
      >>>     endpoint_name=endpoint_name, sagemaker_session=sagemaker_session, serializer=CSVSerializer()
      >>> )
      >>>
      >>> print(predictor.predict(payload))


Question 58:

  Create an ML model to predict which product has the more dedicated following among its customer base. You have a large amount
  of observations using your current product base. You need to perform feature engineering on the large dataset before using it
  to traing your XGBoost algorithm-based model.
  What SageMaker feature can you use to perform the required feature engineering of your dataset in the most efficient way?

  Correct answer: Batch transform
    -> The sageMaker batch transform feature can be used to preprocess your data before using the data in your training runs.

  Incorrect answer: Built-in transforms
    -> built-in transforms are part of the AWS Glue service, not sagemaker.


  Batch transform for inference with Amazon SageMaker AI
    https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html

    Use batch transform when you need to do the following:

      - Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.

      - Get inferences from large datasets.

      - Run inference when you don't need a persistent endpoint.

      - Associate input records with inferences to help with the interpretation of results.


Question 61:

  You need to clear the IoT sensor data before using it for training or use it to provide inferences from your inference endpoint.
  You will use Spark ML with AWS Glue to be your feature transformation code.  Which ML packages and engines are the best choice
  for builing your IoT sensor data transformer task in the simplest way possible (select 3).

  Correct Answer:
    A. MLeap
      -> AWS Glue serializes Spark ML jobs into MLeap containers. You add the MLeap containers to your inference pipeline.
    B. MLlib
      -> Apache Spark MLlib is a ML library that lest you build ML pipeline components to transform your data using the full suite
         of standard transformers such as tokenizers, OneHoteEncoders, normailizers, etc.
    C. SparkML Serving  Container
      -> SparkML Serving Container allows you to deploy an Apache Spark ML pipeline in SageMaker.


     MLeap:
       - MLeap is a common serialization format and execution engine for machine learning pipelines.
       - It supports Spark, Scikit-learn and Tensorflow for training pipelines and exporting them to an MLeap Bundle.
       - Serialized pipelines (bundles) can be deserialized back into Spark for batch-mode scoring or the MLeap runtime to
         power realtime API services.
     MLlib
       - Apache Spark comes with a Machine Learning library called MLlib which lets you build ML pipelines using most of the
         standard feature transformers & algorithms.
     Spark ML Serving Container
       - SageMaker SparkML Serving Container leverages an open source library called MLeap.
       - SageMaker SparkML Serving Container takes a code-free approach for performing inference.
       - You need to pass a schema specifying the structure of input columns and output column.
       - The web server will return you the contents of the output column in a specific format depending on content-type and Accept.


Question 61:

  Based on provided Step Code, what type of metric are you using for your regression evaluation? Additionally, in the HyperparameterTuing
  (XGBoost) steom what happens when the alpha parameter increases for 0 to 1000 (Select 2)?

  snippet of the provided step function code:
  "HyperparameterTuningJobObjective":{"Type": "Minimize", "MetricName":"validation:rmse"},

  Correct Answers:
    D. Root Mean Square Error
    F. As Alpha increases, the model becomes more conservative.


   Xgboost some optional Hyperparameters
     base_score
       - The initial prediction score of all instances, global bias.
       - Optional;  Valid values: Float.; Default value: 0.5
     alpha
       - L1 regularization term on weights. Increasing this value makes models more conservative.
       - Optional;  Valid values: Float.;  Default value: 0

     gamma
       - Minimum loss reduction required to make a further partition on a leaf node of the tree.
       - The larger, the more conservative the algorithm is.
       - Optional;  Valid values: Float. Range: [0,).;  Default value: 0
------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Quiz 3

Question 1:

  Tasked with using the large amount of clickstream depicting user behavorior and product preferences to build a product recommendation
  engine similar to the Amazon.com feature that recommends products through the tagline of "users who bought this also considered these
  items". How should you and your team architect this solution?

  correct answer:  c. Create a recommendation engine based on a neural collaborative filtering model using Tensorflow and run it
                      on Sagemaker
    -> The "amazon.com" recommendation engine is built using a neural collaborative filter method. This method is optimized
       to find similarities in environments when you have a large amount of user actions that you can analyze


  incorrect answer: d. Create a recommendation engine based on a content-based filtering model using Tensorflow and run it on Sagemaker
    -> Content-based filtering relies on similarities between features of the items, wherea as collaborative-based filtering
       relies on the preferences from other users on how they respond to similar items.

  Approaches to build Recommender Systems
    https://analyticsindiamag.com/ai-mysteries/collaborative-filtering-vs-content-based-filtering-for-recommender-systems/

    Collaborative Filtering
      - based on the past interactions that have been recorded between users and items, in order to produce new recommendations
      - Collaborative Filtering tends to find what similar users would like and the recommendations to be provided and in order
      - This approach assumes that if persons A and B share similar opinions on one issue, they are more likely to agree on other
        issues compared to a random pairing of A with another person.

    Content Based filtering
      - The content-based approach uses additional information about users and/or items.
      - This filtering method uses item features to recommend other items similar to what the user likes and also based on
      - If we consider the example for a movies recommender system, the additional information can be, the age, the sex, the
      - The Content-based approach requires a good amount of information about items features, rather than using the users
        interactions and feedback.
        job or any other personal information for users as well as the category, the main actors, the duration or other characteristics
        for the movies i.e the items.
        their previous actions or explicit feedback


    Model based filtering and combinative filtering do not appear in my searches
        to classify the users into clusters of similar types and recommend each user according to the preference of its cluster.
     - The standard method used by Collaborative Filtering is known as the Nearest Neighborhood algorithm


Question 22:

  Which is the  best option to make the SageMaker service avaiable in your company's AWS account without enabling direct internet
  access to your ML specialist's SageMaker notebook instances?

  correct answer: B. Connect directly to the SageMaker runtime (the VPC host SageMaker) through interface endpoint in your corporate VPC.
    -> Using an interface endpoint in your VPC, you can connect directly to the SageMaker APPI or the SageMaker runtime without
       ever connecting over the internet. Using a VPC interface endpoint, communication between your VPC and SageMaker runtime is
       conducted securely with the AWS network.


  incorrect answer: Use Transit Gateway between your corporate VPC and teh VPC hosting SageMaker.
    -> transit gate is a network transit ub used for create a VPC connection between your VPC and an on-premise network. It is not
       used to communicate with AWS like SageMaker. VPC interface adn gateway endpoints are used to accomplisht private VPC
       connections to services like Sagemaker.


   AWS PrivateLink concepts
     https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html

       |---------------------------------------------------------------------|
       | Region                                                              |
       ||------------------------------|      |-----------------------------||
       ||  Service Consumer VPC        |      |    Service Provider VPC     ||
       |||---------------------------| |      ||---------------------------|||
       |||AZ                         | |      ||AZ                         |||
       |||      service  --> VPC  ---------------->load     -----> service |||
       |||                endpoint   | |      ||   Balancer                |||
       |||                           | |      ||                           |||
       |||---------------------------| |      ||---------------------------|||
       |-------------------------------|      |-----------------------------||
       |---------------------------------------------------------------------|

Question 25:

  Amazon fraud detector requires customers to have pre-existing labeled datas from training (True or False)>

  Correct answer: True
    -> Fraud Detector requires customers to provide pre-existing labeled dataset for training. These dataset must include historical
       data of both fraudulanet and legitimate transactions to build, train, and deploy ML models that can predict fraud.

  AWS Certified Machine Learning - Specialty (MLS-C01): Machine Learning Implementation and Operations

    3.12 Detecting Fraud Using Amazon Fraud Detector

      How To use Fraud Detector
        S3 data
          - upload your own historical fraud data to S3
        Fraud detection model type
          - select a fraud detection model type
          - each model type is optimized to detect a specific type of fraud.
          - For example: identity fraud or account takeover fraud.
       Customized model
         - Fraud Detector automatically inspects your data, performs feature analysis, trains, tests, and then deploys a
           custom fraud detection model that is based on your data and requirements.
       Real-time Fraud predictions
         - For real-time fraud detection, there is a prediction API that can be used to receive fraud predictions for real-time
           events, like new account creation or a transaction being attempted.
         - fraud predictions are provided within milliseconds.
       Fraud evaluation
         - The model's output is a score ranging from zero to 1,000, which predicts the likelihood of fraud risk.
         - you can then set up decision logic to interpret the fraud evaluation score and then take some action.
           - for example: if high score, passing the transaction to a human investigator for review.
       Integrate the Amazon Fraud Detector API
         - integrate Fraud detector API into your website so that it becomes integrated,
         - for example, with the account sign-up or order checkout workflow.
        Fraud Detector Dataset requirements
          - MUST have "EVENT_TIMESTAMP" (date and time when event occurred)
          - MUST have "EVENT_LABEL" fields (represents whether event was legitimate or fraud)
          - MUST have at least 10K records

      Types of online fraud,
        - new account or account sign-up
        - identity fraud
        - account takeover,
        - loyalty account fraud,
        - guest checkout fraud,
        - payment fraud,
        - seller fraud

Question 29:

  What is a 'Detector' in the context of Amazon Fraud Detector?

  Correct answer:  A set of rules with ML Model
    -> A "Detector' is a set of rules and ML models used for fraud detection.

  readme.fraud_detector.txt
    (AWS Tutorials - Detect Frauds using Amazon Fraud Detector AI Service

     Fraud Detector has 3 parts:
      1. Build a Fraud Detector model
          - do need to train your model, but simplied by having a framework in place
          - training data is loaded to S3 bucket
          - need training data with a mix of events categorized as "legit" or "fraud"
          - then deploy this model and use API calls to this model
          - configuring fraud detector
              - select data to train model
              - identify features of the data to be used
              - select fraud detection algorithm
              - start training of your model
          - after training:
            - validate the performance [accuracy] of your model
            - if satisfied with performance, host your model

       2. Fraud Detector Detection Logic
         - Combine you model with decision rules to turn model scores into actionable outcomes

       3. Fraud Detector Prediction API
         - call Fraud Detector API with online event data to receive fraud predictions

    [Amazon Fraud Detector] Detector
      https://docs.aws.amazon.com/frauddetector/latest/ug/detector.html


     Create Fraud Detector

      AWS -> Fraud Detector -> Detectors <left tab> -> Create Detectors ->
         Detector Name: dojodector, Event Type: dojo-event -> Next ->
         Add Model -> dojo_fraud_ detection_model, version: 1.0 -> Add model -> Next ->
         Add rules:
            Name: highriskrule, Expression: $dojo_fraud_detection_model_insightscore > 800
         Outcomes: Create a new outcome
            Outcome name: high_risk -> Save outcome
         -> Add rule

         Add another rule:
            Name: mediumriskrule, Expression: $dojo_fraud_detection_model_insightscore <= 800 and $dojo_fraud_detection_model_insightscore > 500
         Outcomes: Create a new outcome
            Outcome name: medium_risk -> Save outcome
         -> Add rule

         Add another rule:
            Name: lowriskrule, Expression: $dojo_fraud_detection_model_insightscore <= 500
         Outcomes: Create a new outcome
            Outcome name: low_risk -> Save outcome
         -> Add rule

         -> Next ->
           Configure rule execution:
             Rule execution models: First matched, keep rule order (high, medium, low)
         -> Next ->
            Create Detector
            # publish detector
            dojodetector (version 1) -> Action -> Publish
            # after publishing, 'dojodector verion 1.0' detector status should now be Active


Question 31:

  To build ML model to assess loan risk based on loan application data. Need to santize your data to ensure it is not biased by
  demographic disparities. Which option is the most efficient approach to used to identify bias in your data prior to training
  your model?

  Correct answer:  Run a sageMaker Clarify job using Total Variation distance (TVD) pretraining metric.
    -> Clarify allow you to identify bias during data preparation using attributs of interest, such as gender or demographics and
       Clarify runs a set of algorithm sot detect the presence of biase in those attributes. The Total Variation Distance (TVD)
       metric measures the difference between distinct distributions of the outcomes associated wit different faces in a dataset
       such as how different are the distributions for load application outcoomes for different demographics.


  Pre-training Bias Metrics
    https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.html
    Total Variation Distance (TVD)
      - Measures half of the L1-norm difference between distinct demographic distributions of the outcomes associated with different
        facets in a dataset.

      - How different are the distributions for loan application outcomes for different demographics?

      - Range for binary, multicategory, and continuous outcomes: [0, +)
        -  Values near zero indicates the labels are similarly distributed.
        - Positive values indicates the label distributions diverge, the more positive the larger the divergence.


 A Cloud Guru: Machine Learning on AWS Deep Dive
   4.7 Detecting Bias with SageMaker Clarify

     Why SageMaker Clarify
       - Detects and measures potential bias using a variety of metrics
       - detects underlining biases in the model and data

     Two Types of ML Bias
       Dataset (pre-training) bias
         - data is imbalanced and doesn't reflect the real world
         - example: loan approval data contains very little data for people who are self-employed but then
           when the bank is using the model in the real world, it has a lot of self-employed people
       Model (post-training) bias
         - bias introduced by the training algorithm
         - example: binary classification algorithm is used to predict fraud or not, but the model was trained
           with data that showed 99% of transactions were not fraud and so the model might have picked up some bias.

     Integration Points
       Using Clarify with other SageMaker Products
         - SageMaker Data Wrangler
           - Use a simple graphical interface
         - SageMaker Data Experiments
           - Get bias results for each experiment
           - Visual results appear alongside other experiment details

     Detecting Pre-training Bias in a Dataset
       - Reviewing the dataset
         - S04/clarify/start/loan_data.csv
           - home loan dataset with features: loan_id,gender,married,dependents,education,self_employed,applicant_income,etc
           - target: approved (Y or N)
           - looking at gender and self_employed for potential bias
       - Creating a new experiment
       - Running the Clarify Processor job
       - Viewing the bias results in the SageMaker

 What is Amazon SageMaker Clarify?
   https://aws.amazon.com/sagemaker-ai/clarify/
   Benefits of SageMaker Clarify
     Evaluate foundation models (FMs) in minutes
       - Automatically evaluate FMs for your generative AI use case with metrics such as accuracy, robustness, and toxicity to
         support your responsible AI initiative.
       - For criteria or nuanced content that requires sophisticated human judgment, you can choose to leverage your own workforce
         or use a managed workforce provided by AWS to review model responses.
     Build trust in ML models
       - Explain how input features contribute to your model predictions during model development and inference.
       - Evaluate your FM during customization using the automatic and human-based evaluations.
     Accessible, science-based metrics and reports
       - Generate easy to understand metrics, reports, and examples throughout the FM customization and MLOps workflow.
     Support compliance programs
       - Detect potential bias and other risks, as prescribed by guidelines such as ISO 42001, during data preparation, model
         customization, and in your deployed models.


Question 32:

  Tasked with building ML model to estimate value of a real estate property listings. You are performing feature engineering
  and you need to encode your categorical features to use in scikit-learn regression algorithm. you have dozens of categorical
  features, with many of the features having 30 to 75 categories. Which encoding should you use for your categorical features?

  correct answer:  Target Encoding with mean transform plus smoothing
    -> combining target encoding using mean transform with smoothing removes the disadvantages of target encoding by calculating
       the avareage of the category and target together with the overall average.

  incorrect answer: Label Encoding
    -> Since label encoding assigns a numberical value that is essentially an incremental count of the number of categories in
       the feature, it runs the risk of your regression algorithm assigning value to the order of the encoding.


  Target Encoding (mean encoding)
    https://www.kaggle.com/code/ryanholbrook/target-encoding

    - A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target.
    - A simple and effective version is to apply a group aggregation, like the mean.

    Smoothing
     Target encoding issues:
       Missing values on future splits:
         - Target encodings create a special risk of overfitting, which means they need to be trained on an independent "encoding" split.
         - When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split.
           These missing values you would have to impute somehow.
       Rare Categories
         - When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate.
      Smooting Solution
        - A solution to these problems is to add smoothing.
        - The idea is to blend the in-category average with the overall average.
        - Rare categories get less weight on their category average, while missing categories just get the overall average.

Question 33:
  Building and  deploying a fraud prediction model usign SM Random Cut Forest. You have created an MLOps Project in SM Studio and chosen
  MLOps template for model building, training, and  deployment project template. You have cloned the model repo to your SM Studio env,
  made the necessary pipeline changes, committed your code and MLOps has triggered a run of your pipeline. What are the steps that follow
  the MLOps trigginer your pipeline?

  correct answer (no longer correct  - project templates that support CodeCommit repository are no longer supported)
    -> MLOps creates a new model version. You approve the new model version, the MLOps deploys your new model version to your stageing
       environment. From CodePipeline, you choose your pipeline and approve the DeployStaging stage which causes the MLOps system to deploy
       the model to the production endpoint.

   What is a SageMaker AI Project?
     https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-whatis.html
     - SageMaker Projects help organizations set up and standardize developer environments for data scientists and CI/CD systems for MLOps engineers.
     - Projects also help organizations set up dependency management, code repository management, build reproducibility, and artifact sharing.

     - You can provision SageMaker Projects from the AWS Service Catalog using custom or SageMaker AI-provided templates.
     - With SageMaker Projects, MLOps engineers and organization admins can define their own templates or use SageMaker AI-provided templates.
     - The SageMaker AI-provided templates bootstrap the ML workflow with source version control, automated ML pipelines, and a set of code
       to quickly start iterating over ML use cases.

   Walk Through a SageMaker AI MLOps Project Using Third-party Git Repos
     https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-walkthrough-3rdgit.html
     - This walkthrough uses the template MLOps templates for model building, training, and deployment with third-party Git using
       CodePipeline to demonstrate how to use MLOps projects to create a CI/CD system to build, train, and deploy models.

       NOTE: Effective September 9, 2024, project templates that use the AWS CodeCommit repository are no longer supported.
             For new projects, select from the available project templates that use third-party Git repositories.

   Topics
     Step 1: Set up the GitHub connection
     Step 2: Create the Project
     Step 3: Make a Change in the Code
     Step 4: Approve the Model
     (Optional) Step 5: Deploy the Model Version to Production
     Step 6: Clean Up Resources

Question 34:

  Client requires marketing data to help determine which marketing will be the most productive for their new product.
  Using SM Studio to create a DataFlow in SM Studio. Plan to use DataFlow to prepare and visualize your data in SM Data Wrangler.
  In SM Data Wrangler, you are building your data preparation pipeline. However, you have discovered that the Data Wrangler built-in
  transforms do not meet your transformation needs. Which options are a viable approach to building your transform in Data Wrangler
  (select 2)?

  correct answers:
    C. Select a custom transform step and write a custom transform in Python (Pandas) programming language.
    e. Select a custom transform step and write a custom transform in Python (PySpark) programming language.
    -> There are 3 options for languages when creating custom transforms in SageMaker Data Wrangler: Python (Pyspark),
       Python (Pandas), and SQL (PySpark SQL).

   incorrect answer:
    A. Select a custom transform step and write a custom transform in Python (SciKit) programming language.
    -> there are only 3 options for the language when creating a custom transform in SM Data Wrangler: Python (PySpark), Python (Pandas),
       SQL (PySpark SQL)

   [Data Wrangler] Custom Transforms
     https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html
     - The Custom Transforms group allows you to use Python (User-Defined Function), PySpark, pandas, or PySpark (SQL) to define
       custom transformations.
     - For all three options, you use the variable 'df' to access the dataframe to which you want to apply the transform.
       To apply your custom code to your dataframe, assign the dataframe with the transformations that you've made to the 'df' variable.

Question 35:

  Tasked with quickly building a ML model to test one of the team's minimum viable products with the intent of persevering or pivoting.
  You are using SM Autopilot to create your experiment. You have selected S3 Bucket data source, and target feature. You are ready to
  select the ML problem type and objective metric. Which are viable combinations.

  Correct answer (provided it is a regression problem which is not clear from the discription)
    -> When running a regression algorithm, the option that make sense in SM AutoPilot available choices are MSE

   [SG AutoPilot objective] Metrics and validation
     https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Question 36:

  Using SM built-in XGBoost to predict user purchase of apps similar to the apps they have already purchased. You are  using
  hyperparameter tuning. Which evaluation metrics and corresponding optimization direction should you choose for your automatic
  model tuning? (select 2)

  correct answer:
    c. ndcg, maximize
      -> XGBoost uses ndgc (Normalized Discounted Cumulative Gain) metric for model validation, and you want to maximize it.
    e. mae, minimize
      -> XGBoost uses mae (Mean Absolute Error) metric for model validation, and you want to minimize it.

  incorrect answer:
    b. map, minimize
      -> XGBoost uses map (mean average precision) metric for model validation. However, you want to maximize it.

  Normalized Discounted Cumulative Gain (NDCG) explained
    https://www.evidentlyai.com/ranking-metrics/ndcg-metric

    - Normalized Discounted Cumulative Gain (NDCG) is a metric that evaluates the quality of recommendation and information
      retrieval systems.
    - NDCG helps measure a machine learning algorithm's ability to sort items based on relevance.
    - It compares rankings to an ideal order where all relevant items are at the top of the list.
    - NDCG can take values from 0 to 1, where 1 indicates a match with the ideal order, and lower values represent a lower quality of ranking.


   search: mean average precision
   AI Overview
     - Mean Average Precision (mAP) is a metric used to evaluate the accuracy of object detection models and information retrieval systems.
     - It's a key metric for machine learning applications like autonomous driving, face recognition, and image segmentation.

     How it's calculated

      1. Calculate the average precision (AP) for each class
      2. Take the mean of all the APs

         mAP = (1/Q) *  AveP(q)

            where:
                             Sum of q=1 to q=Q
               Q              the number of queries in the set
               AveP(q)        the average precision (AP) for a given query Q

      What it measures

        Precision: The number of correct predictions
        Recall: The model's ability to identify positive samples
        Confidence thresholds: The accuracy of predictions at different levels of confidence

       What it's based on

         Confusion matrix: A sub-metric of mAP

         Intersection over Union (IoU): A sub-metric of mAP that measures how well two boxes align

       Why it's important
        - mAP is a comprehensive metric that provides a more robust assessment of model performance than just accuracy or precision alone

Question 37:

  You need to produce visualization to get an idean of which features are useful and which can be improved using dimensionality
  reduction.  You have several data sources that you would like to visualize in QuickSight. Which of these data sources cannot
  be directly use as data sources in Quicksight?

  Correct answer:
    a. DynamoDB
      -> DynamoDB is not supported as a direct data source for QuickSight. You need to uses an intermediary service, such as
         Athena and it data source connectors to make your DynamoDB table data available to Quicksight.
  incorrect answers (can be used as Quicksight data sources)
    b Snowflake
    c. Presto
    d Teradata


  [QuickSight] Supported data sources
    https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html

    - Amazon QuickSight supports a variety of data sources that you can use to provide data for analyses.
    - including
       Connecting to relational databases
         [long list include Presto SQL and Teradata]
         Note: does not include non-relational (NoSQL) databases like DynamoDB
      Importing file data
        [CSV, TSV, ELF, CLF, JSON, XLSX]
      Software as a Service (SaaS) data
        [includes Jira, ServiceNow, Adobe analytics, GitHub, Saleforce]


Question 38:


  You have a recommendation engine model variant that processes inference requests for event content streaming request. When your
  model variant receives these big spikes in inference requests, your streaming services suffers poor performance. You will use
  SM autoscaling to mee the varying model demand. Which type of scaling policy should you use in you SM autoscaling implementation?

  correct answer: a. target-tracking scaling
    -> AWS recommends that you use this scaling policy for your autoscaling configuration because it is fully automated

  incorrect answers:
     b. step scaling
       -> AWS recommends that you use step scaling when you need an advanced configuration, such as specifying how many instances
          to deploy under certain circumstances. You don't have a specialized need like this.
     c. simple scaling
       -> claims this is not a valid option, but AWs doc notes that TargetTrackScaling is also referred to as simple scaling

  Configuring autoscaling inference endpoints in Amazon SageMaker
    https://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/

    Scaling options
      - You can define minimum, desired, and maximum number of instances per endpoint and, based on the autoscaling configurations,
        instances are managed dynamically.

      TargetTrackingScaling (or Simple scaling)
        - Use this option when you want to scale based on a specific Amazon CloudWatch metric.
        - You can do this by choosing a specific metric and setting threshold values.
        - The recommended metrics for this option are average 'CPUUtilization' or 'SageMakerVariantInvocationsPerInstance'.
           - 'SageMakerVariantInvocationsPerInstance' is the average number of times per minute that each instance for a variant is invoked.
           - 'CPUUtilization 'is the sum of work handled by a CPU.
      Step scaling
        - This is an advanced type of scaling where you define additional policies to dynamically adjust the number of instances to scale
          based on size of the alarm breach.
        - This helps you configure a more aggressive response when demand reaches a certain level.
      Scheduled scaling
        - You can use this option when you know that the demand follows a particular schedule in the day, week, month, or year.
        - This helps you specify a one-time schedule or a recurring schedule or cron expressions along with start and end times, which
          form the boundaries of when the autoscaling action starts and stops
      On-demand scaling
        - Use this option only when you want to increase or decrease the number of instances manually.
        - This updates the endpoint weights and capacities without defining a trigger.


Question 40:

  Using SM Studio. You are at the point in data exploration where you need to know the importance of each of the features in
  your training dataset. which option gives you the most efficient view of this feature comparison?

  correct answer:
    d. Use Data Wrangler Quick Model visualization to show the importance score of each feature in a bar chart
      -> Data Wrangler Quick Model visualization helps you evaluate your data by producing importance scores fore each feature.

  incorrect answer.
    a. Use Data Wrangler target leakage visualization to show the importance score of each feature in a bar chart
      -> target leakage visualization shows when there is data in the ML training dataset that is strongly correlated with
         the target label.
    b. Use Data Wrangler bias visualization to show the importance score of each feature in a bar chart
      -> bias visualization helps you identify bias during data preparation.


  Quick Model
   - Use the Quick Model visualization to quickly evaluate your data and produce importance scores for each feature.
   - A feature importance score score indicates how useful a feature is at predicting a target label.
   - The feature importance score is between [0, 1] and a higher number indicates that the feature is more important to the whole dataset.
   - On the top of the quick model chart, there is a model score.
     - A classification problem shows an F1 score. A regression problem has a mean squared error (MSE) score.

Question 42:

  Correct answer:
    A. Stream your data sources via Kinesis Data Firehose to your MongoDB databases, using a lambda function to perform feature transformation.
    -> You can stream your data sources via Kinesis Data Firehose, use a lambda function that you write to perform feature transformations,
       then stream the transformed data to an HTTP endpoint for the MongoDB third party service provider


  What is Amazon Data Firehose?
    https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html
    - Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as S3, Redshift,
      Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, Apache Iceberg Tables, and any custom HTTP endpoint or HTTP endpoints
      owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, Coralogix, and Elastic.

Question 44:

  You work for a online flight booking services that finds the lowest cost flights based on user inputs.  Your team gathers data from
  many sources, include airline flight databases, credit agencies, etc. to use in your model. You need to transform this data for
  your model training and in real-time for your model inference requests. what is the most efficient way to build these
  transformations into  your model flow?

  correct answer:
    B. Uses Apache Spark ML Serving to deploy an inference pipeline that reuses the data transforms you developed for your training model.
      -> You can use SM Spark ML Serving containers that reuse the data transformers developed for training your model in your
         inference endpoints.

     MLeap:
       - MLeap is a common serialization format and execution engine for machine learning pipelines.
       - It supports Spark, Scikit-learn and Tensorflow for training pipelines and exporting them to an MLeap Bundle.
       - Serialized pipelines (bundles) can be deserialized back into Spark for batch-mode scoring or the MLeap runtime to
         power realtime API services.
     MLlib
       - Apache Spark comes with a Machine Learning library called MLlib which lets you build ML pipelines using most of the
         standard feature transformers & algorithms.
     Spark ML Serving Container
       - SageMaker SparkML Serving Container leverages an open source library called MLeap.
       - SageMaker SparkML Serving Container takes a code-free approach for performing inference.
       - You need to pass a schema specifying the structure of input columns and output column.
       - The web server will return you the contents of the output column in a specific format depending on content-type and Accept.


  Inference pipelines in Amazon SageMaker AI
    https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html

    - You can add SageMaker AI 'Spark ML Serving' and 'scikit-learn' containers that reuse the data transformers developed for training models.
    - The entire assembled inference pipeline can be considered as a SageMaker AI model that you can use to make either real-time predictions
      or to process batch transforms directly without any external preprocessing.

    - Within an inference pipeline model, SageMaker AI handles invocations as a sequence of HTTP requests.
    - The first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the
      second container, and so on, for each container in the pipeline. SageMaker AI returns the final response to the client.


Question 46:

  You stream in data from many sources, such as satellite feeds, IoT devices like underwater sensors, etc. You have recently implemented
  SM Feature Store, and you are now implementing the ingestion of batch data from your streaming data sources. Which of the following
  options are viable approaches to streaming data into SM Feature store (Select 2)?

  Correct answers:
    A. Stream your data sources through Apache Kaffka into feature store
    -> Apache Kafka can be used as a streaming data source whenre features are directly fed to the online feature store for feature creation
    b. Stream your data sources through Kinesis Data Analytics and a lambda function into feature store
    -> Kinesis Data Analytics together with a  Lambda Function, can be used as a streaming data source whenre features are directly fed
       to the online feature store for feature creation

  Incorrect Answer:
    C. Stream your data sources through Apache Spare STreaming into feature store
    -> Appatch Spark Streaming is not support as a direct straming feed into SM Feature Store


   [Feature Store] Data sources and ingestion
     https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-ingest-data.html

   [Feature Store] Stream ingestion
     - You can use streaming sources such as Kafka or Kinesis as a data source, where records are extracted from, and directly feed records
       to the online store for training, inference or feature creation.
     - Records can be ingested into your feature group by using the synchronous 'PutRecord' API call. Since this is a synchronous API call
       it allows small batches of updates to be pushed in a single API call.

   [Feature Store] Create feature groups
     - To ingest features into Feature Store, you must first define the feature group and the feature definitions (feature name and data type)
       for all features that belong to the feature group.
     - After they are created, feature groups are mutable and can evolve their schema.
     - Feature group names are unique within an AWS Region and AWS account.
     - When creating a feature group, you can also create the metadata for the feature group.
       - The metadata can contain a short description, storage configuration, features for identifying each record, and the event time.
       - the metadata can include tags to store information such as the author, data source, version, and more.

Question 48:

  Which are viable approaches to streaming data into your SM Feature Store? (Select 2)

  Correct answers:
    A Stream your data sources through Apache Kafka into Feature Store
      -> Kafka can be used as a streaming data source where features are directly fed to the online feature store for feature creation.
    B. Stream your data source through Data Analytics and a Lambda Function into Feature Stores.
      -> Kinesis Data Analytics  together with the use of a Lambda function, can be used as a streaming data source where features
         are directly fed to the online feature store for feature creation.

  Why Amazon SageMaker Feature Store?
    https://aws.amazon.com/sagemaker-ai/feature-store/
    - Amazon SageMaker Feature Store is a fully managed, purpose-built repository to store, share, and manage features for
      machine learning (ML) models.


  [SG Feature Store] Feature Data Ingestion
    https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html
    - Feature generation pipelines can be created to process large batches (1 million rows of data or more) or small batches, and
      to write feature data to the offline or online store.
    - Streaming sources such as Amazon Managed Streaming for Apache Kafka or Amazon Kinesis can also be used as data sources from which
      features are extracted and directly fed to the online store for training, inference, or feature creation.

Question 50:

  Tasked with building a ML model to predict when a customer is about to leave your phone  serice or churn. The inference data from
  your model will allow your marketing department to offer incentives to the customer to get them to stay. Using data generated by
  customer activity with your service offerring, you need to visualize the inference data in a dashboard.
  How can you get your ML inference data into your dashboard visualization in the most efficient manner?

  Correct answer:
    b. Create a JSON schema that contains the metadata that QuickSight needs to process your model data, then use Augment with SM
       feature of QuickSight to visualize your customer churn data.
    -> You can use Augment with SageMaker feature of QuickSight to integrate your SM inference data into your QuickSight
       visualization. This is the most efficient option given.


  Visualizing Amazon SageMaker machine learning predictions with Amazon QuickSight
    - AWS is excited to announce the general availability of Amazon SageMaker integration in QuickSight.
    - You can now integrate your own Amazon SageMaker ML models with QuickSight to analyze the augmented data and use it directly in your
      business intelligence dashboards.
    - As a business analyst, data engineer, or data scientist, you can perform ML inference in QuickSight with just a few clicks.
    - This process makes predictions on new data and uses Amazon SageMaker models for different use cases, such as predicting the
      likelihood of customer churn, scoring leads to prioritize sales activity, and assessing credit risk for loan applications.
     . . .
    - Before you use an Amazon SageMaker model with QuickSight data, you have to create a JSON schema file that contains the metadata
      that QuickSight needs to process the model.
    - It provides metadata about the fields, data types, column order, output, and settings that the model expects, such as type of the
      instance to use for generating the predictions.

Question 51:

  Through your data discovery you have noticed that some of your features are multicollinear. How can you address the
  multicollinearity of your features?

  correct answser:
    C. Use Principal Component Analysis (PCA) to reduce your model's dimensionality, the drop the result components that have
       very low variance.
    -> Using PCA to reduce the dimensionality of your feature set by dropping the components that have very low variance
       will remove the multicollinearity of your features.

  incorrect answer:
    B. Use Linear Discriminant Analysis (LDA) to reduce your model's dimensionality, the drop the result components that have
       very low variance.
    -> LDA is used to reduce dimensionality in multi-class classification problems that predict a categorical target. We are trying
       to solve a continuous target, match point difference or spread.

    search: Use Principal Component Analysis to dropping components
    AI Overview
     - When using Principal Component Analysis (PCA) to drop components, you essentially identify the most important features
       (principal components) based on their variance and remove the remaining components that explain less variance, effectively
       reducing the dimensionality of your data while still capturing most of the essential information within it

Question 55:

  Building a mobile app to identify various types of birds from pictures. You have a large set of unlabeled images of birds that
  you want to uses as your training data for your image recognition app. Which is the most efficient approach to creating a
  labeling job to build the training dataset for your mobile app?

  correct answer:
    C. Use SageMaker Ground Truth to label your unlabeled images, leveraging lambda function to perform annotation consolidation
       and pre-labeling. Leverage SM Image Classification algorithm-based model to perform auto-annotation of your images.
    -> SM Ground Truth is the preferred method of labeling unlabeled image data. Also, using lambda funtions in your labeling
       job allows you to automate the annotation consolidation and pre-labeling tasks. Finally, the SM Image Classification
       built-in algorithm is the best choice for auto-annotation task.
    -> with annotation consolidation, the worker responses gets processed by my post-annotation lambda. You select a built-in lamba
       function or create a custom lambda functions. The built-in lambda functions are designed for specific task types and scoring,
         e.g.: built-in Lambda fcn used for:
         Image classification - Uses a variant of the Expectation Maximization approach to estimate the true class of an image based
         on annotations from individual workers.

  AnnotationConsolidationConfig
    https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AnnotationConsolidationConfig.html
    - Configures how labels are consolidated across human workers and processes output data.
    Contents
     AnnotationConsolidationLambdaArn
       - The Amazon Resource Name (ARN) of a Lambda function implements the logic for annotation consolidation and to process output data.
       - For built-in task types, use one of the following Amazon SageMaker Ground Truth Lambda function ARNs for AnnotationConsolidationLambdaArn


Question 56:

  Building a new data repository for a new home automation device [automated door locks, security cameras, ...]. The new device will
  generate large streams of IoT data using MQTT protocol. You need to build a data repository for use in your ML models that will be
  used to produce future device usage predictions. You management team will use these device usage predictions to inform their mrkt
  campaigns. Which option is the most cost-efficient configuration of AWS services to build your data repository (Select 2).

  correct answers:
    B. Receive your MQTT messages into  IoT Core. Uses an Apache Kafka IoT Core action to send your MQTT messages directly to
       Amazon Managed Streaming for Apache Kafka (MSK) to transform your MQTT data in near real-time. The use an MSK-to-s3-feeder
       appliction to write your transformed MQTT data directly to your S3 bucket that will be the data source of your ML model.
    -> The Apache Kafka IoT Core action allows you to send your MQTT message directly to Amazon MSK to transform your MQTT
       data in near real-time. MSK has an Msk-to-s3-feeder appication that your can use to writh your transformed MQTT data
       directly to your S3 bucket.

    C. Receive your MQTT messages into  IoT Core. Uses an Kinesis Firehose IoT Core action to send your MQTT messages directly to
       Kinesis Data Firehose to transform your MQTT data in near real-time and write your transformed MQTT data directly to your S3
       bucket that will be the data source of your ML model.
    -> Kinesis Data Firehose IoT Core action allows you to use Kinesis Firehose to transform your MQTT message data in near real-tim.
       Kinesis Data Firehose can the write your tranformed MQTT data directly to S3 bucket.


   [AWS IoT Core] AWS IoT rule actions
     https://docs.aws.amazon.com/iot/latest/developerguide/iot-rule-actions.html
     - AWS IoT rule actions specify what to do when a rule is invoked.
     - You can define actions to send data to an Amazon DynamoDB database, send data to Amazon Kinesis Data Streams, invoke an AWS
       Lambda function, and so on. AWS IoT supports
     - Some AWS IoT suported actions include:

       Rule action 	        Description 	                                        Name in API
       --------------           -----------------------------------------               ---------------
       Apache Kafka 	        Sends a message to an Apache Kafka cluster. 	        kafka
       Firehose 	        Sends a message to a Firehose delivery stream. 	        firehose
       Kinesis Data Streams 	Sends a message to a Kinesis data stream. 	        kinesis
       Lambda 	                Invokes a Lambda function with message data as input. 	lambda
       S3                	Stores a message in an Amazon S3 bucket.        	s3


Question 57:

  To produce accurate predictions, you will need to use several ML models that use the real-time streaming data as their training and
  inference data sources.  Since real-time streaming game data is delivered from several different sources, the format and schema of
  the data need transformtion and sanitation. Which option is the most efficient way to perform the feature engineering of your
  real-time streaming data for use in training and inference requests.

  Correct answer:
    C. Ingest the real-time streaming data using Kinesis Data Firehose using kinesis-firehose-process-record lambda blueprint for
       transformation. Stream the output of your Kinesis Data Firehost into SageMaker offline and online feature store FeatureGroups.

    -> You can ingest your streaming data usgin Kinesis Data Firehose and uses kinesis-firehose-process-record lambda blueprint
       for transformation. You will also want to stream the output of your Kinesis Data Firehose into the offline and online
       feature store FeatureGroups since you wish to train using your offline Feature Stores groups and produce real-time
       inferences using your online Feature Store FeatureGroup.

  incorrect answer:
    D. Ingest the real-time streaming data using Kafka, and use kinesis-process-record lambda blueprint for transformation.
       Stream the output of your Kinesis Data Firehost into SageMaker online feature store FeatureGroups.
     -> You can ingest your streaming data using Kafka, and you could uses the kinesis-process-record lambda function blueprint for
        transformation. However, you need to stream to output of your Data Firehose into both offline and online data feature
        store FeatureGroups since you wish to train using your offline Feature Store groups and produce real-time inferences using
        your online Feature Store FeatureGroup.
        -> Note: It does not describe going from Kafka to Firehose.


  Amazon Data Firehose Supported Lambda blueprints
   - These blueprints demonstrate how you can create and use AWS Lambda functions to transform data in your Amazon Data Firehose data streams.
   - To see the blueprints that are available in the AWS Lambda console

    1. Sign in to the AWS Management Console and open the AWS Lambda console at https://console.aws.amazon.com/lambda/
    2. Choose Create function, and then choose Use a blueprint.
    3. In the Blueprints field, search for the keyword firehose to find the Amazon Data Firehose Lambda blueprints.

    List of blueprints:

      Process records sent to Amazon Data Firehose stream (Node.js, Python)
        (kinesis-firehose-process-record ??)
        - This blueprint shows a basic example of how to process data in your Firehose data stream using AWS Lambda.

      Convert Amazon Data Firehose stream records in syslog format to JSON (Node.js)
        - This blueprint shows how you can convert input records in RFC3164 Syslog format to JSON.

Question 61:

   Your department is responsible for leveraging the bank's huge data lake to gain insight and make predictions. Latest project is
   an XGBoost prediction model.  You want to run some additional batch predictions using a batch inference job to make sure your
   model can handle the production prediction workload. In SM notebook, how to extend your estimator to read input data in batch
   from a specified S3 bucket and make predictions?

   correct answer:
     A. Extend the estimator to a transformer object.
     -> You can extend your estimator to a transformer object, which is derived from SM Transformer Class. The batch transformer
        read input data from a specified S3 bucket and make predictions.

  SageMaker Batch Transform
    https://sagemaker.readthedocs.io/en/v2.31.1/overview.html
    - After you train a model, you can use Amazon SageMaker Batch Transform to perform inferences with the model.
    - Batch transform manages all necessary compute resources, including launching instances to deploy endpoints and deleting
      them afterward.

    - If you trained the model using a SageMaker Python SDK estimator, you can invoke the estimators transformer() method to
      create a transform job for a model based on the training job:

      >>> transformer = estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')

     - Alternatively, if you already have a SageMaker model, you can create an instance of the Transformer class by calling its constructor:

       >>> transformer = Transformer(model_name='my-previously-trained-model',
       >>>                           instance_count=1,
       >>>                           instance_type='ml.m4.xlarge')

      - After you create a Transformer object, you can invoke transform() to start a batch transform job with the S3 location
        of your data. You can also specify other attributes of your data, such as the content type.

         >>> transformer.transform('s3://my-bucket/batch-transform-input')


Question 62:

  Large web retailer, needs to gain insight into their sales patterns. They need a way to use visualization to show their sales data
  in near real-time so that they can quickly recognize higher-than-expected sales of specific products.
  Which option is a viable, efficient solution to their problem?

  Correct answer:
    C. Use Kinesis Data Firehose to stream your data to S3. Using QuickSight ML Insights to use the output as source data to
       visualization in QuickSight.
    -> Streaming your data directly to S3 using Data Firehose is very efficent. Also, using QuickSight's integrated ML Insights
       Random Cut Forest Capability requires less development and coding effort than other options.

  incorrect answer:
    D. Use Data Firehose to stream your data to Kinesis Data Analytics application that runs a Random Cut Forest SageMaker model
       on the data continuously, writing the output to S3 which is then used as source data to visualization in QuickSight.
    -> Kinesis Data Analytics has a RCF capability that you can use to detect your sales outliers. However, you would still need
       build your visualizatioin in QuickSight. The option of using ML Insights directly within QuickSight allows you to run
       your anamoly detection and visualize your data more quickly.


   Gaining insights with machine learning (ML) in Amazon QuickSight
   https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html
   - With ML insights, you can avoid spending hours manually analyzing and investigating.
   - You can select from a list of customized context-sensitive narratives, called autonarratives, and add them to your analysis.
   - In addition to choosing autonarratives, you can choose to view forecasts, anomalies, and factors contributing to these.
   - You can also add autonarratives that explain the key takeaways in plain language, providing a single data-driven truth for your company.


   What are ML Insights?
     https://aws.amazon.com/blogs/big-data/introduction-to-amazon-quicksight-ml-insights/
     - QuickSight uses ML to help uncover hidden insights and trends in your data.
     - It does that by using an ML model that over time and with an increasing volume of data being fed into QuickSight, continually
       learns and improves its abilities to provide three key features (as of this writing):

         ML-powered anomaly detection
            Detect outliers that show significant variance from the dataset.
           - This can help identify significant changes in your business metrics such has low-performing stores or products, or top selling items.

         ML-powered forecasting
            Detect trends and seasonality to forecast based on historical data.
           - This can help project sales, orders, website traffic, and more.

         Autonarratives
            Embed narratives in your dashboard to tell the story of your data in plain language.
           - This can help convey a shared understanding of the data within your organization.
           - You can use either the suggested autonarrative or you can customize the computations and language to meet your
             organizations unique requirements.

   How does the ML model work?
     - QuickSight uses a built-in version of the Random Cut Forest (RCF) algorithm.
     - This is a special type of Random Forest (RF) algorithm, a widely used and successful technique in ML.
     - It takes a set of random data points, cuts them down to the same number of points, and then builds a collection of models.
     - In contrast, a model corresponds to a decision treethereby the name forest. Because RCF cant be easily updated in an
       incremental manner, RCFs were invented with variables in tree construction that were designed to allow incremental updates.


------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Core ML Concept Quiz

Question 1:

  When you examine your data visually using python matplotlib library, you find the data has what looks like a non-Gaussian distribution
  of well depth and oil well production

  Which correlation coefficient would you use to best summarize the strength of the correlation between your oil well depth and oil well
  production?

   correct answer:
     c. Spearman's correlation coefficient
       -> Spearmans correlation coefficient is used when you have a non-Gaussian relationship between your variables

   incorrect answer:
     a. Covariance correlation coefficient
       -> Covariance correlation coefficient is used when you have a Gaussian relationship between your variables
     b. Pearson's correlation coefficient
       -> Pearson's correlation coefficient is used when you also have a Gaussian relationship between your variables


  How to Calculate Correlation Between Variables in Python
    https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/

    Covariance [colleration]

      - The calculation of the sample covariance is as follows:

          cov(X, Y) = (sum (x - mean(X)) * (y - mean(Y)) ) * 1/(n-1)

      - The use of the mean in the calculation implies that each data should ideally adhere to a Gaussian or Gaussian-like distribution.

      - The covariance sign can be interpreted as whether the two variables change in the same direction (positive) or change in
        different directions (negative).
      - A covariance value of zero indicates that both variables are completely independent.
      - The magnitude of the covariance is not easily interpreted
      - A problem with covariance as a statistical tool alone is that it is challenging to interpret

    Pearsons Correlation

      - The Pearson correlation coefficient can be used to summarize the strength of the linear relationship between two data samples.
      - Pearsons correlation coefficient is calculated:

           Pearson's correlation coefficient = covariance(X, Y) / (stdv(X) * stdv(Y))

      - It is the normalization of the covariance between the two variables to give an interpretable score.
      - The use of mean and standard deviation in the calculation suggests the need for the two data samples to have a Gaussian
        or Gaussian-like distribution.

      - The coefficient returns a value between -1 and 1, symbolizing the full spectrum of correlation: from a complete negative
        correlation to a total positive correlation.
      - A value of 0 means no correlation.
      - The value must be interpreted, where often a value below -0.5 or above 0.5 indicates a notable correlation, and values below
        those values suggests a less notable correlation.

      - calculate the Pearson's correlation between two variables
          >>> from numpy.random import randn
          >>> from numpy.random import seed
          >>> from scipy.stats import pearsonr
          >>> # seed random number generator
          >>> seed(1)
          >>> # prepare data
          >>> data1 = 20 * randn(1000) + 100
          >>> data2 = data1 + (10 * randn(1000) + 50)
          >>> # calculate Pearson's correlation
          >>> corr, _ = pearsonr(data1, data2)
          >>> print('Pearsons correlation: %.3f' % corr)

     Spearmans Correlation

       - While many data relationships can be linear, some may be nonlinear. These nonlinear relationships are stronger or weaker
         across the distribution of the variables. Further, the two variables being considered may have a non-Gaussian distribution.

       - Spearmans correlation coefficient can be used to summarize the strength between the two data samples.
       - This test of relationship can also be used if there is a linear relationship between the variables but will have slightly less
         power (e.g. may result in lower coefficient scores).

       - the scores are between -1 and 1 for perfectly negatively correlated variables and perfectly positively correlated respectively.

       - Instead of directly working with the data samples, it operates on the relative ranks of data values. This is a common approach
         used in non-parametric statistics, e.g. statistical methods where we do not assume a distribution of the data such as Gaussian.

            Spearman's correlation coefficient = covariance(rank(X), rank(Y)) / (stdv(rank(X)) * stdv(rank(Y)))

       - A linear relationship between the variables is not assumed, although a monotonic relationship is assumed. This is a mathematical
         name for an [consistently] increasing or decreasing relationship between the two variables.

       - If you are unsure of the distribution and possible relationships between two variables, the Spearman correlation coefficient
         is a good tool to use.

      search: relative rank of data values meaning
      AI overview
      "Relative rank of data values" means the position of a data point within a dataset when all the values are ordered from smallest
       to largest, essentially indicating how a particular value compares to the other values in the set,


Question 2:

  You decide to run a Pearson's correlation coefficient to understand your data correlation in a better way. When you calculate your
  Pearson's correlation coefficient of social media advertising ROI< you get a value of 0.35. What conclusion can you draw from this result?

  correct answer:
    D. You cannot declare a notable correlation with confidence based on the result coefficient.
    -> Your coefficient falls into the indeterminate range. For a Pearson's correlation coefficient to indicate a notable correlation,
       the coefficient value should be above 0.5 for a positive correlation or below -0.5 for a negative correlation.

Question 3:

  You need to be able to block web application posts that contain inappropriate words quickly. You have defined a vocabulary of words
  deemed inappropriate your site. Which algorithm is best suited to your task?


  Correct answer:
    B. Bernoulli Naive Bayes
      -> The Bernoulli Naive Bayes algorithm is used in document classification tasks where you wish to know whether a word from your
         vocabulary appears in your observed text or not. 


  Machine Learning Tutorial: The Naive Bayes Text Classifier
    http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/

    What is the Naive Bayes Classifier?
      - The Naive Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem with strong and nave
        independence assumptions.
      - It is one of the most basic text classification techniques with various applications in email spam detection, personal email
        sorting, document categorization, sexually explicit content detection, language detection and sentiment detection.
      - Despite the nave design and oversimplified assumptions that this technique uses, Naive Bayes performs well in many complex
        real-world problems.

      - Even though it is often outperformed by other techniques such as boosted trees, random forests, Max Entropy, Support Vector
        Machines etc, Naive Bayes classifier is very efficient since it is less computationally intensive (in both CPU and memory)
        and it requires a small amount of training data. Moreover, the training time with Naive Bayes is significantly smaller as
        opposed to alternative methods.

  Which Naive Bayes Variation to use?

    Multinomial Naive Bayes
      - used when the multiple occurrences of the words matter a lot in the classification problem.
      - an example is when we try to perform Topic Classification.
    Binarized Multinomial Naive Bayes
      - used when the frequencies of the words dont play a key role in our classification.
      - an example is Sentiment Analysis, where it does not really matter how many times someone mentions the word bad but
        rather only the fact that he does.
    Bernoulli Naive Bayes
      - used when in our problem the absence of a particular word matters.
      - For example Bernoulli is commonly used in Spam or Adult Content Detection with very good results.

      - the Bernoulli variation generates a Boolean indicator about each term of the vocabulary equal to 1 if the term belongs
        to the examining document and 0 if it does not.
      - The model of this variation is significantly different from Multinomial not only because it does not take into consideration
        the number of occurrences of each word, but also because it takes into account the non-occurring terms within the document.
      - While in Multinomial model the non-occurring terms are completely ignored, in Bernoulli model they are factored when
        computing the conditional probabilities and thus the absence of terms is taken into account.

Question 4:

   You are tasked with find novel entries in a web application forms to gather citizen data for census purpose. A novel entry is
   defined as an outlier compared to the established set of citizen entires in your datastore. You cleaned your citizen datastore
   to remove any outliers. You need to build a modiel to determine whether new entires on your web application are nove. Which
   algorithm best fits these requirements.

   Correct answer:
     d. Support Vector Machine
       -> SVM algorithm can be used when your training data has no outliers, and you want to detect whether a new observation is
          a novel entry.

  2.7. Novelty and Outlier Detection
    https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection

    outlier detection:
      - The training data contains outliers which are defined as observations that are far from the others.
      - Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the
        deviant observations.

    novelty detection:
      - The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier.
        In this context an outlier is also called a novelty.

    - Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal
      or unusual observations.
    - Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection.

   2.7.2. Novelty Detection
     - The One-Class SVM has been introduced for that purpose [novelty detection] and implemented in the Support Vector Machines
       module in the 'svm.OneClassSVM object'.
     - The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter.


Question 7:

  How can you change your ML code to get it to use multiple GPUs with the least amount of effort?

  correct answer:
    C. Add Horovod to your code and use its distributed deep learning training framework for TensorFlow
    -> Using  the Horovod distributed deep learning training framework for TensorFlow allows you to distribute your training across many
       GPUs in parallel easily.


  Horovod
  https://github.com/horovod/horovod

    - Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.
      The goal of Horovod is to make distributed deep learning fast and easy to use.

    - The primary motivation for this project is to make it easy to take a single-GPU training script and successfully scale it
      to train across many GPUs in parallel.


  Multi-GPU and distributed training using Horovod in Amazon SageMaker Pipe mode
    https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/

    - In this post, I explain how to run multi-GPU training on a single instance on Amazon SageMaker, and discuss efficient
      multi-GPU and multi-node distributed training on Amazon SageMaker.

    Basics on Horovod
     - When you train a model with a large amount of data, you should distribute the training across multiple GPUs on either
       a single instance or multiple instances.
     - Deep learning frameworks provide their own methods to support multi-GPU training or distributed training.
     - However, there is another way to accomplish this using distributed deep learning framework such as Horovod.
     - Horovod is Ubers open-source framework for distributed deep learning, and its available for use with most popular deep
       learning toolkits like TensorFlow, Keras, PyTorch, and Apache MXNet.
     - It uses the all-reduce algorithm for fast distributed training rather than using a parameter server approach, and includes
       multiple optimization methods to make distributed training faster.

------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Core ML Concept Quiz

Question 2:

  B. Catalog the structured and unstructured manufacturing process data  using a Glue Crawler to populate the Glue catalog.
     Then have your data scientists use Athena to run queries on their manufacturing data. Finally, the data scientist can
     build their KPI dashboards using Quicksight Athena dataset feature.

  [Amazon QuickSight] Supported data sources
    https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html
    - QuickSight supports a variety of data sources that you can use to provide data for analyses.
      The following data sources are supported.

    Connecting to relational data
      - You can use any of the following relational data stores as data sources for Amazon QuickSight (partial list):
         - Amazon Athena
         - Amazon Aurora
         - Amazon OpenSearch Service
         - Amazon Redshift
         - Amazon Redshift Spectrum
         - Amazon S3

    Importing file data
      - You can use files in Amazon S3 or on your local (on-premises) network as data sources.
      - QuickSight supports files in the following formats:

           CSV and TSV  Comma-delimited and tab-delimited text files

           ELF and CLF  Extended and common log format files

           JSON  Flat or semistructured data files

           XLSX  Microsoft Excel files

      - QuickSight supports UTF-8 file encoding, but not UTF-8 (with BOM).

      - Files in Amazon S3 that have been compressed with zip, or gzip  can be imported as-is.
      - If you used another compression program for files in Amazon S3, or if the files are on your local network,
        remove compression before importing them.

Question 3:

  Your ML team will using Amazon Kendra to build an indexed searchable document repository. Your team has created your
  Kendra index and has add your data sources (HTML files, plain text files, PDFs, Word docs, & Powerpower docs) in your
  S3 bucket to your index using Kendra BatchPutDocument API call. However, you see in your CloudWatch logs an HTTP 400
  status code and some of your documents have not bue successfully indexed. What could be the source of the index failure?

  correct answer:
    B. the text extracted from the individual Word document exceeds 5 MB.
      -> One of the limits for Kendra documents is the text extracted from and individual document cannot exceed 5 MB.

  BatchPutDocument
    https://docs.aws.amazon.com/kendra/latest/APIReference/API_BatchPutDocument.html
    - Adds one or more documents to an index.

    - The BatchPutDocument API enables you to ingest inline documents or a set of documents stored in an Amazon S3 bucket.
    - Use this API to ingest your text and unstructured text into an index, add custom attributes to the documents, and
      to attach an access control list to the documents added to the index.

    - The documents are indexed asynchronously. You can see the progress of the batch using AWS CloudWatch.
    - Any error messages related to processing the batch are sent to your AWS CloudWatch log.
    - You can also use the BatchGetDocumentStatus API to monitor the progress of indexing your documents.

    Request Parameters:
    ...
      Documents
        - One or more documents to add to the index.
        - Documents have the following file size limits.

            50 MB total size for any file

            5 MB extracted text for any file

         - Type: Array of Document objects
         - Array Members: Minimum number of 1 item. Maximum number of 10 items.
         - Required: Yes


Question 5:


  You have a large multi-column online product data with one column missing 40% of its data. Your team thinks that you can
  use some of the columns in the dataset to create the missing data.
  Which feature engineering is the best approach to create approximate replacements for the imissing data while also
  preserving the integrity of the dataset.


  A Cloud Guru: AWS Certified Machine Learning - Specialty (MLS-C01): Exploratory Data Analysis

    2.2 Imputing Missing Data
    ...
      Multivariate Imputation By Chained Equation (MICE)
        - a statistical method of imputing missing data
        - runs multiple imputation threads parallelly
        - combines the results to produce the final dataset
        - The reasoning behind running multiple threads is to maintains the relationship between the variables
          in the original data and reduces the bias introduced by the imputed values.
        - effective for datasets with large amounts of missing data
        - computationally intensive because of the mutliple parallel runs
        - Overall MICE has been proven as a valuable tool for addressing missing data that is more reliable and accurate


Question 6:

  You are building a ML video surveillanc service to process streaming video frames to find suspicious objects in the
  video frames on a list of objects identified as potentially dangerous, such as weapons. You are required to label your
  images by identifying the contents of your images at the pixel level for high accuracy.
  Which AWS service gives you the labeling accuracy you project requires?

  correct answer:
    C. SM Ground Truth Image Semantic Segmentation labeling task
      -> Using SM Ground Truth Image Semantic Segmentation labeling task, your workers classify pixels in the image into a
         set of predefined labels or classes. This will give you the pixel-level label identification accuracy you require.
      -> Note: selects only the pixel containing the object instead of creating a bounding box around the object.

  incorrect answer:
    A. SM Ground Truth Bounding Box labeling task
      -> Using SM Ground Truth Bounding Box labeling task, you can identify the pixel location of an object, but not identify
         the contexts of an image at the pixel level.


  Semantic segmentation data labeling
   - To build a machine learning model for semantic segmentation, we need to label a dataset at the pixel level.

   - auto-segment feature to the semantic segmentation labeling user interface to increase labeling throughput and improve accuracy.
   - Instead of drawing a tightly fitting polygon or using the brush tool to capture an object in an image, you only draw
     four points: at the top-most, bottom-most, left-most, and right-most points of the object. Ground Truth takes these four
     points as input and uses the Deep Extreme Cut (DEXTR) algorithm to produce a tightly fitting mask around the object.

Question 7:

  Training data has many features, and performing feature engineering to ensure you don't have any target leakage. Plan to use
  both regression and classification models to see which gives better predictive results. When using SM Data Wrangler to
  visualize your target leakage, which two metrics (for regression and classification) can use to use to measure target leakage
  (Select 2)

  Correct Answers:
    B. R2
    C. AUC - ROC
      The two metrics used by SM Data Wrangler target leakage analysis are AUC - ROC and R2.


  [SG Data Wrangler] Analyze and Visualize
    https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html
    [SageMaker Data Wrangler] Target Leakage

      - Target leakage occurs when there is data in a machine learning training dataset that is strongly correlated with the
        target label, but is not available in real-world data.
      - For example, you may have a column in your dataset that serves as a proxy for the column you want to predict with your model.

      When you use the Target Leakage analysis, you specify the following:

         Target: This is the feature about which you want your ML model to be able to make predictions.

         Problem type: This is the ML problem type on which you are working. Problem type can either be classification or regression.

         (Optional) Max features: This is the maximum number of features to present in the visualization, which shows features
           ranked by their risk of being target leakage.

     For classification:
       - the target leakage analysis uses the area under the receiver operating characteristic, or AUC - ROC curve for each
         column, up to Max features.
     For regression:
       it uses a coefficient of determination, or R2 metric.

     AUC - ROC curve
       - provides a predictive metric, computed individually for each column using cross-validation, on a sample of up
         to around 1000 rows.
       - A score of 1 indicates perfect predictive abilities, which often indicates target leakage.
       - A score of 0.5 or lower indicates that the information on the column could not provide, on its own, any useful
         information towards predicting the target. Although it can happen that a column is uninformative on its own but
         is useful in predicting the target when used in tandem with other features, a low score could indicate the feature
         is redundant.

Question 8:

  You need to rank your features based on impact to the model so that you can use feature selection to reduce the dimensionality
  of your data source. Which  scikit-learn technique can you use to get ranked order of the features based on their relationahip
  with your price target?

  Correct answer:
    B. Use scikit-learn mutual_info_regression metric
      -> The scikit-learn mutual_info_regression metric is the correct metric to use when ranking features for a continuous
         target. Your target, price, is continuous.

  incorrect answer:
    C. Use scikit-learn mutual_info_classif metric
      -> The scikit-learn mutual_info_classif metric is the metric to use with a discrete target.  Your target, price,
         is continuous.

  Introduction

  Mutual Information
    https://www.kaggle.com/code/ryanholbrook/mutual-information

    Introduction
      - when encounter a new dataset, a great first step is to construct a ranking with a feature utility metric, a function
        measuring associations between a feature and the target. Then you can choose a smaller set of the most useful features
        to develop initially and have more confidence that your time will be well spent.

      - The metric we'll use is called "mutual information". Mutual information is a lot like correlation in that it measures a
        relationship between two quantities.
      - The advantage of 'mutual information' is that it can detect any kind of relationship, while correlation only detects
        linear relationships.

      - Mutual information is a great general-purpose metric and especially useful at the start of feature development when you
        might not know what model you'd like to use yet. It is:

          - easy to use and interpret,
          - computationally efficient,
          - theoretically well-founded,
          - resistant to overfitting, and,
          - able to detect any kind of relationship

     Mutual Information and What it Measures

       - Mutual information describes relationships in terms of uncertainty.
       - The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity
         reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be
         about the target?

  [scikit-learn feature_selection] mutual_info_regression
    https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression
    sklearn.feature_selection.mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, ...)

      - Estimate mutual information for a continuous target variable.

      - Mutual information (MI) between two random variables is a non-negative value, which measures the dependency
        between the variables. It is equal to zero if and only if two random variables are independent, and higher values
        mean higher dependency.

  [scikit-learn feature_selection]mutual_info_classif
    https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif
    sklearn.feature_selection.mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, ...)

      - Estimate mutual information for a discrete target variable.

      - Mutual information (MI) between two random variables is a non-negative value, which measures the dependency
        between the variables. It is equal to zero if and only if two random variables are independent, and higher values
        mean higher dependency.

Question 10:

  You are tasked with implementing a multilingual  support chatbot. Chatbot must handle real-time translations and reponse
  generation in multiple languages. You goal is to use SM to develop and deploy a seq2seq model that not only translates but
  also understands and generates culturally relevant responses.

  As part of your data preparatory work, you engage in extensive exploratory data analysis (EDA) to understand the characteristics
  of the multilingual dataset and optimize the seq2seq model's architecture and training process.

  Which two of the following EDA steps would be the most critical to enhancing the perforance and contextual accuracy of a
  multilingual seq2seq model deployed vai AWS SM (Select 2)?

  correct answers:
    A. Implementing a custom tokenization strategies for each language to accurately reflect linguistic nuances, including
       handling idioms and compound words, which are prevalent in Mandarin and Spanish?
       -> Handling language-specific features such as idioms and compound words is essential for accurate translation and
          response generation. Custom tokenization encurse that the model can effectively parse and understand the input
          in different languages, respecting their unique grammatical and syntactic rules. This step directly impacts the
          model's ability to interpret and generate text in a way that is natural and appropriate for each language, which
          is fundamental for a chatbot dealing with multiple language inputs.

    D. Conducting a sentiment analysiss on the training dataset to understand cultural sensitivies and emotional nuances
       specificy  to each language, thus allowing the model to generate more emphathic and contextually appropriate responses.
       -> Sentiment analysis is vitual in understanding the emotional context and cultural sensitivities of the conversation.
          By analyzing sentiment, the model can be trained to recongnize and replicate appropriate emotional responses,
          which enhances the quality of interaction with users. This step encurse that the chatbot is not linguistically
          accurate but also culturally and emotionally aligned with the user.


Question 12:

  Your genomic dataset has over 10K variables. Your current project involves reducing dimensionality using PCA in SM. The
  project's success hinges on your ability to preserve critical biological signals while managing compute resources.
  Using PCA, which parameter adjustment is crucial for maintaining biological signal integrity in the dimensionality
  reduced data without overwhelming computational resources?

  Correct answer:
    B. Explained Variance Ratio
    -> This choice is paramount because adjusting the explained variance ratio directly affects the number of principal
       components retained, ensuring that only components contributing significantly to the data variance are considered.
       This balance is critical for preserving meaningful biological signals relevant to neorodegenerative conditions
       without diluting them with noise or less informative variables.
     -> Note: it is unclear if the explained variance ratio can be accessed from the SM PCA model


    https://saturncloud.io/blog/what-is-sklearn-pca-explained-variance-and-explained-variance-ratio-difference/#explained-variance-in-sklearn-pca

    Explained Variance Ratio in Sklearn PCA
      - Explained variance ratio is a measure of the proportion of the total variance in the original dataset that is
        explained by each principal component.
      - The explained variance ratio of a principal component is equal to the ratio of its eigenvalue to the sum of the
        eigenvalues of all the principal components.

      - In Sklearn PCA, the explained variance ratio of each principal component can be accessed through the
        explained_variance_ratio_ attribute.
      - For example, if pca is a Sklearn PCA object, pca.explained_variance_ratio_[i] gives the explained variance ratio
        of the i-th principal component.

      - The total explained variance ratio of a set of principal components is simply the sum of the explained variance ratios
        of those components.


Question 13:

  The fraud dataset include transactional data, customer interaction logs, and external fraud indicators across multiple
  platforms. You need to effectively integrate these diverse data and prepare the data to uncover intricate fraud patterns
  and reduce false positives.

  In the data preparation phase, which approach should be adopted to optimally integrate and prepare diverse data sources
  to enhance detection accuracy while maintaining computational efficiency?

  correct answer:
    D. Deploy a graph-based approach to link transactions and customer logs, enhancing pattern recognition and anomaly detection.
    -> Graph based approach addresses the need for complex pattern recognition in fraud detection. This method helps in
       visualizing connections and detecting unusual patterns that tranditional flat data structures might miss, such as
       unusual clusters of transactions or rare links between different accounts.

  search: machine learning graph based for diverse data analysis
  AI Overview
    - "Machine learning graph-based" refers to a type of machine learning technique that leverages graph structures to analyze
       complex data where relationships between different data points are crucial, allowing for deeper insights into diverse datasets
       by considering the connections and interactions between entities, often using algorithms like Graph Neural Networks (GNNs)
       to process this information effectively; this is particularly useful for analyzing social networks, biological pathways,
       or customer interaction patterns where connections between data points are vital for understanding the overall system.

    Key points about graph-based machine learning:

      Graph representation:
        - Data is structured as a graph where nodes represent individual data points and edges represent relationships between them.

      Node embedding:
        - Graph algorithms create "embeddings" for each node, capturing information about its neighbors and position within the
          graph, which can then be used for further analysis.

      Message passing:
        - A core mechanism in GNNs where nodes iteratively exchange information with their neighbors, allowing the model
          to learn complex relationships within the graph.

    Applications of graph-based machine learning:
      Social network analysis:
        - Identifying communities, predicting user behavior, and recommending connections based on social ties.
      Bioinformatics:
        - Analyzing protein-protein interactions, drug discovery, and gene regulatory networks.
      Fraud detection:
        - Identifying suspicious patterns in financial transactions by analyzing connections between accounts.
      Recommendation systems:
        - Suggesting products or services to users based on their past interactions and similarities with other users.


Question 14:

  You are charged with optimizing inventory levels across seasons by analyzing sales data, customer demographics, and
  seasonal trends. The data includes historical sales, weather conditions, and promotional  calendars, which need to be
  synthesized to predict seasonal demand spikes.

  What data preparation technique should be prioritized to analyze and predict seasonal inventory needs effectively,
  ensuring the retail chain meets customer demand without overstocking.

  correct answer:
    B. Develop a seasonal decomposition of sales data to extract trends, seasonality, and residuals for precise forecasting.
    -> seasonal decomposition directly addresses cyclical nature of retail sales by breaking down the components into trend,
       seasonality, and residuals. This approach allows for a clearer understanding of underlying patterns, which is crucial
       for accurate inventory planning.

  incorrect answer:
    D. Use a time-series forecasting model that incorporates external regressors like weather and holidays to predict sales
    -> Time-series forecasting with external regressors is effective but may not isolate seasonal component as explicitly
       seasonal decomposition.


Question 15:

  Company is Revamping is recommendation engine. The enhanced engine employs CNN for deep image analysis and RNN for analyzing
  sequential user interaction data from diverse sources including text, speech, images and public datasts. The complex integration
  requires the system to process analyze millions of interactions daily with sub-second latency to offer real-time personalized
  content recommendations.

  Which combinations of AWS services should be employed to deploy this model efficiently and manage the data processing demands
  (select 2)?

  correct answers:
    A. Employ SM endpoints with Elastic Inference for scalable, cost-effective deployment
      -> SageMaker provides managed service for deploying ML models, combined with Elastic Inference to add GPU power as needed.
         This offers scalability and cost-efficiency, particular important for processing large-scale data sets and complex
         models (CNNs and RNNs) involved. SM also allows dynamic scaling which is crucial during peak hours, maintaining
         performance without of provisioning excess static GPU resources.
    B. Utilize AWS Lambda for seamless, real-time data processing via Kinesis Data Streams
      -> Lambda is ideal for handling real-time data processing tasks due to its event-driven nature, scaling automatically based
         on the incoming data flow from Kinesis Data Streams. This setup supports the continuous and dynamic processing of user
         interaction data, ensuring that the recommendatioin engine can promptly adjust to new data without lag.

   https://aws.amazon.com/blogs/machine-learning/model-serving-with-amazon-elastic-inference/
   Note: Amazon Elastic Inference is no longer available. Please see Amazon SageMaker for similar capabilities.



------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Data Engineering Quiz

Question 3:

  For Transcribe to handle network connection when usera are on mobile phone, how can your leverage features of Transcribe to
  keep your solution as cost-effective as possible.

  correct answer:
    B. Use Transcribe HTTP/2 streaming client
    -> Use Transcibe HTTP/2 streaming client to handle retrying the connection when there are intermittant problems on the
       network.
  incorrect answer
    C. Use Transcribe WebSocket protocol
    -> Use Transcibe WebSocket protocol does not provide retry logic to handle connection when there are intermittant problems on the
       network. With this option, you would have to code the retry logic yourself.


  Transcribing with HTTP or WebSockets
    https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-http-websocket.html
    - Amazon Transcribe supports HTTP for both batch (HTTP/1.1) and streaming (HTTP/2) transcriptions.
    - WebSockets are supported for streaming transcriptions.

    - If you're transcribing a media file located in an Amazon S3 bucket, you're performing a batch transcription.
    - If you're transcribing a real-time stream of audio data, you're performing a streaming transcription.

  Search: aws transcribe streaming http/2 retry request
  AI Overview
    - Amazon Transcribe sends an exception response when an error occurs while processing a media stream using HTTP/2 streaming.
    - The response is encoded as an event stream.

    Explanation
      - Amazon Transcribe supports HTTP/2 for streaming transcriptions.
      - HTTP/2 is a revised version of the HTTP protocol that improves performance.
      - When an error occurs, Amazon Transcribe sends an exception response in the form of an event stream.
      - To authenticate requests, you can use AWS Signature Version 4 headers.


Question 4:

  Which AWS Services should you use to provide the live transcriptions feature to your mobile app?

  correct answer:
    C. Stream your data to Transcribe Streaming and uses 'StartStreamTranscription' API to call a bidirectional HTTP stream
       that streams your audio to Transcribe. Transcribe then streams the transcription result to your application and your
       app code produces the live transcription.

    -> Use the Transcribe Streaming service and the StartStreamTranscription API. It starts a bidirectional HTTP/2 stream that
       streams you audio to Transcribe. Then Transcribe steams the transcription results to your application and your app
       produces the live transcription to be displayed in the app user interface.


  [Transcribe] StartStreamTranscription
    - Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to Amazon Transcribe and the transcription results
      are streamed to your application.

  [Transcribe] StartTranscriptionJob
   - [used for batch translation mode]
   - Transcribes the audio from a media file and applies any additional Request Parameters you choose to include in your request.
   - To make a StartTranscriptionJob request, you must first upload your media file into an Amazon S3 bucket; you can then specify
     the Amazon S3 location of the file using the Media parameter.

Question 6:


   Your task is to project the crime rate, or number of crimes per day in the cities in your study group.
   Based on your data source and your visualization, which type of distribution do you have?

   correct answer:
     B. Poisson distribution


  AWS Certified Machine Learning - Specialty (MLS-C01): Exploratory Data Analysis

4.1 Understanding Probability Distributions
  . . .
  Discrete Distributions:
    Bernoulli Distribution
      - an event with a single trial with exactly two possible outcomes
      - The graph of a Bernoulli distribution is a simple bar chart with two values.
        The first bar indicates the outcome 1 (value of P).  The second bar indicates the outcome 2 (value of 1 - P).

    Binomial distribution
      https://medium.com/swlh/binomial-vs-bernoulli-distribution-dd9197c418d
      - repetition of multiple Bernoulli events
      - If Bernoulli distribution is an event with a single trial with exactly two possible outcomes, then binomial
        distribution is nothing but repetition of multiple Bernoulli events.
      - coin toss example: if you repeat the trial n number of times where each trial is independent, the probability
        of heads or tails is same for all the trials.
      - A binomial distribution is better represented as a histogram.

    Poisson distribution
      - the probability that an event will occur within a specific time
      - The rate of occurrence is known, but the actual timing of the occurrence is unknown.
      -  example: Predicting a hospital receiving emergency call. They know on an average they receive two
         calls per day, but they cannot predict the exact timings.
      - This distribution relies on one parameter, X, which is the mean number of events.

      -  Poisson distribution is used to estimate how many times an event is likely to occur within the given period of time


  Continuous Distributions:
    Normal distribution
      - measure and visualize symmetrically distributed data with no skew
      - example:students' scores: most of the students' scores might range between 70 and 90, which forms the cluster at the center.
        Some of the top and bottom performers contribute to the tail at both the ends.  Parting the students' score will result
        in a bell-shaped curve representing the normal distribution of the scores.

    Log-Normal distribution
      - derived from a normally distributed data and represents its logarithmic values
      - often used in financial data to understand future stock prices based on past trends.
      - lognormally distributed data does not form a symmetric shape but rather slants or skews more towards the right.


Question 7:

  You need to stream security master data used to drive ML model for stock selection. You need to stream the master data from
  various sources into your security master dta store in near-real time. Which solutions meet your requirements in the most
  efficient manner (select 2):

  correct answers:
    A. Stream your security master data using Kafka; extract the data into your online feature store using PutRecord API call
       in small batches.
       -> Use can uses SM Feature store to house your security master data. You can ingest data into SM Feature Store using the
          PutRecord API call using small batches of data. Kakfa is a common services used to stream data into SM Feature Store.
    C. Stream your security master data using Kinesis; extract the data into your online feature store using PutRecord API call
       in small batches.
       -> Use can uses SM Feature store to house your security master data. You can ingest data into SM Feature Store using the
          PutRecord API call using small batches of data. Kinesis is a common services used to stream data into SM Feature Store.

          Note: Use small batches of data to meet your real-time requirement [instead of large batches of data]


Question 11:

   Kinesis is designed for real-time data ingestion and processing. It supports streaming data from multiple sources and provides
   features for compliance and data traceability. This makes it well-suited for handling real-time dta processing needs in a
   financial fraud detection system.


Question 12:

   Kinesis vs MSK (Managed streaming services for Apache Kafka)

     - MSK is a powerful for building real-time applications and data pipelines, but it generally overkill for projects that do
       not specifically require Kafka's complex publish-and-subscribe capabilities.

Question 13:

  Tasked with enhancing the fraud detection system through SM's Object2Ved to detect frault in real time and learn new patterns as
  they emerge. The main challenge is configuring Object2Vec to efficiently handle a mixed dataset of numerical and categorical data
  while ensuring high accuracy and adaptability.

  In configuring Object2Vec within SM for a sophisticated fraud detection system which setting is essential for effectively
  managing dynamic transactional data is optimally detect emerging fraud patterns.

  correct answer:
    D. Develop a hybrid approach embedding strategy to accurately process diverse data types.
    -> This option is critical as it addresses the need to integrate and process both numerical and categorical data effectively,
       enhancing the model's capability to learn from complex transactional relationships and adapt to new and evolving fraudulent
       behavior.

Question 14:

  In Configuring SM Data Wrangler to preprocess a heterogenous dataset for a recommendation system which encoding method and
  strategy would be most appropriate to handle the high cardinality and dynamic nature of categorical data like product
  categories and regional codes, ensuring optimal input quaility for ML models?

  Correct answer:
    C. Similarity Encoding to create robust embedding that captures subtle variations and similaritys between categories.
    -> This option is particularly advantegeous in this scenaraio as it handles high cardinality typical of product categories
       and geographic data efficiently. This method is adept at creating low-dimensional, dense embeddings that main the integrity
       and relational aspects of categorical data, making it easier for ML models to identity and learn patterns. Additionally,
       similarity encoding is resilient to noise and minor inconsistencies in data, such as typographical errors, which are
       common in large datasets.

  incorrect answer:
    B. Ordinal Encoding with 'Replace with NaN' for missing values to maintain natural order without introducing bias
    -> Ordinal encoding is user when categorical data maintains a natural order, bit it may introduce artifical ordinal relationships
       when none exists, potentially misleading the model.

  [SM Data Wrangler] Ordinal Encode
    https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-cat-encode
    - Select Ordinal encode to encode categories into an integer between 0 and the total number of categories in the Input column you select.

    Invalid handing strategy: Select a method to handle invalid or missing values.

      - Choose 'Skip' if you want to omit the rows with missing values.

      - Choose 'Keep' to retain missing values as the last category.

      - Choose 'Error' if you want Data Wrangler to throw an error if missing values are encountered in the Input column.

      - Choose 'Replace with NaN' to replace missing with NaN. This option is recommended if your ML algorithm can handle missing
        values.  Otherwise, the first three options in this list may produce better results.

  [SM Data Wrangler] Similarity Encoding

    Similarity encode
      Use similarity encoding when you have the following:
        - A large number of categorical variables
        - Noisy data

      - The similarity encoder creates embeddings for columns with categorical data.
      - An embedding is a mapping of discrete objects, such as words, to vectors of real numbers.
      - It encodes similar strings to vectors containing similar values.
      - For example, it creates very similar encodings for "California" and "Calfornia".

      - Data Wrangler converts each category in your dataset into a set of tokens using a 3-gram tokenizer.
      - It converts the tokens into an embedding using min-hash encoding.

      The similarity encodings that Data Wrangler creates:
        - Have low dimensionality
        - Are scalable to a large number of categories
        - Are robust and resistant to noise

      For the preceding reasons, similarity encoding is more versatile than one-hot encoding.


Question 15:

  Large-scale data analytics platform that uses EMR with Hive to process terabytes of incoming data every hour. The data ingestion
  follows irregular patterns, and complexity arises from varying jobe execution times and the need for multiple crawlers to update
  data catalogs. The specialist needs to design a robust, time-based scheduling system that not reduces the idle time but also
  dynamically adjusts to fluctuating data volumes.

  How should the specialist design a scalable and fault-tolerant solution for EMR Hive jobs and crawlers to handle irregular data
  ingestion and job execution times?

  correct answer:
    C. Set up an AWS Step Function with CloudWatch Events to manage EMR job orchestration and invoke crawlers based on real-time
       patterns using dynamic parallelism.
    -> Step Functions and ClouldWatch Events allows the specialist to orchestrate EMR jobs with precise control over scheduling and
       dynamic scaling, adjusting for irregular patterns. Step Functions provide fault tolerant and retries, ensuring robost job
       execution, while parallelism ensures optimal resource usage. This step minimizes latency and ensures that crawlers are invoked
       at the right time, improving both the data processing accuracy and system performance.

  incorrect answer:
    A. Create a lambda funtion triggered by CloudWatch Events to monitor EMR job execution and dynamically invoke crawlers when jobs
       are completed.

    -> While Lambda can be used for lightweight automation, this approach is not optimal for complex job orchestration with dynamic
       scaling needs. Lambda has execution time limits and lacks the sophisticated state management and error-handling capabilities
       required for large-scale workflows. This would be inefficient for handling retries, parallel procesing, or managing job
       failures in such a dynamic environment.


Question 16:


  Using Kinesis Video Streams to capture high-definition video data for comprehsive surveillance system across its global facilities.
  Video data captured not ony for real-time but also securely and redundantly stored in S3 for subsequent advanced analytics and ML
  processing.
  To achieve these requirements, which configuration should be implemented?

  correct answer:
    B. Configure an S3 endpoint directly in the kinesis Video streams settings to facilitate immediate and automated data delivery.
    -> Configuring a S3 endpoint within Kinesis Video Streams settings in the model direct and efficient method to ensure the video
       data is seamlessly and automatically transferred to S3, meeting the requirements for real-time and redundant data storage. This
       configuration bypasses the need for additional services or complex setups like lambda functions or API Gateway, thus minimizing
       latench and simplifying the infrastructure.

  [Kinesis Video Streams] Automated image generation (Amazon S3 delivery)
    https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/images.html#:~:text=Automated%20image%20generation%20(Amazon%20S3%20delivery)%20Currently%2C,images%20to%20a%20customer%20specified%20S3%20bucket.
    - Currently, customers run and manage their own image transcoding pipeline to create images for various purposes like scrubbing,
      image preview, running ML models on images, and more.
    - Kinesis Video Streams offers the capability to transcode and deliver the images.
    - Kinesis Video Streams will automatically extract images from video data in real-time based on a tag, and deliver the
      images to a customer specified S3 bucket.


Question 17:

  Deploying AWS Panorama Appliance to manage and enhance video analytics capabilities at multiple remote locations. The appliance
  will implement real-time facial recognition and object tracking.

  Given the critical nature of the operations, which configuration strategy should be prioritized when setting up the AWS Panorama
  Application to ensure optimal performance and fault tolerance?

  Correct answer:
    C. Enable multi-AZ deployment with synchonized data replication across the appliance to ensure seamless failover capabilities.
    -> This option enables multi-AZ deployment with synchronized data replication offers the most reobost approach to achieving high
       availability and fault tolerance  for the Panorama Appliance. This configuraiton ensures that operations can continue without
       interruption even if one of the AZ fails by seamlessly failing over to the appliance in another zone.

  search: aws panorama appliance multi availability zones deployment
  AI Overview
   - To achieve a multi-Availability Zone (AZ) deployment with an AWS Panorama appliance, you can configure your network to connect
     the appliance to multiple subnets spanning different Availability Zones within the same AWS region, effectively distributing
     the processing load and ensuring redundancy in case of a single AZ failure; however, the Panorama appliance itself cannot be
     directly deployed across multiple AZs as it is a physical device that needs to be installed in a single location, but you can
     design your network infrastructure to access it through multiple AZs for high availability.

   Key points to consider:

    Network configuration:
      - Create subnets in different Availability Zones within your VPC.
      - Configure the Panorama appliance to connect to these subnets using a router that supports load balancing or failover mechanisms.
      - Utilize a network load balancer to distribute traffic across the different AZs where your cameras are connected to the
        Panorama appliance.


Question 18:

  A global logistics company has integrated Amazon Q with its AWS infrastruce to enhance decision-making and operational efficiency
  across international hubs. The corporation seeks to utilize Amazon Q's capabilities to support dynamic data analysis, while
  rigorously aligning with regional compliance requirements,

  Which Amazon Q feature is crucial for ensuring that real-time analytics comply with diverse international regulations and maintain
  strict data security within a multinational logistics operations.

  Correct answer:
    D. Customizable data access policies for regional compliance.
    -> It directly addresses the scenario's focus on compliance with international regulations while ensuring data security.
       Customization data access policies in Amazon Q allow the system to restrict data access based on regional legal frameworks,
       thereby preventing data breaches and ensuring that analytics outputs are compliant with local laws.


  Amazon Q Business
     - Amazon Q Business makes generative AI securely accessible to everyone in your organization.
     - Leveraging your own company's content, data, and systems, Amazon Q Business makes it easier to get you fast, relevant answers
       to pressing questions, solve problems, generate content, and take actions on your behalf.
     - Amazon Q Business easily and securely connects to commonly used systems and tools so it can synthesize everything and
       provide tailored assistance empowering your teams to be more data-driven, creative, and productive.
  Amazon Q Developer
    - Amazon Q Developer is the most capable generative AI-powered assistant for building, operating, and transforming software,
      with advanced capabilities for managing data and AI/ML.
    - Amazon Q Developer goes beyond coding to help developers and IT professionals with all of their tasks from coding, testing,
      and deploying, to troubleshooting, performing security scanning and fixes, modernizing applications, optimizing AWS resources,
      and creating data engineering pipelines. Data scientists can get guidance to quickly and easily build analytics, AI/ML, and
      generative AI applications.

Question 19:

  Given the scale of data and the proccessing requirements, which feature of Managed Service of Apache Flink should be prioritized
  to achieve the best performance while maintaining fault tolerance in your real-time dat processing architecture?

  correct answer:
    A. Utilizing savepoints for state management to provide consistent snapshots and recovery options.
    -> Utilizing savepoints for state management is crucial for achieving high performance and fault tolerance in a streaming
       architecture using Flink. Savepoints allow for precise state snapshots at defined intervals, which are essential for
       consistent state recovery in case of failures. This ensures that the stream processing can resume from the last consistent
       state, minimizing data loss and processing delays. This feature aligns with the requirements of handling massive, continuous
       data  streams by providing robust recovery mechanisms and maintaining high throughput and low latency, which are critical
       for real-time applications.

  Managed Service for Apache Flink: How it works
    https://docs.aws.amazon.com/managed-flink/latest/java/how-it-works.html

    - Managed Service for Apache Flink is a fully managed Amazon service that lets you use an Apache Flink application to process
      streaming data. First, you program your Apache Flink application, and then you create your Managed Service for Apache Flink
      application.

    Program your Apache Flink application
      - An Apache Flink application is a Java or Scala application that is created with the Apache Flink framework.
      - You author and build your Apache Flink application locally.

      - Applications primarily use either the DataStream API or the Table API
      - The other Apache Flink APIs are also available for you to use, but they are less commonly used in building streaming applications.

      - The features of the two APIs are as follows:

      DataStream API

        - The Apache Flink DataStream API programming model is based on two components:
          Data stream:
            - The structured representation of a continuous flow of data records.
          Transformation operator:
            - Takes one or more data streams as input, and produces one or more data streams as output.

          Applications created with the DataStream API do the following:
             - Read data from a Data Source (such as a Kinesis stream or Amazon MSK topic).
             - Apply transformations to the data, such as filtering, aggregation, or enrichment.
             - Write the transformed data to a Data Sink.

         - Applications that use the DataStream API can be written in Java or Scala, and can read from a Kinesis data stream,
           a Amazon MSK topic, or a custom source.

         Your application processes data by using a connector. Apache Flink uses the following types of connectors:
           Source:
              - A connector used to read external data.
           Sink:
              - A connector used to write to external locations.
           Operator:
              - A connector used to process data within the application.

         - A typical application consists of at least one data stream with a source, a data stream with one or more operators,
           and at least one data sink.

      Table API

        The Apache Flink Table API programming model is based on the following components:
          Table Environment:
            - An interface to underlying data that you use to create and host one or more tables.
          Table:
            - An object providing access to a SQL table or view.
          Table Source:
            - Used to read data from an external source, such as an Amazon MSK topic.
          Table Function:
            - A SQL query or API call used to transform data.
          Table Sink:
            - Used to write data to an external location, such as an Amazon S3 bucket.

        Applications created with the Table API do the following:
          - Create a 'TableEnvironment' by connecting to a Table Source.
          - Create a table in the 'TableEnvironment' using either SQL queries or Table API functions.
          - Run a query on the table using either Table API or SQL
          - Apply transformations on the results of the query using Table Functions or SQL queries.
          - Write the query or function results to a Table Sink.

        - Applications that use the Table API can be written in Java or Scala, and can query data using either API calls or SQL queries.

      Create your Managed Service for Apache Flink application

        Managed Service for Apache Flink is an AWS service that creates an environment for hosting your Apache Flink application
        and provides it with the following settings::
          Use runtime properties:
            - Parameters that you can provide to your application.
            - You can change these parameters without recompiling your application code.
          Implement fault tolerance:
            - How your application recovers from interrupts and restarts.
            https://docs.aws.amazon.com/managed-flink/latest/java/how-fault.html
          Logging and monitoring in Amazon Managed Service for Apache Flink:
            - How your application logs events to CloudWatch Logs.
          Implement application scaling:
            - How your application provisions computing resources.

        - You create your Managed Service for Apache Flink application using either the console or the AWS CLI.

  Allocate Kinesis Processing Units
    https://docs.aws.amazon.com/managed-flink/latest/java/how-scaling.html

    - Managed Service for Apache Flink provisions capacity as KPUs.
    - A single KPU provides you with 1 vCPU and 4 GB of memory.
    - For every KPU allocated, 50 GB of running application storage is also provided.

    - Managed Service for Apache Flink calculates the KPUs that are needed to run your application using the Parallelism
      and ParallelismPerKPU properties, as follows:

       Allocated KPUs for the application = Parallelism/ParallelismPerKPU


Question 20:
    AWS Bedrock APIs examples:

       aws bedrock-runtime invoke-model --model-id amazon.titan-text-express-v1
         -> command used to invoke the 'amazon.titan-text-express-v1' model and execute it with an input payload
       aws bedrock-runtime converse --model-id amazon.titan-text-express-v1
         -> command initiates a conversational interaction with a specifc model, aimed at simulating dialogue exchanges
            It's designed for real-time testing of the model's interactive capabilities
       aws bedrock list-foundation-models --region us-east-1
         -> command lists available foundation models in Bedrock.
            It allows you to confirm which models are accesible within their comfigured region



------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Modeling Quiz

Question 1:

  Need to provide your development team SM Studio juptyer notebooks using Scala Kernel Based on Almond Scala Kernel.
  How can you provided the required development environment to your developers for their jupyter notebooks in the most
  efficient manner?

  correct answer:
    D. Create a custom SM docker image using the Scala kernel based on the Almond Scala Kernel SM customer image sample and
       attach the image to the SM Domain.
       -> The most efficient option is to create a custom SM docker image usging Scala kernel based on Almond Scala Kernel SM
          customer image and attach it to the SM Studio domain.


  SageMaker Studio image support policy
    https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-distribution.html
    - Amazon SageMaker Distribution is a set of Docker images available on SageMaker Studio that include popular frameworks for
      machine learning, data science, and visualization.

    - The images include deep learning frameworks like PyTorch, TensorFlow and Keras; popular Python packages like numpy, scikit-learn
      and pandas; and IDEs like JupyterLab and Code Editor, based on Code-OSS, Visual Studio Code - Open Source.
    - The distribution contains the latest versions of all these packages such that they are mutually compatible.


Question 4:

  Using ML deep learning model to recognize and classify images of potential security risks. However, when your team runs their
  mini-batch training of a neural network, the training accuracy oscillates over your training epochs.
  What is the most probably cause of the training accuracy problem?

  correct answer:
    D. The learning rate is very high.
       -> A high learning rate tends to cause oscillation in yur training accuracy. A high learning rate causes your weight updates
          to be too large, and you will overestimate your goal and oscillate around your true goal.

    other info:
      small mini-batch
         A small mini-batch is used to prevent training process from stopping at local minima. Having a small mini-batch won't
         cause oscillation in you training epoch accuracy.
      large mini-batch
         A large mini-batch size is ued to allow high computational demanding matrix multiplication in your training calculations.
         Having a large mini-batch size will not cause oscillation in your train epoch accuracy.


Question 8:

  Using Amazon Fraud Detector for detecting fraud in the bank's web and mobile applications. When building your Fraud Detector
  model, which model type should you choose?

  correct answer:
    B. ONLINE_FRAUD_DETECTOR


  [Amason Fraud Detctor] Choose a model type
    - The following model types are available in Amazon Fraud Detector. Choose a model type that works for your use case.

      Online Fraud Insights  (modelType = 'ONLINE_FRAUD_INSIGHTS')
        - The Online Fraud Insights model type is optimized to detect fraud when little historical data is available about the entity
          being evaluated, for example, a new customer registering online for a new account.

      Transaction Fraud Insights
        - The Transaction Fraud Insights model type is best suited for detecting fraud use cases where the entity that is being
          evaluated might have a history of interactions that the model can analyze to improve prediction accuracy (for example,
          an existing customer with history of past purchases).

      Account Takeover Insights
        - The Account Takeover Insights model type detects if an account was compromised by phishing or another type of attack.
        - The login data of a compromised account, such as the browser and device used at login, is different from the historical
          login data thats associated with the account.


Question 9:

  Your team is tasked with forecasting the price movement of several stocks in the NASDAQ index. You have decided to use
  historical related time series in your model to improve accuracy.  Your team plans to use Amazon Forecast service.
  What Amazon Forecast algorithm would be the best choice for your stock price movement forecasting problem?

  correct answer:
    D. CNN-QR
      -> The Forecast CNN-QR algorithm is the only Forecast algorithm that accepts related time series data without future values.
      -> That is, CNN-QR is the algorithm that "Accepts historical related time series"

     - Historical related time series contain data points up to the forecast horizon, and do not contain any data points within
       the forecast horizon.
     - Forward-looking related time series contain data points up to and within the forecast horizon.

    Forecast Algorithms:
      https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html
        Neural Networks 	Flexible Local Algorithms 	Baseline Algorithms
        CNN-QR 	DeepAR+ 	Prophet 	                NPTS 	ARIMA 	ETS


Question 10:

    learning rate is too large:
       - A model will overshoot the minimum, oscillating around it and potentially diverging
    learning rate is too small:
       - the model will take an excessive number of iterations to reach a satisfactory solution
       - the model will stop updating too early, getting stuck in a local minimum


Question 12:
  Tasked with analyzing vast arrays of unstructured clinical notes to uncover patterns related to patient outcomes following
  specific treatments.

  In deploying Neural Topic Model (NTM) on SM to analyze clinical notes for associations between treatments and patient
  outcomes, which model configuration parameter is essential to refine to ensure that the model accurately captures the
  relevant medical terminologies and their underlying relationships, considering the diversity in the dataset and the need
  for precise topic discovery?

  correct answer:
    D. Number of Topics
       -> Adjusting the number of topics directly impacts the model's ability to discern and categories the medical terminologies
          and their complex relationships effectively. It ensures that the topics are neigher too broad, which could merge unrelated
          concepts, nor to narrow, which might split significant topics into lesss meaningful subtopics.
   incorrect answer:
     C. Vocabulary size (note: feature_dim)
       -> Vocabulary size is important for model training, but primarily affects which words are included in the model's vocabulary,
          not how they are grouped into topics


  NTM Hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html

     feature_dim
       - The vocabulary size of the dataset.
       - Required;  Valid values: Positive integer (min: 1, max: 1,000,000)

     num_topics
       - The number of required topics.
       - Required;  Valid values: Positive integer (min: 2, max: 1000)

     batch_norm
       - Whether to use batch normalization during training.
       - Optional;  Valid values: true or false
       - Default value: false

     clip_gradient
       - The maximum magnitude for each gradient component.
       - Optional;   Valid values: Float (min: 1e-3)

Question 13:

  You are tasked with analyzed customer feedback collected through various channels to identify prevailing customer issues, sentiments,
  and potential churn. The challenge lies in the heterogeneity of the data, which contains slang, technical jargon, and multiple languages.

  When configuring NTM in SM for analyzing telecommunications customer feedback, which factor must be meticulously optimized to handle
  the heterogenous nature of the data and ensure the extraction of coherent and industry-specific topics that can drive strategic
  customer retention efforts?

  Correct answer:
    C. Preprocessing Pipeline
      -> This is paramount because the effectiveness of topic modeling ins such diverse datasets heavily depends on how well the data
         is cleaned and preparted. Proper preprocesing including handling slang, technical jargon, and multilingual content is crucial
         for ensuring that the NTM processes teh data accurately and efficiently, extracting relevant and actional topics.

  incorrect answer:
    B. Embedding dimensions
      -> Embedding dimensions influence  the representation of words in a lower-dimensional space, but are less critical to the inital
         handling of diverse data types.


Question 14:

  In the context of deploying BlazingText on SM for creating robust multilingual sentiment analysis models, which aspect of BlazingText's
  configuration is crucial to effectively handle the linguistic nuances and the dynamic evolution of language on social media, ensuring
  the embeddings remain relevant and accurate over time?


  correct answer:
    B. subword feature integration
      -> This is vital for handling linguistic nuances and the evolution of language, especially in multilingual settings. BlazingTex's
         ability to train on subwords (character n-grams) allows the model to capture prefixes, suffixes, and internal structures of
         words, which is particularly beneficial for languages wiht rich morphology like Arabic or for adopting to new slang and lexicon
         in rapidly evolving social medla.


Question 15:

  Given the requirements, which combination of ML models should be employed to effectively build the required predictive model?

  correct answer:
    B. Random Forests, RNN, and transfer learning
      -> Random forest:
          - Effective for handling structured patient medical history. Random Forests are ensemble method that combine
            multiple decision trees to improve prediction accuracy whle also offering variable importance measures, whch are crucial for
            understanding the impact of different features (e.g genetic markers) on the adverse drug reactions (ADR) risk.
         RNN (Recurrent Neural Networks)
           - ideap for processing time-series data such as patients' vital signs and lab results. RNN can capture temporal dependencies
             and trends over time, making them well-suited for forecasting ADRs based on evolving clinical data.
         Transfer Learning:
           - this is particularly useful for working with high-resolution medical images where pre-trained CNN models (using transfer
             learning) can be fine-tuned to detect specific patterns associated with ADRs


  search: random cut forest vs random forest
  AI Overview
    - A Random Cut Forest (RCF) is a specific type of Random Forest algorithm primarily designed for unsupervised anomaly detection,
      particularly in streaming data,
    - standard Random Forest is a supervised learning method used for classification and regression tasks, meaning it requires labeled
      data to predict outcomes;
    - the key difference lies in their primary purpose and how they handle data updates, with RCF being optimized for incremental data
      analysis and anomaly detection, whereas a regular Random Forest is better suited for static datasets with known target variables.

    Key points to remember:

    Supervised vs Unsupervised:
      - Random Forest is a supervised learning algorithm,
      - Random Cut Forest is unsupervised, meaning it can identify anomalies without needing labeled data.

    Data Stream Handling:
      - RCF is specifically designed to efficiently handle streaming data, allowing for updates to the model as new data points arrive,
        whereas a standard Random Forest is not optimized for this scenario.

    Anomaly Detection Focus:
      - RCF's primary application is anomaly detection, where it assigns anomaly scores to data points based on how easily they are
        isolated in the decision trees.

    Mechanism of Isolation:
      - Both use a "forest" of decision trees, but RCF employs a different splitting strategy that focuses on randomly partitioning
        the data space to effectively identify outliers.

    When to use which:

      Use Random Forest:
        - When you have labeled data and want to perform classification or regression tasks.

      Use Random Cut Forest:
       - When you need to detect anomalies in a data stream or large datasets where labeling all data points is impractical,
         particularly in scenarios where new data is continuously arriving.


Question 18:

  Tasked with enhancing customer sentiment analysis to improve service offering and marketing strategies. Exploring the Canvas
  Ready-to-use models, specifically focusing on sentiment analysis, intending to integrate advanced features that leverage additional
  AWS services to enrich the data handling and prediction accuracy.

  Which approach would most effectively optimize the sentiment analysis using SM Canvas REady-to-use models which ensuring scalability
  and multilingual support.

  correct answer:
    B. Integrating the sentiment analysis model with Amazon Comprehend for enhanced language support and AWS Auto Scaling for handling
       variable data volumes.

     -> This provides a comprehensive approach that leverages additional AWS services to maximize the effectivenews and scalability of
        the sentiment analysis model. By integrating with Comprehend, themodle gains the ability to prociess and analysis ext in multiple
        languages, which is crucial for a multinational corporation operating in diverse markets. Additionally, Auto Scaling ensures
        that the system can dynamically adjust to its resources based on the volume of data, when enhances efficiency and maintains
        peformance at peak hours.


  [SageMaker Canvas] Ready-to-use models
    https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-ready-to-use-models.html

     - With Amazon SageMaker Canvas Ready-to-use models, you can make predictions on your data without writing a single line
       of code or having to build a modelall you have to bring is your data.
     - The Ready-to-use models use pre-built models to generate predictions without requiring you to spend the time, expertise,
       or cost required to build a model, and you can choose from a variety of use cases ranging from language detection to
       expense analysis.

     - Canvas integrates with existing AWS services, such as Amazon Textract, Amazon Rekognition, and Amazon Comprehend, to analyze
       your data and make predictions or extract insights.
     - You can use the predictive power of these services from within the Canvas application to get high quality predictions for your data.

     Canvas supports the following Ready-to-use models types:

       Sentiment analysis
         - Detect sentiment in lines of text, which can be positive, negative, neutral, or mixed. Currently, you can only do sentiment
          analysis for English language text.
       Entities extraction
         - Extract entities, which are real-world objects such as people, places, and commercial items, or units such as dates and
           quantities, from text.
       Language detection
         - Determine the dominant language in text such as English, French, or German.
       Personal information detection
         - Detect personal information that could be used to identify an individual, such as addresses, bank account numbers, and
       Object detection in images
         - Detect objects, concepts, scenes, and actions in your images.
       Text detection in images
         - Detect text in your images.
       Expense analysis
         - Extract information from invoices and receipts, such as date, number, item prices, total amount, and payment terms.
       Identity document analysis
         - Extract information from passports, driver licenses, and other identity documentation issued by the US Government.
       Document analysis
	 - Analyze documents and forms for relationships among detected text.
       Document queries
	 - Extract information from structured documents such as paystubs, bank statements, W-2s, and mortgage application forms
           by asking questions using natural language.


Question 19:

  News aggregation service focused on enhancing how the the system interprets the contextual relationships with article headlines
  and body text to better align suggestions with user preferences.

  For a recommendation engine that requires a detailed understanding of both the adjacent and non-adjacent linguistic relataionshps
  in text, when method would best enhance the model's ability to interpret complex patterns in news articles?

  correct answer:
    B. Sparse OSB (Orthogonal Sparse Bigram)
    -> Sparse OSB is ideal for the scenario as it involves a method where a sliding window captures pairs of workds within a specific
       range, include non-adjacent ones, bu introductin underscores to denote skipped words. This allows for a rich analysis of the text,
       capturing deeper and more disperesed linguistic relationship than traditional methods, which is crucial for a sophisticated content
       recommendation system. Sparse OSB provides the necessary granularity and content sensitivity needed for effective content
       personalization.



------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Machine Learning Implementation and Operations Quiz

Question 1:

  Your data set, which consists of 17 attributes and humdreds of millions of rows, is stored in libsvm format in an S3 bucket.
  When you attempt to run AutoPilot using your Autopilot job, you get error stating: Could not completed the data build processing
  job. What might be the cause of your Autopilot job failure?

  Correct answer:
    B. The format of your data source is not supported by Autopilot service.
    -> SM Autopilot only support tabular data sets in the CSV format.

  incorrect answer:
    A. Your data source is too large, 17 attributes and hundreds of millions or rows. You need to break your data source into subset
       files.
    -> The AutoPilot service, as built upon SM service, has no limit to the size of your source data.



  [SageMaker] Autopilot datasets, data types, and formats
    https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-datasets-problem-types.html
    - Autopilot supports tabular data formatted as CSV files or as Parquet files: each column contains a feature with a specific
      data type and each row contains an observation.

      CSV (comma separated values)

      Parquet
        - a binary, column-based file format where the data

     - The data types accepted for columns include numerical, categorical, text, and time series that consists of strings of
       comma-separated numbers. If Autopilot detects it is dealing with time series sequences, it processes them through
       specialized feature transformers provided by the tsfresh library

   [SageMaker] Autopilot problem types

     - For the tabular data, you further specify the type of supervised learning problems available for the model candidates as follows:
       Regression
       Binary Classification
       Multiclass Classification

Question 2:

   Your dataset extracted information from scanned bank application contains PII.
   How can your ensure the PIO data remains encypted and the credit card information is secure while ensuring the highest level of
   quality from your scanned forms?

   correct answer:
     D. Use AWS KMS to encrypt the data on S3 and in SM environment. Then obfuscate the credit card numbers from the customer data using
        AWS comprehend. Finally, use SM Augmented AI to provided a human review of the image scan data.
        -> AWS comprehend now provides a very efficient way to obfuscate your credit cared data.


  Detecting PII in Amazon Comprehend
    https://aws.amazon.com/blogs/machine-learning/detecting-and-redacting-pii-using-amazon-comprehend/

    - When you analyze text using Amazon Comprehend real-time analysis, Amazon Comprehend automatically identifies PII, as
      summarized in the following table.
      ...
    - For each detected PII entity, you get the type of PII, a confidence score, and begin and end offset.
    - These offsets help you locate PII entities in your documents for document processing to redact it at the secure storage
      or downstream solutions.

Question 5:

  You model will use a dataset containing customer credit cared information and other PII. The PII data needs to be protected to meet
  the PCI DSS requirements.
  What is the most efficient way to obfuscate the PII data, include the credit card information?

  Correct answer:
    B. Tokenize the PII data
    -> YOu can use tokenization instead of encryption when you only need to protect specific highly sensitive data for regulatory
       compliance requirements such as PCI DSS.

  Incorrect answer:
    A. Use KMS to encrypt the PII data in transit and at rest.
    -> Using KMS and encrypting your data in transit and at rest are more complex and costly thatn using tokenization for the specific
       PII data including the credit card data.


  Best practices for securing sensitive data in AWS data stores
    https://aws.amazon.com/blogs/database/best-practices-for-securing-sensitive-data-in-aws-data-stores/


    Preventative controls
      - You can layer three main categories of preventative controls:

        - IAM
        - Infrastructure security
        - Data protection (encryption and tokenization)

    Encryption and tokenization:
      - Data encryption and tokenization help protect against risks such as data exfiltration through unauthorized access mechanisms.
      - Enable data encryption both at rest and in transit

      - tokenization is an alternative to encryption that helps to protect certain parts of the data that has high sensitivity
        or a specific regulatory compliance requirement such as PCI.
      - Separating the sensitive data into its own dedicated, secured data store and using tokens in its place helps you avoid
        the potential cost and complexity of end-to-end encryption. It also allows you to reduce risk with temporary, one-time-use tokens.


  search: using tokenization with PII data
  AI Overview
    - "Using tokenization with PII data" means replacing sensitive Personally Identifiable Information (PII) like names, addresses,
      or social security numbers with random, meaningless "tokens" to protect the original data from unauthorized access, ensuring
      privacy and compliance with data protection regulations, even if a system containing the tokens is compromised; essentially,
      it's a method to obscure sensitive data by substituting it with a non-sensitive placeholder that only the authorized system
      can decode back to the original PII when needed.

    Key points about tokenization with PII data:

      How it works:
        - When PII is collected, it is immediately replaced with a unique token, which is a random string of characters with no
          inherent meaning.

      Centralized storage:
        - The mapping between the original PII and the token is stored securely in a separate, highly protected "token vault"

Question 6:

  You have used TensorFlow 2.3.0 framework using Horovod on GPU servers using python 3.7. Your training job runs well. But when you
  deploy to your container image for serving inferences, you container fails. Why is your inference container failing?

  correct answer:
    C. You need to remove the Horovod operations from your inference container.
    -> When running inference on SM inference container for TensorFlow that was trained with Horovod, you need to remove references to
       Horovid before deploying for inference.


Question 7:

  Developing model to predict S&P Emini future contract price movement. You have historical data from the past year as your training set.
  Which aWS ML service and algorithm should you use for your model that gives your team the most expeditous results with the least
  amount of administrative overhead?

  correct answer:
    E. Amazon Forecast using DeepAR+ algorithm
    -> DeepAR+ works best with large datasets containing hundreds of features time series. It also works with forward-looking related
       time series.

  incorrect answer:
    B. Amazon Forecast using Prophet algorithm
    -> best suited for time series with strong seasonal effects and several seasons of historical data.
    C. Amazon Forecast using NTPS algorithm
    -> best suited for sparse of intermittent time series.
    D. Amazon Forecast using ARIMA algorithm
    -> best suited for simple datasets under 100 times series.


Question 8:

  You built a classification algorithm to identify the plant from pic uploaded to your app.
  You need to make sure your model produces inferences quickly because your users expect a classification within 3 seconds.
  How should you optimize your model for performancez/

  correct answer:
    C. Use 'accuracy' for your evaluation metric and 'runtime' for your satificing metric.
    -> Using accuracy as your model evaluation metric while limiting your classifier runtime to your satisficing metric


Question 10:

  You are designing a travel booking bot using Lex. You need to slot to capture various departure cities from user worldwide. Which of
  the following approaches would likely present the most significant challenges.

  correct answer:
    D. Freedom Text input that allows users to type the city name without restriction, and use validation logic to confirm location
    -> Note: missed understood question - it is asking for approach giving the challenges, not the best solution


  [Amazon Lex] Built-in Slot Types

    - Amazon Lex supports built-in slot types that define how data in the slot is recognized and handled.
    - You can create slots of these types in your intents.
    - This eliminates the need to create enumeration values for commonly used slot data such as date, time, and location.

    Slot Type 	        Short Description 	                        Supported Locales
    AMAZON.Airport 	Recognizes words that represent an airport. 	All locales
    AMAZON.AlphaNumeric Recognizes words made up of letters and numbers.

    AMAZON.City 	Recognizes words that represent a city. 	All locales
    AMAZON.Country 	Recognizes words that represent a country. 	All locales
    AMAZON.DATE 	Recognizes words that represent a date and converts them to a standard format. 	All locales
    AMAZON.DURATION 	Recognizes words that represent duration and converts them to a standard format. 	All locales
    AMAZON.EmailAddress Recognizes words that represent an email address and converts them into a standard email address. 	All locales
    AMAZON.FirstName 	Recognizes words that represent a first name. 	All locales
    AMAZON.LastName 	Recognizes words that represent a last name. 	All locales
    AMAZON.NUMBER 	Recognizes numeric words and converts them into digits. 	All locales
    AMAZON.Percentage 	Recognizes words that represent a percentage and converts them to a number and a percent sign (%). 	All locales
    AMAZON.PhoneNumber 	Recognizes words that represent a phone number and converts them into a numeric string. 	All locales
    .  .   .

Question 11:

  Tasked with refining a high-frequency trading algorithm using deep learning implemented using multiple LSTM layers integrated with
  attention mechanisms. The model has exhibited challenges in model convergence and meeting latency requirements.
  Which two of the following debugging actions would be pivotal in optimizing model performance regarding convergence and latency issues?

  correct answers:
    A. Use SM Debugger to set gradient hooks to identify and address vanishing gradients during training.
    -> this directly takles one of the most common and challenging issues in training deep learning models, especially those involving
       recurrent networks like LSTMs - vanishing gradients. By setting gradient hooks, SM Debugger can identify when gradients are
       diminishing too quickly, allowing for immediate corrective actions.

    C. Configure CloudWatch for dynamic resource allocation based on real-time latency metrics to ensure trading velocity.
    -> Latency is a critical concern in high-frequency trading algorithms. CloudWatch monitors real-time latency metrics and triggers
       actions (like scaling up or down) through alarm.

   What is Vanishing Gradient?

    - The vanishing gradient problem is a challenge that emerges during backpropagation when the derivatives or slopes of the
      activation functions become progressively smaller as we move backward through the layers of a neural network.
    - This phenomenon is particularly prominent in deep networks with many layers, hindering the effective training of the model.
    - The weight updates becomes extremely tiny, or even exponentially small, it can significantly prolong the training time, and
      in the worst-case scenario, it can halt the training process altogether.


  [SageMaker] Debugger rule
    https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html

    - The following rules are the Debugger built-in rules that are callable using the Rule.sagemaker classmethod.

    - Debugger built-in rules for debugging model training data (output tensors)

      Scope of Validity 	Built-in Rules
        Deep learning frameworks (TensorFlow, MXNet, and PyTorch)
                                - dead_relu
                                - exploding_tensor
                                - poor_weight_initialization
                                - saturated_activation
                                - vanishing_gradient
                                - weight_update_ratio

        Deep learning frameworks (TensorFlow, MXNet, and PyTorch) and the XGBoost algorithm
                                - all_zero
                                - class_imbalance
                                - loss_not_decreasing
                                - overfit
                                - overtraining
                                - similar_across_runs
                                - stalled_training_rule
                                - tensor_variance
                                - unchanged_tensor

        Deep learning applications
                                - check_input_images
                                - nlp_sequence_ratio

        XGBoost algorithm
                                - confusion
                                - feature_importance_overweight
                                - tree_depth

       To use the built-in rules with default parameter values  use the following configuration format:

          from sagemaker.debugger import Rule, ProfilerRule, rule_configs

          rules = [
              Rule.sagemaker(rule_configs.built_in_rule_name_1()),
              Rule.sagemaker(rule_configs.built_in_rule_name_2()),
              ...
              Rule.sagemaker(rule_configs.built_in_rule_name_n())
          ]

Question 12:

  In enhancing the development process of predictive models from high-risk investment strategies using SM Experiments, which step is
  most crucial for ensuring that the models are optimally refined for integration into the real-time decision making platform?

  correct answer:
    D. Utilizing meta-learning to prioritize promising model configuration.
    -> This directly addresses the need to quickly identify and scale the most effective model configurations based on accumulated learning
       from multiple models and experiments. Meta-learning not only improves individual model performance but also enhances the process
       of model selection and adaptation, making it ideal for a senario when rapid and robust decision-make is required.


  What is Meta Learning?
    https://www.geeksforgeeks.org/meta-learning-in-machine-learning/

    - Meta-learning is learning to learn algorithms, which aim to create AI systems that can adapt to new tasks and improve
      their performance over time, without the need for extensive retraining.

    - Meta-learning algorithms typically involve training a model on a variety of different tasks, with the goal of learning
      generalizable knowledge that can be transferred to new tasks.
    - This is different from traditional machine learning, where a model is typically trained on a single task and then used for
      that task alone.

       - Meta-learning, also called learning to learn algorithms, is a branch of machine learning that focuses on teaching
         models to self-adapt and solve new problems with little to no human intervention.


Question 16:

  Balance your data for machine learning with Amazon SageMaker Data Wrangler
    https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

    Data Wrangler now supports the following balancing operators as part of the Balance data transform:
      Random oversampler  Randomly duplicate minority samples
      Random undersampler  Randomly remove majority samples
      SMOTE  Generate synthetic minority samples by interpolating real minority samples

Question 18:

  Which of the following correctly specifies the class weight parameter to balance the classes in an AWS Linear Learner model?

  correct answer:
    C. Apply 'auto' to 'positive_example_weight_mult' for automated adjustment
    -> The AWS linear learner 'positive_example_weight_mult' parameter can be set to 'auto', which automatically adjusts the weights of
       the classess based on the inverse frequency of the classes in the input data. This setting helps in handling imbalance
       effectively by amplifying the minority calss's influence on model training without manual intervention.

    https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html
    linear learning hyperparameters
    positive_example_weight_mult
      - The weight assigned to positive examples when training a binary classifier. The weight of negative examples is fixed at 1.
      - If you want the algorithm to choose a weight so that errors in classifying negative vs. positive examples have equal impact
        on training loss, specify balanced. If you want the algorithm to choose the weight that optimizes performance, specify auto.
      - Optional; Valid values: balanced, auto, or a positive floating-point integer; Default value: 1.0

Question 19:


  Using Glue to process extensive satellite imagery datasets. The current setup does not adequately partition incoming large data
  files, leading to inefficient reads and excessive data shuffling. Exploring advanced Glue partition strategies tailored to specific
  data characteristics and processing demands.
  Which strategies should be employed to improve the partitioning of large input files in AWS Glue, ensuring more efficent grouping
  and reading of data?

  correct answers:
    B. Apply the dynamicframe.repartition() method with a partition key derived from the data patterns.
    -> Utilizing the dynamicframe.repartition() method allows engineers to manually control how data is distributed across partitions
       by specifying a partition key that reflects underlying data characteristics. This approach directly addresses the challenge
       of inefficient reads by logically organizing the data for processing.

    E. Implement a custom Python shell Job in Glue to preprocess data into partitioned formats before ingestion
    -> Implementing a custom python shell job to preprocess and format data into optimally partitioned formats before it is ingested
       by AWS Glue can significantly improve the efficiency of the ETL process. This method provides tailored data manipulation and
       partitioning strategies that go beyond the default functionalities of AWS Glue.


  Managing partitions for ETL output in AWS Glue
    https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html

    - Partitioning is an important technique for organizing datasets so they can be queried efficiently.
    - It organizes data in a hierarchical directory structure based on the distinct values of one or more columns.

    - For example, you might decide to partition your application logs in Amazon Simple Storage Service (Amazon S3) by date,
      broken down by year, month, and day. Files that correspond to a single day's worth of data are then placed under a prefix
      such as s3://my_bucket/logs/year=2018/month=01/day=23/. Systems like Amazon Athena, Amazon Redshift Spectrum, and now AWS Glue
      can use these partitions to filter data by partition value without having to read all the underlying data from Amazon S3.


   DynamicFrame class
    https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html#aws-glue-api-crawler-pyspark-extensions-dynamic-frame-repartition
    . . .
      repartition(numPartitions)  Returns a new DynamicFrame with numPartitions partitions.


Question 21:

  Which parameter in SM Object Detection API should be meticulously configured to improve model performance on small, irregulary
  shaped objects in diverse lighting conditions?

  Correct answer:
    D. Ratio Warp
    -> 'Ratio Warp' is crucial parameter in configuring SM Object Detection model, especially when dealing with high variability
       in object shapes and sizes, such as those encountered in agricultural drone imagery. The parameter adjust the aspect ratio
       handling in the image preprocessing step, which can signficantly impact the model's ability to accurately recongnize small
       and irregular shaped objects across different lighting conditions. Proper tuning of 'RatioWarp' helps in maintaining the
       structural integrity of such objects in the input images, thus enhancing detection accuracy in complex scenarios.


   Note: Google search does NOT find any info  on SageMaker Object detection Ratio Warp


Question 22:

  In the context of SM's image classification on CNNs, which two parameters should be optimally adjusted to dynamically manage
  the learning rate on epoch results and effectively mitigate the influence of noisy data during model training?

  correct answers:
    A. Learning Rate Scheduler

    D. Reduce LROn Plateau


  Note: the Image Classification Learning rate related hyperparameters are:

  learning_rate
    - Initial learning rate.
    - Optional;   Valid values: float. Range in [0, 1].;  Default value: 0.1
  lr_scheduler_factor
    - The ratio to reduce learning rate used in conjunction with the lr_scheduler_step parameter, defined as
      lr_new = lr_old * lr_scheduler_factor.
    - Optional;  Valid values: float. Range in [0, 1].;  Default value: 0.1
  lr_scheduler_step
     - The epochs at which to reduce the learning rate. As explained in the lr_scheduler_factor parameter, the learning rate
       is reduced by lr_scheduler_factor at these epochs. For example, if the value is set to "10, 20", then the learning rate
       is reduced by lr_scheduler_factor after 10th epoch and again by lr_scheduler_factor after 20th epoch. The epochs are
       delimited by ",".
     - Optional;  Valid values: string;  Default value: no default value


  search: linear learner lr on Plateau
  AI Overview
    - "Linear Learner LR on Plateau" refers to a situation where a linear regression model, trained using a "Linear Learner"
       algorithm, experiences a "plateau" in its learning curve, meaning the model's performance stops improving significantly
       during training, despite further iterations, often indicating the need to adjust the learning rate (LR) to escape this stagnation.

    Key points to understand:

      Linear Learner:
        - A machine learning algorithm specifically designed for linear regression tasks, often used in scenarios where the
          relationship between features and target variable is expected to be linear.

      Learning Rate (LR):
        - A hyperparameter that controls how much the model updates its weights during each training iteration.
        - A high LR can lead to large jumps in the parameter space, while a low LR might result in slow learning.

      Plateau:
        - A phase in the training process where the model's performance metric (like accuracy or loss) stops improving
          significantly, appearing as a flat section on the learning curve.

    What happens when a Linear Learner hits a plateau:

      Stuck in a local minima:
        - If the LR is too low, the model might get stuck in a "local minima" where small adjustments in weights
          don't lead to noticeable performance improvement.

       Overfitting:
         - If the LR is too high, the model might overfit to the training data, leading to poor generalization on new data.

    How to address a plateau with a Linear Learner:

      Adjust the learning rate:
        Reduce the learning rate:
           - A common strategy is to gradually decrease the LR using a "learning rate scheduler" when the model hits
             a plateau.

        Cyclic learning rate:
           - Some techniques like cyclic learning rates can help escape local minima by periodically increasing and
             decreasing the LR.


Question 24:

  You are revamping the customer segmentation approach for international online retailer to include real-time behavioral data.
  Your strategy employs k-means clustering algorithm, leveraging its scalability for live data streamed from various digital
  touchpoints. The data, once collected and pre-processedin S3, is ready for clustering.
  Which configuation should you choose for deploying the k-means model in SM to most effectively update customer segments with
  ongoing data influx?

  correct answer:
    A. Update the centroids incrementally per data batch, moderate clusters, and use decay factor for older data.
    -> Because it allows the segmentation model to adjust continuously and smoothly to new customer data. By incrementally updating
       the cluster centroids - the central points that define each cluster - the model can immediately incorporate new puchasing
       behavior without have to reanalyze all past data.  The use of a moderate number of clusters ensures the model remains detailed
       enough to be useful but not som complex that it's hard to interpret.  The decay factor helps by lessening th impact of older
       data and keeping the focus on recent trends, which is essential for real-time relevance.


Question 25:

  Developing a predictive model to forecast quarterly sales using historical sales data segmented by region and product categories.
  The data set hosted on AWS exhibits complexities such as unbalanced categories, missing values, and time series components that
  need to be addressed before model training.
  When configuring the preprocessing workflow in SM Data Wrangler, which feature enables the team to programmactically customize and
  enforce consistent data transformations specific to handling time-series and imbalanced categorical variables across various datasets?

  Correct answer:
    C. Custom Transform scripts
    -> Custom transform scripts in Data Wrangler allow for the execution of user-defined code to apply specific data transforms. this
       feature is essential for addressing complex preprocessing needs such as handling time-series data and balancing categorical
       variables, which are not directly catered bo by the pre-built transformations. The scripts provided the flexibility to implement
       custom logic, ensuring that all nuances of the dataste are appropriately managed, thus maintaining consistency across differen
       data segments and enhancing the model's accuracy.  The other option, while relevant with the SM ecosystem, do not offer the
       same level of customization and direct application to the scenario's specific requirements.


  [Data Wrangler[ Custom Transforms
   - The Custom Transforms group allows you to use Python (User-Defined Function), PySpark, pandas, or PySpark (SQL) to define
     custom transformations.
   - For all three options, you use the variable df to access the dataframe to which you want to apply the transform.
   - To apply your custom code to your dataframe, assign the dataframe with the transformations that you've made to the df variable.
   - If you're not using Python (User-Defined Function), you don't need to include a return statement.
   - Choose Preview to preview the result of the custom transform. Choose Add to add the custom transform to your list of Previous steps.

   - You can import the popular libraries with an import statement in the custom transform code block, such as the following:
       - NumPy version 1.19.0
       - scikit-learn version 0.23.2
       - SciPy version 1.5.4
       - pandas version 1.0.3
       - PySpark version 3.0.0

Question 30:

  correct answer:
    C. AWs IoT Greengrass synchronized with SM endpoints for model management
    -> This option leverages IoT Greengrass for local edge computing, which is crucial for achieving minimal latency in real-time data
       processing. It also incorporates synchronization with SM endpoints, which facilitates efficient model management and updates. This
       setup ensures the ML model can adapt quickly to market changes and maintain accuracy over time, making it high suitable for
       demanding and dynamic environment of financial trend predictions.

       Note: Synchronization may require SM Edge Manager with Edge Manager agent install, but Edge Manager was discontinued April 2024.


Question 32:

  What feature of Random Cut Forest (RCF) algorithm allows the data science team to efficiently adjust the model's to anomalies in
  real-time transaction data without requiring a full retraining of the model?

  correct answer:
    A. The model's ability to update ensemble decision boundaries dynamically as new data points are introduced.
    -> RCF excels in real-time anomaly detection due to it inherent design that supports online learning. This feature allows RCF to
       dynamically update its ensemble of decision trees - each representing a decision boundary - as new data points are introduced.
       The capability to adjust decision boundaries on the fly without retraining the model ensures the detection process remains
       efficient and current, minimizing reponse times to potential threats and reducing the likelihood of missing new types of anomalies.


 [SageMaker Random Cut Forest] tunable hyperparameters

   Choose Hyperparameters

     - The primary hyperparameters used to tune the RCF model are 'num_trees' and 'num_samples_per_tree'.
     num_trees
       - Increasing 'num_trees' has the effect of reducing the noise observed in anomaly scores since the final score is the
         average of the scores reported by each tree.
       - While the optimal value is application-dependent we recommend using 100 trees to begin with as a balance between score
         noise and model complexity.
       - Note that inference time is proportional to the number of trees.
       - training time is also affected it is dominated by the reservoir sampling algorithm describe above.

     num_samples_per_tree
       - The parameter num_samples_per_tree is related to the expected density of anomalies in the dataset.
       - num_samples_per_tree should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous data to normal data.
       - For example, if 256 samples are used in each tree then we expect our data to contain anomalies 1/256 or approximately 0.4%
        of the time. Again, an optimal value for this hyperparameter is dependent on the application.

Question 33:

  Given the startup's need to accurately render pronunciation of multilingual vocabulary with cultural nuances, which capability of
  Polly's custom lexicon is most essential for enhancing the language learning application?

  correct answer:
    B. The use of the International Phonetic Alphabet (IPA) to define exact pronunciation for multilingual terms.
    -> IPA directly addresses the primary need for precise pronunciation in a multilingual education environment.


    [Amazon Polly] Using phonetic pronunciation

     <phoneme>
       - This tag is supported by long-form, neural, and standard TTS formats.
       - To make Amazon Polly use phonetic pronunciation for specific text, use the <phoneme> tag.

       - Two attributes are required with the <phoneme> tag. They indicate the phonetic alphabet Amazon Polly uses and the
         phonetic symbols of the corrected pronunciation:

        alphabet
            ipaIndicates that the International Phonetic Alphabet (IPA) will be used.

            x-sampaIndicates that the Extended Speech Assessment Methods Phonetic Alphabet (X-SAMPA) will be used.

        ph
          - Specifies the phonetic symbols for pronunciation.

       - With the <phoneme> tag, Amazon Polly uses the pronunciation specified by the ph attribute instead of the standard
         pronunciation associated by default with the language used by the selected voice.

Question 34

  pioneering the use of AI to enhance maintenance strategies for aircraft components. This involves assimilating real-time sensor
  data with extensive historical maintenace records to preemptively predict and schedule maintanence.

  Given the intricate nature of the aerospace maintenance, which feature of Bedrock should the company deploy to amalgamate and
  analyze the real-time sensor outputs with archived maintenance logs, thereby optimizing the predictive accuracy of component
  failure while ensuring a high safetwy standard?

  correct answer:
    C. Fine-tuning with domain-specific datasets
    -> Fine-tuning with domain-specific datasets allows the integration and analysis of disparate data types, such as real-time
       sensor outputs and historical maintenance logs. This approach adapts the model more precisely to the unique characteristics
       of the aerospace maintenance data, enhancing its predictive accuracy for component failure and ensuring a higher standard
       of safety.

  other answers:
    A. Retrieval Augmented Generation (RAG)
    -> RAG is primarily used to enhance language models by integrating external knowledge sources dynamically. This feature is less
       applicable to for sensor data integration and predictive maintenance tasks.


  [Amazon Bedrock] Customize your model to improve its performance for your use case
    https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html

    - Model customization is the process of providing training data to a model in order to improve its performance for specific use-cases.
    - You can customize Amazon Bedrock foundation models in order to improve their performance and create a better customer experience.
    - Amazon Bedrock currently provides the following customization methods.

    Continued Pre-training

      - Provide unlabeled data to pre-train a foundation model by familiarizing it with certain types of inputs.
      - You can provide data from specific topics in order to expose a model to those areas.
      - The Continued Pre-training process will tweak the model parameters to accommodate the input data and improve its domain knowledge.

      - For example, you can train a model with private data, such as business documents, that are not publicly available for training
        large language models.
      - Additionally, you can continue to improve the model by retraining the model with more unlabeled data as it becomes available.

    Fine-tuning

      - Provide labeled data in order to train a model to improve performance on specific tasks.
      - By providing a training dataset of labeled examples, the model learns to associate what types of outputs should be generated
        for certain types of inputs.
      - The model parameters are adjusted in the process and the model's performance is improved for the tasks represented by
        the training dataset.


Question 35:


  Considering the necessity for high linguistic adaptability in the global retailer's customer support system, which capability within
  Bedrock should the retailer employ to develop an AI that communicates effectively across various languages and respects cultural nuances?

  correct answerz;
    A. Employ Cohere's Command R+ for its advanced linguistic adaptability.
    -> Employ Cohere's Command R+ for its advanced linguistic adaptability, which would enable the retailer's AI system to handle complex
       multilingual comunication effectively, adapting to regional dialects and cultural nuances. This feature provides specialized
       capability to learn from and respond to the diverse cultural backgrounds of customers worldwide ensuring the AI system can engage
        with uses in a way that is both linguistically accurate and cultural sensitive.


  Cohere in Amazon Bedrock
   https://aws.amazon.com/bedrock/cohere/

    [Cohere's] Command R / R+
      - Command R and R+ are Cohere's powerful, advanced language models for real-world enterprise applications.
      - They balance efficiency and accuracy, enabling businesses to move from proof-of-concept to daily AI utilization.
      - Supporting 10 key languages, these models excel at retrieval-augmented generation (RAG) and long-context tasks.
      - Ideal for global enterprises, Command R and R+ are optimized for RAG use cases and adept at text generation.
      - They're well-suited for full-scale AI implementation, with R+ offering enhanced performance for businesses ready to leverage
        AI across operations

     [Cohere's] Embed
       - Cohere's Embed 3 is an industry-leading embeddings model that generates embeddings from both text and images.
       - It enables enterprises to unlock value from vast image data, creating accurate search systems for complex reports, product
         catalogs, and design files.
       - Supporting 100+ languages and excelling in multimodal search tasks, Embed 3 streamlines advanced AI applications, enhancing
         e-commerce experiences, design asset management, and data-driven decision-making processes.

     [Cohere's] Rerank
       - Cohere's reranker model, Cohere Rerank 3.5, provides a powerful semantic boost to the search quality of any keyword or vector
         search system.
       - In RAG use cases, reranking can help ensure that only the most relevant information is passed to the model.
       - This can provide better responses, reduced latency, and lower costs because the model processes less information.

Question 36:


  In preparation for deploying a foundational model that aedeptly handles the generation of dynamic text responses with their NLP
  applications, what is the initial step the research team should undertake using Bedrock to verify the their chosenmodel conforms to
  the specific demands concerning modality and accessibility across different AWs regions?

  correct answer:
    A. Initiate a ListFoundationModels request to comprehensively review all models accesible along with their detailed attributes and
       features.
    -> This initial step allows the research team to thoroughly explore  the entire catalog of foundation model offerred through
       Bedrock ensuring a broad understanding of each model's capability, output modalities, and availability in various AWS regions.



  [AWS Bedrock API] ListFoundationModels

    - Lists Amazon Bedrock foundation models that you can use.
    - You can filter the results with the request parameters.
    - For more information, see Foundation models in the Amazon Bedrock User Guide.

    Request Syntax
     GET /foundation-models?byCustomizationType=byCustomizationType&byInferenceType=byInferenceType&byOutputModality=byOutputModality&byProvider=byProvider HTTP/1.1


  [Bedrock Python Client] list_foundation_models

    Bedrock.Client.list_foundation_models(**kwargs)

    - Lists Amazon Bedrock foundation models that you can use.
    - You can filter the results with the request parameters. For more information, see Foundation models in the Amazon Bedrock User Guide.

    Request Syntax

      response = client.list_foundation_models(
          byProvider='string',
          byCustomizationType='FINE_TUNING'|'CONTINUED_PRE_TRAINING'|'DISTILLATION',
          byOutputModality='TEXT'|'IMAGE'|'EMBEDDING',
          byInferenceType='ON_DEMAND'|'PROVISIONED'
      )


  [Bedrock Python Client] get_foundation_model

      Bedrock.Client.get_foundation_model(**kwargs)

        - Get details about a Amazon Bedrock foundation model.

        Request Syntax

          response = client.get_foundation_model(
              modelIdentifier='string'
          )


Question 37:

  Which of the following best describes the benefit of using Amazon Titan models within Bedrock environment for handling security
  requirements during model training?

  correct answer:
    D. Titan models automatically encrypt all data in transit and at rest, simplying compliance without additional configuration



------------------------------------------------------
https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/
WhizLabs Final Test (MLS-C01) Quiz

Question 14:

  For your initial analysis, you need to identify the distribution of consultants and their billing hours for the given period.
  What visualization best describes the relationship?

  correct answer:
    B. Histogram
    -> you are looking for distribution on a single dimension: the consultant billing hours. From the Wikipedia article titled
       Histograms, "A histogram is an accurate representation of the distribution of numerical data. It is an estimate of the
       probability  distribution of a continuous variable." The continuous variable in this question: the billing hours, binned
       into ranges (x-axis), at a frequency: the number of consultants at a billing hour range (y-axis).

  incorrect answer:
    C. Scatter Plot
    -> You are looking for a single dimension: the consultant billing hours. A scatter chart [plot] shows multiple distributions
       i.e., two or three measures for a dimension.



Question 16:

  The data that your model needs to process is massive in scale and requires large-scale data processing. How should you build the
  data transformation and feature engineering processing job so that you can process all of the flight data in real-time.

  correct answer:
  C. Run Apache Spark Streaming data processing jobs to perform the transformation and feature engineering on the flight data in
     real-time and save the data to S3 for your model training.
   -> Apache Spark Streaming is an analytics engine used for large-scale data processing that runn distributed data processing jobs.
   You can apply data transformations and extract features (feature engineering) using the Spark Framework

Question 17:


  You need to clearn your data and prepare ot fpr XGBoost algorithm you are going to use. You have written your cleaning/preparation
  code in SageMaker notebook. Based on the following code, what happens on lines 19, 21, 22 (select 3)

  code snippet

  4 import numpy as np
  5 import pandas as pd

  16 data = pd.read_csv('./bank/bank-fullcsv',sep=':')

  19 data['no_previous_campaign'] = np.where(data['contacted'] == 999,1,0)      # np.where(<condition>, <value if true>, <value if false>)
  20 data['not_employed'] = np.where(np.in1d(data['job'],['student', 'retired', 'unempl']), 1,0)
     # In Pandas, the get_dummies() function converts categorical variables into dummy/indicator variables (known as one-hot encoding).
  21 model_data = pd.get_dummies(data)
  22 model_data = model_data.drop(['duration','employee.rate', 'construction.price,idex', 'construction.confidence.idx',
        'lifetime.rate, region'], axis=1)


  correct answers:
    C. set the attribute n_previous_campaign to 1 if the customer in the observation has not bee contacted via a previous campaign or 0
       if they have been contacted
    D. Convert categorical data to a set of indicator variables
    F. REmove features deemed inconsequentials

Question 19:

  After you have cleaned and performed feature engineering on your CSV data, which of the following tasks would you perform next?

  correct answer:
    D. Shuffle you data using a shuffling technique
    -> Once you have cleaned and engineered your data for linear regression, you need to shuffle the data to prevent overfitting
       and reduce variance.

  incorrect answer:
    B. Load your data into pandas DataFrame and remove header rows and any superfluous features
    -> Using Pandas DataFrame to remove superfluous rows and features is part of cleaning and doing feature engineering of your data
       which you have already done.


Question 26:


  Process Amazon Kinesis Data Streams records with Lambda
    https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-create.html
    - To process Amazon Kinesis Data Streams records with Lambda, create a consumer for your stream and then create a Lambda
       event source mapping.

    Configuring your data stream and function
      - Your Lambda function is a consumer application for your data stream. It processes one batch of records at a time from each shard.
        You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with
         enhanced fan-out.

Question 36:

  You have define the containers for your pipeline using CreateModel SageMaker API, and you have create an inference endpoint using
  SM CreateEndpointConfig and CreateEndpoint API. You have decided to change your pipeline to use a different SM feature transformer
  strategy (change the strategy fomr the default to SingleRecord).
  How do you make the change to your inference pipeline?

  Correct answer:
    D. Your pipeline is immutable, but you can change your inference pipeline by deploying a new one using the UpdateEndpoint API


  UpdateEndpoint
    - Deploys the EndpointConfig specified in the request to a new fleet of instances.
    - SageMaker shifts endpoint traffic to the new instances with the updated endpoint configuration and then deletes the old instances
      using the previous EndpointConfig (there is no availability loss).

    - When SageMaker receives the request, it sets the endpoint status to Updating.
    - After updating the endpoint, it sets the status to InService.
    - To check the status of an endpoint, use the DescribeEndpoint API.

Question 43:

  You need to clean polling data on which you want to train your binary classification model (makes over 75K or not). You need to
  remove duplicate fows with erroneous data, transform the income column into a label column with two values, transform the age
  column into categorical features by binning the column, scale the capital gain and capital losses columns, and find split data
  into into train and test dataset.

  Which of the options are the most efficient ways to achieve your data sanitizing and feature preparation? (Select two)


  correct answers:
    B. Create a SageMaker Processing Job using a SageMaker Python SDK with Processing container leveraging the scikit-learn
       SKLearnProcessor package that performs your required preprocessing sanitizing and feature preparation tasks and then splits
       the data into training and test datasets.


    D. Create a SageMaker Processing Job using a SageMaker Python SDK with Processing container leveraging the Spark
       PySparkProcessor package that performs your required preprocessing sanitizing and feature preparation tasks and then splits
       the data into training and test datasets.

    -> SM processing jobs can be written in Python, using SageMaker Python SDK. You can leverage either the 'PySparkProcessor',
       'SparkJarProcessor', or the 'SKLearnProcessor' package to perform your preprocessing sanitizing and feature preparation tasks
       and also split the data into the training and test datasets.

  incorrect answers:
    E. Create a SageMaker Processing Job using a SageMaker Python SDK with Processing container leveraging the Spark
       SparkMLProcessor package that performs your required preprocessing sanitizing and feature preparation tasks and then splits
       the data into training and test datasets.
    -> There is NO SparkMLProcessor package in SageMaker Processing service.


  Amazon SageMaker Processing
    https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html

     - Amazon SageMaker Processing allows you to run steps for data pre- or post-processing, feature engineering, data validation,
       or model evaluation workloads on Amazon SageMaker.

     Data Pre-Processing and Model Evaluation with scikit-learn

       - You can run a scikit-learn script to do data processing on SageMaker using the sagemaker.sklearn.processing.SKLearnProcessor class.
       - You first create a SKLearnProcessor

          from sagemaker.sklearn.processing import SKLearnProcessor

          sklearn_processor = SKLearnProcessor(
              framework_version="0.20.0",
              role="[Your SageMaker-compatible IAM role]",
              instance_type="ml.m5.xlarge",
              instance_count=1,
          )

       - Then you can run a scikit-learn script 'preprocessing.py' in a processing job.
       - In this example, our script takes one input from S3 and one command-line argument, processes the data, then splits the data
         into two datasets for output. When the job is finished, we can retrive the output from S3.

          from sagemaker.processing import ProcessingInput, ProcessingOutput

          sklearn_processor.run(
              code="preprocessing.py",
              inputs=[
                  ProcessingInput(source="s3://your-bucket/path/to/your/data", destination="/opt/ml/processing/input"),
              ],
              outputs=[
                  ProcessingOutput(output_name="train_data", source="/opt/ml/processing/train"),
                  ProcessingOutput(output_name="test_data", source="/opt/ml/processing/test"),
              ],
              arguments=["--train-test-split-ratio", "0.2"],
          )

          preprocessing_job_description = sklearn_processor.jobs[-1].describe()


      Data Processing with Spark

        - SageMaker provides two classes for customers to run Spark applications:
             'sagemaker.spark.processing.PySparkProcessor' and 'sagemaker.spark.processing.SparkJarProcessor'

        PySparkProcessor

          - You can use the 'sagemaker.spark.processing.PySparkProcessor' class to run PySpark scripts as processing jobs.
          - This example shows how you can take an existing PySpark script and run a processing job with the
            'sagemaker.spark.processing.PySparkProcessor' class and the pre-built SageMaker Spark container.

          - First you need to create a PySparkProcessor object

              from sagemaker.processing import PySparkProcessor, ProcessingInput

              spark_processor = PySparkProcessor(
                  base_job_name="sm-spark",
                  framework_version="2.4",
                  py_version="py37",
                  container_version="1",
                  role="[Your SageMaker-compatible IAM role]",
                  instance_count=2,
                  instance_type="ml.c5.xlarge",
                  max_runtime_in_seconds=1200,
                  image_uri="your-image-uri"
              )

          - The 'framework_version' is the spark version where the script will be running.
          - 'py_version' and 'container_version' are two new parameters you can specify in the constructor.
          - They give you more flexibility to select the container version to avoid any backward incompatibilities and unnecessary
            dependency upgrade.

          - If you just specify the 'framework_version', Sagemaker will default to a python version and the latest container version.
          - To pin to an exact version of the SageMaker Spark container you need to specify all the three parameters: 'framework_version',
            'py_version' and 'container_version'.

           - You can also specify the 'image_uri' and it will override all the three parameters.

           - Note that 'command' option will not be supported on either 'PySparkProcessor' or 'SparkJarProcessor'.
           - If you want to run the script on your own container, please use 'ScriptProcessor' instead.

           - Then you can run your existing spark script 'preprocessing.py' in a processing job.

              spark_processor.run(
                  submit_app="preprocess.py",
                  submit_py_files=["module.py", "lib.egg"],
                  submit_jars=["lib.jar", "lib2.jar"],
                  submit_files=["file.txt", "file2.csv"],
                  arguments=["s3_input_bucket", bucket,
                             "s3_input_key_prefix", input_prefix,
                             "s3_output_bucket", bucket,
                             "s3_output_key_prefix", input_preprocessed_prefix],
                  spark_event_logs_s3_uri="s3://your-bucket/your-prefix/store-spark-events"
              )



        SparkJarProcessor

          - Supposed that you have the jar file preprocessing.jar stored in the same directory as you are now, and the java package
            is 'com.path.to.your.class.PreProcessing.java' Heres an example of using 'PySparkProcessor.'

              spark = SparkJarProcessor(
                  base_job_name="sm-spark-java",
                  image_uri=beta_image_uri,
                  role=role,
                  instance_count=2,
                  instance_type="ml.c5.xlarge",
                  max_runtime_in_seconds=1200,
              )

              spark.run(
                  submit_app="preprocessing.jar",
                  submit_class="com.amazonaws.sagemaker.spark.test.HelloJavaSparkApp",
                  arguments=["--input", input_s3_uri, "--output", output_s3_uri]
              )

          - 'SparkJarProcessor' is very similar to 'PySparkProcessor' except that the 'run()' method takes only jar file path, configured
             by 'submit_app' parameter, and 'submit_class' parameter, which is equivalent to class option for spark-submit command.


  https://datascience.stackexchange.com/questions/115302/should-i-split-data-into-train-validation-test-before-feature-scaling-and-featur
  What happens when we perform certain actions before splitting?

    Exploration:
      - if we split the sets ourselves, we assume the distributions having no significant differences, so performing exploratory analysis
        on the full data is acceptable.

    Feature selection:
      - once again, if we assume the distributions to be roughly the same, stats like mutual information or variance inflation factor
        should also remain roughly the same. I'd stick to selection using the train set only just to be sure.

    Imputing missing values:
      - filling with a constant should create no leakage.
      - Strategies like filling with mean values result in a leakage

    Dropping outliers:
      - if we expect the real input data to be filtered the same way, it can be done before splitting.
      - Otherwise it's just cheating (we exclude observations that are hard to predict from all sets).

    Feature encoding:
      - depends on the encoding strategy. One Hot Encoding shouldn't result in a leakage, target encoding would make a huge leak.

    Feature engineering:
      - if we operate within a single observation (like, adding two features' ratio), there's no leak.
      - If it's based on other observations somehow, it's a leak (this applies to most scaling methods just as well).
      - If it considers target too (e.g. SymbolicTransformer()), the leak becomes really huge.

    scaling
      - Strictly you should not act as if you know things in your test set.
      - If you want to perform min-max scaling (for instance) you should fit the min-max scaler to your training set.
      - Then use this to transform your training and test set.

   https://dev.to/gervaisamoah/why-feature-scaling-should-be-done-after-splitting-your-dataset-into-training-and-test-sets-14ia
   The Right Approach: Split, Then Scale

     Step 1: Split the Data
       - Begin by dividing the dataset into training and test sets.
       - A common split ratio is 70-80% for training and 20-30% for testing, though this can vary depending on your dataset size and use case.
     Step 2: Scale the Training Set
       - After splitting, apply feature scaling only to the training set.
       - Calculate the necessary statistics (e.g., mean, standard deviation, minimum, and maximum) using the training data.
       - This ensures that the test set remains independent and unbiased.
     Step 3: Apply Training Set Parameters to the Test Set
       - Use the scaling parameters derived from the training set to transform the test set.
       - This ensures that the test data undergoes the same transformation as the training data without introducing data leakage.

Question 44:

  Tasked with centralizing your company's product, customer, and supplier, and material data in once source. These current data reporitories
  are housed on several different database technologies.

  When you load the  various data sources into your new centralized data source, you need to clean and classify the data as well.
  What is the most expeditious and efficient way to create this new centralized data source?

  correct answer:
    D. Use AWS Data Lake Formation to collect and catalog the data from your disparate data sources, transform (clean and classify) your
       data, and load into your S3 Data lake.
    -> AWS Lake Formation build on the capabilities of AWS Glue to simplify the creation of an S3 data lake. Once your define your
       disparate data sources to AWS Lake Formation, it crawls you data sources and moves the data to your S3 data lake. It uses ML
       algorithms to clean and classify your data. This is the simpliest and most efficient option listed.

  incorrect answer:
    B. Use AWS Glue Crawler to crawl your disparate data sources and create a metastore for your S3 data lake. Use AWS Glue to then extract,
       transform (clean and classify), and load the source data into your S3 data lake
    -> Using AWS Glue and its crawlers will wor to extract, transform, and load your disparate data sources into your S3 data lake.
       But it is not the quickest or simplest option given

  AWS Lake Formation
    https://aws.amazon.com/lake-formation/
    - AWS Lake Formation centralizes permissions management of your data and makes it easier to share across your organization and externally.
    How it works:
                                            |----------------------|
                                            | AWS Lake Formation   |
                                            |                      |
                                            |   Source Crawlers    |          Athena
                                            |   ETL and data prep  |  ----->  Redshift     ------> Users
        S3                       ------>    |   Data catalog       |          Amazon EMR
        Relational DB Instance   ------>    |   Secuity Settings   |
        NoSQL DB instance        ------>    |   Access Control     |
                                            |----------------------|
                                                     ^
                                                     |
                                                     v
                                                S3 Data Lake Storage


Question 54:

  Tasked with producing a ML model that predicts whether a newly released gate will eventually become a successful product and earns
  the company profits. Your data used for the model is product information and product ratings from social media.
  Which model and objective will best match your model requirement?

  correct answer:
    B. XGBoost, binary:logistic
    -> XGBoost is a good choices for your algorithm, and 'binary:logistic' objective is correct objective since it is used for a binary
       classification problem.  You are trying to predict whether your newly released game will eventually succeed in making your
       company money or not.

  incorrect answer:
    B. XGBoost, multi-sofmax

    Note: It was unclear to me whether the goal was binary (product earns profit or not) or determines product ratings (e.g. 1 to 5)


Question 55: (2nd attempt)

  Whih model and SageMaker image will provide the best results for your team in the most efficient manner?

  Note: There is no base TensorFlow image, no base R image, and no base Scala image, so you would need to create custom images
        to use Studio with TensorFlow, R, and Scala


Question 60:

  You will use this custom algorithm to train and run inferences on your model.
  Which of the following steps do you NOT need to complete to create your custom algorithm in SM?

  incorrect answer (that is, you need to create for your custom a
    A. Correct Docker containers for your training and inference code.
    B. Specify the hyperparameters that your algorithm supports.
    C. Specify the metrics that your algorithm sends to cloudwatch when training.
    D. The instance types your algorithm supports for training and inference.

  correct answer:
    E. Whether your algorithm supported distributed inference across multiple instances
    -> When you create a custom algorithm resource in SM, you need to specify whether it supports distributed training across
       multiple instances, NOT not distributed inference.


------------------------------------------------------
