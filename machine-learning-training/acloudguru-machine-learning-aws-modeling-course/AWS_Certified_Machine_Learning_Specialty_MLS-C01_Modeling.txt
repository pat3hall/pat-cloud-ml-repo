------------------------------------------------------

AWS Certified Machine Learning - Specialty (MLS-C01): Modeling

------------------------------------------------------

Chapter 1 Introduction

------------------------------------------------------

Chapter 2 Frame Business Problems as ML Problems

------------------------------------------------------
2.1 Understanding When to Use Machine Learning 


  Machine Learning Life Cycle

     Data Collection --> Data Analysis --> Data Processing --> 

          --> Build Model --> Train Model --> Test model --> Deploy Model --> Monitor Model
                                          <--

    - In this course, we will focus primarily on 'build model', 'test model' and 'train model'

  Is machine learning a good fit?
    - need a large amount of data to derive a reasonably predictable model.
    - need expertise that can study and eliminate the noise around the data
       - This is a huge field in data science called data processing and feature engineering.
    - need computation resources to  pre-processing the data.
    - is your appliation mission-critical  
       - if an error in your model predictions may not be acceptable, machine learning is not the right solution
    - can simple rules solve the business problem
       - if so, implement with simple rules instead of ML

  Machine Learning Types
    Supervised learning   - uses labeled training data
    Unsupervised learning - uses unlabeled data and tries to predict patterns
    Reinforement learning
   
    Supervised: 
      Regression:  used to predict quantitive data
        - examples: predicting house prices, predicting stock prices
        Linear regression
          - you have one dependent and one independent feature.
        multiple regression (or multivariate regression)
          - you have one dependent feature with multiple independent features.
        Polynomial regression 
          - a special case of multiple regression
          - the dependent feature is modeled as a nth degree polynomial of the input feature or the dependent feature.
          - use this when the relationship doesn't look linear,
          - you can use a polynomial line to minimize the error and better fit the model.
  
      Classification:  used to predict [qualitative] discrete questions
        - examples: is a patient is affected by a virus?,  Will the customer renew the service?, 
                    What is the customer rating of a product between one to five?
        Binary Classification:  
           - only two outcomes (ex: true/false, yes/no)
        Multiclass Classification:  
           - when more there are more than two discrete outcomes 
  
      Time Series :  
       - examples: forecast a future demand to a product or a service based on historical sales data, pricing information, 
         and promotional activities;  forecast a sales of a retail store based on the past marketing campaign.

      dataset:
         - trained on training data 
         - evaluated on test data

    Unsupervised: 
      Clustering:  
        - example: customers top complaints
        - a type of unsupervised learning that tries to find patterns of similarity and relationships among the input data 
          samples, and then cluster these samples into various groups such that each group or cluster of data has some similarity.
      Dimensionality reduction
        - when you have a lot of data features, the model training process will become extremely complex
          because of memory and space constraints.
        - called the curse of dimensionality.
        - a type of unsupervised learning to reduce the number of features from the input data.
      anomaly detection.
        - examples: detect security violations, identifying network issues
        - when collecting and processing the data, you may see that some data points are outliers.  
        - These can be rare events and may occur infrequently.

    Reinforcement learning.
       - exampele: It's commonly associated with gaming and robotics applications.
       - business use case examples: like recommendation systems and dynamic price adjustments of a product/service

------------------------------------------------------
2.2 Exploring Supervised Learning Classification

  Classification learning
    - a supervised learning algorithm that uses the training process to classify the elements of a dataset into 
      various categories.
    - dependent feature will always be categorical in nature.
    - Each potential value of a categorical feature is called a class.
    Binary Classification
      - has only two possible classes (ex: true/false, yes/no, bad/good).
      Binary Classification Algorithms:
        - include: Logistic regression  and support vector machines
    Multiclass Classification
      - has more than two possible classes 
      Multiclass Classification algorithms
        K-Nearest Neighbors
        Naive Bayes
    Multilabel Classification
      - multiple predicted class can be assigned to mul
      Multilabel Classification algorithms
        Ensemble Methods
        Deep Learning
    Imbalanced Classification
      - the number of observations is unevenly distributed in each class.
      - example: fraudulent credit card transactions
        minority class: a number of transactions that can be classified as fraud,
        majority class: legitimate transactions
      imbalance challenges.
        - a biased models that prioritize the majority class at the expense of the minority class.
        - generalize poorly to new and unseen data.
      Techniques to handle: 
         SMOTE (Synthetic Minority Over Sampling Technique)
           - as a machine learning technique that generates new synthetic samples to balance imbalanced datasets.

   Classification algorithms 
     Eager learners 
       - spends more time during the training process because of their eagerness to generalize better.
       - spend less time making predictions
       Algorithms:
         Logistic Regressions
         Support Vector Machines (SVM)
     Lazy learners 
       - do not create a model from the training data [just memorize the data]
       - spend more time during the prediction process
       Algorithms:
         K-Nearest Neighbor (K-NN)


    Logistic Regression
      - independent feature can be numeric or categorical.
      - independent features used to determine the dependent feature, which must be categorical.
      - the relation between the dependent and independent feature is a S-shaped sigmoid curve.
      - typically used to calculate a binary outcome like a success or failure or a yes or no answer.
      Business use cases:
         - probability of a purchase
         - probability of a disease
      pros:
         - computationally efficient and can handle large datasets
      cons:
         - Sensitive to outliers
           - The presence of outliers leads to biased results.

    Naive Bayes algorithm
       - based on Bayes theorem.
       - Assumes that each feature is independent of each other.
       -    P(X|Y) = [P(Y|X) * P(X)]  /  P(Y)
           where P(X): probability of X 
                 P(X|Y): probability of X when Y is true
       Businese Cases:
         - commonly used in text classification such as spam filtering and sentiment analysis.
         Pros
           - it is very fast 
           - can handle missing data without requiring imputation.
         Cons
           - the assumption of feature independence as most real world features are somewhat related.
     
    Support Vector Machines (SVM)
      -  In this algorithm, each object is represented on a n-dimensional plane whose coordinates are the features.
      - SVM then performs a classification by drawing a hyperplane that separates these classes.
      - There could be multiple such hyperplanes, but SVM finds a optimal hyperplane that maximizes the distance 
        between two closest point in the categories.
      Business Use Cases
        - customer churn prediction and fraud detection.
      Pros
        - generalizes well to unseen data and is less prone to overfitting.
      Cons:
       - both computationally expensive and memory intensive.

    K-Nearest Neighbors (K-NN or KNN).
     - The value 'K' in the algorithm is a hyperparameter that is explicitly defined.
     - It calculates the distance between two points and assigns the label of new data based on the labels
       of nearest observed data point.
     - k-NN uses the 'Euclidean' or 'Manhattan' distance to compute the closeness or proximity.
     Business Use Cases:
       - customer segmentation 
          - where companies might want to segment their customers into different groups based on demographic, 
            geographical location, or preferences.
       - Intrusion Detection
          - detecting cyber attacks by identifying abnormal network traffic patterns.
       Pros:
         - it does not require a training phase 
         - it just memorizes data and makes predictions based on similarity.
       Cons:
         - it is memory intensive, especially for large datasets because the entire training 
           dataset is loaded in the memory.


------------------------------------------------------
2.3 Exploring Supervised Learning Regression 


  Regression algorithms.
    - a supervised learning algorithm that finds the relationship between two numeric features of a dataset by 
      fitting a mathematical model to the data.
    Business Use Cases:
      - predicting real estate prices

    Learner Regression
      - one dependent (e.g. price) and one independent feature (e.g. home sq ft)
      - formula:   Y = m*x + b
    Multiple Regression
      - one dependent (e.g. price) and multiple independent feature (e.g. home sq ft, bedrms, bathrms, location, no of bathrooms)
      - formula:   Y = m1*x1 + + m2*x2 + ...+ mn*xn + b
    Polynomial Regression
      - relation with independent feature(s) is exponential with dependent feature
      - formula:   Y = m1*x1**2 + + m2*x2 + b
      - graph of x1 vs Y is a logarithmic curve. 
      

  Classification vs Regression Algorithms
    Classification 
      objective: Predicts the category of a new instance based on its features
      output type:  Categorical (e.g. spam or not spam)
      evaluation metrics: Accuracy, Precision, and recall to measure the classifiers performance

    Regression Algorithms
      objective: Predicts a continuosu numerical values of a new instance based on its features
      output type:  Quantitative (e.g. price of a home)
      evaluation metrics: MSE, RMSE, and R-Squared to measure the classifiers performance

  Linear Regresion Demo
    will use the employee.csv file that shows the employee's age, his department and salary.
    - launched SageMaker LinearRegression Notebook

------------------------------------------------------
2.4 Examining Supervised Learning Problem Types Time Series Forecasting 


  time series analysis
    - a statistical technique used to analyze observations that are collected over a period of time, usually at regular intervals.
    - Time series analysis plays a very important role in the field of forecasting analysis.
    - time is always one of the independent features
    Business cases:
      - financial forecasting (e.g. to predict stock prices)
     - business forecasting (e.g.  to predict sales revenues)
     - manufacturing and supply chain management (e.g. forecast the demand for raw materials)
     
     Components of Time series analysis:
       trend
         - represents the directionality of the data over time.
       seasonality
         - represents periodic fluctuations that occur at regular intervals within the data.
       cyclical variations
         - represent a non-repeating fluctuations in the data that may occur at irregular intervals,
           - are not periodic like seasonality.
         - These variations are influenced by other external factors.
         - For example, in some years, summer may be relatively cooler, or the winters may be relatively warmer 
           because of the other environmental factors.
       irregularity
         - represents a randomness or the noise in the data.
         - It represents the random fluctuations that cannot be explained by the trend, seasonality, or cyclical patterns.
   Time Series types 
     Stationary Data
       - Data changes predictably  
       - note: its name is misleading - data does change over time, just in a predictable way)
       time series data is considered stationary if:
          - The mean of the time series data must remain constant over time.
          - The variance of the time series data must remain constant over time, 
          - and the core variance between observations at different time lags must remain constant over time.
      value proposition
         - Since the statistical properties remain constant, they're easier to analyze and model, and they 
           deliver more reliable forecasts
         - Most of the statistical algorithms expects that the time series data are stationary nature.
     Non-Stationary Data
       - the mean of the time series data exhibits a upward or downward trend over time,
       - or the variance of the time series data exhibits periodic fluctuations at fixed intervals.
       challenge for modeling
         - cannot deliver reliable forecasts because their statistical properties change over time,
       techniques to handle non-stationary data
         - detrending, differencing, and transformation techniques to convert to stationary data.

   Limitations of Time-series Analysis
     - cannot handle missing values
       - any missing data can distort the accuracy of the results.
     - assume that the data is stationary,
     - can be overcome by using some techniques to convert non-stationary data to stationary data
     - assumes a linear relationship between the features 
       - may need to use complex modeling approaches for data with non-linear relationships.
     - dependency on historical data to make forecasts.
        - historical data may be limited, especially if we are modeling rare events.

------------------------------------------------------
2.5 Exploring Unsupervised Learning Problem Types Clustering 


  clustering algorithm 
    - an unsupervised learning algorithm that groups data points based on their similarities into different clusters.
    - typically applied to unlabel data.

    Clustering Types
      hard clustering,
        - each data point belongs to one cluster and one cluster only.
        - In other words, each observation is mutually exclusive.
        algorithms:
          k-means
      soft clustering,
        - each data point is assigned a probability of belonging to a cluster.
        algorithms:
          - Fuzzy C-means 

    Goal of a clustering algorithm
      - group the similar data points, and the similarity is determined by the distance between two data points.
      - based on the similarity metrics, clustering algorithms are broadly divided into four major categories.
      Clustering Categories
        centroid-based clustering.
           - partition a dataset into a predefined number of clusters.
           - A cluster center, or centroid, is randomly identified for each such cluster.
           - The algorithm then assigns each data point to the nearest centroid based on a distance metric.
           - The distance is typically calculated using 'Euclidean' or 'Manhattan' distance.
           - The centroid of the clusters are iteratively updated by computing the mean of all the data points 
             assigned to each cluster.
           - The previous two steps are repeated until the centroids no longer change. This point is called the convergence point.
           algoritms:
             K-means 
               - a popular centroid-based clustering algorithm.
        density-based clustering.
          - the number of clusters is not predetermined.
          - capable of discovering clusters of arbitrary shape and size.
          - This algorithm starts by randomly selecting a data point and examining its neighborhood within a specific radius.
          - If the number of data points exceed a threshold, a cluster is formed, and this data point is labeled as a core point.
          - If this core point is closer to other core points, they are grouped into the same cluster.
          - data points that are density reachable are considered as belonging to the same cluster.
             - if two data points are not directly within each other's neighborhood but are density reachable from each other, 
               they are considered as belonging to the same cluster.
          pros:
            - excellent in handling outliers.
          Algorithm:
             DBScan is a well known density-based clustering.

        Hierarchical Clustering (also called connectivity based clustering)
          - groups data together based on similarities and builds a hierarchy of clusters.
          - smaller clusters are merged into larger clusters, forming a tree-like structure called a 'dendrogram'
          Two main approaches.
            Bottom-Up (also called agglomerative hierarchical clustering)
              - Each data point in the dataset is considered a cluster in this algorithm, and based on the similarity 
                computed by the distance between two data points, two clusters are merged recursively.
              algorithm:
               - 'agglomerative hierarchical clustering algorithm'
            Top-Down (also called a divisive technique)
              - all the data points are treated a single cluster to begin with.
              - In every iteration, we compute a measure of dissimilarity between the data points.
              - The one with the highest dissimilarity is split into two new clusters.
              - This process is repeated recursively until all the data points are present in their clusters.

        Distribution-based clustering
          - assumes data points are generated from a mixture of probability distributions and seeks to model these distributions 
            to identify clusters in the data.
          - estimates the mean, variance and mixture weights using the expectation-maximization algorithm.
          - Once they're estimated, the algorithm assigns each data point to the cluster with the highest probability 
            of generating the data point.
          - Since data points are assigned probabilities of belonging to each clusters, these algorithms allow for soft clustering.
          Algorithm:
           - 'Gaussian mixture models' (GMM) is a popular distribution-based clustering algorithm.

    Clustering algorithms Business Use Cases
       customer segmentation
          - develop a focused marketing campaign of customer segmentation.
          - deveop personalized product recommendations
       detect unusual patterns and anomalies.
          - businesses can detect and prevent fraudulent transactions in real time, minimizing financial losses and risks.
       extract insights
         - extract insights, organize textual data, improve the efficiency text processing tasks in the field of NLP

------------------------------------------------------
2.6 Exploring Unsupervised Learning Problem Types : Associations



  Association learning,
    - unsupervised learning type.
    - a data mining technique used to detect implicit relationships, hidden patterns, or dependencies 
      between features in a large dataset.
    Analogy
      - imagine a fruit vendor found that people who buy apple and orange also buy banana.
      - to increase his sales, he wants to keep them together on the same shelf, or he can put two of the 
        three products on sale and drive the overall sales, or he can also run specific marketing campaigns
        targeting to those who buy these products.
    Terminologies
      - works similar to a simple if then statement.
      - For example, if a customer buys apple [ancedent], then he buys a banana [consequent] as well.

         if     [condition]      then     [action]
                Antecedent                Consequent

      - If the association rules involve a single items only, then the relation is called a 'single cardinal relation'
         if     [antecedent - buys apple]      then     [consequent - buys banana]

      - association rules can contain multiple items in both the antecedent and consequent, which may result in a 
        more complex pattern.

     Metrics
       support
         - measures the frequency or percentage of time that items X and Y occur together out of all the transactions.
         - formula:    (Frequency of X and Y)   /   (Total no of transtions)
         - example: if there are 1000 total transactions, how many transactions contain both apple and orange?
       confidence,
         - indicates the percentage of transactions that contain both X and Y out of all the transactions containing X.
         - formula:    (Frequency of X and Y)   /   (frequecy of X)
         - example: if there are 1000 transactions where apples are sold, how many such transactions contain oranges as well?
       lift
         - measures the strength of associations between two items in an association.
         - formula:    (Confidence of X and Y)   /   (Support of Y)
         - Lift metric has three possible values.
           lift > 1 
              - the occurrence of antecedent and consequent are more frequent together 
           lift = 1 
              - the occurrence of antecedent and consequent are independent of each other
           lift < 1 
              - the occurrence of antecedent and consequent are less frequent together 

      Association Learning Algorithms
        Apriori algorithm
          - the most popular
          - discovers the frequently occurring items in a transaction data set and creates association rules.
          - uses a breadth-first search and hash tree to calculate the frequently occurring items.
          - involves generating candidate items, which are a subset of frequently occurring items.

        Frequent Pattern Growth (FP-Growth) algorithm
          - constructs a compact data structure called FP-tree to represent the entire dataset and mines the frequently 
            occurring items directly from the structure
          - algorithm does not generate candidate items
             - With this approach, FP-growth can be more efficient than the Apriori algorithm for large datasets.

         Equivalence Class transformation (Eclat) algorithm
           - Similar to Apriori algorithm,
           - discovers frequently occurring items from a dataset
           - focuses on the depth-first search,
              - makes it memory efficient for large datasets.

      Assoication Learing - Business use cases
         customer segmentation
           - used extensively in customer segmentation to segment them into distinct groups based on their preferences 
             of purchasing history
           - Create focused marketing campaigns in the field of customer segmentation
         market-based analysis,
          - Create cross-selling strategies in the field of market based analysis,
          - creating personalized recommendations based on their historical purchase patterns and preferences.
         supply chain management,
           - effective association rules can help streamline procurement and investment management, leading to cost savings.

------------------------------------------------------
2.7 Understanding Reinforcement Learning 

  Reinforcement Learning
    - It is an autonomous and self-learning machine learning technique that enables an agent to learn in an interactive 
      environment by trial and error to achieve the most optimal results.

  Comparing ML Types
    supervised learning,
      - the model is trained with the labeled data,
    unsupervised learning,
      - the model is trained using unlabeled data,
    reinforcement learning,
      - the model will be trained based on the feedback from the environment.

  Reinforcement Learning Terminologies.
    Agent 
      - the entity that is learning to make decisions based on its feedback from the environment.
    Environment 
      - the domain or the problem space where the agent operates.
      - The environment provides the feedback to the agent in the form of a reward or penalty.
    State 
      - a specific condition within the environment at any given time.
    action 
      - a decision or a choice made by the agent on the current state of the environment.
    reward 
      - a feedback provided by the environment to the agent at the end of each action.
      - The purpose of the agent is the maximize the reward.
    policy 
      - an agent's decision-making process that helps in deciding on the actions based on the current state.


    Reinforcement Learning  Analogy.
      - Imagine a player playing a game where the goal is to navigate through a maze to reach a destination.
      - In this scenario:
         - the player is the 'agent',
         - the maze represents the 'environment'
         - The 'state' represents a position of the player at any given time.
         - These possible moves represents an 'action'.
             - The player can move up, down, or left or right.
         - As a player navigates through the maze and unlock a door, they get a 'feedback'.
         - It will be a 'reward' if they are progressing towards exit, and a 'penalty' if they are moving towards a dead end.
         - Over time, they learn which actions lead to positive 'rewards' and which ones lead to penalty which 
           represents the 'policy' in the reinforcement learning.

  Reinforcement Learning Algorithms.

    Model-Based Reinforcement Learning.
      analogy
        - we enter the maze without any prior knowledge, we'll be randomly choosing our paths and recording our observations.
        - Based on the observation, we eventually started building a mental map of the maze layout about the connections and 
          the dead ends.
        - With this map, we can make an educated decision to reach the exit more efficiently.
        - With this new mental map, we take actions, and follow a specific route.
        - You may need to update the map based on new observations and experiences.
        - In this scenario:
           - the maze is the environment,
           - the mental map is a learned model,
           - planning the route indicates using the model to optimize your actions.
      model-based reinforcement learning overview:
         - the agent builds an internal representation of the environment.
         - acts on the new environment, and notes of the new environment state.
         - associates the reward value with the state transition.
         - The above process repeats until a model is built.
         - Once a model is successfully built, the agent decides on future action sequences based on the optimal 
           cumulative rewards.

    Model-Free Reinforcement Learning.
      analogy 
        - a kid trying to ride a bicycle without support.
        - It starts with a kid having absolute no knowledge of how to balance or pedal.
        - The kid might try a few different approaches, like leaning in on one side, pedaling slower and faster 
          to see what works and what doesn't.
        - Along the way, the kid makes a few mistakes, and adjusts the action based on the feedback received, like 
          leaning on one side might veer the vehicle off course, or pedaling too slow might not gain enough momentum 
          to move forward.
        - With this trial and error approach, the kid learns different strategies and improves their riding skills.
        - With continuous practice and experience, the kid eventually masters stable riding.
        - In this scenario:
            - this learning process through a trial and error without explicit knowledge of the environment's 
              dynamics is a model-free reinforcement learning.

      Model-Free Reinforcement Learning Overview:
        - the agent doesn't build an internal model of the environment.
        - uses the trial and error approach to learn the environment dynamics.
        - records the action taken and the state achieved
        - sequences them to develop a policy.

  Reinforcement Learning Business Use Cases.
    self-driving vehicles
      - the agent learns to navigate from point A to B, reacting to dynamic traffic conditions, obstacles, 
        and traffic rules.
    Robotics 
      - robots learn to perform complex tasks like assembling parts, grasping objects, and responding to instructions,
    Robo [Algorithmic] Trading,
      - the agents learn to buy and sell assets, manage portfolios based on the current market conditions.
    Supply chain management
      - optimize operations by learning demand forecasting models, efficient inventory management policies, 
        and logistic strategies.

------------------------------------------------------
2.8 Exploring Deep Learning 

  Deep learning 
    - a subset of machine learning that is modeled after the human brain focused on solving complex problems 
      by learning from large amounts of data without human intervention.

  Deep learning Drivers
    - high data pre-processing time
    - Feature engineering requires significant domain expertise
    - Traditional machine learning algorithms face challenges with high dimensional input features.
    - With the increase in complexity and size, the computational and memory requirements also increase exponentially.
    - Most of the machine learning algorithms fail to capture the nonlinear relationships in the real world data.

  Perceptron
    - an elemental building block of an artificial neural network,
    - it has four important components, input values, weights and biases, net sum, and activation function.

                      --------------------------------------
                      |              Error                 |    
                      V                                    |
       input        weights &                              | 
       values       biases                                 | 
         X1 ------>  w1     -------->                      |
         X2 ------>  w2     -------->|-> Net   -------->  Activation   -------> Output
         X3 ------>  w3     -------->|   Sum              Function
                                          ^
                                          |
                                          Bias (one per neuron) 


    - A perceptron accepts multiple input values, applies the weights on the inputs, sums all the inputs, and then 
      applies the transformation function or activation function to create an output signal or an output value.

    - bias is one per neuron and not one per input value.

  Activation Function.
    Step function (0 or 1) or Sign Function (-1 or 1)
      - An activation function can be a simple step function or a sign function that outputs zero or one or false or true 
        when the sum of the weighted inputs fall short or exceeds a threshold value.
      - An output of one or two indicates that a neuron is triggered,
      - an output of zero or false, indicates not being triggered.
      - This predicted output is then compared to the known output, and if there is an error in the prediction, this error 
        is propagated backwards to adjust the weights and recompute the training process.
      -The step function or a sign function can be used for a typical binary classification or a linear problem.
    Sigmoid function
       - use to address a non-linear problem,
       - a simple logistic function that represents a probability of the value between zero and one.
       - useful when we are interested in probability mapping instead of a simple zero or one.
     Rectified linear unit (ReLU) function,
       - allows to eliminate negative units in an artificial neural network.
       - equation:  ReLU(z) = max(0,z)
       - use if you want an guarantee an output is always positive
       key benefits of a ReLU function
         - it performs a faster and effective training of a deep neural network 
         - it scales very well.

  Neural Network
     - It's a computing system made up of a number of simple, highly interconnected neurons producing a certain output.
    multi-layered neural network includes:
       - input layer, one or more hidden layer and an output layer.

          input layer ---------> hidden layer(s)  -------> Output layer

       simple network with three layers
         - has one input layer, one hidden layer, and one output layer.
         - feed the labeled training data at the input layer along with weights for each connection.
         - An activation function is executed at the hidden layer to produce an output.
           - This is called a forward propagation.
         - This output could be the right prediction or the wrong one, and this value is compared to the actual value, 
           and the error is computed.
         - This error is also called as a cost function, and our goal is to minimize this cost function.
         - There are many optimization techniques like stochastic gradient descent to achieve this.
         - This error is fed back to the input layer and weights and biases are readjusted, and this process is 
           called back propagation or backward propagation.
         - This is an iterative training process to get an optimal training score.
         - Once the training process is perfected, you can feed in the test data and check how the prediction works 
           on unseen test data.

  Deep Learning challenges 
    - require high computational power to complete the training process
    - require large amounts of data to generalize well on any new unseen data.
    - tend to have more hyperparameters, they require careful tuning to achieve optimal performance.

  Deep Learning Business Use Cases 
    Sentiment Analysis
      - Since deep learning can process both structure and unstructured data, it is used in the field of 
        sentiment analysis to detect customer reviews and feedback.
    Chat bots [Intelligent Agents]
       - used in various customer service portals 
       - since deep learning excel in the field of natural language processing and visual recognition.
    Healthcare
       - support medical imaging specialists and radiologists, allowing them to analyze more images in less time.
    Detecting Criminal Activities
      - speech recognition and computer vision techniques can help improve the efficiency of investigative analysis 
        in detecting fraudulent or criminal activity.

------------------------------------------------------
2.9 Exploring Computer Vision Using Convolutional Neural


  Convolution Neural Network (CNN)
    - It is a deep learning technique inspired by human visual object recognition, designed to process images and video.
    - It uses a series of layers to extract features and then uses them to classify or detect objects.

  CNN Usages
    facial recognition
      - Unlocking your phone using the facial recognition feature,
    photoshop images
      - applying a filter to a photo to transform it before uploading to social media,
    automatic image tagging or content moderating
      - flag inappropriate contents are all CNN in action.
      - As the image goes through these layers, CNN fine-tunes the image recognition process until it finally 
        identifies the object.

  CNN Architecture
    - It begins with an input layer, where the image is fed into the network.
    - CNNs typically consist of three or more layers:

                                                                                      Fully-
      input   ------>  Convolutional  ------> Pooling Layer ------> Flattening  ----> Connected
      layer                layers               Layer(s)              Layer           Layer

    - The first one is a convolution layer, which is followed by a pooling layer, followed by a flattening layer.
      And the fully-connected layer, or the FC layer, is the final layer.


    Convolution layer.
      - They are the core building blocks of CNN.
      - convolution refers to a multiplication operation between two functions resulting in an output value.
      image
        - an image is composed of pixels.  And each pixel value is represented by a numerical value,
        -  an image is represented as a 'm * n' matrix, where m = number of rows  and  n = number of columns.
        grayscale image 
           - pixel values are either zero or one.

      convolution operation 
        - We will use a filter or a kernel that slides over the input image to perform the convolution operation.
        - This operation uses dot matrix multiplication between these two matrices, resulting in a convoluted matrix as the output.
        - This two-dimensional output matrix is called a feature map.
       Convolution Layer Hyperparameters
         Padding
           -  refers to the process of adding additional border pixels with zero values around the input image before
              performing the convolution operation.
           - helps preserve the dimension of the original image.
         stride.
           - the step size at which the filter moves across the input data during the convolution operation.
           - A value of one indicates that the kernel moves one pixel at a time.
           - increasing the stride value may result in faster computation, but also results in loss of information, 
             as a kernel covers fewer positions in the input data.

    Activation Layer or ReLU layer 
      - introduces non-linearity to CNN.
         - Without this additional function (e.g. ReLU) we run into the risk of reducing CNN to a linear model.
      - ReLU equation:  O(x) = max(0,x)
        - ReLU outputs the input for a positive value, or zero for negative values
      - If your source data has complex features, you may use multiple convolution layers.
          - The initial convolutional layers learn simple features, and the deeper layers learn more complex features.

    Pooling Layer
      - used to downsample the feature map, reducing the spatial dimensions of the input while preserving the relevant information.
      Pooling Strategies.
        Max Pooling,
          - uses the most significant element from each feature region in the feature map.
        Average Pooling
          - calculated by computing the average value of each feature region in the feature map.

    Flattening Layer.
       - Converts the two-dimensional arrays into a linear vector
         - Flattening is a process of converting all the resultant two-dimensional arrays from pooled feature maps 
           into a single, long, continuous linear vector, which is one-dimensional.
       - serves as a bridge between the convolution layer and the fully connected layer.

    Fully Connected Layer, (or dense layer)
      - each neuron receives input from all the neurons in the previous layer and produces a single output value.
      - usually in the final stages of a CNN
      - Makes predictions
         - enabling them to learn from raw input data, extract features, and make predictions.

  CNN business use cases
     Computer vision 
       - is a growing area that derives meaningful information from digital images.
     social media platform
       - auto-tagging the friends and families in the pictures.
     healthcare
       - used in medical imaging analysis, for tasks such as disease diagnosis and treatment planning.
     Visual search 
       - retail ecommerce websites use to effortlessly search for specific items.
     Object Detection
       - Object detection, which is a subset of computer vision, is a game-changer in the field of autonomous 
         driving, robotics, and surveillance systems.

------------------------------------------------------
2.10 Processing Sequential Data Using Recurrent Neural Networks 




  Recurrent Neural Networks (RNNs)
    - an artificial neural network designed to process sequential data using internal memory.
    - It is a popular technique commonly used in Google's Translate and Apple Siri.

  Sequential Data
    - Sequential data is an ordered data where related things follow each other.
    Time Series Data
      - the observations are collected over time.
      - Typical examples are weather data and stock prices.
    Text Data 
      - Word sequence convey a specific meaning.
    Audiovisual data 
      - sequence of speech or images captured over time.

  Feedforward Neural Networks (FNN) vs Recurrent Neural Networks (RNNs)
    FNN
      - A traditional feedforward network assumes that the inputs and the outputs are independent of each other.
      - the information flows in one direction, from the input layer through the hidden layer, to the output layer.
      - no memory of the past input and cannot predict what's coming next.
      - cannot maintain historical data.
      - address typical classification or regression problems.
    RNN
      - the output depends on all the elements that are part of the sequence.
      - the information flows in both the directions which allow it to maintain an internal memory.
      - considers a current input and the learnings from the past input.
      - RNNs's ability to capture and maintain temporal dependency makes it well-suited for speech recognition 
        and text prediction that involves text data and time series data.

   RNN simple architecture diagram
      - simple architecture diagram of a typical RNN that is unrolled in time.
      - Each Recurrent Neuron has two sets of weights, one for the input X at time t, and the other from the 
        previous time interval, t-1.
      - This is also called a memory cell because at any point in time, the output is a function of all inputs 
        from the previous steps.

           Y          Y(t-1)    Y(t)     Y(t+1)
           ^           ^        ^         ^            a -> activation function
           |           |        |         |            b -> bias
           |           |b       |b        |b           w -> weights
           |           |        |         |
           |---        |    a   |    a    |     a
           h  |     h(t-1) --- h(t) ---- h(t+1)
           |---        |        |         |
           |           |w       |w        |w
           |           |        |         |
           X          X(t-1)    X(t)     X(t+1)


       - It uses the same set of activation functions, weights, and biases for each input, and performs the same task 
         on all the input unhidden layers to produce the output.
       - instead of creating multiple hidden layers, RNNs will create one hidden layer and loop over it as many 
         times as required.

  RNN types:

    LSTM (long short-term memory)
      Issue that LSTMs help address:
        - Consider predicting the last word in the following phrase:  
          "I am an expert in Python, and it is very important in the field of data science."
        - If we were to predict the 'field', it would be much easier to have the context of 'Python' ahead of time.
        - As the gap between the relevant context gets wider, RNNs become less effective.
      - LSTM is a type of RNN that addresses this problem by capturing long-range dependencies over extended sequences.
      - contain memory cells 
        - that enable them to retain information over long sequences.
      - use a series of gates that controls the information flow in and out of the memory cells at every step.

   GRU (Gradient Recurrent Unit)
     - another technique that can capture long-range dependencies.  Similar to LSTM,
     - equipped with control gates that direct the information flow within the network.
     - simpler architecture with fewer gating units to selectively update the hidden state at each step,
     - computationally less expensive and makes the model easier to train.

  RNN configurations
    one-to-one architecture

        X  ----->  h  -------> Y

      - a single input resulting in a single output.
      - practical application: predicting the next word or character.
    one-to-many architecture 

        X  ----->  h  -------> Y
                   |
                   V
                   h  -------> Y1
                   |
                   V
                   h  -------> Y2
                   |
                   V
                   h  -------> Y3
                   |
                   V
                   h  -------> Y4

      - the input is single but results in multiple outputs.
      - practical application:  an image captioning where the input is a single image, but the output is a sentence 
         containing multiple words predicting the image.
      - For example, the diagram below shows an image of a human, and the output results in a text saying, "This is a human."
   many-to-one architecture

        X  ----->  h  
                   |
                   V
        X1 ----->  h  
                   |
                   V
        X1 ----->  h  -------> Y
                   

     - multiple inputs generate a single output.
     - practical application:  sentiment analysis.
        - For example, consider the following example where the text, "I like this," resulting in a positive sentiment.
   many-to-many architecture


        X  ----->  h 
                   |
        X1 ----->  V
                   h 
                   |
                   V
                   h  -------> Y3
                   |
                   V
                   h  -------> Y4


     - multiple inputs generate multiple outputs
     - practical application: language translation.
       - For example, consider the following scenario where we translate a simple sentence from English to Spanish 
         where multiple words as input resulting in multiple translated words as output.

------------------------------------------------------
2.11 Employing Transfer Learning 

  Transfer Learning
    - an ML technique in which the knowledge of a previously trained model on one task is leveraged to improve 
      performance on a new related task.
    - By leveraging the features and knowledge from the original task, the amount of label data and the 
      computation resources required to train the new task are considerably reduced.

  Transfer Learning in Action.

       Common inner layers 
         - The initial layer, such as convolution layer, pooling layer, and flattening layers are common inner 
           layers that focus on extracting image features that can be easily reused.
       task specific layers
         - The later layers are task-specific that specialize on the classification or regression task.

      input   ------>  Convolutional  ------> Pooling Layer ------> Flattening  ----> Connected  ----> Task 
      layer                layers               Layer(s)              Layer           Layer            Specific
                                                                                                       Layers

      <------------------ Common inner layers --------------------------------->  <------- task related layers --->

    Task: 
      we have an image dataset containing many animals, and our requirement is to classify them.
    Steps:
       select a pre-trained model.  
          - And our scenario, since our dataset is limited, we can leverage a pre-trained CNN model that has been 
            trained on images to extract high level features from the input images.
       freeze the weights of the common layers
         - freeze the weights of the common layers that are obtained from the original task,
         - unfreeze the later layers that are specific
       introduce new layers 
        - introduce new layers that are specific to the new task.
       pass the new input data to the network,
         - leverage the feature extraction capability from the previously trained convolution and pooling layers, 
           and use them as input to the later task-specific layers.
       monitor the model performance,
         tune hyper parameters to improve the output.

  Transfer learning benefits,
    Addresses data scarcity issues
      - In scenarios where label data for the new task is limited, transfer learning enables a model to 
        generalize better by leveraging knowledge from related tasks with abundant data.
    Reduced computational resources.
      - Instead of training a model from scratch on the new task, transfer learning leverages pre-trained models.
      - This significantly reduces their time on the computational resources required for training because the model 
        starts with a good initialization and only needs to learn the task-specific features.
    Enhanced performance.
      - Transfer learning leverages knowledge and representations learned from the original task to improve performance 
        on the new task.
      - By transferring useful features or representations, the model can achieve better performance, especially when 
        the new task has limited label data.

  Transfer Learning Business use cases
    customer sentiment analysis
      - where we have limited data on customer sentiments.
    Image classification.
      - Training ML models on images takes a lot of time and they are computationally expensive.
      - By leveraging models that are pre-trained on images, we can enhance a performance of new models.
    Speech recognition and voice processing,
      - enables more natural and accurate interactions with users.

------------------------------------------------------
2.12 Frame Business Problem as ML Problems Review 


  Certification Task Statement 3.1
                                          |-> Determine when to use and when not to use ML
                                          |
       Task statement 3.1                 |
       (Frame business problems    -------|-> Know the difference between supervised and 
       as ML Problems)                    |     unsupervised learning
                                          |
                                          |-> Select from amount the classification, regression,
                                                forecasting, clustering, and recommendation models

  Determine when to use and when not to use ML.
     - Machine learning requires: 
       - significant amount of historical data
       - domain expertise to eliminate the noise around the data,
       - large computational resources
     - mission critical application
        - if an error in your model predictions may not be acceptable, machine learning is not the right solution
     - a business problem that that can be solved with simple rules is not a good candidate for ML

  Know the difference between supervised and unsupervised learning.
    training data
      - In supervised learning, the training data is labeled,
      - unsupervised learning deals with unlabeled data
    Learning type
      - supervised learning focuses  prediction or classification type tasks
      - unsupervised learning focuses on uncovering hidden patterns in the data 
    Techniques
      - common supervised training techniques: linear regression, logistic regression, and time-series forecasting
      - common unsupervised learning techniques: clustering, association learning, and dimensionality reduction.
    Feedback
      - Supervised learning: Since the actual value of the target feature is known ahead of time, the difference between 
        the actual and predicted value is provided as feedback.
       - unsupervised: no feedback is possible in unsupervised learning.

  Select from classification, regression, forecasting, clustering, and recommendation models.
    classification
      - Classification algorithms are used when the dependent feature is categorical in nature.
        - Binary classification , multi-class classification, and multi-label classification are some of the variations
    regression 
      - A regression algorithm is used when the target feature is quantitative, or continuous in nature.
        - Linear regression, multiple regression, and polynomial regression are some of the variations.
    time-series focecasting
      - time-series forecasting is used when the input data is collected at regular intervals
        Four core components of time-series forecasting are:
          trend
            - the directionality of the data over time;
          seasonality 
            - a periodic fluctuations that occur at regular intervals within the data;
          cyclical variations 
            - a non-repeating fluctuation in the data that may occur at irregular intervals;
          irregularity 
            - the randomness or the noise in the data.
    clustering 
      - Clustering is an unsupervised learning algorithm that groups data points based on the similarities 
        into different clusters.
      - The goal of a clustering algorithm is to group similar data points, and similarity is determined 
        by the distance between two data points.
      Clustering Algorithm categories
        - Based on the similarity metrics, clustering algorithms are broadly divided into four major categories:
        centroid-based clustering 
          - requires a number of clusters to be predetermined.
        Density-based clustering 
          - does not require the number of clusters to be predetermined,
        Hierarchical clustering
          - groups data together based on similarities and builds a hierarchy of clusters
        Distribution-based clustering,
          - assumes data points are generated from a mixture of probability distributions, and seeks to model 
            these distributions to identify clusters in the data.

  Deep Learning
    Deep learning drivers:
      - As the data complexity and size increase, the computational and memory requirements also increase exponentially.
      - Most of the [non-deep learning] machine learning algorithms fail to capture the nonlinear relationships that exist 
        in the real world data.

    Deep learning technique 
      - addresses most of these limitations by leveraging neural networks with multiple layers of interconnected neurons.
      - a subset of machine learning that is modeled after the human brain, focused on solving complex problems by 
        learning from large amounts of data without human intervention.

    Convolution Neural Network (CNN)
      - uses a series of layers to extract features, and then use them to classify or detect objects.

    Recurrent Neural Networks (RNNs)
      - designed to process sequential data using internal memory.

    Transfer Learning 
      - a ML technique in which the knowledge of a previously trained model on one task is leveraged to improve 
        performance on a new related task.


------------------------------------------------------
2.13 Frame Business Problems as ML Problems - Quiz

-----------------
Question 2
You are currently working on a dataset that shows an individual's height and weight over a period of time, but this data 
is not a regular interval. You have been assigned to create a machine learning (ML) model to predict an individual's weight, 
given his height. What can you use to solve this problem?

Choices: 
  Binary classification algorithm

  Logistic regression algorithm

  Multivariate regression algorithm

  Linear regression algorithm                 <--- Correct Answer

Good work!
    Linear regression is commonly used when you have one dependent and one independent feature.


------
What is Multivariate Regression ?
  https://towardsdatascience.com/applied-multivariate-regression-faef8ddbf807

  - Multivariate regression is an extension of simple linear regression. 
  - It is used when we want to predict the value of a variable based on the value of two or more different variables. 
  - The variable we want to predict is called the Dependent Variable, while those used to calculate the dependent variable 
    are termed as Independent Variables.

  Note: main difference with linear regression is predicting a targets based on more than 1 variable?

-----------------
Question 3

You are currently working on training a large image dataset. However, you have limited data and don't have a lot of 
computational resources.  How would you approach training the data?

  Use a convolutional neural network.

  Use a recurrent neural network.

  Use supervised learning.

  Use transfer learning.                 <--- Correct Answer

Good work!
  With the limitations in place, transfer learning is the right approach.


-----------------
Question 4

You are working on a clustering problem where the number of clusters cannot be easily predetermined. Upon analyzing the 
data, you found that your data contains many outliers. Which clustering algorithm would you choose to address this problem?

  Distribution-based clustering

  Hierarchical clustering                  <--- Incorrect Answer

  Density-based clustering                 <--- Correct Answer

  Centroid-based clustering

Sorry!
  Since the data contain outliers, hierarchical clustering is not a good choice.
Correct Answer
  Density-based clustering is excellent in handling data containing outliers.


------

A Cloud Guru:
AWS Certified Machine Learning - Specialty (MLS-C01): Exploratory Data Analysis
4.5 Performing Cluster Analysis Using the Elbow Method
  ...
  Cluster analysis Technique Categories
    - three major categories based on the number of observations, presence of outliers, and the features in the dataset
    Paritioning clustering
      - user must specify the number of custers indicated by a variable 'K'.
      - each cluster will contain at least one observation and they will be unique
      k-means
        - popular partitioning clustering algorithm
    Hierarchical clustering
      - user is not required to specify the number of clusters 
      - cluster assignments are determined by creating a hierarchy and cutting the tree at a specified depth
      - creating a hierarchy either in a bottom-up approach or a top-down approach, and cutting the tree at 
        a specified depth
      Agglomerative clustering 
        - a popular hierarchical clustering bottom-up approach algorithm
    Density-Based clustering
      - user is not required to specify the the number of clusters
      - cluster assignments are determined based on the density of the data points
      - This technique works based on a distance-based parameter that determines the distance between 
        the two points.
      DBScan 
        - popular density-based clustering algorithm

------
      DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 
         https://www.newhorizons.com/resources/blog/dbscan-vs-kmeans-a-guide-in-python
         - It is a density-based algorithm that groups together points that are close to each other based on a density criterion. 
         - Points that are not part of any cluster are considered noise. 
         - DBSCAN is particularly useful when dealing with datasets that have irregular shapes and different densities.
------
   DBSCAN [density] vs Hierarchical Clustering
     https://medium.com/@amit25173/dbscan-vs-hierarchical-clustering-3bc4d9635bc8

     performance:
       - DBSCAN is generally more efficient when dealing with large datasets, especially if the data isn’t too varied in density
       - Hierarchical Clustering, however, tends to be more computationally expensive

     Handling Noise and Outliers
       - DBSCAN is almost a natural-born noise detector. Since it only groups points that are in dense regions, any point that 
          doesn’t fit in gets classified as noise. This makes DBSCAN particularly robust in messy datasets where outliers can skew results
       - Hierarchical Clustering doesn’t inherently deal with noise. It tries to cluster every point, even if it doesn’t belong anywhere.

     Cluster Shape and Size
       - DBSCAN is the clear winner for flexibility when it comes to cluster shapes. It doesn’t assume any specific shape for the clusters, 
         allowing it to find those tricky, irregularly shaped clusters that other algorithms might miss.
       - Hierarchical Clustering, however, is more structured. It works well when clusters are roughly spherical or elliptical and 
         when you’re more interested in understanding the hierarchy of clusters than their exact shapes

     Parameter Sensitivity
        - With DBSCAN, the key parameters are epsilon (ε) — the maximum distance between points in a cluster — and minPts, the minimum 
          number of points required to form a dense region. Getting these parameters right is crucial. Set them too high or too low, 
          and you might end up with too few or too many clusters, or miss clusters entirely.
        - Hierarchical Clustering requires you to choose a linkage criterion — how you measure the distance between clusters as they merge. 
          Whether you choose single, complete, or average linkage will significantly impact the shape of your clusters and the resulting 
          dendrogram. This choice can be somewhat subjective, and small changes can lead to different clustering outcomes.

      Use Cases
        - DBSCAN is your go-to for spatial data or any dataset where clusters might have irregular shapes and you need to handle 
          noise robustly.
        - Hierarchical Clustering shines in scenarios where understanding the structure of the data is more important than the specific 
          clusters themselves. 

-----------------
Question 5
You are currently working on an ML problem that requires classifying the customer reviews of a product between 1 and 5. What type 
of classification problem is this?

  Multilabel classification

  Imbalanced classification

  Multiclass classification                 <--- Correct Answer

  Binary classification

Good work!
  Multiclass classification is the right approach when we have more than two classes.


-----------------
Question 6
You are working on training a set of images using a convolution neural network. Which hyperparameter is used to prevent the image from 
shrinking in size during the convolution operation?

  Stride

  Padding                 <--- Correct Answer

  Shrink

  Pooling

Good work!
  Padding adds an additional layer of pixels around the images to prevent the images from shrinking in size.



    Number of Layers

------
How to decide the hyperparameters in CNN
  https://medium.com/@sengupta.joy4u/how-to-decide-the-hyperparameters-in-cnn-bfa37b608046

  Number of Layers
    - The number of layers in a CNN is a critical hyperparameter that determines the depth of the network. 
    - A deeper network can learn more complex features and patterns from the data, but it is also more prone to overfitting. 
    - A good starting point is to use a small number of layers and gradually increase their depth until the desired performance is achieved.

  Filter Size
    - The filter size is another important hyperparameter that determines the receptive field of each convolutional layer. 
    - A larger filter size can capture more information from the input image, but it also increases the number of parameters in the network. 
    - A smaller filter size can reduce the number of parameters, but it may not be able to capture all the relevant features in the image.

  Stride
    - The stride is a hyperparameter that determines the number of pixels by which the filter moves across the input image. 
    - A larger stride can reduce the size of the output feature maps, but it can also lead to information loss. 
    - A smaller stride can preserve more information, but it also increases the computation time and memory requirements. 
    - Default stride in CNN is 1

  Padding
    - Padding is a technique used to preserve the spatial dimensions of the input image while applying convolutional layers. 
    - It involves adding zeros around the border of the input image to create a padded image that can be convolved with the filter. 
    - Padding can help preserve the information at the edges of the image and prevent the loss of spatial resolution. 
    - However, it also increases the memory requirements and computation time of the network. 
    - Therefore, it is important to experiment with different padding techniques and choose the one that gives the best performance.

  Learning Rate
    - The learning rate is a hyperparameter that determines the step size at which the network updates its parameters during training. 
    - A large learning rate can lead to rapid convergence but may result in unstable and oscillating training. 
    - A small learning rate can ensure stable and smooth training but may result in slower convergence

  Batch Size
    - The batch size is a hyperparameter that determines the number of samples that are processed by the network in each training iteration. 
    - A larger batch size can reduce the variance of the gradient estimates and improve the stability of the training.  However, it also 
      increases the memory requirements and may lead to slower convergence. 
    - A smaller batch size can reduce the memory requirements and improve the convergence speed but may lead to noisy gradient estimates. 

-----------------
Question 7
You are the chief architect for your company. Management has approached you to devise an inventive solution for their mission-critical 
application that involves simple business rules. You do have lots of data for training purposes and the company has allocated you enough 
budget to purchase the required computing resources. What solution will you suggest?

  Supervised machine learning algorithms

  Unsupervised machine learning algorithms

  Deep learning solution using convolutional neural networks

  Simple rule-based computer program                 <--- Correct Answer

Good work!
  Since the application is mission-critical and based on simple business rules, suggesting a machine-learning solution would be overkill.


-----------------
Question 8
You are currently working on a problem of image captioning and have been instructed to use a recurrent neural network. 
You want to predict the image and give a meaningful description. Which RNN configuration would you use?

  One-to-many                 <--- Correct Answer

  Many-to-many                <--- Incorrect Answer

  Many-to-one

  One-to-one

Sorry!
  Many-to-many is not a good solution.
Correct Answer
  A one-to-many configuration is commonly used in image captioning.

------
Introduction to Recurrent Neural Networks
https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/

  - In simple terms, RNNs apply the same network to each element in a sequence, RNNs preserve and pass on relevant information, 
    enabling them to learn temporal dependencies that conventional neural networks cannot.
  - Recurrent Neural Networks (RNNs) solve this by incorporating loops that allow information from previous steps to be fed back 
    into the network. This feedback enables RNNs to remember prior inputs, making them ideal for tasks where context is important. 

  Key Components of RNNs
    1. Recurrent Neurons
      - The fundamental processing unit in a Recurrent Neural Network (RNN) is a Recurrent Unit, which is not explicitly called 
         a “Recurrent Neuron.” 
      - Recurrent units hold a hidden state that maintains information about previous inputs in a sequence. 
      - Recurrent units can “remember” information from prior steps by feeding back their hidden state, allowing them to capture 
        dependencies across time.
    2. RNN Unfolding
      - RNN unfolding, or “unrolling,” is the process of expanding the recurrent structure over time steps. 
      - During unfolding, each step of the sequence is represented as a separate layer in a series, illustrating how information 
        flows across each time step. 
      - This unrolling enables backpropagation through time (BPTT), a learning process where errors are propagated across time 
        steps to adjust the network’s weights, enhancing the RNN’s ability to learn dependencies within sequential data.

   Types Of Recurrent Neural Networks
     - There are four types of RNNs based on the number of inputs and outputs in the network:

     1. One-to-One RNN
       - One-to-One RNN behaves as the Vanilla Neural Network, is the simplest type of neural network architecture. 
       - In this setup, there is a single input and a single output. Commonly used for straightforward classification tasks 
         where input data points do not depend on previous elements.

     2. One-to-Many RNN
       - In a One-to-Many RNN, the network processes a single input to produce multiple outputs over time. 
       - This setup is beneficial when a single input element should generate a sequence of predictions.
       - For example, for image captioning task, a single image as input, the model predicts a sequence of words as a caption.

     3. Many-to-One RNN
       - The Many-to-One RNN receives a sequence of inputs and generates a single output. 
       - This type is useful when the overall context of the input sequence is needed to make one prediction.
       - In sentiment analysis, the model receives a sequence of words (like a sentence) and produces a single output, 
         which is the sentiment of the sentence (positive, negative, or neutral).

     4. Many-to-Many RNN
       - The Many-to-Many RNN type processes a sequence of inputs and generates a sequence of outputs. 
       - This configuration is ideal for tasks where the input and output sequences need to align over time, often in a one-to-one 
         or many-to-many mapping.
       - In language translation task, a sequence of words in one language is given as input, and a corresponding sequence in 
         another language is generated as output.

-----------------
Question 9
In reinforcement learning terminology, what is an environment?

  The domain where the agent operates                 <--- Correct Answer

  An entity that is learning to make decisions

  The feedback that is provided to the agent

  An agent decision-making process

Good work!
  The environment is the domain where the agent operates.

------
AWS Certified Machine Learning - Specialty (MLS-C01)

7.10 Reinforcement Learning

  Reinforcement Learning Strategies
    Positive
      - provide a positive reward thereby motivating the subject to repeat the behavior, presumably
        for another positive reward
    Negative
      - provide a displeasurable experience or response thereby motivating the subject to NOT repeat the
        undesired behavior

  Reinforcement Learning (RL)
    - RL is a machine learning technique that attempts to learn a strategy, called a policy, that optimizes
      for an agent acting in an environment
    - well suited for solving problems where an agent can make autonomous decisions

    - find the path to the greatest rewards

  Markov Decision Process (MDP)
    Agent:
      - the thing that's going to be doing the activity
    Environment
      - the agent is placed in an environment
      - the environment could be real-world or simulated
    Reward
    State
      - state is the information about the environment and maybe any past steps that might be relevant
        to any sort of future steps
    Observation
      - the information that's available to the agent at each state or each step
    Episodes
      - episodes are just iterations from start to finish that the agent takes while it's accumulating reward
    Policy
      - the goal is to create a policy that will help it make those decisions in the future, given a similar objective 
      - in developing that path, it's going to learn some lessons that will help it make those decisions 
        in the future, given a similar objective.  And this is called the policy.
      - It's the decision making part of the reinforcement learning model that will make choices which will 
        hopefully maximize our reward.


-----------------
Question 10
You are currently working on a deep learning problem, and upon analyzing the data, you found that the data contains a non-linear 
relationship between the features. Which activation function would you choose?

  Step function

  Sigmoid function                 <--- Correct Answer

  Sign function

  Multi-step function

Good work!
  A sigmoid function is good for addressing non-linear data.

------

Step Activation Function vs. Sigmoid Activation Function: A Detailed Comparison
  https://www.linkedin.com/pulse/step-activation-function-vs-sigmoid-detailed-babu-chakraborty/

  Step Activation Function
   - The step activation function is a simple but effective activation function. 
   - It takes a single real number as input and outputs a 0 or 1, depending on whether the input is greater than or equal 
    to a threshold value. The equation for the step activation function is as follows:

      f(x) = 1 if x >= threshold
      f(x) = 0 if x < threshold 

   - The step activation function is often used in binary classification problems, where the goal is to classify an input into 
     one of two categories. For example, the step activation function could be used to classify a tumor as benign or malignant. 

   OReily Note: step function only has flat segments, so there is no gradient to work with (gradient descent cannot work
                on a flat surface

  Sigmoid Activation Function
    - The sigmoid activation function is a more complex activation function that produces a smooth, S-shaped curve. 
    - The equation for the sigmoid activation function is as follows:

      f(x) = 1 / (1 + e^(-x)) 

    - The sigmoid activation function is often used in multi-class classification problems, where the goal is to classify 
      an input into one of several categories. 
    - For example, the sigmoid activation function could be used to classify a flower as a rose, tulip, or daisy. 

------------------------------------------------------

Chapter 3 Select the Appropriate Model(s) for a Given ML Problem 

------------------------------------------------------
3.1 Reviewing SageMaker's XGBoost Algorithm 

  XGBoost Algorithm.
    - a popular implementation of gradient boosted trees algorithm.
    - uses: ensemble learning, boosting, and gradient boosting
    Ensemble Learning
     - multiple ML models are combined to improve prediction accuracy as compared to a single ML model.
    Boosting 
      - an ensemble learning technique that sequentially combines their predictions and improve the overall 
        performance of the model.
      steps:
        - start by initializing the same weights to all the models.
        - The model is then trained against a subset of training data.
        - The error of the weak learner is computed.
        - Models with larger error rate are assigned higher weights compared to models that perform better, 
          and they are retrained.
        - The previous two steps are repeated multiple times.
        - The final prediction is based on the weighted total of all the weak learners.
      Boosting Algorithm Types 
        Adaptive BOosting 
          - extensively used in classification problem
      Gradient BOosting 
        - can be applied both to regression and classification problems.
        - uses gradient descent algorithm to minimize the errors.

    Decision tree algorithm
      - can be used to predict a category in a classification problem or a continuous numeric value in a 
        regression problem.
      - divide the dataset into smaller subsets based on their features.
      - predicts the output by evaluating a sequence of if-then-else and true or false feature questions 
        and estimating the minimum number of branches needed to assess the probability of making the right decision.

    Gradient Boosted Decision Tree
      - a decision tree ensemble learning algorithm that combines multiple machine learning model algorithms 
         to obtain a better model.
      - XGBoost is an implementation of gradient boosted decision tree algorithm.

  SageMaker XGBoost Algorithm Attributes
      - both as a built-in algorithm or as a framework 
    Learning Type:
      - both classification and regression 
    File/Data Types:
      - libsvm, CSV, parquet, and protobuf.
    Instance Type:
      - CPU and GPU training process.
    Hyperparameters
      - required: num_round and num_class
      - 30+ optional hyperparameters to tune
      Num_round 
        - the number of rounds to run the training
      num_class
        - number of classes
    Metrics:
      - reports MAE, MSE, RMSE, and MAP as metrics to measure a regression problem,
      - reports accuracy, area under curve (AUC), and F1 score to measure a classification problem.

      Metric Notes:
         MAE: Mean absolute error
            MAE(X,h) =  (1/m) SUM |(h(xi) - yi)|  where SUM is from i=1 to i=m
         MSE: Mean Square Error
            MSE(X,h) = [ (1/m) SUM (h(xi) - yi)**2 ] where SUM is from i=1 to i=m
         RMSE: Root Mean Square Error
            RMSE(X,h) = [ (1/m) SUM (h(xi) - yi)**2 ]**1/2 where SUM is from i=1 to i=m

              m: number of instances in the dataset
              xi: a vector of all the features values of the ith instances of the dataset, and
                  yi is the label (desired output)
              X: is a matrix containing all the features excluding labels of all instances in the dataset
              h: your systems prediction's function, also called the 'hypothesis'

         MAP: Mean Absolute Precision
             MAP = 1/m SUM AP_i    where SUM is from 1=1 to i=m; AP_i: average precision of instance i

         MAPE: Mean Absolute Percentage Error
            MAPE(X,h) =  (100/m) SUM |[(h(xi) - yi)/ yi]|  where SUM is from i=1 to i=m

  Business Use Cases
    finance domain
      - detect frauds 
      - predict stock prices.
    e-commerce
      - predict customer churn 
      - forecast sales.
    marketing
      - to predict ad-click revenue
      - customer segmentation.

  Note: Resource section is missing link to XGBoost regression notebook 


  XGBoost algorithm with Amazon SageMaker AI
  https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html

  XGBoost sample notebooks
    https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-sample-notebooks.html
      Regression with Amazon SageMaker XGBoost (Parquet input)


  The following list contains a variety of sample Jupyter notebooks that address diffeerent use cases of Amazon SageMaker AI XGBoost algorithm.

    How to Create a Custom XGBoost container
    – This notebook shows you how to build a custom XGBoost Container with Amazon SageMaker AI Batch Transform.
      https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/sagemaker_studio_image_build/xgboost_bring_your_own/Batch_Transform_BYO_XGB.html
         Save to: notebooks/xgboost_notebooks/Batch_Transform_BYO_XGB.ipynb

   Regression with XGBoost using Parquet
    – This notebook shows you how to use the Abalone dataset in Parquet to train a XGBoost model.
      https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_parquet_input_training.html
         Save to: notebooks/xgboost_notebooks/xgboost_parquet_input_training.ipynb

   How to Train and Host a Multiclass Classification Model
    – This notebook shows how to use the MNIST dataset to train and host a multiclass classification model.
         https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_mnist/xgboost_mnist.html
         Save to: notebooks/xgboost_notebooks/xgboost_mnist.ipynb

------------------------------------------------------
3.2 Introducing Amazon SageMaker 

  Amazon SageMaker service
    - a fully managed machine learning service that provides data engineers with the services to prepare, build, 
      train, and deploy machine learning models at scale.  

     Data Collection --> Data Analysis --> Data Processing --> 

          --> Build Model --> Train Model --> Test model --> Deploy Model --> Monitor Model
                                          <--


  SageMaker Services for Machine Learning Life Cycle
    Data collection phase
      - can use Amazon SageMaker Ground Truth,
      - is a fully managed labeling service that helps build a highly accurate training set.
    Data analysis phase
      - can use Amazon SageMaker Data Wrangler,
      - simplifies the process of data preparation and enables users to analyze and visualize data without 
        writing code.
    Data processing phase
       - SageMaker Feature Store
       - simplifies feature processing, storing, and retrieving the features for model development.
    Build Model
       - can use Amazon SageMaker notebooks,
         - a fully managed Jupyter Notebook,
       - can use SageMaker studio,
          - an IDE for machine learning, and provides a visual interface for all the steps of the ML lifecycle.
    Train and test ML model 
       - there are multiple options.
         - can use any built-in algorithm provided by the SageMaker.
         - can also use custom code to train with deep learning frameworks.
         - Write your own custom algorithm
         - use an algorithm from the AWS marketplace.
    Deploying model
       - can use Amazon SageMaker hosting services that can deploy trained models for inference at scale 
         with low latency.
        - can use Amazon SageMaker Batch Transform that can run batch inference on large data sets asynchronously.
    Monitor model:
      - can use SageMaker Model Monitor to continuously monitor deployed models for its performance and drift.

          S3        ------>|                   |
          Training         | compute instances |
          Data             | training code     |  -----> S3
                           |                   |         output data
          container ------>|                   |
          image

  Algorithm Implementation options
     built-in algorithm.
        - the easiest option and requires no coding to start running the experiments.  
        - requires algorithm with data, hyperparameters, and computing resources.
     script mode
        - in a supported framework, like Psyche learn, TensorFlow, pyarch, or MXNet.
        - In this mode, you write a custom Python script and leverage the additional Python libraries that 
          comes preloaded with these frameworks for training an algorithm.
     custom docker image,
        - if your use case is not addressed by previous two options.
        - This option requires Docker knowledge, of course.
        - the Docker image must be uploaded to Amazon ECR before you can start training the model.

------------------------------------------------------
3.3 Exploring SageMaker's Linear Learner Algorithm 

  Sagemaker linear learner algorithm.
    - A linear learner is a supervised learning algorithm that can be used to address classification or 
      regression problems.
    - It can handle large data sets and high dimensional features.

  Linear learner
      - formula:   Y = m1*x1 + + m2*x2 + ...+ mn*xn + b

      - Training the linear learner model is like adjusting these weights and biases to minimize the difference 
        between the predictor and actual home price.
      - uses a stochastic gradient descent to best fit the line to the data points.
      - It iteratively adjusts the model parameters to minimize the loss function.

  Sagemaker linear learner algorithm Attributes
    Learning Type:
      - classification and regression 
    File/Data Types:
      -  CSV, protobuf, JSON (inference only)
    Instance Type:
      - CPU and GPU training process.
    Hyperparameters
      - required: num_class and predictor_type
      num_class
        - number of classes
      predictor_type 
        - Specifies the type of target variable as a binary classification, multiclass classification, or regression.
    Metrics:
      - Cross entropy loss, absolute error, and MSE (regression metrics)
      - Precision, Recall, accuracy (classification metrics)

  Amazon SageMaker -> Developer Guide -> Linear learner hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html
    num_classes 	
      - The number of classes for the response variable. The algorithm assumes that classes are labeled 0, ..., 
        num_classes - 1.
      - Required when predictor_type is multiclass_classifier. Otherwise, the algorithm ignores it.
      - Valid values: Integers from 3 to 1,000,000
    predictor_type 	
      - Specifies the type of target variable as a binary classification, multiclass classification, or regression.
      - Required
      - Valid values: binary_classifier, multiclass_classifier, or regressor


  Linear Learner business use cases
     processing loan applications
       - based on financial history,
     detecting email spam 
       - based on content and sender information,
    recommendation systems
      - to create personalized recommendations for product, music and movies.

Note: Resource section Missing: sample notebook demonstrating SageMaker's linear learner algorithm.


   Linear Learner Algorithm
   https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html

      An Introduction with the MNIST dataset

        Using the MNIST dataset, we train a binary classifier to predict a single digit.
          https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.html
             saved to: notebooks/linear_learner_notebooks/linear_learner_mnist.ipynb


------------------------------------------------------
3.4 Understanding SageMaker's K Nearest Neighbors (k-NN or KNN) Algorithm 

  SageMaker KNN algorithm.
    - It is an index-based, non-parametric algorithm that can be used to address both classification and 
      regression problems.
    - It is index-based because it operates on indexed data structures
    - non-parametric means they make no assumptions on the underlying data and uses data structures to 
      organize and retrieve data quickly.

    - addressing a regression problem by calculating the average of the K closest points.

    The training process steps: 
       sampling,
         - specify the sample dataset size, using a 'sample_size' hyperparameter so that the dataset can 
           fit into memory.
       perform dimensionality reduction
         - reduces the number of features in the dataset.
         - supports two types of dimension reduction.
         - you specify the type using the 'dimension_reduction_type' hyperparameter.
       build the index
         - enables efficient lookups between data points.
         - The current implementation supports three types of indexing, and you need to use the 
           'index_type' hyperparameter to specify its value.


Amazon SageMaker -> Developer Guide -> k-NN Hyperparameters
  https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_hyperparameters.html

  key hyperparameters:
    feature_dim 	
      - The number of features in the input data.
      - Required; Valid values: positive integer.
    k 	
      - The number of nearest neighbors.
      - Required; Valid values: positive integer
    predictor_type 	
      - The type of inference to use on the data labels.
      - Required;  Valid values: 'classifier' for classification or 'regressor' for regression.
    sample_size 	
      - The number of data points to be sampled from the training data set.
      - Required;   Valid values: positive integer
    dimension_reduction_target 	
      - The target dimension to reduce to.
      - Required when you specify the dimension_reduction_type parameter.
      - Valid values: positive integer greater than 0 and less than feature_dim.
    dimension_reduction_type 	
      - The type of dimension reduction method.
      - Optional 
      - Valid values: 'sign' for random projection or 'fjlt' for the fast Johnson-Lindenstrauss transform.  
      - Default value: No dimension reduction
    index_type 	
      - The type of index.
      - Optional 
      - Valid values: 'faiss.Flat', 'faiss.IVFFlat', 'faiss.IVFPQ'.
      - Default values: faiss.Flat


-----
How the k-NN Algorithm Works
  https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_how-it-works.html
  - The Amazon SageMaker AI k-nearest neighbors (k-NN) algorithm follows a multi-step training process which includes 
    sampling the input data, performing dimension reduction, and building an index. 
  - The indexed data is then used during inference to efficiently find the k-nearest neighbors for a given data point 
    and make predictions based on the neighboring labels or values.

  Step 1: Sample
    - To specify the total number of data points to be sampled from the training dataset, use the 'sample_size' parameter. 
    - For example, if the initial dataset has 1,000 data points and the 'sample_size' is set to 100, where the total number 
      of instances is 2, each worker would sample 50 points. A total set of 100 data points would be collected. 
    - Sampling runs in linear time with respect to the number of data points.

  Step 2: Perform Dimension Reduction
    - The current implementation of the k-NN algorithm has two methods of dimension reduction. 
    - You specify the method in the 'dimension_reduction_type' hyperparameter. 
    - The 'sign' method specifies a random projection, which uses a linear projection using a matrix of random signs
    - the 'fjlt' method specifies a 'fast Johnson-Lindenstrauss' transform, a method based on the Fourier transform. 
    - Both methods preserve the 'L2' and inner product distances. 
    - The 'fjlt' method should be used when the target dimension is large and has better performance with CPU inference. 
    - Using dimension reduction introduces noise into the data and this noise can reduce prediction accuracy.

  Step 3: Build an Index
    - During inference, the algorithm queries the index for the k-nearest-neighbors of a sample point. 
    - Based on the references to the points, the algorithm makes the classification or regression prediction. 
    - It makes the prediction based on the class labels or values provided. k-NN provides three different types of 
      indexes: a flat index, an inverted index, and an inverted index with product quantization. 
    - You specify the type with the index_type parameter.

   Serialize the Model
     - When the k-NN algorithm finishes training, it serializes three files to prepare for inference.
        - model_algo-1: Contains the serialized index for computing the nearest neighbors.
        - model_algo-1.labels: Contains serialized labels (np.float32 binary format) for computing the predicted label 
                               based on the query result from the index.
        - model_algo-1.json: Contains the JSON-formatted model metadata which stores the k and predictor_type hyper-parameters 
                               from training for inference along with other relevant state.
     - With the current implementation of k-NN, you can modify the metadata file to change the way predictions are computed
     - For example, you can change k to 10 or change predictor_type to regressor.
        {
          "k": 5,
          "predictor_type": "classifier",
          "dimension_reduction": {"type": "sign", "seed": 3, "target_dim": 10, "input_dim": 20},
          "normalize": False,
          "version": "1.0"
        }
-----

  Sagemaker KNN algorithm Attributes
    Learning Type:
      - classification and regression 
    File/Data Types:
      -  CSV, protobuf, JSON (inference only)
    Instance Type:
      - CPU and GPU training and inference processes.
    Hyperparameters
      - required: feature_dim, K, predictor_type, sample_size, dimension_reduction_target
      feature_dim 
        - determines the number of features in the input dataset,
      K 
        - determines the nearest neighbors,
      predictor_type 
        - determines the inference type,
        - Valid values: classifier for classification or regressor for regression.
      sample size 
        - identifies the number of data points to be sampled from the training data,
      dimension_reduction_target
        - the target dimension to reduce to.
    Metrics:
      - MSE (regression metrics)
      - accuracy (classification metrics)


  KNN business use cases
    determine credit ratings,
      - the algorithm can group people together for credit risk based on the attributes they share.
    recommendation system
      - algorithm can recommend a product or a service based on a customer's current likes and preferences.
    fraud detection
      - the algorithm can identify suspicious transactions by comparing them to previous fraud cases.

  Note: Missing: In the resources section, you will see a page containing a sample notebook demonstrating 
        SageMaker's KNN algorithm.


  K-Nearest Neighbors (k-NN) Algorithm
  https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html

    Multi-Class Classification using Amazon SageMaker k-Nearest-Neighbors (kNN)
      https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/k_nearest_neighbors_covtype/k_nearest_neighbors_covtype.html

      Saved to:  notebooks/knn_notebooks/k_nearest_neighbors_covtype.ipynb


------------------------------------------------------
3.5 Examining SageMaker's Factorization Machines Algorithm 

  SageMaker's Factorization Machines (FM) Algorithm.
    - It is a supervised machine learning algorithm that is an extension of a linear model designed to capture 
      the higher order relationships between features in a dataset.

  higher-order vs Non-Linear Relationship
    higher-order
      - refers to interactions between three or more features in a dataset,
    non-linear 
      - refers to a relationship between features that cannot be expressed by a linear function.
      - Since it involves two or more features and non-linear, the relationships is not necessarily higher order.

  factorization machine algorithm Equation
    simple linear regression equation:
       Y =  b + ∑ (m_i * x_i)     where ∑ (SUM) is from i=1 to i=n

            Y -> dependent feature or the feature we want to predict.
            b is a global bias.
            m_i represents the weight of the ith independent feature
            x_i -> one-hot encoded features

    pre-processing step
      - must convert all the categorical features into one-hot encoded features.

    equation to capture the higher order interaction.
       Y =  b + ∑_i (m_i * x_i) + ∑_i ∑_j <v_i dot v_j> x_i dot x_j     
                where ∑ (SUM) is from i=1 to i=n

            v_i, v_j ->  latent vector representations of a feature.

       - The higher order is captured by the sum of sum of multiplying each one hot encoded columns 
          alongside the dot product between the latent vector representation.

   Recommendation System Course 
     - help understand factorization Machine Algorithm:
        Content-Based Recommendation Systems on Google Cloud
        - offered by Google Cloud
        - focus on 3rd module

  latent vector example
    - Considering a movie recommendation system where users rate movies on a scale of 1 to 5,
    - each movie is associated with certain features like genre, actors and directors.
    - Representing all these hidden features in a numerical fashion forms a latent vector.

  FM limitations with these algorithm
   considers only pairwise features.
     - In other words, it's only going to analyze the relationship between two pairs of features at a time, 
       and that could be limiting depending on what you are trying to do with your problem.
   Does NOT support CSV support
     - does not support CSV, 
   Does not support multi-class classification.
     - It only works for either binary classification or regression problems.
   Requires lots of data
     - to make up for the sparseness of the data, in other words, the missing features, it really needs 
       a lot of data, and AWS recommends anywhere between 10,000 to 10 million rows in the dataset 
   recommends CPUs only
     - use CPUs to give us the most efficient experience.
   does not perform well on dense data.


  Sagemaker Factorization Machine algorithm Attributes
    Learning Type:
      - Binary classification and regression 
    File/Data Types:
      -  protobuf (with flow 32 tensors) and JSON (inference only)
    Instance Type:
      - CPU training and inference processes.
    Hyperparameters
      - required: feature_dim, num_factors, & predictor_type
      feature_dim,
        - determines the dimension of the input feature space.
      num_factors 
        - determine the dimensionality of factorization  
      predictor type,
        - determines the type of predictor.
    Metrics:
      - RMSE (regression metrics)
      - accuracy and cross-entropy (classification metrics)

  Factorization Machines Hyperparameters
    https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html

    feature_dim 	
      - The dimension of the input feature space. This could be very high with sparse input.
      - Required
      - Valid values: Positive integer. Suggested value range: [10000,10000000]

    num_factors 	
     - The dimensionality of factorization.
     - Required
     - Valid values: Positive integer. Suggested value range: [2,1000], 64 typically generates good 
       outcomes and is a good starting point.
     -  [dimension of matrix used algorithm calculation] 

    predictor_type 	
      - The type of predictor.
      - binary_classifier: For binary classification tasks.
        regressor: For regression tasks.
       - Required
       - Valid values: String: binary_classifier or regressor

  Factorization machines Business Use Cases:
    recommendation systems and  ad-click prediction
       - where the data set is high dimensional and the data is sparse.

   Note: Missing: 
      In the resources section, you will see a page containing sample notebook demonstrating SageMaker's 
      factorization machine algorithm.

      Factorization Machines Algorithm
        https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html

        An Introduction to Factorization Machines with MNIST
        https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.html
           Saved to: notebooks/factorization_machines_notebooks/factorization_machines_mnist.ipynb

------------------------------------------------------
3.6 Discovering SageMaker's DeepAR Forecasting Algorithm 


  SageMakers's DeepAR algorithm
    - It is a supervised learning algorithm designed for forecasting one-dimensional time-series data 
      using a recurrent neural network.
    one-dimensional time-series data 
      - sequential data where each observation corresponds to a single variable measured at regular time intervals.
      - For example, daily temperature measurements and hourly stock prices.

  DeepAR algorithm - Analogy
    - a shoe company getting ready to introduce a new shoe design, and wants to predict how many they 
      expect to sell
    cold start problem.
      - means that we don't have enough history to pour into the forecasting model and deliver how many we 
        expect to sell.
    - want to create a model that allows us to combine those patterns of multiple shoes to create an aggregate 
      forecast that's probably a more realistic model for our brand new shoe.
    - can use DeepAR to look at multiple datasets of historic data for multiple shoes.
    
  DeepAR types of forecasts:
    point-in-time forecast
      - provides a single predicted value for each time step in the forecast period.
      - an example of that is just the number of sneakers that we might sell in a week is X.
    probabilistic forecast,
      - provides a range of possible future values for each time step along with associated probabilities.
      - an example of this might be the number of sneakers sold in a week is between X and Y with Z probability.

  DeepAR input file format:
    - Each observation in the input file should contain the following fields:
      Start
         - required field; a string representation of a timestamp
         - A string with the format YYYY-MM-DD HH:MM:SS
         - it cannot contain time zone information.
      target
          - required field; an array of floating-point values or integers that represents the time-series data.
      dynamic_feat field
        - optional;  an array of arrays of floating-point values or integers that represents the vector 
          of custom feature time-series data.
        - For example, if the time-series data represents the stock prices, an associated dynamic_feat might be a 
          Boolean condition that indicates if it's a favorable economic condition or not.
      cat
        - optional; an array of categorical features that can be used to encode the groups that the record belongs to.
        - if you use this field, all time-series must have the same number of categorical features.

  deepAR file format info:
    Use the SageMaker AI DeepAR forecasting algorithm
      https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html

  Sagemaker DeepAR algorithm Attributes
    Learning Type:
      - Forecasting (time-series)
    File/Data Types:
      - training and testing dataset can be provided in JSON Lines format.
      - input files can be in gzip and parquet format.
      - JSON for inferencing
    Instance Type:
      - train on CPU and GPU in both single and multi-machine settings
      - recommended starting with CPU [ex: ml.c4.2xlarge, ml.c4.4xlarge] and switch to GPU on when necessary
    Hyperparameters
      - required: context_length, epochs, prediction_length, & time_freq
      context_length
        - specifies the number of time points that the model gets to see before making the prediction.
      epochs 
        - a maximum number of passes over the training data.
      prediction_length
        - the number of time steps that the model is trained to predict.
      time_freq
        - the granularity of the time-series in the dataset.
        - It can be monthly, weekly, daily, or hourly.
    Metrics:
      https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html
      test:RMSE 	
        - The root mean square error between the forecast and the actual target computed on the test set.
         - Minimize
      test:mean_wQuantileLoss 	
        - The average overall quantile losses computed on the test set. 
        - To control which quantiles are used, set the test_quantiles hyperparameter.
        - Minimize
      train:final_loss 	
        - The training negative log-likelihood loss averaged over the last training epoch for the model.
        - Minimize


  DeepAR Business Use Cases:
    forecasting.
      demand forecasting
          - to forecast demand for products and services.
      Sales forecasting 
         - to forecast sales volumes of a product for a time period,
      financial forecasting 
        - to forecast market trends and exchange rates.
    Risk assessment and mitigation
      - forecasting risks in credit defaults or fraud incidents,


  Note: Missing: In the resources section, you will see a page containing a sample notebook demonstrating 
        SageMaker's DeepAR algorithm.


  Use the SageMaker AI DeepAR forecasting algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html

      DeepAR Sample Notebooks
        https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html
        Save as: 
          notebooks/deepar_notebooks/DeepAR-Electricity.ipynb

------------------------------------------------------
3.7 Reviewing SageMaker's Principal Component Analysis (PCA) Algorithm 


  Principal Component Analysis (PCA) algorithm overview
    - It is an unsupervised learning algorithm that reduces the number of features in a dataset without 
      losing meaningful information.
    - This algorithm helps overcome the curse of dimensionality problem.

  Principal Component Analysis (PCA) algorithm
    - combines uncorrelated, original features into components.
      - In other words, they are new features that are linear combinations of the original features.
    - The first component captures the maximum variance in the data, 
    - The second component captures the remaining maximum variance orthogonal to the first component.
    - Eigenvalues and Eigenvectors are used to determine the magnitude and the direction of these components.

  PCA analogy
    - trying to photograph a 3D object that represents high dimensional data because the object has depth, 
      width, height, color, and so on.
    - We take multiple pictures of this object at the best possible angles to capture all the important 
      features in a two dimensional representation.
    - These best angles are analogous to the principal components trying to capture the most important information.


  PCA Modes of Operation
     Regular mode 
       - specific for data sets with sparse data and a moderate number of features.
     Randomized.  
       - For datasets with both a large number of observations and features. 
       - This mode uses an approximation algorithm. 


  Sagemaker PCA algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html
    Learning Type:
      - Dimensionality Reduction (unsupervised)
    File/Data Types:
      - training dataset: CSV and protobuf in File mode or Pipe mode  
      - or inference, PCA supports text/csv, application/json, and application/x-recordio-protobuf
    Instance Type:
      - PCA supports CPU and GPU instances for training and inference (depends on training data)
    Hyperparameters
      - required: feature_dim, mini_batch_size, num_components
      feature_dim
        - that specifies the input dimension.
      mini_batch_size 
        - the number of rows in a mini batch
      num_components
        - the number of principal components to compute.

  PCA hyperparameters:
    https://docs.aws.amazon.com/sagemaker/latest/dg/PCA-reference.html
    feature_dim 	
      - Input dimension.
       - Required 
       - Valid values: positive integer
    mini_batch_size 	
      - Number of rows in a mini-batch.
      - Required
      - Valid values: positive integer
    num_components 	
      - The number of principal components to compute.
      - Required
      - Valid values: positive integer

  PCA Business Use Cases:
    image compression,
       - reducing the pixel data by preserving the important features.
    financial analysis.
       - By reducing the number of variables, PCA helps in identifying the factors driving market movements.
    Customer feedback analysis.
       - By reducing the dimensionality of the data, PCA can help identify the themes and sentiments provided 
         in the user feedback.

  Note: Missing:  In the resources section, you will see a page containing sample notebook demonstrating 
        SageMaker's PCA algorithm.


  Principal Component Analysis (PCA) Algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html

       PCA Sample Notebooks
        https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/pca_mnist/pca_mnist.html
          -> save to notebooks/pca_notebooks/pca_minst.ipynb

------------------------------------------------------
3.8 Exploring SageMaker's Random Cut Forest (RCF) Algorithm 


  SageMaker's Random Cut Forest (RCF) algorithm.
    - It is an unsupervised algorithm primarily used to detect anomalies in a dataset.

  anomaly, or an outlier 
    - is an extreme value that deviates significantly from most data points.

  Random Cut Forest (RCF)
    - The algorithm computes the distance from the mean for every data point and assigns a score 
      to each of them
      - A low score that is a score less than three standard deviations from the mean score indicates 
        a normal data point.
      - a high score indicates an anomaly.
    - Reservoir sampling is a common algorithm often used to effectively draw random samples from a dataset.



  Sagemaker RCF algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html
    Learning Type:
      - Anomaly detection
    File/Data Types:
      - training dataset: CSV and protobuf in File mode or Pipe mode  
      - Train and test data content types can be either application/x-recordio-protobuf or text/csv formats. 
      - For the test data, when using text/csv format, the content must be specified as text/csv;label_size=1 
        where the first column of each row represents the anomaly label: "1" for an anomalous data point and "0" 
        for a normal data point. 
      - You can use either File mode or Pipe mod
      - For inference, RCF supports application/x-recordio-protobuf, text/csv and application/json input dat
    Instance Type:
      - recommends only CPU instances (does not take advantage of GPU hardware)
    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_hyperparameters.html
      - required: feature_dim
      feature_dim
        - The number of features in the data set. 
        - (If you use the Random Cut Forest estimator, this value is calculated for you and need not be specified.)
        - Required
        - Valid values: Positive integer (min: 1, max: 10000)
    Metrics
      -  The optional test channel is used to compute accuracy, precision, recall, and F1-score metrics on labeled data.


 Random Cut Forest (RCF) business use cases
   anomaly detection.
     fraud detection 
       - by detecting unusual patents in financial transaction data,
     Security breaches
       - detecting security breaches by analyzing network traffic that could signify security breaches 
         or cyber attacks.
   e-commerce
     - to detect unusual shopping patterns that might indicate bot activities or fraudulent purchases.
   

  Note: Missing: In the Resources section, you will see a page containing a sample notebook demonstrating 
    SageMaker's Random Cut Forest algorithm.


  Random Cut Forest (RCF) Algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html
      RCF Sample Notebooks
        https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.html
        Saved to: notebooks/rcf_notebooks/random_cut_forest.ipynb

------------------------------------------------------
3.9 Understanding SageMaker's IP Insights Algorithm 


  SageMaker's IP Insights algorithm 
    - It's an unsupervised learning algorithm that learns the usage patterns for IPv4 addresses, capturing 
      associations between IP addresses and entities like user IDs and account numbers.

  IP Insights example
    - Imagine you shop online on your favorite e-commerce platform from your home regularly.
    - Typically, you're shopping is on the weekend evenings.
    - You decided to go on a vacation to an international country.  On your vacation, you decided to do 
      some shopping, but when you logged in, you are alerted with additional security questions and an 
      OTP to your mobile phone to validate your authenticity.
      OTP: One time password or one-time pin
    - This is IP Insights algorithm in action,

  IP Insights algorithm 
    - stores a usual IP address and the entity details like user ID or account number information as key value pairs.
    - Queries historical data for any login attempts and returns a score
    - A high score indicates an anomalous behavior
    - Uses a neural network

  Sagemaker IP Insight algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html
    Learning Type:
      - Anomaly detection (unsupervised)
    File/Data Types:
      - Training and validation data content types need to be in text/csv format. 
      - The first column of the CSV data is an opaque string that provides a unique identifier for the entity. 
      - The second column is an IPv4 address in decimal-dot notation. 
        example CSV format:
           entity_id_1, 192.168.1.2
           entity_id_2, 10.10.1.2
      - supports only File mode. 
      - For inference, IP Insights supports text/csv, application/json, and application/jsonlines data
    Instance Type:
      - run on both GPU and CPU 
      - For training jobs, we recommend using GPU instances.
      - For inference, we recommend using CPU instances
    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights-hyperparameters.html
      required hyperparameters
        num_entity_vectors
          - The number of entity vector representations (entity embedding vectors) to train. 
        vector_dim 
          - represents the size of the key value pairs representing the entities and IP addresses.
          - The size of embedding vectors to represent entities and IP addresses. 
          - The larger the value, the more information that can be encoded using these representations.
    Metrics
      -  It uses the optional validation channel to compute an area-under-curve (AUC) score on a predefined 
         negative sampling strategy. 
      - The AUC metric validates how well the model discriminates between positive and negative samples.

   JSONLines:
     - Each line of a JSONL file is a self-contained JSON object, like a mini database record. 
num_entity_vectors

  IP Insights Business use cases
    Anomaly detection
      - fraud detectioin
         - fraudulent transactions and account takeovers.
      - compliance detection
          - compliance with regional regulations by detecting access from IP addresses located in 
            restricted or high-risk regions.
    Geolocation based personalization
      - helps provide localized content and services based on the location of the IP addresses accessing the platform.

-----
An Introduction to the Amazon SageMaker IP Insights Algorithm
  https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/ipinsights_login/ipinsights-tutorial.html

[IPInsight] Model Hyperparameters
    num_entity_vectors: 
      - the total number of embeddings to train. 
      - use an internal hashing mechanism to map the entity ID strings to an embedding index; therefore, using 
        an embedding size larger than the total number of possible values helps reduce the number of hash collisions. 
      - recommend this value to be 2x the total number of unique entites (i.e. user names) in your dataset;

    vector_dim: 
      - the size of the entity and IP embedding vectors. 
      - The larger the value, the more information can be encoded using these representations but using too 
        large vector representations may cause the model to overfit, especially for small training data sets;

-----


  Note: Missing: 
     In the resources section, you will see a page containing a sample notebook demonstrating SageMaker's 
       IP Insights algorithm.

  IP Insights
    https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html

      IP Insights Sample Notebooks

       An Introduction to the Amazon SageMaker IP Insights Algorithm
       https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/ipinsights_login/ipinsights-tutorial.html
          saved to: notebooks/ip_insights_notebooks/ipinsights-tutorial.ipynb

------------------------------------------------------
3.10 Examining SageMaker's K Means Algorithm 


  SageMaker K-means algorithm Overview
    - It's an unsupervised learning algorithm that groups similar data.
    - The similarity is determined based on the attributes we specify.
    - 'k' number must be predetermined for the algorithm.


   Amazon SageMaker 
     - it uses a modified version of K-means algorithm and it is more accurate.
     - Expects tabular data 
        - each row represents the observation, and 
        - each column represents the attribute or the features of the observations.
     - Euclidean distance is used to measure the similarity among the observations.



  Sagemaker K-Means algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html
    Learning Type:
      - clustering (unsupervised)
    File/Data Types:
      -  Both recordIO-wrapped-protobuf and CSV formats are supported for training. 
      - You can use either File mode or Pipe mode to train models on data that is formatted as 
        recordIO-wrapped-protobuf or as CSV.
      - For inference, text/csv, application/json, and application/x-recordio-protobuf are supported. 
        - k-means returns a closest_cluster label and the distance_to_cluster for each observation.
    Instance Type:
      - For training jobs, we recommend using CPU instances.
      - You can train on GPU instances, but should limit GPU training to single-GPU instances (such as 
        ml.g4dn.xlarge) because only one GPU is used per instance

      - For inference, we recommend using CPU instances
    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html
      required hyperparameters
        feature_dim 	
          - The number of features in the input data.  
          - Valid values: Positive integer
        k 	
          - The number of required clusters.
          - Valid values: Positive integer
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-tuning.html
      - The k-means algorithm computes the following metrics during training. 
      - When tuning a model, choose one of these metrics as the objective metric. 
      test:msd 	
         - Mean squared distances (MSD) between each record in the test set and the closest center of the model.
         - Minimize
      test:ssd 	
        - Sum of the squared distances (SSD) between each record in the test set and the closest center of the model.
        - Minimize


  K-Means Business use cases
    e-commerce and retail
      - to group customers based on their purchasing behavior and demographics.
    market segmentation
      - to group markets based on customer characteristics like age, income, interests, and buying habits.
    recommendation systems
      - to recommend products and services based on customer preferences.

  Note: Missing 
     in the resources section, you will see a page containing a sample notebook demonstrating SageMaker's 
       K-means algorithm.

  K-Means Algorithm
    https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html#km-instances

    K-Means Sample Notebooks
      - For a sample notebook that uses the SageMaker AI K-means algorithm to segment the population of counties 
        in the United States by attributes identified using principle component analysis
        https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans/sagemaker-countycensusclustering.html

         Saved to: notebooks/kmeans_notebooks/sagemaker-countycensusclustering.ipynb
  
------------------------------------------------------
3.11 Discovering SageMaker's Object2Vec Algorithm `


  Object2Vec algorithm Overview
    - It is a highly customizable neural embedding algorithm that can be used to create vector representations of objects.
    - The objects can be anything such as words, sentences, or abstract entities like users or products.

  Object2Vec algorithm analogy
    - Imagine you are a librarian and your task is to organize a large number of books so that the readers 
      can easily find them.
    - Each book has many attributes like genre, author, publication year, and so on.
    - Similarly, the readers have favorite genre, favorite authors, and preferences based on past reading habits.

    - The books are like the objects that this algorithm deals with.  Initially, the books may be placed in 
      random order, but eventually you decide to create groups of books that are similar in any of the choose and attribute.
    - You mentally map out where each book should go, so similar books are placed together on the shelves.
    - This mental map is similar to creating an embedding space to organize the objects.
    - As readers start borrowing books, you start observing a pattern that some readers often borrow books from 
      specific clusters, and based on this observation, you update the mental map, which makes it even easier for them.
    - This is like iterative adjustment of embeddings to better reflect the relationship between the objects.

  SageMaker Object2Vec algorithm analogy
    - During the training process, the algorithm accepts pairs of objects and the relationship labels as inputs.
      - For example, in a typical recommendation system, these pairs could be user and item.
      - These pairs are associated with labels indicating the nature of the relationships, whether the user 
        liked or disliked the item.
    - Each object is initially presented as a random vector.
    - The goal is to adjust these vectors such that the objects with similar relationships are closer together.
    - Object2Vec uses a neural network to understand and learn embeddings.
    - For each object pair the neural network process their embeddings and predicts their relationships.
    - The predictor relationship is compared to the actual relationship using the loss function.
    - The error is then propagated back through the network, and the embeddings are updated to minimize the loss.
    - This process iteratively refines the embeddings to better capture the relationships between the objects.


  Sagemaker Object2Vec algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html
    Learning Type:
      - general-purpose neural embedding algorithm
    File/Data Types:
      - This algorithm expects the data to be provided in a sentence -sentence pair, label-sentence pair. 
        and other pairs 
          Sentence-sentence pairs
	    "A soccer game with multiple males playing." and "Some men are playing a sport."
          Labels- sequence pairs
	    The genre tags of the movie "Titanic", such as "Romance" and "Drama", and its short description: 
            "James Cameron's Titanic is an epic, action-packed romance set against the ... of April 15, 1912."
          Customer-customer pairs
            The customer ID of Jane and customer ID of Jackie.
          Product-product pairs
            The product ID of football and product ID of basketball.

      - the data must be pre-processed and transformed into supported formats.
      - For training, the data must be in jsonlines format,
      - for inference, the data format must be in JSON or jsonlines format.
    Instance Type:
      - Amazon recommends CPU and GPU instances for training purposes.

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html
      required hyperparameters
        enc0_max_seq_len
          - The maximum sequence length for the enc0 encoder.
          - Valid values: 1 ≤ integer ≤ 5000
        enc0_vocab_size 	
          - The vocabulary size of enc0 tokens.
          - Valid values: 2 ≤ integer ≤ 3000000
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-tuning.html
      - This algorithm reports means square error (mse) for any regression tasks.
      - For classification tasks, it reports accuracy and cross entropy.

  Object2Vec Business use cases
     user behavior analysis
       - for creating detailed user profiles based on their interactions and behavior, which can be used 
         for personalized marketing.
     natural language processing (NLP)
        - to detect spam and perform sentiment analysis.
     social network analysis
        - to identify groups of users with similar interests or behavior, which can be used for targeted advertising.


    Note: Missing: 
      in the resources section, you will see a page containing a sample notebook demonstrating SageMakers 
      Object2Vec algorithm.


  An Introduction to SageMaker ObjectToVec model for sequence-sequence embedding
    https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/object2vec_sentence_similarity/object2vec_sentence_similarity.html
     Saved to: notebooks/object2vec_notebooks/object2vec_sentence_similarity.ipynb.txt

------------------------------------------------------
3.12 Reviewing SageMaker's Latent Dirichlet Allocation (LDA) Algorithm 


  Latent Dirichlet allocation (LDA) Algorithm Overview
    - an unsupervised learning algorithm that attempts to describe a set of observations as a mixture 
      of different categories.
    - Each observation is considered a document.
    - The features are the presence of each word and the categories are the topics.
    - a generative probabilistic model used for discovering the underlying topics in a collection of documents.
    - extensively used in the field of natural language processing for topic modeling purposes.

  LDA Algorithm - an Analogy
             represent
      books     ->    Documents in dataset
      Genre     ->    Hidden Topic
      words     ->    words in the Documents


  Sagemaker LDA algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html
    Learning Type:
      - Unsupervised learning commonly used for topic modeling (to discover a user-specified number of 
        topics shared by documents within a text corpus.
    File/Data Types:
     - For training: CSV (dense only; file mode) and protobuf (sparse and dense; file and pip mode) format
     - For inference: CSV, protobuf, and JSON format
     - JSON is also supported during the inference stage.
    Instance Type:
      - supports single-instance CPU training. 
      - CPU instances are recommended for hosting/inference.
    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html
      required hyperparameters
       num_topics 	
         - The number of topics for LDA to find within the data.
         - Valid values: positive integer
       feature_dim 	
         - The size of the vocabulary of the input document corpus.
         - Valid values: positive integer
       mini_batch_size 	
         - The total number of documents in the input document corpus.
         - Valid values: positive integer
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/lda-tuning.html
      test:pwll 	
        - Per-word log-likelihood on the test dataset. 
        - The likelihood that the test dataset is accurately described by the learned LDA model.
        - goal: Maximize

  LDA Business use cases
    customer feedback analysis
      - to identify the common themes and topics mentioned in the customer reviews.
    social media trend analysis
      - to identify trending topics on social media and understand public sentiment and emerging issues.
    content creation 
      - to generate ideas from blog posts, articles, and marketing content based on the topics that are 
        of interest to the target audience.

  Note: Missing: 
    in the resources section, you will see a page containing a sample notebook demonstrating SageMaker's LDA Algorithm.


    An Introduction to SageMaker LDA
      https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/lda_topic_modeling/LDA-Introduction.html

        Saved to:  notebooks/lda_notebooks/LDA-Introduction.ipynb


------------------------------------------------------
3.13 Exploring SageMaker's Neural Topic Model (NTM) Algorithm 


  Neural Topic Model (NTM) algorithm Overview
    - an unsupervised learning algorithm that is used to organize a corpus of documents into topics that 
      contain word groupings based on their statistical distribution.

    - Similar to LDA, NTM is extensively used in the field of topic modeling.
    - Behind the scenes, NTM uses neural networks to capture complex patterns and provide a more nuanced 
      understanding of the topics.
  LDA vs NTM Algorithms
    LDA
      modeling:
        - probabilistic graphical model using Dirichlet distributions
      scalability:
        - single instance CPU - as a result, it can be computationally expensive for large data sets 
      interpretability:
        - highly interpretable due to its explicit probabilistic model
    NTM
      modeling:
        - uses neural network model.
      scalability:
        - can leverage both CPU and GPU instances and is very effective in handling large data sets.
      interpretability:
        - less interpretable because of the black box nature of the neural networks.



  Sagemaker NTM algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html
    Learning Type:
      - unsupervised learning algorithm that specializes in text processing.
      - unsupervised learning algorithm that is used to organize a corpus of documents into topics that 
        contain word groupings based on their statistical distribution
    File/Data Types:
     - For training, test, validate: CSV (dense only; file and pip mode) and protobuf (sparse and dense; 
       file and pip mode) format
     - For inference: CSV, protobuf, JSON and JSONLines format
    Instance Type:
      - training supports both GPU and CPU instance types. 
      - recommend GPU instances, but for certain workloads, CPU instances may result in lower training costs. 
      - CPU instances should be sufficient for inference
    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html
      required hyperparameters
       num_topics 	
         - The number of required topics
         - Valid values: positive integer (min: 1, max: 1,000,000)
       feature_dim 	
         - The vocabulary size of the dataset
         - Valid values: positive integer (min: 1, max: 1,000,000)
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/ntm-tuning.html
      validation:total_loss 	
        - Total Loss on validation set
        - goal: Minimize

  NTM Business use cases
    customer feedback analysis
      - to uncover customer pain points and suggest product improvements.
    personalized recommendations
      - to recommend articles, blog posts, and other contents based on the topic that a user has shown interest
    market sentiment analysis
      - to extract topics from financial news and social media 
      - to gauge market sentiment and inform trading strategies.

   Note: Missing: 
      Finally, in the resources section, you will see a page containing a sample notebook demonstrating 
      SageMaker's NTM algorithm.

      Introduction to Basic Functionality of NTM
        https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/ntm_synthetic/ntm_synthetic.html

          Saved to: notebooks/ntm_notebooks/ntm_synthetic.ipynb.txt
            
------------------------------------------------------
3.14 Understanding SageMaker's BlazingText Algorithm 


   SageMaker's BlazingText algorithm overview
     - It is a highly optimized implementation of the word2vec and text classification algorithms.

   SageMaker's BlazingText algorithm
     - algorithm based on Facebook's FastText,
         - BlazingText is greatly scaled beyond FastText's performance.
           - where FastText may have taken days to train models, BlazingText can do that in minutes 
           - inferences performed in real time, whereas FastText inference could only perform in batch mode.

     - BlazingText expects a single pre-processed text file.
        - Each line of the file should contain a single sentence,
        - if you need to train on multiple text files, just concatenate them all together, and upload as 1 file

      - AWS improved the scalability of the original word2vec algorithm, as well as the original, Facebook's 
        FastText text classifier
           - BlazingText is 20 times faster than Facebook's FastText algorithm

   SageMaker's BlazingText algorithm mode

       Modes 	                Word2Vec                Text Classification
                                (Unsupervised           (Supervised  
                                Learning)               Learning)
     -------------------        ----------------        ------------------

     Single CPU instance        cbow, Skip-gram          supervised
                                Batch Skip-gram

      Single GPU instance       cbow                     supervised with one GPU
      (with 1 or more GPUs)     Skip-gram

      Multiple CPU instances    Batch Skip-gram 	 None


-----
 skip-gram
   -"Skip-Gram" predicts the surrounding context words based on a single target word
   - essentially, Skip-Gram focuses on learning word representations by predicting context from a single word
 CBOW (Continuous Bag-of-Words) 
   - CBOW predicts a target word based on its surrounding context words; 
   - CBOW learns by predicting a word from its surrounding context
 skip-gram vs CBOW
   - Skip-Gram generally better for rare words and CBOW faster for large datasets with frequent words
-----
    
   SageMaker's BlazingText algorithm mode
     Word2Vec algorithm (mode = batch_skipgram, skipgram, or cbow) 
       - The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as 
         sentiment analysis, named entity recognition, machine translation, etc
       - maps words to high-quality distributed vectors. 
       - The resulting vector representation of a word is called a word embedding. 
       - Words that are semantically similar correspond to vectors that are close together. That way, word 
         embeddings capture the semantic relationships between word
       - provides the Skip-gram and continuous bag-of-words (CBOW) training architecture
       - BlazingText can generate meaningful vectors for out-of-vocabulary (OOV) words by representing their 
         vectors as the sum of the character n-gram (subword) vectors.
     TextClassification algorithm (mode=supervised)
       - Text classification is an important task for applications that perform web searches, information retrieval, 
         ranking, and document classification.
       - Ability to perform high speed multi-class and and multi-label text classification. 
       - The goal of text classification is to automatically classify the text documents into one or more defined 
         categories, like spam detection, sentiment analysis, or user reviews categorization.


  Sagemaker BlazingText algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html
    Learning Type:
      - unsupervised word2vec algorithm
      - supervised text classification algorithm
    File/Data Types:
      - expects a single pre-processed text file with space-separated tokens during the training, test, and validation stages.  
      - JSON and jsonlines are supported during the inference stage.
    Instance Type:
      - single CPU and GPU instances for cbow and skip-gram modes, for Batch skip-gram,
      - support single or multiple CPU instances  (see previous table)

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html
      required hyperparameters
        mode
          - for word2vec architecture, valid values are: batch_skipgram, skipgram, or cbow
          - for text classification architecture, valid value: supervised
    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html
      Word2Vec algorithm reports
        train:mean_rho 	
          - The mean rho (Spearman's rank correlation coefficient) on WS-353 word similarity datasets
          - goal: Maximize
      text classification algorithm reports
        validation:accuracy 	
          - The classification accuracy on the user-specified validation dataset
          - goal: Maximize

  BlazingText Business use cases
    Word2Vec:
       - vectorize text (converted into real-valued vectors) for downstream natural language processing (NLP) tasks 
         like sentiment analysis, named entity recognition, and machine translation 
    Text Classification
       sentiment analysis
         - to evaluate customer reviews on a social media and decide if it is a positive or a negative sentiment.
       classify documents.
          - to crawl a series of documents in our enterprise and call out certain documents that may contain 
            some sensitive information and be able to set those aside or give those documents extra protection.
       recommendation systems,
          - in recommending products, articles, or content based on user preferences.

  Note: Missing: 
     In the Resources section, you will see a page containing a sample notebook demonstrating SageMaker's 
     BlazingText algorithm.

     
    BlazingText Sample Notebooks
      For a sample notebook that trains and deploys the SageMaker AI BlazingText algorithm to generate word vectors, 
      see Learning Word2Vec Word Representations using BlazingTex

        Learning Word2Vec Word Representations using BlazingText
           https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/blazingtext_word2vec_text8/blazingtext_word2vec_text8.html
           Saved to: notebooks/blazingtext_notebooks/blazingtext_word2vec_text8.ipynb


------------------------------------------------------
3.15 Examining SageMaker's Sequence-to-Sequence (Seq2Seq) Algorithm 


  SageMaker sequence-to-sequence (Seq2Seq) algorithm Overview:
    - a supervised learning algorithm that uses a neural network architecture where a sequence of input 
      tokens is transformed to another sequence of tokens as output.
    - Example applications include: machine translation (input a sentence from one language and predict what that 
      sentence would be in another language), text summarization (input a longer string of words and predict a shorter 
      string of words that is a summary), speech-to-text (audio clips converted into output sentences in tokens).

  Sequence-to-sequence algorithm's layers
     embedding layer
       - the encoded input tokens are mapped to a dense feature layer
       - It is a standard practice to initialize this embedding layer with pre-trained word vector, 
         like FastText, and learn the parameters during the training process.
     encoder layer
       - compresses the input into a fixed-length feature vector.
       - Typically, an encoder is made of RNN network, like LSTM or GRU.
     decoder layer 
       - converts the encoder feature to an output sequence of tokens.
       - This layer also is typically built with RNN architecture.


  Sagemaker Sequence-to-Sequence (Seq2Seq) algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html
    Learning Type:
      - supervised learning algorithm that specializes in language processing.
    File/Data Types:
      - training, test, and validation states expects data in RecordIO-Protobuf format
      - protobuf and JSON supported during the inference stage.
    Instance Type:
      - single GPU instances 

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html
      No required hyperparameters

    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-tuning.html
      validation:accuracy 	
         - Accuracy computed on the validation dataset.
         - goal: Maximize
      validation:bleu 	
        - BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has 
          been machine-translated from one natural language to another. 
        - BLEU score computed on the validation dataset. 
        - Because BLEU computation is expensive, you can choose to compute BLEU on a random subsample of the validation 
          dataset to speed up the overall training process. 
        - Use the bleu_sample_size parameter to specify the subsample.
        - goal: Maximize
      validation:perplexity 	
        - perplexity is a loss function computed on the validation dataset. 
        - Perplexity measures the cross-entropy between an empirical sample and the distribution predicted by a model 
          and so provides a measure of how well a model predicts the sample values, 
        - Models that are good at predicting a sample have a low perplexity.
        - goal: Minimize

  Sequence-to-Sequence (Seq2Seq) Business use cases
    language translations
      - to translate it in sequence of words from one language to another language.
    speech-to-text conversion.
      - given an audio vocabulary, you can then predict the textual representation of those spoken words.
    code generation and auto completion 
      - assist developers by generating code snippets or completing the code based on content.

   Note: Missing: 
     Finally, in the Resources section, you will see a page containing a sample notebook demonstrating SageMaker's 
     sequence-to-sequence algorithm.


     Sequence-to-Sequence Sample Notebooks
       - For a sample notebook that shows how to use the SageMaker AI Sequence to Sequence algorithm to train 
         a English-German translation model, see Machine Translation English-German Example Using SageMaker AI Seq2Seq


         Machine Translation English-German Example Using SageMaker Seq2Seq
         https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/seq2seq_translation_en-de/SageMaker-Seq2Seq-Translation-English-German.html
            Save to: notebooks/seq2seq_notebooks/SageMaker-Seq2Seq-Translation-English-German.ipynb.txt

------------------------------------------------------
3.16 Discovering SageMaker's Image Classification Algorithm 

  SageMaker's image classification algorithm Overview:
    - It is a supervised learning algorithm that accepts labeled images as input and classifies them into one of 
      the output categories.
    - uses a CNN to train from scratch or leverage transfer learning when the training data is limited.

  Image classification modes
    full training mode,
      - the image training is performed from the scratch by initializing the network with random weights and 
        training requires a large data set.
    transfer learning mode,
      - can leverage previously trained images and the network is initialized with pre-trained weights.
      - the training can be achieved with a smaller dataset.

   CNN Architecture
     - typically will contain many layers.

                                                                                         Fully-
      input   ------>  Convolutional  ------> activation  ------> Pooling Layer ------> Flattening  ----> Connected
      layer                layers               fcn                 Layer(s)              Layer           Layer


    convolution layer 
      - to extract features using learn filters.
    activation function
      - applies a nonlinear activation function like ReLU
    pooling layer 
      - reduces spatial dimensions, which prevents overfitting.
    flattened layer 
      - converts the 2D matrix to a 1D vector.
    fully connected layer
      - performs a classification task.


  Sagemaker Image Classification algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html
    Learning Type:
      - supervised learning algorithm that specializes in classifying images
    File/Data Types:
      - recordIO format or image formats (JPG or PNG)
      - requires a single tab separated .LST file that contains a list of image files.
    Instance Type:
      - recommends using GPU instances with more memory for training the images  
      - CPU or a GPU can be used in the inference stage.

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html
      required hyperparameters
        num_classes 	
          - Number of output classes. 
          - This parameter defines the dimensions of the network output and is typically set to the number of classes in the dataset.
          - Besides multi-class classification, multi-label classification is supported too. 
          - Please refer to Input/Output Interface for the Image Classification Algorithm for details on how to work with 
            multi-label classification with augmented manifest files.
          - Valid values: positive integer
        num_training_samples 	
          - Number of training examples in the input dataset.
          - If there is a mismatch between this value and the number of samples in the training set, then the behavior 
            of the lr_scheduler_step parameter is undefined and distributed training accuracy might be affected.
          - Valid values: positive integer

    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/IC-tuning.html 
      validation:accuracy 	
         - The ratio of the number of correct predictions to the total number of predictions made.
         - goal: Maximize

  Image Classification Business use cases
     healthcare
       - to classify medical images like X-rays and CT scans to assist in diagnosing diseases.
     autonomous vehicles
       - to classify the moving objects like pedestrians, vehicles, animals in the images captured by the vehicle cameras.
     security
       - to classify images from surveillance cameras, to detect unauthorized persons or suspicious activities.

  Note: Missing: 
    In the resources section, you will see a page containing a sample notebook demonstrating SageMaker's image 
    classification algorithm.

    Image Classification Sample Notebooks
      - For a sample notebook that uses the SageMaker AI image classification algorithm, see Build and Register an 
        MXNet Image Classification Model via SageMaker Pipelines
          https://github.com/aws-samples/amazon-sagemaker-pipelines-mxnet-image-classification/blob/main/image-classification-sagemaker-pipelines.ipynb
            Saved to: notebooks/ic_notebooks/image-classification-sagemaker-pipelines

------------------------------------------------------
3.17 Reviewing SageMaker's Object Detection Algorithm 

  SageMaker's Object Detection algorithm Overview
    - It is a supervised learning algorithm that takes images as input, and identifies all instances of objects 
      within the image scene.

    - The object is categorized into one of the classes in a specified collection with a confidence score that 
      it belongs to the class. 
    - Its location and scale in the image are indicated by a rectangular bounding box

    - It uses the Single Shot multibox Detector (SSD) framework and supports two base networks: VGG and ResNet.

    - Supoorts full training mode and transfer learning mode 


  Sagemaker Object Detection algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html
    Learning Type:
      - supervised learning algorithm that specializes in classifying and detecting images.

    File/Data Types:
      - recordIO format or image formats (JPG or PNG). 
      - Each image also needs a .json file for annotation, and the file name must match the image name.
        - annotations provides object classes and locations for each image

    Instance Type:
      - recommends using GPU instances with more memory for training the images  
      - CPU or a GPU can be used in the inference stage.

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html
      required hyperparameters
        num_classes 	
          - Number of output classes. 
          - This parameter defines the dimensions of the network output and is typically set to the number of classes in the dataset.
          - Valid values: positive integer
        num_training_samples 	
          - Number of training examples in the input dataset.
          - If there is a mismatch between this value and the number of samples in the training set, then the behavior 
            of the lr_scheduler_step parameter is undefined and distributed training accuracy might be affected.
          - Valid values: positive integer

    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html
        validation:mAP 	
          - Mean Average Precision (mAP) computed on the validation set.
          - goal: Maximize

  Object Detection Business use cases

    automated checkout system
       - to detect and identify items in a shopping cart at a checkout counter.
    manufacturing quality control
       - to detect defects in the products during the manufacturing process
    finance 
      - to detect and verify information in scanned documents like checks, invoices and ID cards.

  Note: Missing: 
    Finally, in the resources section, you will see a page containing a sample notebook demonstrating SageMaker's 
    object detection algorithm.

    Object Detection Sample Notebooks
      - For a sample notebook that shows how to use the SageMaker AI Object Detection algorithm to train and host 
        a model on the Caltech Birds (CUB 200 2011) dataset using the Single Shot multibox Detector algorithm, 
        see Amazon SageMaker AI Object Detection for Bird Species

        Amazon SageMaker Object Detection for Bird Species
          https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/object_detection_birds/object_detection_birds.html
          Saved to: notebooks/object_detection_notebooks/object_detection_birds.ipynb


------------------------------------------------------
3.18 Exploring SageMaker's Semantic Segmentation

  SageMaker's Semantic Segmentation Algorithm 
    - It is a supervised learning algorithm that tags every pixel in an image with a class label from a 
      predetermined set of classes.

    - Since the tagging is done at the pixel level, this algorithm provides information about the shapes 
      of the objects present in the image.
    - The output produced by this algorithm is represented as a grayscale image, also called a segmentation mask.

  Image Processing Algorithms
    Image classification 
      - analyzes all the images and classifies them into one or more multiple output categories.
    Object detection algorithm 
      - detects and classifies all instances of an object present in an image
      - It indicates the location and scale of each object in the image with a rectangular bounding box
    Semantic Segmentation 
      - classifies every pixel present in an image.
      - it also provides information about the shapes of the objects contained in the image


  Sagemaker Semantic Segmentation algorithm Attributes
    https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html
    Learning Type:
      - supervised learning algorithm that specializes in as assigning class labels to images at pixel level

    File/Data Types:
      - the data to be provided in four separate channels, two for images and two for annotations:
          train (jpg), train_annotation (png), validation (jpg), and validation_annotation (png)
      - Annotations are expected to be uncompressed PNG images
      - Every JPG image in the train and validation directories have a corresponding PNG label image with 
        the same name in the train_annotation and validation_annotation directories.

    Instance Type:
      - recommends using GPU instances with more memory for training the images  
      - CPU or a GPU can be used in the inference stage.

    Hyperparameters
      https://docs.aws.amazon.com/sagemaker/latest/dg/segmentation-hyperparameters.html
      required hyperparameters
        num_classes 	
          - Number of output classes to segment 
          - Valid values:  2 ≤ positive integer ≤ 254
        num_training_samples 	
          - The number of samples in the training data. 
          - The algorithm uses this value to set up the learning rate scheduler.
          - Valid values: positive integer

    Metrics
      https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation-tuning.html
        validation:mIOU 	
          - IoU = (Area of overlap) / (Area of Union);  and MIoU is the mean IoU
          - The area of the intersection of the predicted segmentation and the ground truth divided by the 
            area of union between them for images in the validation set. Also known as the Jaccard Index.
          - goal: Maximize
	
        validation:pixel_accuracy 
          - The percentage of pixels that are correctly classified in images from the validation set. 	
          - goal: Maximize

  Semantic Segmentation Business use cases

    satellite and aerial imagery
      - to segment different land cover types, such as urban areas or water bodies.
    retail 
      - to segment products on retail shelves.
    entertainment and media 
      - to segment inappropriate or harmful content in images and videos.

   Note: Missing:
      Finally, in the Resources section, you will see a page containing sample notebook demonstrating 
      SageMaker's semantic segmentation algorithm.


  Semantic Segmentation Sample Notebooks
    https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html#semantic-segmentation-sample-notebooks
    - For a sample Jupyter notebook that uses the SageMaker AI semantic segmentation algorithm to train a model 
      and deploy it to perform inferences, see the Semantic Segmentation Example. 
      https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.html

      Amazon SageMaker Semantic Segmentation Algorithm

      Saved to: notebooks/semantic_segmentation_notebooks/semantic_segmentation_pascalvoc.ipynb.txt

------------------------------------------------------
3.19 Comparing Machine Learning Algorithms on a Single Dataset using Amazon SageMaker


Saved files:
    completed python jupyter notebook:
      compare_algorithms_finished.ipynb
    extracted python code from jupyter notebook:
      compare_algorithms_finished.py

About this lab

  Imagine you are the data engineer, and you have been assigned the task of finding an optimal ML algorithm 
  by comparing multiple algorithms. This lab will take the California housing dataset and predict the median housing value.

  In this hands-on lab, you will learn how to train multiple regression algorithms, predict for test data and compare 
  core regression metrics.

Learning objectives
  - Launch SageMaker Notebook
  - Load Libraries and Fetch the Data (California Housing data)
  - Train the Model with Multiple Algorithms (Scikit Linear Learn, Random Forest, Ridge models)
  - Predict and Compare


Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. If you are prompted to select a kernel, please choose conda_tensorflow2_p310.
    In the search bar, type "SageMaker" to search for the SageMaker service. Click on the Amazon SageMaker result to go directly to the SageMaker service.
    Expand Applications and IDEs and select Notebooks to display the notebook provided by the lab.
    Check to see if the notebook is marked as InService. If so, click on the Open Jupyter link under Actions.
    Click on the compare_algorithms.ipnyb file.

Load the Dataset and Split the Data

Note: If this is your first time running a notebook, each cell contains Python commands you can run independently.

    Click the first cell that imports the required Python libraries, and use the Run button at the top to execute the code.

    Note: A * inside the square braces indicates the code is running, and you will see a number once the execution is complete.

    This cell indicates all the libraries we will import from Pandas and sklearn. It may take a few minutes to complete the operation.

    The next cell fetches the housing dataset from sklearn library. After loading the dataset, we create feature variables (X) and target variables (Y). Click Run to execute this cell.

    Now that the data is imported, it must be split for training and testing purposes. Use the following code snippet and Run the cell to perform this operation.

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

Initialize and Train the algorithms

    The next cell initializes LinearRegression and applies fit operation on the training data we fetched in the previous step. Run this cell to train this algorithm.

    In the same fashion, lets initialize RandomForestregressor algorithm and apply fit function to train the model. Copy the following code and click Run

    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)

    We will compare three algorithms in this lab. As the third algorithm, we will use Ridge. Use the following code to train this model.

    ridge = Ridge()
    ridge.fit(X_train, y_train)

Make Predictions

    In this section, we will use all the three trained models and predict values for test data. The first cell under this category uses LinearRegression to predict. Run this cell to complete the prediction.

    Use the following code snippet to run predictions on RandomForestRegressor and Ridge models.

    rf_predictions = rf_model.predict(X_test)
    ridge_predictions = ridge.predict(X_test)

Evaluate the Model

    The first cell under this section, evaluates the performance of these models using three metrics MAE (mean absolute error), R2 Score and RMSE (Root mean squared error).

    The cell contains code to fetch the metrics for LinearRegression predictions, but we want to fetch the metrics for all three models. Copy the following code snippet and add it to the bottom of the cell and Run it to fetch the metrics for all three models.

    rf_mae = mean_absolute_error(y_test, rf_predictions)
    rf_r2 = r2_score(y_test, rf_predictions)
    rf_rme = root_mean_squared_error(y_test, rf_predictions)

    ri_mae = mean_absolute_error(y_test, ridge_predictions)
    ri_r2 = r2_score(y_test, ridge_predictions)
    ri_rme = root_mean_squared_error(y_test, ridge_predictions)

Validate the Output

    Highlight the cell under the section and click Run to print the metrics from all the three models.
    Based on the results, the Random Forest Regression model would be the best model to use in this case.

   --------------------------------------------
   code: Compare algorithm lab 

      >>> # Comparing Machine Learning Algorithms on a Single Dataset using Amazon SageMaker

      >>> # # Introduction
      >>> # 
      >>> # In this lab, you will learn how to import a dataset, split it into training and test data, initialize multiple algorithms, train them, predict for test data and compare the metrics against the test data.

      >>> # # How to Use This Lab
      >>> # 
      >>> # Most of the code is provided for you in this lab as our solution to the tasks presented. Some of the cells are left empty with a #TODO header and its your turn to fill in the empty code. You can always use our lab guide if you are stuck.

      >>> # # 1) Import the Libraries

      >>> import pandas as pd
      >>> from sklearn.model_selection import train_test_split
      >>> from sklearn.datasets import fetch_california_housing
      >>> from sklearn.model_selection import train_test_split
      >>> from sklearn.linear_model import LinearRegression
      >>> from sklearn.linear_model import Ridge
      >>> from sklearn.ensemble import RandomForestRegressor
      >>> from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error


      >>> # # 2) Load the Dataset

      >>> # Load the Dataset and create feature and target variables
      >>> california_housing = fetch_california_housing()
      >>> X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)
      >>> y = pd.Series(california_housing.target, name='MedHouseVal')


      >>> # # 3) Split the Data

      >>> # TODO: Use `train_test_split` function and split the data 80, 20 ratio. Assign the result to X_train, X_test, y_train, y_test
      >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


      >>> # # 4) Initialize and Train the Algorithms

      >>> # Train a Linear Regression model
      >>> lr_model = LinearRegression()
      >>> lr_model.fit(X_train, y_train)


      >>> # TODO: Train a Random Forest Regression algorithm. pass two parameters n_estimators with a value of 100 and random_state with a value 42.
      >>> # TODO: Assign the result to rf_model. Fit the training data similar to linear regression algorithm.
      >>> rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
      >>> rf_model.fit(X_train, y_train)


      >>> # TODO: Train a Ridge algorithm. This algorithm doesnt require any parameters.
      >>> # TODO: Assign the result to ridge. Fit the training data similar to linear regression algorithm.
      >>> ridge = Ridge()
      >>> ridge.fit(X_train, y_train)


      >>> # # 5) Make Predictions

      >>> # Make predictions on the testing set
      >>> lr_predictions = lr_model.predict(X_test)


      >>> # TODO: Make predictions on the random forest regression model and ridge model. Assign the predictions to `rf_predictions` and `ridge_predictions`
      >>> rf_predictions = rf_model.predict(X_test)
      >>> ridge_predictions = ridge.predict(X_test)


      >>> # # 6) Evaluate the Model

      >>> # Evaluate the performance of all the models
      >>> lr_mae = mean_absolute_error(y_test, lr_predictions)
      >>> lr_r2 = r2_score(y_test, lr_predictions)
      >>> lr_rme = root_mean_squared_error(y_test, lr_predictions)

      >>> #TODO: In the same fashion, fetch MAE (mean absolute error), R2 Score and RMSE (Root mean squared error) for the remaining two models.
      >>> rf_mae = mean_absolute_error(y_test, rf_predictions)
      >>> rf_r2 = r2_score(y_test, rf_predictions)
      >>> rf_rme = root_mean_squared_error(y_test, rf_predictions)

      >>> ri_mae = mean_absolute_error(y_test, ridge_predictions)
      >>> ri_r2 = r2_score(y_test, ridge_predictions)
      >>> ri_rme = root_mean_squared_error(y_test, ridge_predictions)


      >>> # # 7) Validate the Output

      >>> print("Linear Regression:")
      >>> print(f"Mean Absolute Error: {lr_mae}")
      >>> print(f"R-squared: {lr_r2}")
      >>> print(f"Root Mean Squared Error: {lr_rme}")

      >>> print("\nRandom Forest Regression:")
      >>> print(f"Mean Absolute Error: {rf_mae}")
      >>> print(f"R-squared: {rf_r2}")
      >>> print(f"Root Mean Squared Error: {rf_rme}")

      >>> print("\nRidge Regression:")
      >>> print(f"Mean Absolute Error: {ri_mae}")
      >>> print(f"R-squared: {ri_r2}")
      >>> print(f"Root Mean Squared Error: {ri_rme}")

          Linear Regression:
          Mean Absolute Error: 0.5332001304956557
          R-squared: 0.5757877060324508
          Root Mean Squared Error: 0.7455813830127764
          
          Random Forest Regression:
          Mean Absolute Error: 0.32754256845930246
          R-squared: 0.8051230593157366
          Root Mean Squared Error: 0.5053399773665033
          
          Ridge Regression:
          Mean Absolute Error: 0.5332039182571163
          R-squared: 0.5758549611440127
          Root Mean Squared Error: 0.7455222779992701
   --------------------------------------------

------------------------------------------------------
3.20 Select the Appropriate Model for a Given ML Problem Review

  Certification Task Statement 3.2
                                          |-> XGBoost, logistic regression, k-means, linear learner, 
                                          |   decision trees, random forests, RNN, CNN, ensemble, transfer learning
       Task statement 3.2                 |
       (Select the appropriate     -------|
       model(s) for a give problem)       |    
                                          |
                                          |-> Express the inituition behind models
                                                

  Built-in algorithms 5 major categories.
      tabular data
        XGBoost 
          - an implementation of gradient boosted trees algorithm that combines an ensemble 
            of estimates from weaker models
        Linear Learner
          - learns a linear functon for regression or a linear threshold for classification
        K-Nearest Neighbors
          - an index-based non-parametric algorithm that can be used to address both regression 
             and classification problems
        Factorization Machines
          - an extension ofa linear learner model designed to capture the higher-order relationships 
            between features in a dataset

      time-series data
        - Time-series data is where you have data that is recorded over consistent interval in time.
        - Forecasting product demand, analyzing server loads, and web traffic are common examples.
        DeepAR algorithm
          - forecasts one-dimensional time-series data usesin recurrent Neural Networks (RNNs)

      clustering
         - unsupervised learning category where label data is not available.
         - these algorithms are used for tasks like clustering, demonstrated reduction, and anomaly detection.
         Principal Component Analysis (PCA) Algorithm
           - overcomes the "curse of dimensionality" problem by reducing the number of features
         Random Cut Forest (RCF) Algorithm 
           - used to detect anomalies in a dataset
         IP Insights Algorithm
           - learns the usage pattern for IPv4 addresses
         K-Means Algorithm
           - groups similar data

      text data 
        - used in the field of natural language processing (NLP) for document summarization, topic modeling, 
          and language translation.
        Object2Vec 
          - general purpose neural embedding algorithm that can be used for recommendataion systems
          - customizable neural embedding algorithm that can be used to create vector representations of object
          - It can find relationship between things based on pairings
        Latent Dirichlet Allocation (LDA) Algorithm
          - determines topics in a set of documents
        Neural Topic Model (NTM) Algorithm
          - Uses neural network to determine topics in a set of documents
        BlazingText Algorithm
          - A highly optimized implementation of word2vec and text classification algorithms
        sequence-to-Sequence (Seq2Seq) Algorithm
          - used from transforming a sequence of input tokens to another sequence of output tokens

      Vision / image data
        image classification
          - used to classify images into one of the output categories
        object detection
          - uses a neural network to detect and classify objects in an image
        semantic segmentation
          - tags every pixel in an image with a class lable from a predefined set of classes
          - provides information about the shapes of the objects present in the image.

------------------------------------------------------
3.21 Select the Appropriate Model(s) for a Given ML Problem - Quiz

-----------------
Question 1

You have been asked to develop an ML model that can forecast prices for a product your company manufactures. However, your 
development team has no expertise in ML or Python. What strategy will you follow in developing the model within the limited budget?

  Use a custom Docker image.

  Use built-in algorithms.                                             <--- Correct Answer
     Info: This is the best no-code option.

  Outsource your work to a company that has ML expertise.               <--- Incorrect Answer
    Info: The company has a limited budget. So, this may not be a feasible option.

  Use script mode in a supported framework.

------
-----------------
Question 2
You are currently working on developing a model. Upon analyzing the dataset, you found out that the dataset has a very 
large number of features. You have been asked to reduce the number of features without losing any valuable information. 
How would you approach the problem?

  Use Principal Component Analysis algorithm and perform dimensionality reduction.          <--- Correct Answer

  Use Linear Learner algorithm that can learn about the redundant features

  Use DeepAR algorithm and perform dimensionality reduction.

  Ask the business owner which features are redundant.

Good work!
  PCA is primarily used for dimensionality reduction.

------
-----------------
Question 3
You are working in the retail industry. You have been asked to develop an ML model to segment the customers into distinct 
groups based on their purchasing behavior and demographics. Which algorithm will you use to achieve this?

  DeepAR algorithm

  K-Means algorithm                            <--- Correct Answer

  Random Cut Forest algorithm

  Semantic Segmentation algorithm

Good work!
  K-Means algorithm is a good choice to group data based on specific attributes.

------
-----------------
Question 4
You are working on developing an ML model to block traffic originating from a specific set of IP addresses. 
Which algorithm will you use?

  XGBoost

  K-Nearest Neighbors

  Random Cut Forest

  IP Insights                            <--- Correct Answer

Good work!
  IP Insights algorithm can detect restricted IP addresses.

------
-----------------
Question 5
You are working on developing an ML model that must identify the individual objects within an image. Which algorithm will 
you use to develop this model?

  Image Classification

  Semantic Segmentation

  Object Detection                            <--- Correct Answer

  Object2Vec

Good work!
  Object Detection algorithm can detect objects inside an image.

------
Amazon SageMaker Object2Vec: Turning Objects into Meaningful Embeddings (with Python Example)
  https://www.linkedin.com/pulse/amazon-sagemaker-object2vec-turning-objects-python-ramirez-sosa-72e7e/

  What is Amazon SageMaker Object2Vec?
    - Object2Vec is a feature engineering algorithm offered within the Amazon SageMaker suite. 
    - It transforms various types of objects, including text, categorical data, and user interactions, into low-dimensional 
      vector representations known as embeddings. 
    - These embeddings capture the semantic similarities and relationships between objects, enabling various machine learning 
      algorithms to better understand and leverage the data.

  Key Advantages of Object2Vec:
    - Efficient Information Encoding: Object2Vec compresses complex data into smaller, more manageable vector representations, 
      facilitating faster processing and reducing storage requirements.
    - Enhanced Feature Extraction: It goes beyond simply representing individual features, capturing the relationships and 
      context between them, leading to richer and more informative data exploration.
    - Improved Machine Learning Performance: By providing meaningful embeddings, Object2Vec empowers machine learning 
      algorithms to handle complex relationships and unlock better performance in various tasks.

  Use Cases of Object2Vec:
    Object2Vec finds applications in diverse domains:
      Recommendation Systems: 
        - By embedding user profiles and item descriptions, Object2Vec can power personalized recommendations, suggesting 
          items similar to those users have interacted with or favored based on their inherent relationships.
      Anomaly Detection: 
        - Object2Vec can identify anomalies in user behavior or system logs by comparing their embeddings to expected patterns. 
        - Deviations from the norm might signal potential issues or suspicious activities.
    Sentiment Analysis: 
      - Object2Vec can be used to understand the sentiment expressed in text data (e.g., reviews, comments) by capturing the 
        underlying emotions and sentiment associated with the embedded representations.


-----------------
Question 6
You are currently working on a labeled dataset that exhibits a higher-order relationship. Which algorithm will 
you use to model the data?

  XGBoost

  Principal Component Analysis                     <--- Incorrect Answer

  Factorization Machines                            <--- Correct Answer

  Linear Learner

Sorry!
  Principal Component Analysis is an unsupervised learning algorithm.

Correct Answer
  Factorization Machines algorithm captures the higher-order relationship in a dataset.

------
Recommendation Systems using Factorization Machines with examples and codes
  https://medium.com/data-science-in-your-pocket/recommendation-systems-using-factorization-machines-with-examples-and-codes-48a8dcb9d5ea

  What are higher-order feature interactions?
    - Higher-order interactions, refer to the combined effect of two or more features on the target variable, where the 
      impact is not linear and cannot be represented by the sum of individual feature effects. For example,

      - Assume we have classification data for whether the user will click on an ad. The feature-set has

         User-id, User Age, Ad Type, Ad-ID, Click or Not (label)

      - We might observe that the effect of “Ad Type” on the likelihood of a user clicking on the ad depends on the “User Age.” 
      - For example, younger users may be more likely to click on image ads, while older users may prefer video ads. 
      - This interaction implies that the impact of “Ad Type” is not uniform across all age groups. 
      - To capture this higher-order interaction, the model needs to account for how “Ad Type” and “User Age” interact 
        to influence the click-through rate.

  Factorization Machine can help us capture such high-order interactions that Linear Regression ignores.
    - The FM model equation incorporates n-way interactions between features of various orders. 
    - The most prevalent configuration is the second-order model, which encompasses weights for individual features and 
      interaction terms for every pair of features in the dataset. 

-----------------
Question 7
You are working on a document publishing company. Your company wants to develop an ML model to detect plagiarized 
content. Which algorithm will you use for this purpose?

  Object2Vec

  K-Means

  Principal Component Analysis

  BlazingText                            <--- Correct Answer

Good work!
  BlazingText is the ideal candidate to process a large corpus of text data.

------

SageMaker unsupervised algorithms
ByMichael Stainsbury Last updated date:
17 March, 2023

  - There are five SageMaker unsupervised algorithms that process tabular data. 
    - Unsupervised Learning algorithms process data that has not been labeled. 
    1. IP Insights is an anomaly detection algorithm to detect problems and threats in an IR network. 
    2. K-Means is a clustering algorithm. 
    3. Object2Vec translates input data to vectors. 
    4. Principal Component Analysis (PCA) algorithm is used in Feature Engineering to reduce the number of features in data. 
    5. The Random Cut Forest (RCF) is a general purpose anomaly detection algorithm.

-----------------
Question 8
Your company has large amounts of labeled data and wants to develop a model that offers high speed and performance. 
They also want you to leverage an algorithm that combines the prediction accuracy of multiple algorithms. 
Which SageMaker built-in algorithm should you choose?

  Random Cut Forest

  K-Nearest Neighbors

  Linear Learner

  XGBoost                            <--- Correct Answer

Good work!
  This built-in algorithm combines the prediction accuracy of multiple ML models.

------
  
What is XGBoost Algorithm?
  https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/

  - XGBoost is a machine learning algorithm that belongs to the ensemble learning category, specifically the gradient boosting framework. 
  - It utilizes decision trees as base learners and employs regularization techniques to enhance model generalization. 
  - XGBoost is famous for its computational efficiency, offering efficient processing, insightful feature importance analysis, and 
    seamless handling of missing values. 
  - It’s the go-to algorithm for a wide range of tasks, including regression, classification, and ranking. 

  - The algorithm works by sequentially adding weak learners to the ensemble, with each new learner focusing on correcting the 
    errors made by the existing ones. 
  - It uses a gradient descent optimization technique to minimize a predefined loss function during training.
-----------------
Question 9
You are working in the automotive industry, which supplies software for self-driving cars. You have been assigned 
the task to develop an ML model that will detect lane boundaries to keep the vehicle within lanes. Which SageMaker 
built-in algorithm will you use to solve this problem?

  Image Classification

  Image Detection

  Object Detection

  Semantic Segmentation                            <--- Correct Answer

Good work!
  Semantic Segmentation algorithm helps classify images at pixel level.

------
-----------------
Question 10
You have been assigned a task of developing a model using a dataset in CSV format. Which algorithm is NOT 
a good choice?

  K-Nearest Neighbors

  Linear Learner

  Factorization Machines                            <--- Correct Answer

  XGBoost

Good work!
  Factorization Machines algorithm does not support CSV data.

------
-----------------
------
-----------------
------------------------------------------------------

Chapter 4 Train ML Models 

------------------------------------------------------
4.1 Exploring the SageMaker Infrastructure 


  CPU vs GPU
                          CPU                    GPU
                          --------------         ---------------------
      Model Complexity    Simplier models        Complex models
      Model Size          Smaller models         Larger models
      Inference           Low latency            High throughput
      Cost                Cost sensitive tasks   Expensive solution


    - For simpler models, like linear regression, logistic regression, or decision trees, CPU will be good enough
    - complex models involving any deep learning or recurrent learning networks, then GPU will be a better option.

    - For larger models, GPU is preferred for training and inference as they can handle large memory requirement and 
      offer enhanced performance.

    - for models with low-latency requirements, like recommendation systems, CPUs are more suitable,
    - models with high throughput requirements, GPU may be a better choice as it can handle large batches of data simultaneously.

    - cost-efficiency perspective, CPUs may be a better choice if you have budget constraints.  
    - GPUs are relatively expensive, but do keep in mind they come with significant performance gains in training.
 

  SageMaker EC2 instances
      Genearl Purpose:           ml.m5.xlarge, ml.m5.4xlarge, ml.m5.12xlarge
      Compute-optimized:         ml.c5.xlarge, ml.c5.4xlarg2, ml.c5.8xlarge
      Accelerated computing:     ml.p3.xlarge, ml.p3.8xlarg2, ml.p3.16xlarge

  SageMaker Inference recommender
    - reduces the time required to get the ML to production
    - helps you decide the ideal instance, instance count, container parameters, and so on

  Spot instances
    - a cost-effective solution
    - instances that AWS offers at a significantly reduced cost compared as much as 90% cheaper than On-Demand Instances,
    - AWS can reclaim these instances when it needs the capacity back with a short notice,
    checkpoints
      - it's recommended to use checkpoints to save the data.
      - For every save checkpoint, SageMaker copies checkpoint data from a local path to Amazon S3.
      - When the job is restarted, SageMaker then restarts from the same point where it stopped before by copying 
        the data back from S3
  Distributed training.
    - is a process where an ML model is trained across multiple machines to reduce training time and handle large datasets.
    -  useful with deep learning tasks,
       - like computer vision and natural language processing.
    two main approaches to distributor training.
       data parallelism.
         - used this strategy when the dataset is large.
         - the training dataset is split across multiple GPU instances.
         - Each GPU instance contains the same replica of the model but operates on different batches of data.
         -  AWS uses a method of weighted average of the gradients resulting from all the instances.
       model parallelism.
          - You use this strategy when the model has too many layers or parameters that cannot be fit in a single GPU instance.
          - In this strategy, each GPU instance carries a subset of the model through which the entire dataset flows.

   

------------------------------------------------------
4.2 Splitting, Shuffling, and Bootstrapping Data for Training 
------------------------------------------------------

  Datasets:
    Training data               Trains the ML Model
    Validation data             [optional] Measures the model performance during the training
    test data                   Determines how well the model generalizes on the unseen data

  Cross validation
    - This process of validating the model against the fresh, unseen data is called Cross-validation.

    K-Fold Cross-validation
     - the entire dataset is divided into K-Folds.
     Each 'i' interation (from 1 to 'k')
       - The 'i' set is used for testing purpose, and the model is trained with a remaining K minus one sets.
       - Once the iternation training is complete, the 'i' interation test error rate is computed.
       - repeat for all 'k' iterations
      After 'k' iterations;
        - The mean of errors from all the iterations is calculated as a Cross-validation test error estimate.
      Result:
       - Though this technique is effective in preventing overfitting, it is computationally expensive.

    stratified K-Fold Cross-validation.
      - similar to K-Fold Cross-validation, except this technique ensures that each fold maintains the same class distribution
        as the entire dataset.
      - important when you are working with a classification problem having imbalanced sets.

    Leave-one Out Cross-validation.
      - the data is trained on all the dataset, leaving out one data point for testing purposes.
      - The model is then iteratively tested on the remaining N minus one data points, and the error is computed at each step.
      - extremely computationally expensive and NOT often used


  Data Shuffling
    - ensures daata distribution is randomized
    - prevents bias
    - enhances model generation
    - built-in algorithms perform internal data shuffling that can be configured
    - If you're running custom Python scripts, you may need to explicitly shuffle the data before the training process.

  bootstrapping.
    - a statistical technique used to create multiple samples of data with replacement.
    - By replacement we mean that the same data point can appear in multiple times in a re-sample dataset.
    Bootstrapping is performed
      - to improve the model stability and variability.
      - allows for the estimation of confidence intervals for model performance metrics
      - help understanding the bias-variance trade-off in model predictions.
      - requires custom scripts to perform bootstrapping as part of model training process.

------------------------------------------------------
4.3 Optimization Techniques in Machine Learning Training 

  Why Optimize
    - improves model accuracy by minimizing the cost function
    - Accuracy is measured by the loss function or the cost function
       - the loss function is to capture the difference between the actual and predicted values for a single record 
       - whereas cost functions aggregate the difference for the entire training dataset
    - prevents overfitting and underfitting 
      - optimization techniques help in finding the right balance between the bias and variance of a model

  Optimization Techniques

    Linear Learner 
      - minimize the residuals 
      - find the line of a best fit in a linear regression by computing the error between the actual output y and the 
        predicted output y1

    Gradient Descent Technique
      - based on a convex function,
      - needs one additional parameter called learning rate, also known as the step size.  
         - It is a step size taken to reach the minimal error.
         - A larger learning rate may possess the risk of overshooting the minimum,
         - smaller learning rate has many small step sizes.
             - taking smaller step size has the advantage of more precision, it is computationally expensive.
       - The stable point found at the end of this gradient process is called a convergence point.
     For non-convex problem:
       local minimum 
         - Though this point seems to be the point where the error value is minimum, but they're not the absolute minimum.
       global minimum.
         - absolute minimum
       - Gradient decent algorithm may get stuck in local minimum during the training of the models because finding the 
         local minimum is easier compared to the global minimum.

    Stochastic gradient descent.
      - A common approach used to overcome getting stuck at local minimum 
      - randomly sampling a subset of the data points at each iteration
      - then using the gradients of those data points to update the model parameters.
      - In other words, by introducing the noise in the gradient calculation, stochastic gradient descent escapes the problem of local minimum.
      - the convergence may oscillate around the minimum rather than converging smoothly.

    Batch gradient descent.
      - batch gradient descent uses the entire dataset in calculating gradient.
      - The parameters are updated after going through the entire dataset.
      - can achieve stable convergence, and the gradient is accurate since all the data points are considered at once.
      - drawbacK: needs to load entire dataset into memory
        - may be infeasible for large datasets.
------------------------------------------------------
4.4 Training A SageMaker Model Using a Built In Algorithm 


  Saved files:
    completed python jupyter notebook:
      linear_learner_mnist.ipynb
    extracted python code from jupyter notebook:
      linear_learner_mnist.py


  Notebook:
     https://github.com/aws/amazon-sagemaker-examples/blob/main/aws_sagemaker_studio/sagemaker_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb

  dataset:
    MNIST database, which is a large database of handwritten digits as the input data.

  SageMaker Linear Learner Algorithm
    Learning type:   Classification / regression 
    file type:       CSV, [recordio] protobuf, and JSON (inferernce only)
    instance type:   CPU, GPU 
    Hyperparameters: num_class, predictor_type
    Metrics:         Cross entrop loss,  absolute error,  MSE (regression
                     Precision, recall, accuracy (classification)

   Training a model using a Built-in Algorithm
      processing data
        - convert the data from pickled NumPy array on disk to protobuf format.
        - split into training and validation data and uploaded to S3 bucket.
      training stage
        - then trained using Amazon SageMaker, Python SDK, which will provision the instance for the training purposes,
        - use Amazon SageMaker Notebook for the training purposes with a execution role
        - have 

 SageMaker Linear Linear python SDK 
   https://sagemaker.readthedocs.io/en/v2.15.2/algorithms/linear_learner.html
     class sagemaker.LinearLearner(role, instance_count, instance_type, predictor_type, ...)
        <- child class of the: sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase 


   Code: Linear Learning MNIST sample Notebook

      >>> # # An Introduction to Linear Learner with MNIST
      >>> # 

      >>> # ---
      >>> # 
      >>> # This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. 
      >>> # _**Making a Binary Prediction of Whether a Handwritten Digit is a 0**_
      >>> # 
      >>> # 1. [Introduction](#Introduction)
      >>> # 2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)
      >>> #   1. [Permissions and environment variables](#Permissions-and-environment-variables)
      >>> #   2. [Data ingestion](#Data-ingestion)
      >>> #   3. [Data inspection](#Data-inspection)
      >>> #   4. [Data conversion](#Data-conversion)
      >>> # 3. [Training the linear model](#Training-the-linear-model)
      >>> # 4. [Set up hosting for the model](#Set-up-hosting-for-the-model)
      >>> # 5. [Validate the model for use](#Validate-the-model-for-use)
      >>> # 

      >>> # ## Introduction
      >>> # 
      >>> # Welcome to our example introducing Amazon SageMaker's Linear Learner Algorithm!  Today, we're analyzing the [MNIST]
      >>> #  (https://en.wikipedia.org/wiki/MNIST_database) dataset which consists of images of handwritten digits, from zero to nine.  
      >>> #  We'll use the individual pixel values from each 28 x 28 grayscale image to predict a yes or no label of whether the 
      >>> #  digit is a 0 or some other digit (1, 2, 3, ... 9).
      >>> # 
      >>> # The method that we'll use is a linear binary classifier.  Linear models are supervised learning algorithms used for 
      >>> #  solving either classification or regression problems.  As input, the model is given labeled examples ( **`x`**, `y`). **`x`** 
      >>> #  is a high dimensional vector and `y` is a numeric label.  Since we are doing binary classification, the algorithm expects 
      >>> #  the label to be either 0 or 1 (but Amazon SageMaker Linear Learner also supports regression on continuous values of `y`).  
      >>> #  The algorithm learns a linear function, or linear threshold function for classification, mapping the vector **`x`** to 
      >>> #  an approximation of the label `y`.
      >>> # 
      >>> # Amazon SageMaker's Linear Learner algorithm extends upon typical linear models by training many models in parallel, in a 
      >>> #  computationally efficient manner.  Each model has a different set of hyperparameters, and then the algorithm finds the 
      >>> #  set that optimizes a specific criteria.  This can provide substantially more accurate models than typical linear 
      >>> #  algorithms at the same, or lower, cost.
      >>> # 
      >>> # To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on.

      >>> # ## Prequisites and Preprocessing
      >>> # 
      >>> # The notebook works with *Data Science* kernel in SageMaker Studio.
      >>> # 
      >>> # ### Permissions and environment variables
      >>> # 
      >>> # _This notebook was created and tested on an ml.m4.xlarge notebook instance._
      >>> # 
      >>> # Let's start by specifying:
      >>> # 
      >>> # - The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region 
      >>> #  as the Notebook Instance, training, and hosting.
      >>> # - The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. 
      >>> #  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto 
      >>> #  regexp with a the appropriate full IAM role arn string(s).

      >>> ! pip install --upgrade sagemaker

      >>> import sagemaker

      >>> #bucket = sagemaker.Session().default_bucket()
      >>> bucket = "pat-demo-bkt-e2"
      >>> prefix = "sagemaker/DEMO-linear-mnist"

      >>> # Define IAM role
      >>> import boto3
      >>> import re
      >>> from sagemaker import get_execution_role

      >>> sess = sagemaker.Session()
      >>> region = boto3.Session().region_name
      >>> print(f"region: {region}")

      >>> role = get_execution_role()
      >>> print (f"role: {role}")

      >>> # S3 bucket where the original mnist data is downloaded and stored.
      >>> # downloaded_data_bucket = f"sagemaker-example-files-prod-{region}"
      >>> # downloaded_data_prefix = "datasets/image/MNIST"


      >>> # ### Data ingestion
      >>> # 
      >>> # Next, we read the dataset from an online URL into memory, for preprocessing prior to training. This processing 
      >>> # could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset 
      >>> # is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. 
      >>> # For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets.

      >>> %%time 
      >>> import pickle, gzip, numpy, urllib.request, json
      >>> 
      >>> fobj = (
      >>>     boto3.client("s3")
      >>>     .get_object(
      >>>         Bucket=f"sagemaker-example-files-prod-{boto3.session.Session().region_name}",
      >>>         Key="datasets/image/MNIST/mnist.pkl.gz",
      >>>     )["Body"]
      >>>     .read()
      >>> )
      >>> 
      >>> with open("mnist.pkl.gz", "wb") as f:
      >>>     f.write(fobj)
      >>> 
      >>> # Load the dataset
      >>> with gzip.open("mnist.pkl.gz", "rb") as f:
      >>>     train_set, valid_set, test_set = pickle.load(f, encoding="latin1")
      >>> ')

      >>> # ### Data inspection
      >>> # 
      >>> # Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand 
      >>> # the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right 
      >>> # here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset.

      >>> get_ipython().run_line_magic('matplotlib', 'inline')
      >>> import matplotlib.pyplot as plt

      >>> plt.rcParams["figure.figsize"] = (2, 10)


      >>> def show_digit(img, caption="", subplot=None):
      >>>     if subplot == None:
      >>>         _, (subplot) = plt.subplots(1, 1)
      >>>     imgr = img.reshape((28, 28))
      >>>     subplot.axis("off")
      >>>     subplot.imshow(imgr, cmap="gray")
      >>>     plt.title(caption)


      >>> show_digit(train_set[0][30], "This is a {}".format(train_set[1][30]))


      >>> # ### Data conversion
      >>> # 
      >>> # Since algorithms have particular input and output requirements, converting the dataset is also part of the process 
      >>> # that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation 
      >>> # of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.
      >>> # 
      >>> # Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as `sagemaker` below.

      >>> import io
      >>> import numpy as np
      >>> import sagemaker.amazon.common as smac

      >>> vectors = np.array([t.tolist() for t in train_set[0]]).astype("float32")
      >>> labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype("float32")

      >>> buf = io.BytesIO()
      >>> smac.write_numpy_to_dense_tensor(buf, vectors, labels)
      >>> buf.seek(0)


      >>> # MINST dataset: 50000 images at 28 x 28 pixel per image
      >>> # labels: for image of '0', label[i] = 1, else 0

      >>> vectors.size


      >>> show_digit(train_set[0][21], "This is a {}".format(train_set[1][21]))


      >>> # ## Upload training data
      >>> # Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it.

      >>> import boto3
      >>> import os

      >>> key = "recordio-pb-data"
      >>> boto3.resource("s3").Bucket(bucket).Object(os.path.join(prefix, "train", key)).upload_fileobj(buf)
      >>> s3_train_data = "s3://{}/{}/train/{}".format(bucket, prefix, key)
      >>> print("uploaded training data location: {}".format(s3_train_data))


      >>> # Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm.

      >>> output_location = "s3://{}/{}/output".format(bucket, prefix)
      >>> print("training artifacts will be uploaded to: {}".format(output_location))


      >>> # ## Training the linear model
      >>> # 
      >>> # Once we have the data preprocessed and available in the correct format for training, the next step is to actually 
      >>> # train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of 
      >>> # the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.
      >>> # 
      >>> # Again, we'll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed.  
      >>> # In this example that takes between 7 and 11 minutes.  Despite the dataset being small, provisioning hardware and 
      >>> # loading the algorithm container take time upfront.
      >>> # 
      >>> # First, let's specify our containers.  Since we want this notebook to run in all 4 of Amazon SageMaker's regions, 
      >>> # we'll create a small lookup.  More details on algorithm containers can be found in 
      >>> # [AWS documentation](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html).

      >>> from sagemaker.image_uris import retrieve

      >>> container = retrieve("linear-learner", boto3.Session().region_name)


      >>> session = boto3.Session()
      >>> s3 = session.resource('s3')

      >>> my_bucket = s3.Bucket(bucket)

      >>> for my_bucket_object in my_bucket.objects.all():
      >>>     print(my_bucket_object.key)


      >>> # Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters.  Notice:
      >>> # - `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.
      >>> # - `predictor_type` is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.
      >>> # - `mini_batch_size` is set to 200.  This value can be tuned for relatively minor improvements in fit and speed, 
      >>> # but selecting a reasonable value relative to the dataset is appropriate in most cases.

      >>> import boto3

      >>> sess = sagemaker.Session()

      >>> linear = sagemaker.estimator.Estimator(
      >>>     container,
      >>>     role,
      >>>     train_instance_count=1,
      >>>     train_instance_type="ml.c4.xlarge",
      >>>     output_path=output_location,
      >>>     sagemaker_session=sess,
      >>> )
      >>> linear.set_hyperparameters(feature_dim=784, predictor_type="binary_classifier", mini_batch_size=200)

      >>> linear.fit({"train": s3_train_data})


      >>> # ## Set up hosting for the model
      >>> # Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  
      >>> # This will allow out to make predictions (or inference) from the model dyanamically.
      >>> # 
      >>> # _Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice 
      >>> # of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon 
      >>> # Athena, or other deployment target._

      >>> from sagemaker.serializers import CSVSerializer
      >>> from sagemaker.deserializers import JSONDeserializer

      >>> linear_predictor = linear.deploy(
      >>>     initial_instance_count=1,
      >>>     instance_type="ml.m4.xlarge",
      >>>     serializer=CSVSerializer(),
      >>>     deserializer=JSONDeserializer(),
      >>> )


      >>> # ## Validate the model for use
      >>> # Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  
      >>> # To make this easier, we'll again use the Amazon SageMaker Python SDK and specify how to serialize requests and deserialize 
      >>> # responses that are specific to the algorithm.

      >>> # Now let's try getting a prediction for a single record.

      >>> result = linear_predictor.predict(train_set[0][30:31])
      >>> print(result)


      >>> # OK, a single prediction works.  We see that for one record our endpoint returned some JSON which contains `predictions`, 
      >>> # including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing 
      >>> # the probability we think the digit is a 0 or not.  `predicted_label` will take a value of either `0` or `1` where (somewhat 
      >>> # counterintuitively) `1` denotes that we predict the image is a 0, while `0` denotes that we are predicting the image is not of a 0.
      >>> # 
      >>> # Let's do a whole batch of images and evaluate our predictive accuracy.

      >>> import numpy as np

      >>> predictions = []
      >>> for array in np.array_split(test_set[0], 100):
      >>>     result = linear_predictor.predict(array)
      >>>     predictions += [r["predicted_label"] for r in result["predictions"]]

      >>> predictions = np.array(predictions)


      >>> import pandas as pd

      >>> pd.crosstab(
      >>>     np.where(test_set[1] == 0, 1, 0), predictions, rownames=["actuals"], colnames=["predictions"]
      >>> )


      >>> # As we can see from the confusion matrix above, we predict 931 images of 0 correctly, while we predict 44 images as 
      >>> # 0s that aren't, and miss predicting 49 images of 0.

      >>> # ### (Optional) Delete the Endpoint
      >>> # 
      >>> # If you're ready to be done with this notebook, please run the delete_endpoint line in the cell below.  This will 
      >>> # remove the hosted endpoint you created and avoid any charges from a stray instance being left on.


      >>> sagemaker.Session().delete_endpoint(linear_predictor.endpoint)


------------------------------------------------------
4.5 Training s SageMaker Model Using a Training Script 

  Saved files:
    SageMaker entrypoint training script
      train.py
    original python jupyter notebook:
      sklean_byom.ipynb
    completed python jupyter notebook:
      sklean_byom_output.ipynb
    extracted python code from jupyter notebook:
      sklean_byom.py


  Using Scikit-learn with the SageMaker Python SDK
    https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html

  Algorithm Implementation options
    built-in algorithm.
      - no code required
      - requires algorithm with training data, hyperparameters, and computing resources.
    script mode in supported framework
      - Develop a custom Python script
      - in a supported framework, like Psyche learn, TensorFlow, pyarch, or MXNet.
      - leverage the additional Python libraries that are preloaded with these frameworks for 
        training an algorithm.
    custom docker image,
      - requires docker expertise
      - if your use case is not addressed by previous two options.
      - the Docker image must be uploaded to Amazon ECR before you can start training the model.

  Demo Example
    - IRIS dataset, which represents instances of various plant species.
    - multi-class classification model that will predict the plant species based on the sepal and 
      the petal length and width.
    - using random forest regressor in this demo.
    - pre-processing data and convert the class labels from string to integer.
    - data is split into training and validation data in a 80/20 ratio and uploaded to S3 bucket.
    - need  actual training script along with the training data.



   SageMaker Notebook instance -> Jupyter -> SageMaker Examples -> SageMaker Script Mode -> sklearn_byom.ipynb -> Use

   -> downloads: 
       sklearn_2024-12-13/sklearn_pyom.ipynb
       sklearn_2024-12-13/sklearn_pyom_outputs.ipynb
       sklearn_2024-12-13/train.py


  Traning script:
    - A typical training script loads data from the input channels, configures training with hyperparameters, 
      trains a model, and saves a model to model_dir so that it can be hosted later. 
    - Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.

    Training Script env variables:
      SM_MODEL_DIR: 
        - the path to the directory to write model artifacts to. 
      SM_OUTPUT_DATA_DIR: 
        - path to write output artifacts to. Output artifacts 
      SM_CHANNEL_TRAIN: 
        - A string representing the path to the directory containing data in the ‘train’ channel
      SM_CHANNEL_TEST: 
        - Same as above, but for the ‘test’ channel.
    

    Code:  Train a SKLearn Model using Script Mode

      >>> # 
      >>> # The aim of this notebook is to demonstrate how to train and deploy a scikit-learn model in Amazon SageMaker. The 
      >>> # method used is called Script Mode, in which we write a script to train our model and submit it to the SageMaker 
      >>> # Python SDK. For more information, feel free to read [Using Scikit-learn with the SageMaker Python SDK]
      >>> # (https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html).
      >>> # 
      >>> # ## Runtime
      >>> # This notebook takes approximately 15 minutes to run.
      >>> # 
      >>> # ## Contents
      >>> # 1. [Download data](#Download-data)
      >>> # 2. [Prepare data](#Prepare-data)
      >>> # 3. [Train model](#Train-model)
      >>> # 4. [Deploy and test endpoint](#Deploy-and-test-endpoint)
      >>> # 5. [Cleanup](#Cleanup)

      >>> # ## Download data 
      >>> # Download the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris), which is the data used to 
      >>> # trained the model in this demo.

      >>> get_ipython().system('pip install -U sagemaker')


      >>> import boto3
      >>> import pandas as pd
      >>> import numpy as np

      >>> s3 = boto3.client("s3")
      >>> s3.download_file(
      >>>     f"sagemaker-example-files-prod-{boto3.session.Session().region_name}",
      >>>     "datasets/tabular/iris/iris.data",
      >>>     "iris.data",
      >>> )

      >>> df = pd.read_csv(
      >>>     "iris.data", header=None, names=["sepal_len", "sepal_wid", "petal_len", "petal_wid", "class"]
      >>> )
      >>> df.head()


      >>> # ## Prepare data
      >>> # Next, we prepare the data for training by first converting the labels from string to integers. Then we split 
      >>> # the data into a train dataset (80% of the data) and test dataset (the remaining 20% of the data) before saving 
      >>> # them into CSV files. Then, these files are uploaded to S3 where the SageMaker SDK can access and use them to 
      >>> # train the model.

      >>> # Convert the three classes from strings to integers in {0,1,2}
      >>> df["class_cat"] = df["class"].astype("category").cat.codes
      >>> categories_map = dict(enumerate(df["class"].astype("category").cat.categories))
      >>> print(categories_map)
      >>> df.head()


      >>> df_rand= df.sample(frac=1)
      >>> df_rand.head()


      >>> # Split the data into 80-20 train-test split
      >>> num_samples = df_rand.shape[0]
      >>> split = round(num_samples * 0.8)
      >>> train = df_rand.iloc[:split, :]
      >>> test = df_rand.iloc[split:, :]
      >>> print("{} train, {} test".format(split, num_samples - split))


      >>> # Write train and test CSV files
      >>> train.to_csv("train.csv", index=False)
      >>> test.to_csv("test.csv", index=False)


      >>> # Create a sagemaker session to upload data to S3
      >>> import sagemaker

      >>> sagemaker_session = sagemaker.Session()

      >>> # Upload data to default S3 bucket
      >>> prefix = "DEMO-sklearn-iris"
      >>> training_input_path = sagemaker_session.upload_data("train.csv", key_prefix=prefix + "/training")


      >>> # ## Train model
      >>> # The model is trained using the SageMaker SDK's Estimator class. Firstly, get the execution role for training. 
      >>> # This role allows us to access the S3 bucket in the last step, where the train and test data set is located.

      >>> # Use the current execution role for training. It needs access to S3
      >>> role = sagemaker.get_execution_role()
      >>> print(role)


      >>> # Then, it is time to define the SageMaker SDK Estimator class. We use an Estimator class specifically desgined to train 
      >>> # scikit-learn models called `SKLearn`. In this estimator, we define the following parameters:
      >>> # 1. The script that we want to use to train the model (i.e. `entry_point`). This is the heart of the Script Mode method. 
      >>> # Additionally, set the `script_mode` parameter to `True`.
      >>> # 2. The role which allows us access to the S3 bucket containing the train and test data set (i.e. `role`)
      >>> # 3. How many instances we want to use in training (i.e. `instance_count`) and what type of instance we want to use in 
      >>> # training (i.e. `instance_type`)
      >>> # 4. Which version of scikit-learn to use (i.e. `framework_version`)
      >>> # 5. Training hyperparameters (i.e. `hyperparameters`)
      >>> # 
      >>> # After setting these parameters, the `fit` function is invoked to train the model.

      >>> # Docs: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html

      >>> from sagemaker.sklearn import SKLearn

      >>> sk_estimator = SKLearn(
      >>>     entry_point="train.py",
      >>>     role=role,
      >>>     instance_count=1,
      >>>     instance_type="ml.c5.xlarge",
      >>>     py_version="py3",
      >>>     framework_version="1.2-1",
      >>>     script_mode=True,
      >>>     hyperparameters={"estimators": 20},
      >>> )

      >>> # Train the estimator
      >>> sk_estimator.fit({"train": training_input_path})


      >>> # ## Deploy and test endpoint
      >>> # After training the model, it is time to deploy it as an endpoint. To do so, we invoke the `deploy` function within 
      >>> # the scikit-learn estimator. As shown in the code below, one can define the number of instances (i.e. `initial_instance_count`) 
      >>> # and instance type (i.e. `instance_type`) used to deploy the model.

      >>> import time

      >>> sk_endpoint_name = "sklearn-rf-model" + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())
      >>> sk_predictor = sk_estimator.deploy(
      >>>     initial_instance_count=1, instance_type="ml.m5.large", endpoint_name=sk_endpoint_name
      >>> )


      >>> # After the endpoint has been completely deployed, it can be invoked using the [SageMaker Runtime Client]
      >>> # (https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html) 
      >>> # (which is the method used in the code cell below) or [Scikit Learn Predictor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-predictor). If you plan to use the latter method, make sure to use a [Serializer](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) to serialize your data properly.

      >>> import json

      >>> client = sagemaker_session.sagemaker_runtime_client

      >>> request_body = {"Input": [[9.0, 3571, 1976, 0.525]]}
      >>> data = json.loads(json.dumps(request_body))
      >>> payload = json.dumps(data)

      >>> response = client.invoke_endpoint(
      >>>     EndpointName=sk_endpoint_name, ContentType="application/json", Body=payload
      >>> )

      >>> result = json.loads(response["Body"].read().decode())["Output"]
      >>> print("Predicted class category {} ({})".format(result, categories_map[result]))


      >>> # ## Cleanup
      >>> # If the model and endpoint are no longer in use, they should be deleted to save costs and free up resources.

      >>> sk_predictor.delete_model()
      >>> sk_predictor.delete_endpoint()


    Code:  Entrypoint script 'train.py' called by above SageMaker script mode script

      >>> import argparse, os
      >>> import boto3
      >>> import json
      >>> import pandas as pd
      >>> import numpy as np
      >>> from sklearn.model_selection import train_test_split
      >>> from sklearn.preprocessing import StandardScaler
      >>> from sklearn.ensemble import RandomForestRegressor
      >>> from sklearn import metrics
      >>> import joblib

      >>> if __name__ == "__main__":

      >>>     # Pass in environment variables and hyperparameters
      >>>     parser = argparse.ArgumentParser()

      >>>     # Hyperparameters
      >>>     parser.add_argument("--estimators", type=int, default=15)

      >>>     # sm_model_dir: model artifacts stored here after training
      >>>     parser.add_argument("--sm-model-dir", type=str, default=os.environ.get("SM_MODEL_DIR"))
      >>>     parser.add_argument("--model_dir", type=str)
      >>>     parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN"))

      >>>     args, _ = parser.parse_known_args()
      >>>     estimators = args.estimators
      >>>     model_dir = args.model_dir
      >>>     sm_model_dir = args.sm_model_dir
      >>>     training_dir = args.train

      >>>     # Read in data
      >>>     df = pd.read_csv(training_dir + "/train.csv", sep=",")

      >>>     # Preprocess data
      >>>     X = df.drop(["class", "class_cat"], axis=1)
      >>>     y = df["class_cat"]
      >>>     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
      >>>     sc = StandardScaler()
      >>>     X_train = sc.fit_transform(X_train)
      >>>     X_test = sc.transform(X_test)

      >>>     # Build model
      >>>     regressor = RandomForestRegressor(n_estimators=estimators)
      >>>     regressor.fit(X_train, y_train)
      >>>     y_pred = regressor.predict(X_test)

      >>>     # Save model
      >>>     joblib.dump(regressor, os.path.join(args.sm_model_dir, "model.joblib"))

      >>> # Model serving
      >>> # INFERENCE
      >>> # SageMaker uses four functions to load the model and use it for inference: model_fn, input_fn, output_fn, and predict_fn

      >>> """
      >>> Deserialize fitted model
      >>> """
      >>> def model_fn(model_dir):
      >>>     model = joblib.load(os.path.join(model_dir, "model.joblib"))
      >>>     return model

      >>> """
      >>> input_fn
      >>>     request_body: The body of the request sent to the model.
      >>>     request_content_type: (string) specifies the format/variable type of the request
      >>> """
      >>> def input_fn(request_body, request_content_type):
      >>>     if request_content_type == "application/json":
      >>>         request_body = json.loads(request_body)
      >>>         inpVar = request_body["Input"]
      >>>         return inpVar
      >>>     else:
      >>>         raise ValueError("This model only supports application/json input")

      >>> """
      >>> predict_fn
      >>>     input_data: returned array from input_fn above
      >>>     model (sklearn model) returned model loaded from model_fn above
      >>> """
      >>> def predict_fn(input_data, model):
      >>>     return model.predict(input_data)

      >>> """
      >>> output_fn
      >>>     prediction: the returned value from predict_fn above
      >>>     content_type: the content type the endpoint expects to be returned. Ex: JSON, string
      >>> """
      >>> def output_fn(prediction, content_type):
      >>>     res = int(prediction[0])
      >>>     respJSON = {"Output": res}
      >>>     return respJSON

------------------------------------------------------
4.6 Using Apache Spark with Amazon SageMaker 


  Saved files:
    completed python jupyter notebook:
      pyspark_mnist_xgboost.ipynb
    extracted python code from jupyter notebook:
      pyspark_mnist_xgboost.py


   Apache Spark  
     - an open source distributed computing system designed for fast processing of large scale data by distributing 
       the computational tasks across multiple nodes in a cluster.

   Training a Model Using Apache Spark
      - use the MNIST database, which is a large database of handwritten digits as the input data.
      - In the setup process:
          - from SageMaker Notebook, we will create a Spark session,
          - create a Spark data frame, and load the input data into this data frame.
          -  split data into training and validation data set.
       - use XGBoost SageMaker Estimator


   SageMaker Notebook instance -> Jupyter -> SageMaker Examples -> SageMaker Spark -> pyspark_minst_xgboost.ipynb -> Use

   You can visit SageMaker Spark's GitHub repository at 
      https://github.com/aws/sagemaker-spark to learn more about SageMaker Spark.

   You can visit XGBoost's GitHub repository at 
     https://github.com/dmlc/xgboost to learn more about XGBoost

  LibSVM format:
    - The first row contains the class label, in this case 0 or 1. 
    - Following that are the features, here there are two values for each one; the first one is the feature 
      index (i.e. which feature it is) and the second one is the actual value.
    - The feature indices starts from 1 (there is no index 0) and are in ascending order. 
    - The indices not present on a row are 0.
    - In summary, each row looks like this;
      <label> <index1>:<value1> <index2>:<value2> ... <indexN>:<valueN>
    - This format is advantageous to use when the data is sparse and contain lots of zeroes. 
    - All 0 values are not saved which will make the files both smaller and easier to read.
    
  What is Spark’s JAR Folder?
    https://medium.com/@Nelsonalfonso/understanding-sparks-jar-folder-its-location-and-usage-7817db37cb27
    - The Spark JAR folder is the repository of library files that Spark uses during its operations. 
    - These library files or JAR files contain compiled Java classes and associated metadata that encapsulate 
      the core functionalities of Spark. 
    - The location of the Spark JAR folder varies depending on the Spark installation method and the 
      operating system in use.


   PyShark Notebook demo
     - Change Kernel from Sparkmagic(Pyshark) the conda3_python3

     - in the notebook is an access denied exception because of insufficient privileges, (due to need EMR privileges)?
       and you can address it by adding the required policy to the role from IAM.

     - can safely ignore the warning message that says it is unable to load the native Hadoop library.

     - In a practical application, we'll be connecting to a remote Spark cluster under EMR Spark clusters 
       now come pre-installed with SageMaker Spark.

     - Since the train and test data are already preloaded at the SageMaker sample data locations, we don't 
       need to perform the split or pre-process the data explicitly.

     - The load of data is in the Lib SVM format.  The first column is a label column, and the subsequent 
       columns are vectors of doubles for features.

      - The next and the final step is to train the model.
        - We'll use XGBoost's SageMaker Estimator from the SageMaker PySpark algorithms package.
          - We pass the role, the training, and the endpoint hosting instance type and count.
          - Though NUMClasses and NUMRound are the only required hyper parameters,
          - Finally, this code snippet invoke the fit operation on the training data.

   Code: pyspark MINST EMR XGBoost demo

         Notes: 


         Introduction

         This notebook will show how to classify handwritten digits using the XGBoost algorithm on Amazon 
         SageMaker through the SageMaker PySpark library. We will train on Amazon SageMaker using XGBoost on the 
         MNIST dataset, host the trained model on Amazon SageMaker, and then make predictions against that hosted model.

         Unlike the other notebooks that demonstrate XGBoost on Amazon SageMaker, this notebook uses a SparkSession 
         to manipulate data, and uses the SageMaker Spark library to interact with SageMaker with Spark Estimators 
         and Transformers.

         You can visit SageMaker Spark's GitHub repository at https://github.com/aws/sagemaker-spark to learn 
         more about SageMaker Spark.

         You can visit XGBoost's GitHub repository at https://github.com/dmlc/xgboost to learn more about XGBoost

         This notebook was created and tested on an ml.m4.xlarge notebook instance.

         Setup

         First, we import the necessary modules and create the SparkSession with the SageMaker Spark dependencies.

      >>> import os

      >>> from pyspark import SparkContext, SparkConf
      >>> from pyspark.sql import SparkSession

      >>> import sagemaker
      >>> from sagemaker import get_execution_role
      >>> import sagemaker_pyspark

      >>> role = get_execution_role()

      >>> # Configure Spark to use the SageMaker Spark dependency jars
      >>> jars = sagemaker_pyspark.classpath_jars()

      >>> classpath = ":".join(sagemaker_pyspark.classpath_jars())

      >>> # See the SageMaker Spark Github repo under sagemaker-pyspark-sdk
      >>> # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.
      >>> spark = (
      >>>     SparkSession.builder.config("spark.driver.extraClassPath", classpath)
      >>>     .master("local[*]")
      >>>     .getOrCreate()
      >>> )


          Loading the Data

          Now, we load the MNIST dataset into a Spark Dataframe, which dataset is available in LibSVM format at

          s3://sagemaker-sample-data-[region]/spark/mnist/train/

          where [region] is replaced with a supported AWS region, such as us-east-1.

          In order to train and make inferences our input DataFrame must have a column of Doubles (named "label" 
          by default) and a column of Vectors of Doubles (named "features" by default).

          Spark's LibSVM DataFrameReader loads a DataFrame already suitable for training and inference.

          Here, we load into a DataFrame in the SparkSession running on the local Notebook Instance, but you can 
          connect your Notebook Instance to a remote Spark cluster for heavier workloads. Starting from EMR 5.11.0, 
          SageMaker Spark is pre-installed on EMR Spark clusters. For more on connecting your SageMaker Notebook 
          Instance to a remote EMR cluster, please see this blog post.

      >>> import boto3

      >>> cn_regions = ["cn-north-1", "cn-northwest-1"]
      >>> region = boto3.Session().region_name
      >>> endpoint_domain = "com.cn" if region in cn_regions else "com"
      >>> spark._jsc.hadoopConfiguration().set(
      >>>     "fs.s3a.endpoint", "s3.{}.amazonaws.{}".format(region, endpoint_domain)
      >>> )

      >>> trainingData = (
      >>>     spark.read.format("libsvm")
      >>>     .option("numFeatures", "784")
      >>>     .option("vectorType", "dense")
      >>>     .load("s3a://sagemaker-sample-data-{}/spark/mnist/train/".format(region))
      >>> )

      >>> testData = (
      >>>     spark.read.format("libsvm")
      >>>     .option("numFeatures", "784")
      >>>     .option("vectorType", "dense")
      >>>     .load("s3a://sagemaker-sample-data-{}/spark/mnist/test/".format(region))
      >>> )

      >>> trainingData.show()

          Training and Hosting a Model

          Now we create an XGBoostSageMakerEstimator, which uses the XGBoost Amazon SageMaker Algorithm to train 
          on our input data, and uses the XGBoost Amazon SageMaker model image to host our model.

          Calling fit() on this estimator will train our model on Amazon SageMaker, and then create an Amazon 
          SageMaker Endpoint to host our model.

          We can then use the SageMakerModel returned by this call to fit() to transform Dataframes using our hosted 
          model.

          The following cell runs a training job and creates an endpoint to host the resulting model, so this cell 
          can take up to twenty minutes to complete.

      >>> import random
      >>> from sagemaker_pyspark import IAMRole, S3DataPath
      >>> from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator

      >>> xgboost_estimator = XGBoostSageMakerEstimator(
      >>>     sagemakerRole=IAMRole(role),
      >>>     trainingInstanceType="ml.m4.xlarge",
      >>>     trainingInstanceCount=1,
      >>>     endpointInstanceType="ml.m4.xlarge",
      >>>     endpointInitialInstanceCount=1,
      >>> )

      >>> xgboost_estimator.setEta(0.2)
      >>> xgboost_estimator.setGamma(4)
      >>> xgboost_estimator.setMinChildWeight(6)
      >>> xgboost_estimator.setSilent(0)
      >>> xgboost_estimator.setObjective("multi:softmax")
      >>> xgboost_estimator.setNumClasses(10)
      >>> xgboost_estimator.setNumRound(10)

      >>> # train
      >>> model = xgboost_estimator.fit(trainingData)


          Inference

          Now we transform our DataFrame. To do this, we serialize each row's "features" Vector of Doubles into 
          LibSVM format for inference against the Amazon SageMaker Endpoint. We deserialize the CSV responses 
          from the XGBoost model back into our DataFrame. This serialization and deserialization is handled 
          automatically by the transform() method:

      >>> transformedData = model.transform(testData)

      >>> transformedData.show()

          How well did the algorithm perform? Let us display the digits corresponding to each of the labels 
          and manually inspect the results:

      >>> from pyspark.sql.types import DoubleType
      >>> import matplotlib.pyplot as plt
      >>> import numpy as np

      >>> # helper function to display a digit
      >>> def show_digit(img, caption="", xlabel="", subplot=None):
      >>>     if subplot == None:
      >>>         _, (subplot) = plt.subplots(1, 1)
      >>>     imgr = img.reshape((28, 28))
      >>>     subplot.axes.get_xaxis().set_ticks([])
      >>>     subplot.axes.get_yaxis().set_ticks([])
      >>>     plt.title(caption)
      >>>     plt.xlabel(xlabel)
      >>>     subplot.imshow(imgr, cmap="gray")


      >>> images = np.array(transformedData.select("features").cache().take(250))
      >>> clusters = transformedData.select("prediction").cache().take(250)

      >>> for cluster in range(10):
      >>>     print("\n\n\nCluster {}:".format(int(cluster)))
      >>>     digits = [img for l, img in zip(clusters, images) if int(l.prediction) == cluster]
      >>>     height = ((len(digits) - 1) // 5) + 1
      >>>     width = 5
      >>>     plt.rcParams["figure.figsize"] = (width, height)
      >>>     _, subplots = plt.subplots(height, width)
      >>>     subplots = np.ndarray.flatten(subplots)
      >>>     for subplot, image in zip(subplots, digits):
      >>>         show_digit(image, subplot=subplot)
      >>>     for subplot in subplots[len(digits) :]:
      >>>         subplot.axis("off")

      >>>     plt.show()

          Since we don't need to make any more inferences, now we delete the endpoint:

      >>> # Delete the endpoint

      >>> from sagemaker_pyspark import SageMakerResourceCleanup

      >>> resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)
      >>> resource_cleanup.deleteResources(model.getCreatedResources())



  SageMaker PySpark Example notebooks:

    SageMaker Notebook instance -> Jupyter -> SageMaker Examples -> SageMaker Spark ->

       pyspark_mnist_custom_estimator.ipynb
       pyspark_mnist_kmeans.ipynb
       pyspark_mnist_pca_kmeans.ipynb
       pyspark_mnist_pca_mllib_kmeans.ipynb
       pyspark_mnist_xgboost.ipynb
  

------------------------------------------------------
4.7 Debugging ML Models Using Amazon SageMaker Debugger 


  Amazon SageMaker -> Developer Guide -> List of Debugger built-in rules
    https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html

  SageMaker Debugger Features
    - debug model parameters like weights, gradients, and biases of our training job.
   - it helps to monitor and profile the training jobs by saving the internal model state at 
     regular intervals.
   - Detects non-convergence conditions quickly
      - Debugger helps you detect these non-convergent conditions quickly.
   convergence point.
     - During the training process, it is important for the model's performance metrics like loss function, 
       and accuracy, to stabilize and reach an optimal value after a set of number of training iterations.
   - optimizes cost by reducing training time [via detecting non-convergence, etc]
   - handles anomalies using integrated tools
      - send alerts when anomalies occur during training.
      - offers built-in rules to identify computation issues, system bottlenecks, and over-utilization of resources.
   - supports multiple ML frameworks and algorithms 
      - TensorFlow, PyTorch, MXNet, and XGBoost.

    Note: Missing: Please find the resources section for a link where you can find all the built-in rules 
       offered by SageMaker Debugger.

   Debugger Architecture
     - modifying the training script with the `sageMaker-debugger` option.
     - configure training jobs with SageMaker Debugger
        - by modifying the SageMaker estimator API.
        - Debugger rules are added from the list of debugger built-in rules.
      - The training process is started and monitored.
      - send alerts and take action
      - you can set up your own actions using Amazon CloudWatch or SNS.

  SageMaker Debugger best practices
   - Saving the training data output to Amazon S3 bucket
     - helps you analyze the logs from the training script and derive meaningful information.
   - Use SageMaker Debugger built-in rules to analyze metrics and tensors collected during the training 
     process with no additional cost.
   - Automate actions based on built-in rule status
   - Using the debugger, we can monitor the hardware resource utilization of the EC2 instances
      - This monitoring will give you insights if you need to switch to a larger or smaller instances,
   - Deep Dive into your training data using the SMDebug Client library
     and provide the visualization tools like line charts and HeatMap to track the system utilization.


-----
Example notebooks and code samples to configure Debugger hook
    https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-save-tensors.html

  Save tensors using Debugger built-in collections
    - You can use built-in collections of tensors using the CollectionConfig API and save them using the DebuggerHookConfig API. 
    - The following example shows how to use the default settings of Debugger hook configurations to construct a SageMaker AI TensorFlow estimator. 
    - You can also utilize this for MXNet, PyTorch, and XGBoost estimators.

    Code: SageMaker Debugger python example code
      >>> import sagemaker
      >>> from sagemaker.tensorflow import TensorFlow
      >>> from sagemaker.debugger import DebuggerHookConfig, CollectionConfig
      >>> 
      >>> # use Debugger CollectionConfig to call built-in collections
      >>> collection_configs=[
      >>>         CollectionConfig(name="weights"),
      >>>         CollectionConfig(name="gradients"),
      >>>         CollectionConfig(name="losses"),
      >>>         CollectionConfig(name="biases")
      >>>     ]
      >>> 
      >>> # configure Debugger hook
      >>> # set a target S3 bucket as you want
      >>> sagemaker_session=sagemaker.Session()
      >>> BUCKET_NAME=sagemaker_session.default_bucket()
      >>> LOCATION_IN_BUCKET='debugger-built-in-collections-hook'
      >>> 
      >>> hook_config=DebuggerHookConfig(
      >>>     s3_output_path='s3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}'.
      >>>                     format(BUCKET_NAME=BUCKET_NAME, 
      >>>                            LOCATION_IN_BUCKET=LOCATION_IN_BUCKET),
      >>>     collection_configs=collection_configs
      >>> )
      >>> 
      >>> # construct a SageMaker TensorFlow estimator
      >>> sagemaker_estimator=TensorFlow(
      >>>     entry_point='directory/to/your_training_script.py',
      >>>     role=sm.get_execution_role(),
      >>>     base_job_name='debugger-demo-job',
      >>>     instance_count=1,
      >>>     instance_type="ml.p3.2xlarge",
      >>>     framework_version="2.9.0",
      >>>     py_version="py39",
      >>>     
      >>>     # debugger-specific hook argument below
      >>>     debugger_hook_config=hook_config
      >>> )
      >>> 
      >>> sagemaker_estimator.fit()
-----

------------------------------------------------------
4.8 Using Spot Instances for XGBoost Training 


  Using spot instances considerations
    - AWS can terminate the instances with short notice, typically two minutes,
    - Since the training process, which uses spot instances, can be interrupted, the jobs can take 
      longer time to complete,
    - training jobs must be configured to use a checkpoint
       - SageMaker will promptly copy the checkpoint data to an S3 bucket and will restart from this 
         point instead of starting from the beginning.
    - The training script must be robust enough to handle such interruptions and restarts.
    - Setup Cloudwatch events to be notified once a job is terminated or interrupted.

   Using spot instance important concepts
     billable time.
       - absolute wall clock time.
       - the billable time is multiplied by instance count.
     training time.
       - duration during which the model is being trained actively.
       - In other words, this is the time computing resources are actively used to update weights 
         and model parameters.

   Spot Savings Formula
     (1 - (BillableTimeInSecond/TrainingTimeInSec) * 100%)


  Configure Training Job with Spot Instances


  from sagemaker import Estimator

    estimator parameters to configure:

      use_spot_instances=True
        - setting it to true will allow the training job to use spot instances
      max_run 
        - the maximum duration in seconds the training job can run.
      max_wait 
        - a maximum duration in seconds the training job is allowed to run, including the time spent 
          waiting for spot instances.
        - must be set when the use_spot_instances is set to true and must be greater than the max_run value.
      checkpoint_s3_uri
        -  S3 location where checkpoints will be saved.
      checkpoint_local_path 
        – the local path where the model saves the checkpoints periodically in a training container. 
        - The default path is set to '/opt/ml/checkpoints'

        estimator = Estimator(
            image_uri='[your-container-uri]',
            role='[your-execution-role]',
            instance_count=1,
            instance_type='ml.m4.xlarge',  # A common general-purpose instance
            use_spot_instances=True,
            max_run=300,      # Maximum training time in seconds - 5 mins
            max_wait=600,     # Maximum time to wait for spot capacity in seconds - 10 mins
            output_path='s3://your-bucket/output'
            # Parameters required to enable checkpointing
            checkpoint_s3_uri=checkpoint_s3_bucket,
            checkpoint_local_path=checkpoint_local_path
        )

------------------------------------------------------
4.9 Distributed Training In Amazon Sagemaker 


  SageMaker Distributed training 
    - distributed training is all about running complex models with large datasets that require 
      substantial computational resources across multiple instances.

  SageMaker Distributed training Major Challenges
    Large dataset
      Addressed using 'Data Parallelism' stragety
        - the training dataset is split across multiple GPUs with each GPU instance containing a model 
          replica that handles different batches of the training data.
       
    complex model
      Addressed using 'Model Parallelism' strategy
        - the model is partitioned across multiple GPUs in a single cluster.
        - Each GPU instance will carry a subset of the model through which the same set of training 
          data flows, and the transformations are performed.

  Data Parallelism vs Model Paralelism
    Data Parallelism
      - SageMaker offers SageMaker Distributed Data Parallelism (SMDDP) Library
      - used to improve the compute performance and perform optimized node-to-node communications by 
        fully leveraging AWS network infrastructure.
      - SMDDP Library is supported by PyTorch, PyTorch Lighting, and Hugging Face Transformers frameworks
      - SMDDP library is supported by ml.p3dn.24xlarge, ml.p4d.24xlarge, ml.p4de.24xlarge instance types.
    
    Model Parallelism
      - SageMaker also offers SageMaker Model Parallelism Library (SMP v2)
      - used to accelerate the training and fine-tuning of large and complex models.
      - SMP v2 Library is supported by PyTorch, PyTorch Lighting, Hugging Face Transformers,
        and Hugging Face Accelerate frameworks.
      - SMP v2 is supported by ml.p4d.24xlarge, ml.p4de.24xlarge, and ml.p5.48xlarge instance types.


   Distributed training Best Practices
     selecting the instance types
       - Use ml.p3 and ml.p4 instances for deep learning tasks to benefit from high computation power of GPUs.
     selecting instance count
       - based on the model complexity and dataset size by balancing cost and training time.
     distributed training libraries
       - utilize distributed training libraries offered by SageMaker to optimize training for large models 
         and datasets.
     hyperparameter tuning
       - Use SageMaker's hyperparameter tuning feature to automatically search for the best hyperparameters 
         to improve model accuracy and performance.
     Monitor with CloudWatch
       - Finally, use Amazon CloudWatch to monitor resource utilization, log training metrics, and set 
         alarms to detect unusual behavior.


   AWS Accelerated Computing instances

     p4d & p4de
       - provide high performance for machine learning training and high performance computing in the cloud.
       Features:
       - 3.0 GHz 2nd Generation Intel Xeon Scalable processors (Cascade Lake P-8275CL)
       - Up to 8 NVIDIA A100 Tensor Core GPUs
       - only have 24xlarge instances (8 GPUs, 96 vCPUs, 1152 GiB)
       
     p5, p5e, & p5en
       - the latest generation of GPU-based instances and provide the highest performance in Amazon EC2 for 
         deep learning and high performance computing (HPC).
       Features:
       - Intel Sapphire Rapids CPU and PCIe Gen5 between the CPU and GPU in P5en instances; 
         3rd Gen AMD EPYC processors (AMD EPYC 7R13) and PCIe Gen4 between the CPU and GPU in P5 and P5e instances.
       - Up to 8 NVIDIA H100 (in P5) or H200 (in P5e and P5en) Tensor Core GPUs  
       - only have 48xlarge instances (8 H100/H200 GPUs, 192 vCPUs, 2 TiB)

    Inf1
      - instances are built from the ground up to support machine learning inference applications.
      Features:
      - Up to 16 AWS Inferentia Chips
      - Supported by AWS Neuron SDK
      - High frequency 2nd Generation Intel Xeon Scalable processors (Cascade Lake P-8259L)
      - inf1.xlarge (1 inf1, 4 vCPU, 8 GiB) to inf1.24xlarge (16 inf1, 96 vCPU, 192 GiB)

    Inf2
      - purpose built for deep learning inference. 
      - They deliver high performance at the lowest cost in Amazon EC2 for generative artificial intelligence models, 
        including large language models and vision transformers. 
      - Inf2 instances are powered by AWS Inferentia2. These new instances offer 3x higher compute performance, 
        4x higher accelerator memory, up to 4x higher throughput, and up to 10x lower latency compared to Inf1 instances
      Features:
      - Up to 12 AWS Inferentia2 chips
      - Supported by AWS Neuron SDK
      - Dual AMD EPYC processors (AMD EPYC 7R13)
      - inf2.xlarge (1 inf1, 4 vCPU, 8 GiB) to inf2.48xlarge (12 inf1, 192 vCPU, 768 GiB)
    
------------------------------------------------------
4.10 Retraining a ML Model Using Amazon SageMaker Canvas 

  Amazon SageMaker Canvas.
    - a drag and drop UI designed for non-technical users to quickly create machine learning predictions
    - In other words, it is a no-code ML service to build, train, and deploy ML models.

  Amazon SageMaker Canvas key features.
    - import data from CSV files, databases are data lakes, and export predictions on model results back 
      to the same source.
    - visually explore and prepare data 
       - including tasks like cleaning, identifying missing values, detecting outliers, and joining data sets for lookup.
    - automatically selects the best ML algorithm.
       - Based on the imported data and the select target column
    - automatically tune the hyperparameters to optimize model performance
    - Model can be easily shared with data scientists for further refinement in SageMaker Studio.
    - continuously trained and retrained the model whenever newer version of the data sets become available.

  SageMaker Canvas demo - train and retrain a model

    AWS SageMaker -> <left tab> Canvas -> Open Canvas 
       -> <left tab> Datasets
       -> select "canvas-sample-retial-electronics-forecasting.csv
           6 features x 40,000 
       -> <upper right> Create Model -> model name: Canvas model,
          -> "predictive analysis" <- canvas automatically selected based on data
          -> Create
          <under "Buid">
          target column: price
            Model type: Time Series forecasting <- model type was automatically selected based on 'target'
         -> configure model: 
             Item ID Column: item_id
             Group Column: Product_category
             time stamp column: time_stamp
             months [to forecast]: 3
             -> Save
             issue: Quick build is disable due to 'demand' column invalid name
             -> demand: product_demand
             -> Quick Build
               - Expected Build time: 14 - 20 min
               - After model build completed, 'Model Status' reports various model metrics
               - Column impact: reports percentage prediction impact for each column
             -> Predict
                choice: Batch prediction, Single Prediction


            How to retrain if you have fresh data
            -> select "canvas-sample-retial-electronics-forecasting.csv
               -> <top right> update dataset 
                  options: Manual update   or Automatic update (at preset intervals)

   -> log out of Canvas as SageMaker bills you for the duration of the session.

------------------------------------------------------
4.11 Linear Regression Performed Using Amazon SageMaker

Saved files:
    completed python jupyter notebook:
      train.ipynb
    extracted python code from jupyter notebook:
      train.py
    html view from completed jupyter notebook:
      train.html

About this lab

Imagine you are the data engineer at your company and your company just selected AWS as the preferred cloud provider. 
You have been asked to train an ML model using linear learner algorithm. In this lab, you will fetch the iris data and 
use that as the input dataset. Once the data is split, the data is uploaded to S3 bucket. Then the Sagemaker estimator 
is configured before initiating the training process.

Learning objectives
  - Launch SageMaker Notebook
  - Install dependencies and import the libraries
  - Download the data and upload them to S3 bucket
  - Set up training and validation data
  - Train the model

--------------------------
Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. If you are prompted to select a kernel, please choose conda_tensorflow2_p310.
    In the search bar, type "SageMaker" to search for the SageMaker service. Click on the Amazon SageMaker result to go directly to the SageMaker service.
    Click on the Notebooks link under the Applications and IDEs section to view the notebook provided by the lab.
    Check if the notebook is marked as** InService.** If so, click the Open Jupyter link under** Actions.**
    Click on the train.ipnyb file.

Load the Dataset and Split the Data

Note: If this is your first time running a notebook, each cell contains Python commands you can run independently. Also, a * inside the square braces indicates the code is running, and you will see a number once the execution is complete.

    Click the first cell that installs boto3 and sagemaker. Use the Run button at the top to execute the code.
    Click the next cell and the Run button to import the required Python libraries, initialize the SageMaker session and define the output bucket. This cell lists all the libraries that will be imported from Pandas and sklearn. It may take a few minutes to complete the operation.
    Copy the following code snippet and paste into the next cell, click Run to perform this operation. This cell fetches the IAM role using the get_execution_role function.

    role = get_execution_role()

Download the data and upload them to the S3 bucket

    Select the next cell and click Run to execute. This cell fetches the iris dataset from the sklearn library. After loading the dataset, we create feature variables (X) and target variables (Y).

    Copy the following code snippet and paste into the next cell. Select the cell and click Run to split the data for training and testing purposes.

    train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)

    Select the next cell and click Run to convert the training and validation data to CSV format.

    Select the next cell and click Run to upload the training CSV file to the S3 bucket using the upload_file function.

    Copy the following code snippet and paste into the next cell. Click Run to execute the code that will upload the validation.csv file.

    s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')

Setup Training and Validation Data

    Select the next cell and click Run to initialize the path from which training data and validation data will be read. These values will be used to create the input parameter for the estimator object.
    Select the next cell and click Run. This code uses the TrainingInput function and creates the train_data input parameter. Please pay attention; we are passing the path from which the training data will be read.
    Copy the following code snippet and paste into the next cell, and click Run to create an input parameter for validation_data,

    validation_data = sagemaker.inputs.TrainingInput(
        s3_validation_data,
        distribution="FullyReplicated",
        content_type="text/csv",
        s3_data_type="S3Prefix",
        record_wrapping=None,
        compression=None,
    )

Fetch the Algorithm and Train the Model

In this section, we will see how to train the linear learner model.

    Select the first cell and click Run. This cell fetches the linear-learner algorithm specific to the region.
    Select the next cell and click Run to initialize the Estimator object. Please make sure you choose the correct instance type and instance count.
    Copy the following code snippet and paste into the next cell, click Run to configure the hyperparameters manually.

    linear.set_hyperparameters(
        feature_dim=4,  # Adjust this to match your feature dimension
        predictor_type='regressor',  # Use 'classifier' for classification
        mini_batch_size=20
    )

    Select the final cell and click Run. This cell uses the fit function to initiate the training process.

Once the training process starts, you may switch to SageMaker console and monitor the training process.
--------------------------

    code:  Linear Learner regressor lab

      >>> # Linear Regression Performed Using Amazon SageMaker

      >>> # # Introduction
      >>> # 
      >>> # In this lab, you will learn how to import the iris dataset, split it into training and validation data, upload 
      >>> # them to S3 bucket, fetch the linear learner algorithm, initialize the estimator object, set the hyperparameters 
      >>> # and train the model.

      >>> # # How to Use This Lab
      >>> # 
      >>> # Most of the code is provided for you in this lab as our solution to the tasks presented. Some of the cells are left 
      >>> # empty with a #TODO header and its your turn to fill in the empty code. You can always use our lab guide if you are stuck.

      >>> # # 1) Install dependencies and import the required libraries

      >>> # Install Sagemaker
      >>> get_ipython().system('pip install boto3 sagemaker')


      >>> # 1. We will use the iris dataset as our input data. 
      >>> # 2. The S3 bucket that you want to use for training data must be within the same region as the Notebook Instance.
      >>> # 3. The IAM role is used to provide training and hosting access to your data. See the documentation for how to create these. 
      >>> # Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp 
      >>> # with an appropriate full IAM role arn string(s).

      >>> import sagemaker
      >>> import boto3
      >>> from sagemaker import get_execution_role
      >>> from sagemaker.inputs import TrainingInput
      >>> import pandas as pd
      >>> from sklearn.datasets import load_iris
      >>> from sklearn.model_selection import train_test_split

      >>> # Initialize the SageMaker session
      >>> sagemaker_session = sagemaker.Session()

      >>> # Define the S3 bucket and prefix to store data
      >>> output_bucket = sagemaker.Session().default_bucket()
      >>> output_prefix = 'sagemaker/linear-learner'


      >>> #TODO: Fetch the IAM role using the get_execution_role function and assign the value to a variable `role`
      >>> role = get_execution_role()


      >>> # # 2) Download the data and upload them to S3 bucket

      >>> # 1. load_iris function is used to download the input data
      >>> # 2. The data is split into training and validation data in the ratio of 80 - 20
      >>> # 3. The data is saved under 'train.csv' and 'validation.csv'

      >>> # Load the Iris dataset
      >>> iris = load_iris()
      >>> X = iris.data
      >>> y = iris.target

      >>> # Convert to DataFrame for easier manipulation
      >>> data = pd.DataFrame(X, columns=iris.feature_names)
      >>> data['target'] = y


      >>> data.head()


      >>> #TODO: Use the `train_test_split` function and split the data in a 80 - 20 ratio. 
      >>> #TODO: Assign the values to variables `train_data` and `validation_data` 
      >>> train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)


      >>> # Save to CSV
      >>> train_data.to_csv('train.csv', index=False, header=False)
      >>> validation_data.to_csv('validation.csv', index=False, header=False)


      >>> # Let's use the upload_file function and upload the .csv files to the S3 buckets

      >>> # Upload data to S3
      >>> s3 = boto3.client('s3')
      >>> s3.upload_file('train.csv', output_bucket, f'{output_prefix}/train/train.csv')


      >>> #TODO: Using the strategy we followed to upload the training data, as shown above, please upload the validation 
      >>> # data to the output bucket.
      >>> s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')


      >>> # # 3) Set up training and validation data

      >>> # Create three separate variables that are dynamically constructed, which will be used as one of the input 
      >>> # parameters while generating training inputs.

      >>> # creating the inputs for the fit() function with the training and validation location
      >>> s3_train_data = f"s3://{output_bucket}/{output_prefix}/train"
      >>> print(f"training files will be taken from: {s3_train_data}")
      >>> s3_validation_data = f"s3://{output_bucket}/{output_prefix}/validation"
      >>> print(f"validation files will be taken from: {s3_validation_data}")
      >>> output_location = f"s3://{output_bucket}/{output_prefix}/output"
      >>> print(f"training artifacts output location: {output_location}")


      >>> # Let's create the sagemaker.session.s3_input objects from our data channels. Note that we are using the 
      >>> # content_type as text/csv. We use two channels here, one for training and the second for validation.

      >>> # generating the session.s3_input() format for fit() accepted by the sdk
      >>> train_data = sagemaker.inputs.TrainingInput(
      >>>     s3_train_data,
      >>>     distribution="FullyReplicated",
      >>>     content_type="text/csv",
      >>>     s3_data_type="S3Prefix",
      >>>     record_wrapping=None,
      >>>     compression=None,
      >>> )


      >>> #TODO: Following the same strategy shown above, please set up a training input for validation data.
      >>> #TODO: Name it as `validation_data`
      >>> validation_data = sagemaker.inputs.TrainingInput(
      >>>     s3_validation_data,
      >>>     distribution="FullyReplicated",
      >>>     content_type="text/csv",
      >>>     s3_data_type="S3Prefix",
      >>>     record_wrapping=None,
      >>>     compression=None,
      >>> )


      >>> # # 4) Fetch the algorithm and train the model

      >>> # Let's retrieve the image for the Linear Learner Algorithm according to the region.

      >>> # Fetch the linear learner image according to the region
      >>> from sagemaker.image_uris import retrieve

      >>> container = retrieve("linear-learner", boto3.Session().region_name, version="1")
      >>> print(container)
      >>> deploy_amt_model = True


      >>> # Then, we create an estimator from the SageMaker Python SDK using the Linear Learner container image 
      >>> # and set the training parameters.

      >>> %%time
      >>> import boto3
      >>> import sagemaker
      >>> from time import gmtime, strftime

      >>> sess = sagemaker.Session()

      >>> job_name = "linear-learner-iris-regression-" + strftime("%Y%m%d-%H-%M-%S", gmtime())
      >>> print("Training job", job_name)

      >>> linear = sagemaker.estimator.Estimator(
      >>>     container,
      >>>     role,
      >>>     instance_count=1,
      >>>     instance_type="ml.m5.large",
      >>>     output_path=output_location,
      >>>     sagemaker_session=sagemaker_session,
      >>> )


      >>> # The hyperparameters are manually configured

      >>> # TODO: Use the set_hyperparameters function and set the following hyperparameters on linear learner
      >>> # feature_dim=4, predictor_type='regressor', mini_batch_size=20
      >>> linear.set_hyperparameters(
      >>>     feature_dim=4,  # Adjust this to match your feature dimension
      >>>     predictor_type='regressor',  # Use 'classifier' for classification
      >>>     mini_batch_size=20
      >>> )


      >>> # 1. The following cell will train the algorithm. Training the algorithm involves a few steps. First, the instances 
      >>> #    that we requested while creating the Estimator classes are provisioned and set up with the appropriate libraries. 
      >>> #    Then, the data from our channels is downloaded into the instance. Once this is done, the training job begins. 
      >>> #    The provisioning and data downloading will take time, depending on the size of the data. Therefore, it might be 
      >>> #    a few minutes before we start getting data logs for our training jobs.
      >>> # 2. The log will print the objective metric details.
      >>> # 3. The training time takes between 4 and 6 minutes.

      >>> %%time
      >>> linear.fit(inputs={"train": train_data, "validation": validation_data}, job_name=job_name)


--------------------------
4.12 Train ML Models Review 

------------------------------------------------------


   

                             |---> Split data between training and validation
                             |
                             |---> Understand optimization techniques for ML training
    task statement 3.3 ------- 
    (Train ML model)         |---> choose appropriate compute resources
                             |
                             |---> update and retrain models


  split data between training and validation.
    - A dataset is split into training data, testing data, and validation data.
      - The training data is used to train the model.
      - The validation data (optional) used to measure the model performance during the training and 
        to tune any hyperparameters of the model.
      - testing data is used to determine how well the model generalizes on unseen data.
    cross-validation
      - the process of validating the model against fresh, unseen data is called cross-validation.

    Common cross-validation techniques.
      - K-fold cross-validation,
      - stratified K-fold cross-validation,
      - leave-one-out cross-validation.

   understand optimization techniques for ML training.
    
    loss fucntion
      - the loss function is the difference between predicted output and the actual output.
      - The focus of an optimization function is to minimize the loss function.
      - Effective optimization techniques ensure that the model converge quickly,
    Gradient Decent algorithm
      - A commonly used optimization technique is gradient decent algorithm.
      - A common challenge with gradient decent technique is that this algorithm may get stuck in 
        local minima during the training process.
     Two common approaches used to overcome this problem 
       Stochastic gradient descent.
         - A common approach used to overcome getting stuck at local minimum 
         - randomly sampling a subset of the data points at each iteration
         - then using the gradients of those data points to update the model parameters.
         - In other words, by introducing the noise in the gradient calculation, stochastic gradient descent escapes the problem of local minimum.
         - the convergence may oscillate around the minimum rather than converging smoothly.
       Batch gradient descent.
         - batch gradient descent uses the entire dataset in calculating gradient.
         - The parameters are updated after going through the entire dataset.
         - can achieve stable convergence, and the gradient is accurate since all the data points are considered at once.
         - drawbacK: needs to load entire dataset into memory
           - may be infeasible for large datasets.

  choose appropriate compute resources.
    Use CPUs for:
      - simpler classification and regression problems
      - smaller models with fewer parameters and smaller footprint
      - models with low latency requirements
      -  models budget constraints
    Use GPUs for:
      - complex models involving deep learning networks,
      - complex models with large number of parameters needing significant memory
      - models with high throughput requirements 
      - Models with significant performance requirements

    Distributed training
      - for training complex models with large datasets that require substantial computational resources 
        across multiple instances.
      - SageMaker recommends 'data parallelism' and 'model parallelism' to overcome the challenges in distributed training.
      - SageMaker provides prebuilt docker images that include Apache Spark and other dependencies needed to run distributed 
        data processing jobs.

  Update and retrain model
    SageMaker Canvas
    - SageMaker Canvas is a drag and drop UI designed for non-technical users like business analysts to quickly create 
      machine learning predictions.
    - SageMaker Canvas offer the features either to manually update or automatically schedule the update at a preset interval.  
    - Retraining a model with the latest dataset keeps it up to date and makes more accurate predictions.

------------------------------------------------------
4.13 Train ML Models - Quiz

-----------------
Question 1
You are working on developing a machine learning (ML) model and you must ensure that the model generalizes well with 
the unseen new data. How will you accomplish this?

  Split the input dataset and use the training data.           <--- Incorrect Answer

  Split the input dataset and use the testing data.             <--- Correct Answer

  Split the input dataset and use the validation data.

  Split the input dataset and use the generalization data.

Sorry!
  Training data trains the model.

Correct Answer
  Testing data determines how well the model generalizes with unseen data.

------
-----------------
Question 2

As the chief architect of a company, you want to develop a prototype of a machine learning (ML) model. However, all your 
ML engineers are busy with their current assignments, and you wanted to leverage the business analysts. Which SageMaker 
service can you leverage for this purpose?

  Amazon CloudWatch

  Amazon SageMaker Canvas                                   <--- Correct Answer

  Amazon SageMaker Studio

  Amazon SageMaker Notebook

Good work!
  This drag-and-drop UI is designed for business analysts to develop an ML model.

------
Introduction: AWS’s Machine Learning Suite — Empowering Every User
  https://medium.com/@salmananwaar1127/aws-sagemaker-studio-vs-aws-canvas-choosing-the-right-tool-for-your-ml-needs-a27d0d85bc74

  AWS SageMaker Studio and AWS Canvas are two of these powerful offerings, each designed to meet specific user needs 
    within the ML spectrum. 

  SageMaker Studio, 
    - a fully integrated development environment (IDE) for ML, is targeted at data scientists and ML engineers who require a 
      complete set of tools for end-to-end ML workflows. 
    - It’s built for those with technical expertise who need to dive deep into model customization, hyperparameter tuning, and deployment. 

   SageMaker Canvas 
     - a no-code platform that empowers business analysts, marketers, and other domain experts to leverage ML without 
       needing any coding experience. 
     - Canvas uses automated ML (AutoML) techniques to simplify model creation and deployment, making it accessible 
       to a broader audience.

-----------------
Question 3

Imagine you are training an ML model using a custom script. Which one of the following is a mandatory task while configuring 
the SageMaker estimator object?

  Set the parameter "custom_script."

  Set the parameter "Script_mode."                              <--- Correct Answer

  Set the parameter "instance_type."                            <--- Incorrect Answer

  Set the parameter "instance_count."

Sorry!
  This parameter is used to specify the instance type.

Correct Answer
  Setting "script_mode" to true is mandatory for training with a custom script.

------
-----------------
Question 4

You are working on a classification problem, and the dataset is highly imbalanced. Which strategy will you use for cross-validation?

  K-fold cross-validation

  Stratified cross-validation                                  <--- Correct Answer

  Rolling cross-validation

  Leave-one-out cross-validation

Good work!
  An imbalanced dataset has uneven class distribution, and stratified cross-validation is a good strategy

------
-----------------
Question 5

You are developing a complex machine learning (ML) model, and the dataset is in terabytes. What will be your strategy in 
training this complex model?

  Use general-purpose EC2 instances.

  Use storage-optimized EC2 instances.

  Use spot instances for training.

  Use distributed training.                                     <--- Correct Answer

Good work!
  Distributed training is an excellent option for a complex model with large datasets.

------
-----------------
Question 6

You are developing a complex model and need to monitor various common conditions critical to the success of a training job. 
How will you achieve this?

  Creating custom rules using the Debugger Rule APIs              <--- Incorrect Answer

  Using built-in rules provided by SageMaker Debugger             <--- Correct Answer

  Using SageMaker Canvas

  Using SageMaker Studio

Sorry!
  We don't need to create custom rules to monitor common conditions.

Correct Answer
  SageMaker's built-in rules allow us to monitor various common conditions critical to the success of a training job

------
-----------------
Question 7

You are currently training a machine learning (ML) model, and the input dataset is multiple terabytes in size. What 
distribution training strategy will you use?

  Warm pools

  Spot instances

  Data parallelism                                                 <--- Correct Answer

  Model parallelism

Good work!
  Data parallelism is used to split a large dataset. Each instance contains the same model replica and handles different 
    batches of training data.

------
-----------------
Question 8

You are currently developing a machine learning (ML) model and have received strict instructions to limit the training cost 
within the budget. What strategy will you use?

  Use on-demand instances for model training.

  Use distributed training for model training

  Use spot instances for model training.                               <--- Correct Answer

  Use the instances already preselected by your team lead.

Good work!
  Spot instances are a cost-effective solution for training a model.

------
-----------------
Question 9

You are a machine learning (ML) engineer training a model using spot instances. What is a big challenge you must be aware of?

  Spot instances are expensive.

  The training jobs can take longer to complete.                          <--- Correct Answer

  You need to sign a contract with AWS to use spot instances.

  Spot instances often crash and cannot be relied on to train a model.

Good work!
  Since spot instances can be terminated, the training jobs can take longer.

------
-----------------
Question 10

You are a data engineer assigned to develop a machine learning (ML) model. You will be working on a classification problem 
and are told that the company has strict budget constraints. Which hardware will you select to train the model?

  GPU

  The resource that the algorithm demands

  CPU                                                                     <--- Correct Answer

  TPU

Good work!
  CPU is a good option for projects with budget constraints.

------
-----------------

Chapter 5  Perform Hyperparameter Optimization 

------------------------------------------------------
5.1 Understanding Hyperparameter Tuning 

Saved files:
    completed python jupyter notebook:
      demos.ipynb
    extracted python code from jupyter notebook:
      demos.py
    html view from completed jupyter notebook:
      demos.html

  Resource:
    Perform Hyperparameter Optimization - Demos
      downloaded to:
      labs/lesson5_1_hyperparameter_optimization_demo/demos.ipynb


  parameters vs hyperparameters
    parameters
      - parameters are learned during the training process.
      - optimized during the training process.
      - specific to a model
      - The accuracy of these parameters directly affect the model's predictions.
    Hyperparameters
      - hyperparameters are set before the training process,
      - they define the training process and the model structure,
        - such as its complexity and the speed at which it learns.
      - Hyperparameters require tuning to find the best combination that yields optimal model performance.
        - start with an initial set and then based on the model's performance


  Parameters and Hyperparameters in Machine Learning and Deep Learning
    https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac
    Hyperparameters
     - Hyperparameters are parameters whose values control the learning process and determine the values of model 
       parameters that a learning algorithm ends up learning. 
     - you choose and set hyperparameter values that your learning algorithm will use before the training of the model even begins. 
     - In this light, hyperparameters are said to be external to the model because the model cannot change its values during learning/training.
     - Hyperparameters are used by the learning algorithm when it is learning but they are not part of the resulting model. 
    Hyperparameters examples:
      - Train-test split ratio
      - Learning rate in optimization algorithms (e.g. gradient descent)
      - Choice of optimization algorithm (e.g., gradient descent, stochastic gradient descent, or Adam optimizer)
      - Choice of activation function in a neural network (nn) layer (e.g. Sigmoid, ReLU, Tanh)
      - The choice of cost or loss function the model will use
      - Number of hidden layers in a nn
    Parameters
      - Parameters on the other hand are internal to the model. 
      - That is, they are learned or estimated purely from the data during training as the algorithm used tries to 
        learn the mapping between the input features and the labels or targets.
      - Model training typically starts with parameters being initialized to some values (random values or set to zeros). 
      - As training/learning progresses the initial values are updated using an optimization algorithm (e.g. gradient descent).
    Parameter Examples 
      - The coefficients (or weights) of linear and logistic regression models.
      - Weights and biases of a nn
      - The cluster centroids in clustering

  hyperparameter tuning approaches
    Grid Search.
      - Grid search is an exhaustive search method that evaluates every possible combination of the specified 
        hyperparameter values
      - Only categorical parameters are supported when using the grid search strategy.
      - The model is trained and evaluated for each combination of hyperparameters in the grid using strategies 
        like cross-validation.
      - The model is then predicted using evaluation metric like accuracy and F1 score.
      - the combination that yields a best performance according to the evaluation metric is selected as the optimal 
        set of hyperparameters.
    Random Search
      - samples hyperparameter values from specific distributions and evaluates a set number of random combinations.
      - more efficient than Grid Search
      - For each such combination, the model is trained and evaluated using strategies like cross-validation
      - the model with best performance metrics is selected
      - offers a better trade-off between exploration and computational cost as compared to grid search
    Bayesian Optimization
      - an advanced strategy for hyperparameter tuning that uses probabilistic model of the cost function
        or the objective function and using it to select the most promising hyperparameters to evaluate.
      - more efficient than grid search and random search, because it intelligently selects the next set of 
        hyperparameters to evaluate based on the previous results.
      - often finds better hyperparameters with fewer evaluations
      - this technique is more complex to understand and implement compared to grid search and random search.

    Amazon SageMaker Automatic Model Tuning (AMT) 
       - SageMaker alternatively refers to hyperparameter tuning as Automatic Model Tuning (AMT)
       - AMT uses the algorithm and ranges of hyperparameters that you specify.
       - It then chooses the hyperparameter values that creates a model that performs the best as measured by 
         a metric that you choose.


    Code: Perform Hyperparameter Optimization - Demos
          For Iris dataset, Uses sklearn GridSearchCV and RandomSearchCV to find the best DecisionTree hyperparameters

      >>> import numpy as np
      >>> from sklearn.datasets import load_iris
      >>> from sklearn.tree import DecisionTreeClassifier
      >>> from sklearn.model_selection import GridSearchCV, train_test_split
      >>> from sklearn.metrics import accuracy_score


      >>> data = load_iris()
      >>> X, y = data.data, data.target
      >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


      >>> #3 Define the model and setup the hyperparameter ranges

      >>> tree = DecisionTreeClassifier()

      >>> param_grid = {
      >>>     'max_depth': [None, 10, 20, 30, 40, 50],         # Maximum depth of the tree
      >>>     'min_samples_split': [2, 5, 10],                 # Minimum number of samples required to split an internal node
      >>>     'min_samples_leaf': [1, 2, 4],                   # Minimum number of samples required to be at a leaf node
      >>> }


      >>> # Step 4: Set up Grid Search with cross-validation
      >>> # n_jobs=-1 -> use all CPUs during the model process
      >>> grid_search = GridSearchCV(estimator=tree, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)


      >>> # Step 5: Fit the Grid Search to the training data
      >>> grid_search.fit(X_train, y_train)


      >>> # Step 6: Evaluate the best model
      >>> print("Best hyperparameters found:")
      >>> print(grid_search.best_params_)

      >>> best_dt = grid_search.best_estimator_
      >>> y_predict = best_dt.predict(X_test)
      >>> print("\nValidation Accuracy:", accuracy_score(y_test, y_predict))


      >>> from sklearn.model_selection import RandomizedSearchCV

      >>> # Step 7: Set up Randomized Search with cross-validation
          # n_iter -> controls the number of different combinations of hyperparameters that will be tried during the tuning process.
          #        -> only 6 x 3 x 3 =54 possible combinations of hyperparameters, 54 iteration will be tried instead 100
      >>> random_search = RandomizedSearchCV(estimator=tree, param_distributions=param_grid, n_iter=100, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)


      >>> # Step 8: Fit the Randomized Search to the training data
      >>> random_search.fit(X_train, y_train)


      >>> # Step 9: Evaluate the best model
      >>> print("Best hyperparameters found:")
      >>> print(random_search.best_params_)

      >>> best_dt = random_search.best_estimator_
      >>> y_pred = best_dt.predict(X_test)
      >>> print("\nValidation Accuracy:", accuracy_score(y_test, y_pred))


------------------------------------------------------
5.2 Understanding Overfitting and UnderFitting in Machine Learning


  Overfitting
    - model performs well against the training data, but the model performs very poorly with new unseen data.
    - an overfit model memorizes the training data, treats noise as a signal, and makes poor generalizations.

  Impacts of Overfitting
    - An overfit model will lead to poor predictive performance, which will have a direct impact on 
      user experience and brand reputation.

  Overfitting and model complexity
    - An overfit model tends to be highly complex.
       - typically has a large number of model parameters, which allows the model to fit the training data very closely.
       - as the model complexity increases, the ability to generalize decreases.


  Note: employee salary dataset not provided (features: name (first/last), age, gendere, department; target: salary)


  Overfitting Metrics
    Mean Absolute Error (MAE) 
      - also called a L1 loss.
      - MAE is the average of the absolute differences between the actual and predicted values.

            MAE = (1/n) ∑ |y_pred_i - y_i|    from i=1 to i=n 
                          where:   y_i:       actual value for instance i
                                   y_pred_i:  predicted value for instance i

      - overfitting when MAE has a low value against the training data and a high value against the test data
      - The MAE is commonly used to measure the accuracy of the regression models.
      Mean Absolute Error vs Accuracy
        - Both these metrics are inversely proportional
          - in the sense that a high MAE indicates a low accuracy and vice versa.

    R-squared.
      - R-squared is a statistical measure of how well the model approximates the actual data.
      mean model
        - A mean model is a simplest model you can build by calculating the mean of all the Y values.
        - the mean model will predict the same value for all the X values.
        Total Sum of Squares (TSS) [or SST ?]
            - total variance
            - the total sum of squares is calculated by summing the square of each error (actual vs mean).
                TSS =   ∑ (y_mean_i - y_i)**2    from i=1 to i=n 
                          where:   y_i:          actual value for instance i
                                   y_mean_i:     mean value for instance i
             
        Regression Sum of Squares (RSS) [or SSR ?]
            - total squared error
            - the regression sum of squares is calculated by summing the square of each error (actual vs predicted).
               RSS =    ∑ (y_regr_i - y_i)**2    from i=1 to i=n 
                          where:   y_i:          actual value for instance i
                                   y_regr_i:     regression model's predicted value for instance i
    R-squared.
             
        R**2 =  1  -  (RSS/TSS)

      - The regression sum of squares divided by the total sum of squares is a total variance that a 
        regression model wasn't able to explain,
      - subtracting this value from one is a fraction of variance that our regression model was able to explain.
      - r-squared will range from zero to one.
      - the closer to 1, the better the fit of the model to the data. For example, if the R-squared value 
        is 0.80, 80% of the model variance explains the variability of the data points.

     Using R-squared
       - the R-squared value may be high, but the predictive ability of the model could be low.
       - It is therefore important to note that R-square alone is not sufficient to detect overfitting
       - R-squared alone cannot be used to judge the goodness of a model.
       - It must be considered along with other metrics.

   overfitting/underfitting metrics
     bias 
       - high bias model is a simple model that doesn't capture the complexity of the data.
       - bias is the error introduced because of an over-simplistic model
     variance.
       - an overly-trained model has a high variance
       - a model with high variance is overly trained to the training data, capturing all the 
         intricacies of the data and interpreting the noise as a signal.

     bias-variance tradeoff
       - This bias-variance tradeoff is typically represented as a bullseye diagram.
       - A model has high bias if the dot throws are clustered, but away from the center.
       - A high bias and high variance model indicates the throws are not only away from the center,
         but widely scattered.
       - A model with a perfect fit has low bias and low variance.
       overfit
        - A high variance and a low bias model is an overfit model,

------------------------------------------------------
5.3 Using Regularization Techniques to Improve Accuracy 


  Regularization
    - regularization can help us prevent the overfitting problem
    - especially useful if you have limited amount of training data and your model is 
      complex with a large number of parameters.
    model performance 
      - measured by the loss function or the cost function,
        - measures the difference between the predictor and the actual values for a given set of inputs:
               loss function =  (1/n)  ∑ (y_pred_i - y_i)**2    from i=1 to i=n 
                          where:   y_i:                         actual value for instance i
                                   y_pred_i:                    predicted value for instance i

      - An ideal model will have a very low value for the loss function.

    L1 (lasso) regularization.
      - L1 regularization adds a term to the loss function that is proportional to the absolute value 
        of the coefficients.
      - It tends to minimize the impact of the coefficients by multiplying their sum and the penalty factor lambda.

                loss + L1 = (1/n)  ∑ (y_pred_i - y_i)**2  + λ * ([c1| + |c2|)

      - L1 regularization is effective in minimizing the impact of redundant or irrelevant features.
      - L1 focused dropping unwanted features

    L2 (ridge) regularization
      - when the model is too focused only on certain features in the data 
      - L2 regularization adds a penalty to the loss function, but it adds to the square of the model coefficients (weights).

                loss + L2 = (1/n)  ∑ (y_pred_i - y_i)**2  + λ * ([c1|**2 + |c2|**2)

      - L2 regularization is focused on distributing the impact of all the important features.

   Early Stopping.
     - early stopping is a technique where we consciously stop training the model when the error on the test data increases.

   dropout regularization
     - extensively used in deep neural networks to prevent overfitting 
     - dropout technique randomly drops out a set of neurons along with their connections in each layer
     - introduces an element of randomness and forces the remaining set of neurons to learn generalizable features.
     - The number of neurons to be dropped is a hyperparameter that can be set during the model training.
     dropout rate 
       - binary value that indicates if the element will participate in training or no
       - probability of a neuron being dropped during the training phase.
     binary mask 
       - created for each layer.  Each element in the mask has a binary value of zero or one.  
       - A value of one indicates that the element will participate in the training and zero otherwise.
     forward pass 
       - a learning process where the data flows from the input to the output layer
     backward pass 
       - model learns from the mistakes and calculates the error between the predicted and the actual values

------------------------------------------------------
5.4 Prevent Overfitting Using Cross Validation Techniques 

  cross-validation techniques to prevent over fitting.
    - cross-validation is a statistical technique that partitions the dataset into training and testing data sets 
      used to assess the performance and generalizability of a model.

  When to use cross-validation
    - When you have a limited amount of data 
       - cross-validation allows you to use your data more effectively by training and validating the model on different 
         subsets, providing a more reliable estimate of performance.
    - Model performance assessment
      - Whenever we need to assess our model's performance on new, unseen data, we can use this technique to get 
        more accurate estimate compared to a single trained test split.

  cross-validation techniques.
    K-fold cross-validation.
      - the dataset is split into 'K' number of folds.
      - Then the model is trained with 'K' minus one folds and tested with a Kth fold.
      - This process is repeated K number of times, and each time using a different fold for testing or validating the model.
      - With this approach, we get K number of model performance predictions, which can then be averaged to get the 
        final evaluation score.
     - because the K-fold cross-validation randomly splits the data and will not perform well with imbalanced data sets.
     - Note: the final model is generated using the full dataset; k-fold CV is to predict the quality of the final model

    Stratified K-fold cross-validation.
      - similar to K-fold cross-validation, except that each fold will maintain the same class distribution as the original dataset.

    Time Series Cross Validation
      - designed to handle time series data.
      - the folds are created sequentially based on time intervals.
      - This ensures that the training and testing data are in chronological order.
      - The training process may start with a smaller subset of data and a much smaller test data.
      - As new data come in, the test fold gets added to the training fold, and the new data is used for testing purposes 
        in the next iteration, which ensures that the testing is always performed against future data.
  
------------------------------------------------------
5.5 Optimizing Hyperparameters in a Linear Model 


  Saved files:
    completed python jupyter notebook:
      Linear_Learner_Regression_csv_format.ipynb
    extracted python code from jupyter notebook:
      Linear_Learner_Regression_csv_format.py
    html view from completed jupyter notebook:
      Linear_Learner_Regression_csv_format.html


   Choosing the right combination of hyperparameters
     - is critical for the efficiency and effectiveness of the optimization process.
     - finding the right combinatin of hyperparameter values will help the model predict accurately

     Hyperparameter optimization
        - previously learned the different strategies used to tune the hyperparameters like grid search and random search.
        Gradient Descent algorithm
          - also can use gradient descent to find optimal hyperparameter values.
          - In this approach, the hyperparameters are adjusted by computing the gradients of the objective function 
            with respect to these hyperparameters and updating them iteratively.

     Effect or the learning rate 
       small learning rate
         - With a slow learning rate, the algorithm makes tiny updates to the parameters needing a large number 
           of iterations to converge.
         - The algorithm may also get stuck in a local minimum, especially in a non convex problem.
       large learning rate
         - A large learning rate may overshoot the minimum, causing the parameters to oscillate around the minimum 
           and never converging making the training process unstable.

     How to define hyperparameters
       - during the tuning process using a parameter range.
       parameter range
         categorical parameter range.
           - You define the different categories of hyperparameter values that you want the tuning job to select.
            "CategoricalParameterRanges":
             [
               {
                  "Name": "tree-method", 
                  "Values": ["auto", "exact", "approx"]
               }
             ]

         continuous parameter range.
           - You specify the minimum and the maximum value of the hyperparameter range, and the tuning job will 
             select a value between this range during the tuning process.

            "ContinuousParameterRages":
             [
               {
                  "Name": "eta", 
                  "MaxValues": 0.5, "MinValue": 0, "ScalingType": "Auto"
               }
             ]


         integer parameter range.
           - You specify the minimum and the maximum hyperparameter range value as well, but the values need to 
             be an integer value.

            "IntegerParameterRages":
             [
               {
                  "Name": "max_depth", 
                  "MaxValues": 10, "MinValue": 1, "ScalingType": "Auto"
               }
             ]


      Hyperparameter scaling Types
        auto.
          - SageMaker hyperparameter tuning chooses the best scale for the hyperparameter.
        linear.
          - The tuning job selects the value in a linear fashion starting from lowest to the highest incrementing in smaller intervals.
        logarithmic.
          - The scaling works only for positive values.  
          - Use logarithmic scaling when you are searching a range that spans a several order of magnitude.
        reverse logarithmic.
          - This is supported only in continuous parameter range only and not in integer parameter range.
          - It works only for ranges that are between zero and one, 
          - choose this option when you are working on a range that is highly sensitive to small changes.

   Note: Missing - Link to demo notebook - appear to be the below:
   Demo: Optimiziation hyperparameters of a linear learning using AMT

        Using the MNIST dataset, we train a binary classifier to predict a single digit - includes AMT option.
          https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.html
             saved to: notebooks/Linear_Learner_Regression_csv_format.ipynb

         also at:
           
           SageMaker Notebook instance -> Jupyter -> SageMaker Examples -> Introduction to Amazon Algorithms 
               -> Linear_Learner_Regression_csv_format.ipynb-> Use

      
    Linear Learning Hyperparameters in demo:
      wd 	
        - The weight decay parameter, also known as the L2 regularization parameter. 
        - If you don't want to use L2 regularization, set the value to 0.
        - Optional
        - Valid values:auto or non-negative floating-point integer
        - Default value: auto
  
      learning_rate 	
        - The step size used by the optimizer for parameter updates.
        - Optional
        - Valid values: auto or positive floating-point integer
        - Default value: auto, whose value depends on the optimizer chosen.
  
      mini_batch_size 	
       - The number of observations per mini-batch for the data iterator.
       - Optional
       - Valid values: Positive integer
       - Default value: 1000

    Code: Linear Linear Demo Automatic Model Tuning code only

          Training with Automatic Model Tuning (HPO)

          As mentioned above, instead of manually configuring our hyper parameter values and training with SageMaker Training, 
          we'll use Amazon SageMaker Automatic Model Tuning.

          The code sample below shows you how to use the HyperParameterTuner. For recommended default hyparameter ranges, 
          check the Amazon SageMaker Linear Learner HPs documentation.

          The tuning job will take 8 to 10 minutes to complete.

      >>> import time
      >>> from sagemaker.tuner import IntegerParameter, ContinuousParameter
      >>> from sagemaker.tuner import HyperparameterTuner

      >>> job_name = "DEMO-ll-aba-" + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())
      >>> print("Tuning job name: ", job_name)

      >>> # Linear Learner tunable hyper parameters can be found here https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html
      >>> hyperparameter_ranges = {
      >>>     "wd": ContinuousParameter(1e-7, 1, scaling_type="Auto"),
      >>>     "learning_rate": ContinuousParameter(1e-5, 1, scaling_type="Auto"),
      >>>     "mini_batch_size": IntegerParameter(100, 2000, scaling_type="Auto"),
      >>> }

      >>> # Increase the total number of training jobs run by AMT, for increased accuracy (and training time).
      >>> max_jobs = 6
      >>> # Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.
      >>> # if max_jobs=max_parallel_jobs then Bayesian search turns to Random.
      >>> max_parallel_jobs = 2

      >>> hp_tuner = HyperparameterTuner(
      >>>     linear,
      >>>     "validation:mse",
      >>>     hyperparameter_ranges,
      >>>     max_jobs=max_jobs,
      >>>     max_parallel_jobs=max_parallel_jobs,
      >>>     objective_type="Minimize",
      >>> )

      >>> # Launch a SageMaker Tuning job to search for the best hyperparameters
      >>> hp_tuner.fit(inputs={"train": train_data, "validation": validation_data}, job_name=job_name)


------------------------------------------------------
5.6  Optimizing Hyperparameters in a [Decision] Tree Based Model 


  Decision Tree algorithm
    - a supervised learning algorithm that progressively divides the data and builds a model in the form 
      of a tree structure, which is used to make predictions based on the input features.

  Decision Tree terminology
    root node
      - top node is the 'root node'.
    condition
      - Each node splits the data based on a 'condition'.
    sub-nodes
      - These intermediate decision points are called 'sub-nodes'.
    branches or edges
      - The connection between the nodes represent the outcome of a decision, and they are referred 
        as 'branches' or 'edges'.
    terminal nodes
      - The 'terminal nodes' represent the final outcome or prediction where no further split is possible.

  Note: Missing: sklearn Notebook not provided
      -> same as 5.1 lab
    5.1 Resource:
      Perform Hyperparameter Optimization - Demos
        downloaded to:
        labs/lesson5_1_hyperparameter_optimization_demo/demos.ipynb

  Demo: Compare the performancees of two optimizations (grid search and random search)
     -> see 5.1 demo

------------------------------------------------------
5.7 Understanding Neural Network Architecture 

  Neural Network Architecture
    - includes input layer, hidden layer, output layer, weights and biases.

                                           
      input   ------>  Hidden  ------> Output  -----> predictions
      layer            layers          Layer      |
         ^                                        |
         |----------------------------------------|
                         Error

    Input Layer
      - The input data will first be fed into the neural network via the input layer.
      - The primary function of this layer is to receive data from the outside and pass it to the first hidden layer.
      - Each neuron in this layer represents an input feature.
    Hidden Layers
      - The next section of a neural network is a series of hidden layers that take in the past data and perform 
        various transformations.
      - Hidden layers are where relationships and informations about the data are derived.
    Output layer 
      - where the predictions are made.
      - The number of neurons in this layer corresponds to the number of possible outcomes or classes.

  Activation Functions
                      |---------------------|
                      |       Error         |
                      V                     |
        X1  ------>   W1 ----V              |
        X2  ------>   W2 ---> SUM ---->  Activation  --->
        X3  ------>   W3 ----^ ^          Function
                               |
                              Bias
     weights
       - Weights are often initialized randomly, usually with small values to break symmetry and allow the 
         network to learn diverse features.
       - Each connection between the neuron has an associated weight.
       - These weights determine the importance of the input features and are adjusted during the training 
         process to minimize the error in predictions.
     biases
       - Biases are constants added to the network, one per neuron, allowing the activation function to shift 
         towards a positive or negative side.
       - This helps a model fit the data better.

     Activation functions 
       - help a neural network learn patterns within the data.
       - Without the addition of activation functions, a neural network would be similar to a linear regression model.
       - They introduce a non-linearity into the network, enabling the network to learn complex patterns.

     Activation functions Types
       Softmax function 
         - is primarily used in the output layer for classification tasks.
         - It converts a vector of raw score into probabilities, which sum to one.
       Tanh function
         - outputs values between minus one to plus one.
         - Since it is zero centered, it often leads to faster convergence.
       Sigmoid function 
         - used to map any value between zero and one,
         - primarily used for binary classification.
       ReLU 
         - outputs the input directly if it is positive, otherwise, it outputs zero.

  Hidden Layers Types
    Fully Connected Layer (or Dense Layer)
      - In this layer, each neuron receives input from all neurons in the previous layer.
      - It is a most common type of layer in a neural network.
    convolution layer 
      - commonly used in CNNs, where each layer is used to process images.
      - It uses kernels to convolve the input and extract features like edges and textures.
    Pooling Layer (from lesson 2.9)
      - used after a convolution layer to reduce spatial dimension
      - used to downsample the feature map, reducing the spatial dimensions of the input while preserving the relevant information.
      Pooling Strategies.
        Max Pooling,
          - uses the most significant element from each feature region in the feature map.
        Average Pooling
          - calculated by computing the average value of each feature region in the feature map.
   Recurrent layer 
     - used primarily in RNNs, 
     - used to process sequential data.
     - Neurons in this layer have connections to the previous time step neurons, allowing the network to 
       maintain memory of the previous inputs.

------------------------------------------------------
5.8 Optimizing Hyperparameters in a Neural Network 


  Neural Network important hyperparameters 

    Learning Rate.
      - controls the step size while iterating towards the minimum value of the loss function.
      Performance
        - a high learning rate will converge quickly, but it may overshoot the true minima
        - a low learning rate will result in a very slow convergence, but will not overshoot the true minima
      Memory usage
        - minimal impact to memory usage,
      Cost
        - A low learning rate will take a longer time to complete the process having a significant impact on cost.

    Batch Size.
      - the number of training samples used in one iteration to update the model's weights and train the neural network.
      Performance
        - A smaller batch size, (referred to as mini batch) results in a slow learning and a stable model
        - A large batch size will result in fast learning and unstable model
            - it may miss the true minimum and oscillate.
            - Oscillations are when you see your model weights are updating rapidly with big numbers.
      Memory Usage
         - A smaller batch size will require more computing resources.
      Cost
         - a smaller batch size will result in a higher cost.

    Epochs
      - the number of times the entire training dataset is passed forward and backward through the neural network.
      Performance
        - More epochs can lead to better training, but cause overfitting, if the model starts to learn the noise in the training data.
      Memory Usage
        - minimal impact 
      Cost
        - the number of epochs will increase the computation cost linearly.

    Number of layers 
      - the depth of the neural network.
      Performance
        - Deeper networks can capture more complex patterns, but are harder to train and more prone to over fitting.
      Memory Usage
        - As the number of layer increases, the memory usage will increase
      cost
        - Deeper networks will increase the computational cost

  Resource Limits
     -  automated model tuning resource limits enforced by the SageMaker.
     Parallel hyperparameter tuning jobs 
       limit: 100
     Maximum number of training jobs per hyperparameter tuning job 
       limit: 750 
        - irrespective of the strategy
     Concurrent training jobs per hyperparameter tuning job 
       limit: 10.
     Hyperparameters that can be searched during a specific job 
       limit: 20.
     Maximum number of metrics that are defined per hyperparameter tuning job
       cannot exceed 20
     Maximum runtime for a hyperparameter tuning job 
        cannot exceed 30 days.

  SageMaker Tuning Process best practices 
    Use a smaller number of hyperparameters  
       - allows up to 20 hyperparameters in a tuning job, but should use small number
    Use smaller ranges for hyperparameters  
       - The range of the hyperparameters that you choose will have a significant impact on the resource, 
         so it's recommended to use a much smaller range than a larger one.
    Use logarthmic scaling
       - Converting a parameter scale from a linear scale to a logarithmic scale is a time consuming process.
       - So if you know that a hyperparameter should be log scaled, you can convert it yourself and mention 
         it during configuration.
    Limit the number of concurrent training jobs
       - A tuning job improves only after every successful rounds of experiments, so it's recommended to 
         limit the number of training jobs that run concurrently.
    Enable distributed training
        - recommends enabling distributed training by training the jobs in multiple instances.

  Early stopping 
    - terminating a training job when the objective metric computed by this job is significantly lower 
      than the best training job.
    - converse compute resources.
    - helps you avoid overfitting the model.
    To configure early stopping 
       - you need to set the variable "early_stopping_type" to "auto" during the configuration process.
    After each epoch of training:
      - gets the value of objective metric
      - Computes a median of the objective metric for all the previous training jobs up to the same epoch.
      - compare with previous jobs
      - if the value of the objective metric is higher than the previous jobs, SageMaker stops the current 
        job to conserve computing resources.

    algorithms supporting early stopping include:
        Linear Learner, XGBoost, Image Classification, Object Detection, Sequence-to-Sequence, and IP Insights.

  Warm Start to hyperparameter tuning job
     - is a process of leveraging or reusing the previously concluded training jobs.
     leverage previously concluded training jobs
       - The results of the previous jobs are used to inform which combination of hyperparameters are effective 
         in the newly starting job.
     conserves resources
       - With the knowledge of previously tuned jobs, every current job doesn't need to start from scratch,
       -  it helps reduce the time it takes to identify the best hyperparameters combination.
       - Warm starting helps save significant time, effort, and computing resources and eventually saves cost.
     longer start time
       - Tuning jobs with warm start usually takes longer to start than standard tuning jobs because the results 
         from the parent jobs needs to be loaded and studied.

------------------------------------------------------
5.9 Hyperparameter Tuning Job Created Using Amazon SageMaker


Saved files:
    completed python jupyter notebook:
      tune.ipynb
    extracted python code from jupyter notebook:
      tune.py
    html view from completed jupyter notebook:
      tune.html


About this lab

Imagine you are a data engineer, and your company just selected AWS as the preferred cloud provider. You have been 
asked to find the optimal set of hyperparameters for the linear learner model by leveraging SageMaker's Automatic 
Model Tuning (AMT). In this lab, you will fetch the iris data and use that as the input dataset. Once the data is 
split, the data is uploaded to an S3 bucket. Then the Sagemaker estimator is configured to run four tuning jobs, 
and will identify the job that minimizes the objective metric.

Learning objectives
 - Launch SageMaker Notebook
 - Install dependencies and import the libraries
 - Download the data and upload it to an S3 bucket
 - Set up training and validation data
 - Initialize the estimator
 - Tune hyperparameters

-------------------------
Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1, N. Virginia, region. (If you are prompted to select a kernel, choose conda_tensorflow2_p310.)
    In the search bar, enter SageMaker then click on Amazon SageMaker.
    On the left, under the Applications and IDEs section, click on Notebooks.
    Ensure the Notebook is marked as InService (if it's Pending wait for a 30 seconds or a minute), then under Actions, click the Open Jupyter link.
    Click on the tune.ipnyb file.

Load the Dataset and Split the Data

Each cell contains Python commands you can run independently.

    Click the first cell that installs boto3 and sagemaker. Use the Run button at the top to execute the code. A * inside the square braces indicates the code is running, and you will see a number once the execution is complete.
    Run the next cell, which imports the required Python libraries, initializes the sagemaker session, and defines the output bucket. This cell indicates all the libraries you will import from pandas and sklearn. It may take a few minutes to complete the operation.
    The next cell fetches the IAM role using the get_execution_role function. Paste in the following code snippet, ensuring you overwrite the existing comments, and Run the cell to perform this operation.

    role = get_execution_role()

Download the Data and Upload It to the S3 Bucket

    The next cell fetches the iris dataset from the sklearn library. After loading the dataset, you create a feature variable (X), and a target variable (y). Click Run to execute this cell.
    Now that the data is imported, it must be split for training and testing purposes. Use the following code snippet, and Run the cell to perform this operation.

    train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)

    Run the next cell converts the training and validation data to CSV format.
    Then, you will upload the training CSV file to the S3 bucket using the upload_file function. (Run this cell.)
    Use the following code snippet, and run it in the next cell to upload the validation.csv file.

    s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')

Set Up Training and Validation Data

    Run the first cell in this section, which initializes the path from which training data and validation data will be read. These values will be used to create the input parameter for the estimator object.
    Then, use the TrainingInput function, and create the train_data input parameter. This passes the path from which the training data will be read.
    Use the following code snippet and create an input parameter for validation_data,

    validation_data = sagemaker.inputs.TrainingInput(
        s3_validation_data,
        distribution="FullyReplicated",
        content_type="text/csv",
        s3_data_type="S3Prefix",
        record_wrapping=None,
        compression=None,
    )

Fetch the Algorithm and Initialize the Estimator

    In this section, you will see how to configure the estimator object. The first cell under this category fetches the linear-learner algorithm specific to the region. Run this cell to fetch the algorithm.
    The next cell initializes the Estimator object. It is important to ensure you choose the correct instance type and instance count.

Define the Hyperparameter Ranges and Tune the Model

    Use the following code snippet to configure the hyperparameters manually.

    linear.set_hyperparameters(
        feature_dim=4,  # Adjust this to match your feature dimension
        predictor_type='regressor',  # Use 'classifier' for classification
        mini_batch_size=20
    )

    Then set the hyperparameter ranges for learning_rate and wd (weight decay).

    # Define the hyperparameter ranges
    hyperparameter_ranges = {
        'learning_rate': ContinuousParameter(0.01, 0.2),
        'wd': ContinuousParameter(0.0, 0.1)
    }

    Now create the HyperparameterTuner object. It is important to set the variables max_jobs and max_parallel_jobs appropriately for your capacity. The goal is to minimize rmse, or Root Mean Square Error, to improve accuracy.
    Initiate the tuning process by invoking the fit function.

    # Launch the hyperparameter tuning job
    tuner.fit({'train': train_data, 'validation': validation_data})

    You will get a couple No finished training job messages, which is expected, as no training was done before tuning.
    Go back to SageMaker in the console, on the left scroll down and expand Training, then click Hyperparameter tuning jobs to monitor the tuning.
-------------------------


    code: hyperparameter Tuning Job Created Using Amazon SageMaker

      >>> # # Introduction
      >>> # 
      >>> # In this lab, you will learn how to import the iris dataset, split it into training and validation data, upload them to the 
      >>> # S3 bucket, fetch the linear learner algorithm, initialize the estimator object, and automatically tune the hyperparameters 
      >>> # using Amazon SageMaker's Automatic Model Tuning (AMT).

      >>> # # How to Use This Lab
      >>> # 
      >>> # Most of the code is provided for you in this lab as our solution to the tasks presented. Some of the cells are left empty 
      >>> # with a #TODO header, and it's your turn to fill in the empty code. You can always use our lab guide if you are stuck.

      >>> # # 1) Install dependencies and import the required libraries

      >>> # Install Sagemaker
      >>> !pip install boto3 sagemaker


      >>> # 1. We will use the iris dataset as our input data. 
      >>> # 2. The S3 bucket you want to use for training data must be within the same region as the Notebook Instance.
      >>> # 3. The IAM role is used to provide training and hosting access to your data. See the documentation for how to create these. 
      >>> #    Note that if more than one role is required for notebook instances, training, and/or hosting, please replace the boto 
      >>> #    regexp with an appropriate full IAM role arn string(s).

      >>> import sagemaker
      >>> import boto3
      >>> from sagemaker import get_execution_role
      >>> from sagemaker.inputs import TrainingInput
      >>> import pandas as pd
      >>> from sklearn.datasets import load_iris
      >>> from sklearn.model_selection import train_test_split
      >>> from sagemaker.tuner import HyperparameterTuner, ContinuousParameter

      >>> # Initialize the SageMaker session
      >>> sagemaker_session = sagemaker.Session()

      >>> # Define the S3 bucket and prefix to store data
      >>> output_bucket = sagemaker.Session().default_bucket()
      >>> output_prefix = 'sagemaker/linear-learner'


      >>> #TODO: Fetch the IAM role using the get_execution_role function and assign the value to a variable `role.`
      >>> role = get_execution_role()


      >>> # # 2) Download the data and upload them to S3 bucket

      >>> # 1. load_iris function is used to download the input data
      >>> # 2. The data is split into training and validation data in the ratio of 80 - 20
      >>> # 3. The data is saved under 'train.csv' and 'validation.csv'

      >>> # Load the Iris dataset
      >>> iris = load_iris()
      >>> X = iris.data
      >>> y = iris.target

      >>> # Convert to DataFrame for easier manipulation
      >>> data = pd.DataFrame(X, columns=iris.feature_names)
      >>> data['target'] = y


      >>> iris.feature_names


      >>> data.head()


      >>> #TODO: Use the `train_test_split` function and split the data in an 80 - 20 ratio. 
      >>> #TODO: Assign the values to variables `train_data` and `validation_data`.
      >>> train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)


      >>> # Save to CSV
      >>> train_data.to_csv('train.csv', index=False, header=False)
      >>> validation_data.to_csv('validation.csv', index=False, header=False)


      >>> # Let's use the upload_file function and upload the .csv files to the S3 buckets.

      >>> # Upload data to S3
      >>> s3 = boto3.client('s3')
      >>> s3.upload_file('train.csv', output_bucket, f'{output_prefix}/train/train.csv')


      >>> #TODO: Using the strategy we followed to upload the training data, as shown above, please upload the validation data to 
      >>> # the output bucket.
      >>> s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')


      >>> # # 3) Set up training and validation data

      >>> # Create three separate variables that are dynamically constructed, which will be used as one of the input parameters 
      >>> # while generating training inputs.

      >>> # creating the inputs for the fit() function with the training and validation location
      >>> s3_train_data = f"s3://{output_bucket}/{output_prefix}/train"
      >>> print(f"training files will be taken from: {s3_train_data}")
      >>> s3_validation_data = f"s3://{output_bucket}/{output_prefix}/validation"
      >>> print(f"validation files will be taken from: {s3_validation_data}")
      >>> output_location = f"s3://{output_bucket}/{output_prefix}/output"
      >>> print(f"training artifacts output location: {output_location}")


      >>> # Let's create the sagemaker.session.s3_input objects from our data channels. Note that we are using the content_type as 
      >>> # text/csv. We use two channels here, one for training and the second for validation.

      >>> # generating the session.s3_input() format for fit() accepted by the sdk
      >>> train_data = sagemaker.inputs.TrainingInput(
      >>>     s3_train_data,
      >>>     distribution="FullyReplicated",
      >>>     content_type="text/csv",
      >>>     s3_data_type="S3Prefix",
      >>>     record_wrapping=None,
      >>>     compression=None,
      >>> )


      >>> #TODO: Following the above strategy, please set up a training input for validation data.
      >>> #TODO: Name it as `validation_data`.
      >>> validation_data = sagemaker.inputs.TrainingInput(
      >>>     s3_validation_data,
      >>>     distribution="FullyReplicated",
      >>>     content_type="text/csv",
      >>>     s3_data_type="S3Prefix",
      >>>     record_wrapping=None,
      >>>     compression=None,
      >>> )

      >>> # # 4) Fetch the algorithm and initialize estimator

      >>> # Let's retrieve the image for the Linear Learner Algorithm according to the region.

      >>> # Fetch the linear learner image according to the region
      >>> from sagemaker.image_uris import retrieve

      >>> container = retrieve("linear-learner", boto3.Session().region_name, version="1")
      >>> print(container)
      >>> deploy_amt_model = True


      >>> # Then, we create an estimator from the SageMaker Python SDK using the Linear Learner container image and set the 
      >>> # training parameters.

      >>> %%time
      >>> import boto3
      >>> import sagemaker
      >>> from time import gmtime, strftime

      >>> sess = sagemaker.Session()

      >>> job_name = "linear-learner-iris-regression-" + strftime("%Y%m%d-%H-%M-%S", gmtime())
      >>> print("Training job", job_name)

      >>> linear = sagemaker.estimator.Estimator(
      >>>     container,
      >>>     role,
      >>>     instance_count=1,
      >>>     instance_type="ml.m5.large",
      >>>     output_path=output_location,
      >>>     sagemaker_session=sagemaker_session,
      >>> )

      >>> # # 5) Define hyperparameter ranges and invoke tuning job

      >>> # Set the initial values for the hyperparameters.

      >>> # TODO: Use the set_hyperparameters function and set the initial hyperparameters on linear learner
      >>> # feature_dim=4, predictor_type='regressor', mini_batch_size=20
      >>> linear.set_hyperparameters(
      >>>     feature_dim=4,  # Adjust this to match your feature dimension
      >>>     predictor_type='regressor',  # Use 'classifier' for classification
      >>>     mini_batch_size=20
      >>> )


      >>> # Lets use the Continous parameter range and define the values for `learning rate` and `wd` (weight decay - L2 regularization).

      >>> #TODO: Define the hyperparameter ranges
      >>> #1. 'learning_rate': ContinuousParameter(0.01, 0.2)
      >>> #2. 'wd': ContinuousParameter(0.0, 0.1)
      >>> # Define the hyperparameter ranges
      >>> hyperparameter_ranges = {
      >>>     'learning_rate': ContinuousParameter(0.01, 0.2),
      >>>     'wd': ContinuousParameter(0.0, 0.1)
      >>> }


      >>> # 1. Instead of manually configuring our hyperparameter values and training with SageMaker Training, we'll use Amazon 
      >>> # SageMaker Automatic Model Tuning.
      >>> # 2. The code sample below shows you how to use the HyperParameterTuner. It accepts the hyperparameter ranges we set previously.
      >>> # 3. Based on your capacity, you can adjust the `max_jobs` and `max_parallel_jobs.`
      >>> # 4. The goal of the tuning job is to minimize `rmse.`
      >>> # 5. The tuning job will take 8 to 10 minutes to complete.

      >>> # Create a HyperparameterTuner object
      >>> tuner = HyperparameterTuner(
      >>>     estimator=linear,
      >>>     objective_metric_name='validation:rmse',
      >>>     hyperparameter_ranges=hyperparameter_ranges,
      >>>     metric_definitions=[
      >>>         {'Name': 'validation:rmse', 'Regex': 'validation rmse=([0-9\\.]+)'}
      >>>     ],
      >>>     max_jobs=4,
      >>>     max_parallel_jobs=2,
      >>>     objective_type='Minimize'
      >>> )


      >>> #TODO: Initiate the tuner job by invoking the fit function.
      >>> #2. Pass the train_data and validation_data as input parameters.
      >>> # Launch the hyperparameter tuning job
      >>> tuner.fit({'train': train_data, 'validation': validation_data})


      >>> tuner_name = tuner.describe()['HyperParameterTuningJobName']
      >>> print(f'tuning job submitted: {tuner_name}.')


      >>> tuner.best_training_job


      >>> # Retrieve analytics object
      >>> #tuner_analytics = tuner.analytics()

      >>> # Look at summary of associated training jobs
      >>> #tuner_analytics_dataframe = tuner_analytics.dataframe()



-------------------------
5.10 Perform Hyperparameter Optimization Review 


                             |---> Perform regularization
                             |
                             |---> Perform cross validation
                             |
                             |---> Initialize models
    task statement 3.4 ------- 
    (Perform Hyperparameter  |---> Understand neural network architecutre, learning rate, and activation functions
     optimization)           |
                             |---> Understand tree-based models
     optimization)           |
                             |---> Understand linear models


  Perform regularization.
    - Overfitting happens when the model performs well against training data, but performs poorly with 
      new unseen data.
    - Underfitting happens when the model is unable to learn the hidden patterns in the dataset, resulting 
      in poor prediction.
    Regularization 
      - a technique used to prevent the overfitting problem and improve model performance.
      L1 regularization 
        - adds the sum of the absolute values of the model's coefficients to the loss function.
        - L1 regularization is very effective in minimizing the impact of redundant or irrelevant features.
      L2 regularization 
        - adds the sum of the squared values of the model's coefficients to the loss function.
        - It is focused on distributing the impact of all the important features.
      Early stopping
        - another regularization technique,
        - used to consciously stop training the model when the model's performance on a validation set stops improving.

  Perform cross validation.
    Cross Validation
      - Cross validation is another technique used to prevent overfitting.
      - It is a statistical technique that partitions the dataset into training and testing data to assess the performance 
        and generalizability of a model.
      - This technique is very effective when you have limited data so that you can train the model on different subsets of data.
      K-fold cross-validation
        - the dataset is split into K-number of folds.
        - Then the model is trained with K-minus-one folds and tested with a K-fold.
        - This process is repeated K-number of times, and each time using a different fold for testing or validating the model.
      Stratified K-fold cross-validation,
         - similar to K-fold cross-validation, except that each fold will maintain the same class distribution as a original dataset.
         - This technique is effective with an imbalanced dataset.
      time series cross-validation 
        - the folds are created sequentially based on time intervals.
        - This ensures that the training and testing data are in chronological order.
        - effective when the order of data is important.

  Initialize Models.
    - Before tuning the models, the algorithms are initialized with initial set of values for the hyperparameters that needs tuning.
    - The hyperparameters also needs to be initialized with their ranges, 
    - tuning jobs find the best hyperparameter values by searching over the range of hyperparameter values that you define.  
    Hyperparameter ranges:
       - CategoricalParameterRanges 
       - ContinuousParameterRanges
       - IntegerParameterRanges

  neural network architecture, learning rate, and activation functions 
     Neural network three main components:
        input layer 
          - The input layer receives data from external sources and passes them to the hidden layer
        hidden layer 
          - There are many hidden layers and their primary responsibility is to transform the input data.
        output layer
          - The output layer is the final layer where the predictions are made.
     Weights 
       - determine the importance of the input features,
       - they are adjusted during the training process to minimize the error in predictions.
     Biases 
       - one per neuron,
       - helps a model fit the data better.
     activation function 
       - introduces non-linearity into the network, enabling the network to learn complex patterns.
       Activation functions include:
          - Sigmoid
          - Tanh
          - ReLU
          - and Softmax.

  Tree-Based Models
    - It is a supervised learning algorithm that progressively divides the data and builds a model in the form of a restructure.
    root node
      - The top node, representing the entire dataset, is a root node.
    condition
       - Each node splits the data based on a condition.
    sub-nodes
       - These intermediate decision points are called sub-nodes.
    branches or edges.
       - The connection between the nodes represent the outcomes of a decision,
    terminal nodes 
       - terminal nodes represent the final outcome or prediction where no further split is possible.
    key [decision] tree-based hyperparameters 
       https://www.geeksforgeeks.org/how-to-tune-a-decision-tree-in-hyperparameter-tuning/
       number_of_folds
          - number of cross validation folds
       max_depth 
         - the maximum depth to which the decision tree is allowed to grow
         - When the max_depth is deeper it allows the tree to capture more complex patterns in the training data 
           potentially reducing the training error. 
         - However, setting max_depth too high can lead to overfitting where the model memorizes the noise in the training data. 
       min_sample_split 
         - minimal number of samples that are needed to split a node
       min_samples_leaf.
         - required minimal amount of samples to be present at a leaf node. 
         - It acts as a threshold for halting the splitting process and designating a node as a leaf. 
         - The condition for splitting is that it must leave at least min_samples_leaf samples on both resulting child nodes, 
           this ensures that the splitting process doesn’t continue indefinitely

  linear models.
    Gradient descent 
      - is an advanced algorithm used to find optimal hyperparameter values.
    Learning rate 
       - choosing the right learning rate is critical for the efficiency and effectiveness of the optimization process.
    alpha hyperparameter [wd - weight decay?]
       - controls the strength of regularization in ridge regression.
       - Tuning this value is important to find the right balance between bias and variance.



------------------------------------------------------
5.11 Perform Hyperparameter Optimization - Quiz


-----------------
Question 2

You are working on developing a machine learning (ML) model and have discovered that you need to reduce its variance. 
Which of the following techniques is an acceptable solution?

choices:
  Increase learning rate.

  Reduce learning rate.                             <--- Incorrect Answer

  Add more input features.

  Reduce the number of input features.              <--- Correct Answer

Sorry!
  The learning rate does not influence variance.

Correct Answer
  Reducing the number of input features can reduce the complexity of the model, which will directly impact the variance.

-----------------

Question 3

Which hyperparameter defines the number of times the learning algorithm will work through the entire dataset?
choices:

  Batch size

  Number of epochs              <--- Correct Answer

  Learning rate

  Number of layers

Good work!
  The number of epochs defines the number of times the learning algorithm will work through the entire dataset.

------
-----------------

Question 4

Imagine you are your company's machine learning (ML) engineer, and you are currently tuning a model's hyperparameters. 
You have been instructed to save computational costs. Which tuning strategy will you choose?

choices:

  Manual hyperparameter tuning

  RandomSearch                    <--- Correct Answer

  Bayesian optimization           <--- Incorrect Answer

  GridSearch

Sorry!
  This is an advanced strategy and an expensive solution.

Correct Answer
  RandomSearch offers the perfect trade-off between exploration and computation cost.

------
-----------------

Question 5

As a machine learning (ML) engineer, you are currently working on tuning the hyperparameters of a model. Because of the 
limited budget, you have been instructed to use the computer resources wisely. What regularization technique will you use 
to meet this requirement?

choices:
  L1 regularization

  Early stopping              <--- Correct Answer

  L2 regularization

  L3 regularization

Good work!
  Early stopping can reduce the hyperparameter tuning time and help save computational resources.

------
-----------------
Question 6

During the hyperparameter tuning process, you observed that one of the hyperparameters is very sensitive to small changes. 
Which scaling type will you use during the tuning process?

choices:
  Reverse Logarithmic              <--- Correct Answer

  Auto

  Incremental change

  Logarithmic

Good work!
  The reverse logarithmic option is perfect when you are working on a range that is sensitive to minor changes.

------
-----------------

Question 7

You are currently working on a deep learning project, using convolutional neural network (CNN) algorithms to classify images. 
Which activation function will you use for faster convergence?

choices:
  tanh                            <--- Incorrect Answer

  Sigmoid

  ReLU                             <--- Correct Answer

  Softmax

Sorry!
  The tanh option is preferred only in shallower networks.

Correct Answer
  ReLU is the preferred option for faster convergence.

------
-----------------
Question 8

You are developing a machine learning (ML) model, and the input dataset has many irrelevant features. Which technique will 
you use during the training process to improve the model prediction?

choices:

  L1 regularization              <--- Correct Answer

  Warm start

  L2 regularization

  Early stopping

Good work!
  Choosing L1 regularization can drive the coefficients of irrelevant features to zero, effectively resulting in a sparse model.

------
How to Choose an Activation Function for Deep Learning
  https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/

Activation for Hidden Layers
  - There are perhaps three activation functions you may want to consider for use in hidden layers; they are:
    - Rectified Linear Activation (ReLU)
    - Logistic (Sigmoid)
    - Hyperbolic Tangent (Tanh)

  ReLU (rectified linear ) Hidden Layer Activation Function
    - most common function used for hidden layers.
    - It is common because it is both simple to implement and effective at overcoming the limitations of other previously 
      popular activation functions, such as Sigmoid and Tanh. 
    - Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, 
      although it can suffer from other problems like saturated or “dead” units.

    - The ReLU function is calculated as follows:

      max(0.0, x)

    - shape: x < 0 --> 0
             x > 0 --> X

    - When using the ReLU function for hidden layers, it is a good practice to use a “He Normal” or “He Uniform” weight 
      initialization and scale input data to the range 0-1 (normalize) prior to training.

  Sigmoid Hidden Layer Activation Function (or logistic function)
    - It is the same function used in the logistic regression classification algorithm.

    - The function takes any real value as input and outputs values in the range 0 to 1. 
    - The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the 
      input (more negative), the closer the output will be to 0.0.

     - The sigmoid activation function is calculated as follows:

         1.0 / (1.0 + e^-x)

     - shape: S-shaped between 0 and 1

     - When using the Sigmoid function for hidden layers, it is a good practice to use a “Xavier Normal” or 
       “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) 
       and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training.

  Tanh Hidden Layer Activation Function (or hyperbolic tangent activation function) 
    - It is very similar to the sigmoid activation function and even has the same S-shape.

     - The function takes any real value as input and outputs values in the range -1 to 1. 
     - The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller 
       the input (more negative), the closer the output will be to -1.0.

     - The Tanh activation function is calculated as follows:

          (e^x – e^-x) / (e^x + e^-x)

     - shape: S-shaped between -1 and 1

     - When using the TanH function for hidden layers, it is a good practice to use a “Xavier Normal” or 
      “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and 
      scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training.

  How to Choose a Hidden Layer Activation Function

    - A neural network will almost always have the same activation function in all hidden layers.
    - The activation function used in hidden layers is typically chosen based on the type of neural network architecture.


      Multilayer Perceptron (MLP): 
        - ReLU activation function.
      Convolutional Neural Network (CNN): 
        - ReLU activation function.
      Recurrent Neural Network (RNN): 
        - Tanh and/or Sigmoid activation function.
        - For example, the LSTM commonly uses the Sigmoid activation for recurrent connections and 
          the Tanh activation for output.
  

Activation for Output Layers

  - The output layer is the layer in a neural network model that directly outputs a prediction.

  - All feed-forward neural network models have an output layer.

  - There are perhaps three activation functions you may want to consider for use in the output layer; they are:
    Linear
    Logistic (Sigmoid)
    Softmax


  Linear Output Activation Function [or “identity” (multiplied by 1.0) or “no activation.”]
   - This is because the linear activation function does not change the weighted sum of the input in any way 
     and instead returns the value directly.

   - shape: a diagonal line shape where inputs are plotted against identical outputs

  Sigmoid Output Activation Function
   - shape: S-shaped between 0 and 1
   - Target labels used to train a model with a sigmoid activation function in the output layer will have the values 0 or 1.

  Softmax Output Activation Function
    - The softmax function outputs a vector of values that sum to 1.0 that can be interpreted as probabilities 
      of class membership.

    - It is related to the argmax function that outputs a 0 for all options and 1 for the chosen option. 
    - Softmax is a “softer” version of argmax that allows a probability-like output of a winner-take-all function.

    - As such, the input to the function is a vector of real values and the output is a vector of the same length 
      with values that sum to 1.0 like probabilities.

    - The softmax function is calculated as follows:

         e^x / sum(e^x)

         Where x is a vector of outputs and e is a mathematical constant that is the base of the natural logarithm.

  How to Choose an Output Activation Function
   - You must choose the activation function for your output layer based on the type of prediction problem that you are solving.
   - Specifically, the type of variable that is being predicted.
   - For example, you may divide prediction problems into two main groups, predicting a categorical variable 
     (classification) and predicting a numerical variable (regression).


     Regression: 
        - One node, linear activation.

     Classification:
       Binary Classification: 
         - One node, sigmoid activation.
       Multiclass Classification: 
         - One node per class, softmax activation.
       Multilabel Classification: 
         - One node per class, sigmoid activation

-----------------
------------------------------------------------------

Chapter 6  Evaluate ML Models 

------------------------------------------------------
6.1 Evaluating a Binary and Multi-class Classification Model Using a Confusion Matrix 

  confusion matrix
    - It is a table used to evaluate the performance of a classification model by comparing the 
      actual values with the predicted values.
     - used to evaluate binary and multiclass classification models.



        Confusion Matrix - binary classification:

                        |             actual  Class                     | 
                        |-----------------------------------------------|
                        |  Positive            |     Negative           |
                        |  (SPAM)              |     (Not Spam)         |
          --------------|----------------------|------------------------|
          Predict Class | True Postive (TP)    |  False Positive (FP)   |   
                        |                      |    (TYPE 1 ERROR)      |    
          Positive      | SPAM was             |Not SPAM was incorrectly|    
          (SPAM)        | correctly predicted  | predicted as SPAM      |
                        | SPAM                 |                        |    ^
          --------------|----------------------|----------------------  |    |
          Predict Class | False Negative (FN)  |  True Negative (TN)    |    |
                        |   (TYPE 2 ERROR)     |                        |    |
          Negative      | Not SPAM was         | Not SPAM was correctly |    |
          (Not Spam)    | incorrectly predicted| predicted as Not SPAM  |    recall   
                        | as SPAM              |                        |    
          --------------|----------------------|------------------------|   
                                                  <-------- Precision


  confusion matrix - multiclass classification problem,
    - if a particular fruit is an apple, banana, or orange.
    - Since there are three possible outcomes, the confusion matrix will be a three by three 
      matrix, and along the same lines for a classification problem with N possible outcomes, 
      the confusion matrix will be a N by N matrix.

------------------------------------------------------
6.2 Evaluating a Classification Model Using Core Metrics 

  Metric for Classification Problems

     Accuracy: (TP + TN)  / (TP + FP + TN + FN)
        - percentage of predictions that were correct:
        - use only with a balanced dataset
        - less effective with a lot of true negatives
           - example: predicting fraud with little to no fraud data

     Precision: (TP)  / (TP + FP)
        - accuracy of positive predictions
        - percentage of positive predictions that were correct:
        - minimizes false positives, but does not consider false negatives
        - Use when the cost of false positives is high
           - example: an email is flagged and deleted as spam when it really isn't

     Recall: (TP)  / (TP + FN)
        - also called sensitivity or true positive rate (TPR)
        - percentage of actual positive predictions that were correctly identified:
        - minimizes false negatives, but does not consider false positives
        - Use when the cost of false negatives is high
           - example: someone has cancer, but screening does not find it

     Specificity, TNR  =  TN  / (TN + FP)         
        - probability that a negative cases will be true negative
        - also called: TNR (True Negative Rate) and TNF (True Negaive Fraction)
        - strength: ability to correctly identify negative instances; 
        - weakeness: doesn't consider false negatives
        - use where correctly identifying negative cases is crucial.
        - example: Out of all the people that do not have the disease, how many got negative results?


     F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
       - combined precision and recall score
       - harmonic mean of the precision and recall 
       - regular mean treats all values equally, the harmonic mean give more weight to low values
       - classifiers will only get high F1 Score if both recall and precision values are high

     Equation 3-3: F1 score:

     F1 = 2 / [ (1/precision) + (1/recall)]  =  2 x [( precision x recall) / (precision + recall)]

        = (TP)  / [TP + ((FN + FP) / 2)]


 AUC (Area under the Curve) - ROC (receiver operator characteristic) curve,
    - plots a relation between true positive rate versus a false positive rate.

  ROC curve (receiver operating characteristic curve) 
    - is a graph showing the performance of a classification model at all classification thresholds. 
    - This curve plots two parameters:
        True Positive Rate (TPR)   on Y-axis
        False Positive Rate (FPR)  on X-axis

    True Positive Rate (TPR) (as called recall) 
       TPR = TP / (TP + FN)

    False Positive Rate (FPR):
       FPR = FP / (FP + TN)
         - The false positive rate is the relative frequency of no disease when disease is predicted 
 

  AUC: Area Under the ROC Curve
    - measures the entire two-dimensional area underneath the entire ROC curve (think integral 
      calculus) from (0,0) to (1,1).
    - provides an aggregate measure of performance across all possible classification thresholds. 
    - One way of interpreting AUC is as the probability that the model ranks a random positive 
      example more highly than a random negative example. 
    - AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; 
      one whose predictions are 100% correct has an AUC of 1.0.`


------------------------------------------------------
6.3 Assessing a Regression Model Using Core Metrics 


  Mean Absolute Error (MAE) (L1 Loss)
     MAE =  (1/n) SUM |y_pred_i - y_i|  where SUM is from i=1 to i=n

     - strength: not sensitive to outliers.
     - weakness: treats all the errors equally and does not provide information on 
                 the direction of errors.

  Mean Squared Error (MSE)
     MSE =  (1/n) SUM (y_pred_i - y_i)**2  where SUM is from i=1 to i=n

     - MSE is generally larger compared to MAE, and it penalizes larger errors, making it useful 
       for optimization in ML algorithms.
     - weakness: is that it is less interpretable because we square the values.


  Root Mean Squared Error (RMSE)
     RMSE =  √[(1/n) SUM (y_pred_i - y_i)**2]  where SUM is from i=1 to i=n

    - represents a standard deviation of the residuals
    - in other words, how large the residuals are dispersed from the mean.
    - RMSE is recommended over MSE because they can be easily interpreted as they match the 
      units of the output.
    - weakness: sensitive to outliers.

  Mean Absolute Percentage Error (MAPE)
     MAPE =  (100%/n) SUM |(y_pred_i - y_i)/ y_i|  where SUM is from i=1 to i=n

     - a percentage instead of a positive number.
     - provides a relative measure of error making it useful for comparing the model performance.
     - limitation: its sensitivity to zero values because any data point with a zero value 
                   involves a division by zero operation.

------------------------------------------------------
6.4 Performing Online Model Evaluation 

 offline vs online model evaluation,
    offline evaluation
      - uses a static dataset to assess a model's performance
      - split the data into training and testing datasets.
    online evaluation
      - involves continuously assessing a model's performance using live data in a production environment
      - The model makes predictions on real-time data
      Advantages
        - provides real-time feedback, and immediate insights.
        - Any performance degradation can be quickly deducted.
      Drawbacks:
        - resource intensive
          - needs infrastructure for continuous data processing and monitoring.
        - If not properly monitored, the model can suffer from data drift, leading to degraded 
          performance.

  online model evaluation Metrics
    latency 
      - time taken for a model to generate predictions from the moment it starts receiving data
    throughput
      - defines the number of predictions the model can make per unit of time
    data drift 
      - similarity of the data on which the model was initially trained
      - a Data drift in the model indicates that the model needs retraining

   online business metrics
     click through-rate
       - a ratio of users who click on a specific link to the total number of users to whom 
         the recommendations are made.
     conversion rate 
       - percentage of successful outcomes from the total number of predictions of recommendations made

  A/B testing
    - implemented by deploying multiple versions of the model.
    - SageMaker allows you by running multiple production variance on an endpoint.

                                          Prod Variant 1
                                         |--80%------>
                                         |
                              SageMaker  |Prod Variant 2
       incoming request --->  endpoint   |--10%------>
                                         |
                                         |Prod Variant 3
                                         |--10%------>

      - Amazon SageMaker enables you to test multiple models or model versions behind the same 
        endpoint using production variants.
      - Each production variant identifies an ML model and the resources deployed for hosting the model
      distributing traffic by weights
      - need to specify the traffic percentage that will be sent to each model by specifying 
        the weight for each production variant.
      - SageMaker will distribute the traffic based on the specified weight.
      target variant option
         - set the target variant header in the incoming request to specifically target the model 
           of your choice and to send the traffic to the specific version of that model.

------------------------------------------------------
6.5 Comparing ML Models Using Production Parameters 

  computational complexity,
     - measures the computing resources that a particular algorithm needs during the modeling
    Metrics:
      Time Complexity
        - measure of time taken by an algorithm for a given input size
        - measured with respect to the input data size
        - can be very different during the training and testing phases.
        Big O
          - Big O notation is often used for calculating the time complexity of an algorithm
          - describes the time taken to complete a task in relation to the number of steps required 
            to complete it.
        Time Complexities for some algorithms: 
            n: number of training samples
            f: number of features
          linear regression 
            - may take a longer time during the training phases as compared to the testing phase.
            - training complexitity:   O(n * f**2  + f**3)
            - testing complexitity:    O(f)
          logistic regression 
            - training complexitity:   O(n * f)
            - testing complexitity:    O(f)

      Space Complexity 
        - amount of additional memory an algorithm needs to complete its execution.
        - measured with respect to the input size.
        - crucial metric in determining the efficiency of an algorithm.
          Space Complexities for some algorithms: 
            linear regression  ->  O(f) 
            logistic regression -> O(f)

      Sample Complexity
        - measures the number of training samples required to train an algorithm in order to gain 
          the desired level of performance, such as accuracy and generalizability.
        - A higher dimensionality dataset can increase the sample complexity even further
        - a dataset has high noise levels needs more samples to achieve the same performance level

      Parametricity 
        - refers to whether a model is parametric or non-parametric
        Parametric models
          - A parametric model has a fixed number of parameters.
          - A parametric model is usually faster to train and easier to interpret.
          - Examples: Linear regression and logistic regression 
        Non-Parametric models
          - the number of parameters can grow with the amount of training data
          - more flexible as they make fewer assumptions about the data distribution.
          - Examples: KNNs nearest neighbors and decision trees 

  Comparing the ML models.
    define the production environment.
      - clear picture of the available computation resources in production
      - measure the training and inference times
      - measure the memory consumed during training and inference stages
      - Evaluate the model's latency on throughput to make a prediction.
      - repeat the same steps for the second model that you would like to compare against.

------------------------------------------------------
6.6 Training Reports Utilized in SageMaker Debugger to Improve Your Models

Saved files:
    completed python jupyter notebook:
      eval.ipynb
    extracted python code from jupyter notebook:
      eval.py
    html view from completed jupyter notebook:
      eval.html
    SageMaker Debugger report and notebook
      profiler-report.html
      profiler-report.ipynb
    SageMaker Profiler report and notebook
      xgboost_report.html
      xgboost_report.ipynb


About this lab

Imagine you are the data engineer at your company, and your company has just selected AWS as the 
preferred cloud provider. You have been given a dataset to predict if an individual makes more than 
$50K in salary. As part of the modeling process, you have been asked to generate a summary of the model
training evaluation results, insights into the model performance, and interactive graphs. In this lab, 
you will fetch the census data and use that as the input dataset. Once the data is split, the data is 
uploaded to the S3 bucket. Then, the Sagemaker estimator is configured with the debugger hook and 
Sagemaker built-in rules to generate performance metric reports.

Learning objectives
  - Launch SageMaker Notebook
  - Install dependencies and import the libraries
  - Download the data and upload them to S3 bucket
  - Set up training and validation data
  - Configure and run the estimator
  - View the generated reports
                                                       Debugger
                                                       Hook         
                                                         |          |---> Training
                     |----> train      --> S3 ---|       V          |     Report
  Shap --> Census  --|                           |---> SageMaker ---|
  library  Dataset   |----> validation --> S3 ---|       ^          |
                                                         |          |----> Profile
                                                        Rules              Report

   Shap Census Dataset
     - Standard UCI income dataset with 48K observations with 14 features
     - split 80% / 20% for trainining/testin

------------------
Solution
Launch SageMaker Notebook

    To avoid issues with the lab, open a new Incognito or Private browser window to log in to the lab. This ensures that your personal account credentials, which may be active in your main window, are not used for the lab.
    Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region.
    In the search bar, type "SageMaker" to search for the SageMaker service. Click on the Amazon SageMaker result to go directly to the SageMaker service.
    Click on the Notebooks link under the Applications and IDEs section to view the notebook provided by the lab.
    Check if the notebook is marked as InService. If so, click the Open Jupyter link under Actions.
    Click on the eval.ipnyb file.

Install dependencies and import the libraries

Note: If you are prompted to select a kernel, please choose conda_tensorflow2_p310.

If this is your first time running a notebook, each cell contains Python commands you can run independently. A * inside the square braces indicates the code is running, and you will see a number once the execution is complete.

    Click the first cell that installs sagemaker, smdebug, numpy, and shap libraries. Use the Run button at the top to execute the code.

    The next cell imports the required Python libraries, initializes the sagemaker session and defines the output bucket. Click the cell and use the Run button to execute the code. This will import libraries from: Pandas, sklearn, and sagemaker.debugger. It may take a few minutes to complete the operation.

    The next cell fetches the IAM role using the get_execution_role function. Copy the following code snippet and paste into the empty code block and use the Run button to perform this operation.

    role = get_execution_role()

Download the data and upload them to the S3 bucket

    The next cell fetches the adult income dataset from the shap library and assigns them two variables X and y. Click Run to execute this cell.

    Now that the data is imported, it must be split for training and testing purposes. Copy the following code snippet and paste into the cell, and Run the cell to perform this operation.

    # Split into training and validation sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)

    The next cell uses pd.concat to concatenate the train and test data along the first axis and assign them to variables train_data and validation_data, respectively. Click the cell and click Run to execute this cell.

    The next cell converts the training and validation data to CSV format. Click the cell and click Run to execute this cell.

    Next, we will upload the training CSV file to the S3 bucket using the upload_file function. Click the cell and click Run to upload the CSV to the S3 bucket.

    Copy the following code snippet and paste into the cell, and Run the code to upload the validation.csv file.

    s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')

Set Up Training and Validation Data

    Click the next cell and click Run to execute. This initializes the path from which training data and validation data will be read. These values will be used to create the input parameter for the estimator object.
    Then click the next cell and click Run to execute. This uses the TrainingInput function to create the train_data input parameter. Please pay attention; we are passing the path from which the training data will be read.
    Click the next cell and click Run to create an input parameter for validation_data.

Fetch the algorithm and initialize estimator

    Copy the following code snippet and paste above the print function. Click Run to execute. This will fetch the 1.2.1 version of xgboost algorithm.

    container = retrieve('xgboost', boto3.Session().region_name, version='1.2-1')

    The next cell sets the values for the hyperparameters and defines our objective metric. Select it and click `Run' to execute.
    Click the next cell and click Run to initialize the profiler_config, one of the estimator object's input parameters.
    Click the next cell and click Run to create the Estimator object and pass profiler_config, Debugger_hook_config, and rules.
    Finally, copy the following code snippet and paste into the next cell, and click `Run' to initiate the training process.

    xgboost_estimator.fit({"train": train_data, "validation": validation_data})

Note: This will take a few minutes to complete. Once complete, you can locate and download the xgboost_report.html and profiler-report.html files from the S3 bucket, to review and obtain further information.
------------------

  profiler-report.html and profiler-report.ipynb
  -> at: 
      xgboost-iris-debugger-20250102-18-17-28-2025-01-02-18-17-46-238/ rule-output/ ProfilerReport/ profiler-output/

  xgboost_report.html and xgboost_report.ipynb
    -> at:
     xgboost-iris-debugger-20250102-18-17-28-2025-01-02-18-17-46-238/ rule-output/ CreateXgboostReport/

------------------------------------------------------
6.7 Utilizing Training Reports in SageMaker Debugger to Improve Your Models

  -> lab would not open







------------------------------------------------------
6.8 Evaluate ML Models Review 


                                               |---> Avoid overfitting or underfitting
                                               |
                                               |---> Evaluate metrics
                                               |
                                               |---> Interpret confusion matrix
    task statement 3.5 (evaluate ML Models) ---|
                                               |---> Perform offline and online model evaluation
                                               |
                                               |---> Compare models by using metrics
                                               |
                                               |---> Perform cross-validation


  evaluate metrics,

    Accuracy: (TP + TN)  / (TP + FP + TN + FN)
      - used for a balanced dataset because it is simple and effective,
      - the cost of different types of errors is similar.

    Precision: (TP)  / (TP + FP)
      - used for an imbalance dataset with a high cost of false positives
      - use for applications like fraud detection or spam filtering.

    Recall: (TP)  / (TP + FN)
      - used for an imbalance dataset with a high cost of false negatives
      - use for applications like disease detection or safety systems.

    F1 Score: (TP)  / [TP + ((FN + FP) / 2)]
      - used where you need to balance both false positives and false negatives
      - use for applications like text classification where both types of errors [precision and recall] are important.

    AUC (Area under the Curve) - ROC (receiver operator characteristic) curve,
      - plots a relation between true positive rate versus a false positive rate.
      - used in a binary classification task where you want to evaluate model's discrimination ability.
      ROC curve (receiver operating characteristic curve) 
        - is a graph showing the performance of a classification model at all classification thresholds. 
        - This curve plots two parameters:
            True Positive Rate (TPR)   on Y-axis
            False Positive Rate (FPR)  on X-axis

  regression metrics.

    Mean Absolute Error (MAE) (L1 Loss)
       MAE =  (1/n) SUM |y_pred_i - y_i|  where SUM is from i=1 to i=n

      - used when dealing with data that has outliers, and you don't want them to disproportionately affect the metric.

    Mean Squared Error (MSE)
      MSE =  (1/n) SUM (y_pred_i - y_i)**2  where SUM is from i=1 to i=n

      - means Square error can heavily penalize large errors, this metric is used when large errors are more 
        problematic than small errors.

    Root Mean Squared Error (RMSE)
      RMSE =  √[(1/n) SUM (y_pred_i - y_i)**2]  where SUM is from i=1 to i=n
      - retains the properties of MSE but in the same units as the target variable.
      - it is heavily used where larger errors are significantly more problematic, and you need a metric to reflect this.

    Mean Absolute Percentage Error (MAPE)
      MAPE =  (100%/n) SUM |(y_pred_i - y_i)/ y_i|  where SUM is from i=1 to i=n
      - expresses errors as percentage, so it is ideal in a situation where you need to express any metric in a percentage format.

  confusion matrices.
    - A confusion matrix is not a metric of its own, and it forms the basis of multiple other performance metrics.
    True positive 
      - the number of instances where the model correctly predicted the positive class.
    True negative 
      - the  number of instances where the model correctly predicted the negative class.
    false positive (Type I error) 
      - the number of instances where the model incorrectly predicted the positive class.
    False negatives (Type II error)
      - the number of instances the model incorrectly predicted the negative class.

  perform online and offline model evaluation.
    offline evaluation
      - uses a static dataset to assess a model's performance
      - split the data into training and testing datasets.
    Online model evaluation 
      - involves continuously assessing a model's performance using live data in a production environment.
      - The model makes predictions on real-time data.  
      Online performance metrics 
        latency
          - time taken for a model to generate predictions from the moment it starts receiving data
        throughput
          - defines the number of predictions the model can make per unit of time
        data drift
          - similarity of the data on which the model was initially trained
          - a Data drift in the model indicates that the model needs retraining

      business metrics 
        click through-rate
          - a ratio of users who click on a specific link to the total number of users to whom 
            the recommendations are made.
        conversion rate 
          - percentage of successful outcomes from the total number of predictions of recommendations made

      A/B testing 
        - used to evaluate multiple versions of the model by running them parallelly and distributing the traffic among them.

   compare ML models by using metrics.
     computational complexity 
       - another metric used to evaluate an ML model.
     commonly used computational complexity metrics
       time complexity
         - a measure of time taken by an algorithm for a given input size.
       Space complexity 
         - the amount of additional memory and algorithm needs to complete its execution.
       Sample complexity 
         - measures the number of training samples required to train an algorithm in order to gain the desired 
           level of performance.
       Parametricity 
         - refers to whether a model has fixed or a dynamic number of parameters.

------------------------------------------------------
6.9 Evaluate ML Models - Quiz

-----------------
Question 1

Your manager asks you to test the new version of the machine learning (ML) model by diverting 5% of the live 
traffic to it. What strategy will you use to accomplish this?

choices:
  Use A/B testing and split the input data set as training and testing data in the ratio of 95% and 5%.

  Use A/B testing and split the input data set as training and testing data in the ratio of 5% and 95%.

  Use A/B testing and create a production variant that accepts 5% of the incoming traffic.        <--- Correct Answer

  Use A/B testing and create a production variant that accepts 95% of the incoming traffic.

Good work!

  Using the A/B testing strategy, we can divert 5% of the traffic to the version we want to test.

------
-----------------

Question 2

You are working on a linear regression problem. Upon analyzing the input dataset, you find that the data has many outliers. 
Which is the preferred metric to use in this scenario?

choices:

  Root mean squared error

  Mean squared error

  Mean absolute error        <--- Correct Answer

  Mean absolute percentage error

Good work!

  Mean absolute error is the perfect choice while dealing with a dataset that has outliers.

------
-----------------
Question 3

You are currently working on a binary classification task and you need to find the number of instances where the model 
incorrectly predicted the negative class. Which outcome of the confusion matrix will provide this information?

choices:

  False negatives        <--- Correct Answer

  False positives

  True negatives

  True positives

Good work!

  False negatives indicate the number of instances where the model incorrectly predicted the negative class.


------
-----------------
Question 4

You work in the financial industry and were asked to develop a model that will detect if a credit card transaction 
is valid or not. Which metric will you use to project the model output?

choices:

  Accuracy

  F1score

  AUC curve

  Precision        <--- Correct Answer

Good work!

  Since this is a perfect example of an imbalanced dataset, Precision is the best choice.

------
-----------------
Question 5

You are currently working on a regression problem and the input dataset contains many zero values. Which metric is 
not a good fit in this scenario?

choices:

  Mean absolute error

  Mean absolute percentage error        <--- Correct Answer

  Mean absolute error

  Root mean square error

Good work!

  Mean absolute percentage error is sensitive to zero values, as it involves the division operation.

------
-----------------
Question 6

You are working on training a model and would like to measure the computational complexity of the model in terms 
of the number of samples it will take to train it. Which metric will you use in this scenario?

choices:

  Data complexity

  Sample complexity        <--- Correct Answer

  Space complexity

  Time complexity

Good work!

  Sample complexity measures the number of samples it takes to train a model.

------
-----------------

Chapter 7  Conclusion 

------------------------------------------------------
------------------------------------------------------
7.1 Course Summary


  Domain 3: Modeling

                 |---> frame a business problems as ML problems 
                 |
                 |---> selecting the appropriate models for a given ML problem, 
     modeling ---|
                 |---> training ML models
                 |
                 |---> performing hyper parameter optimization
                 |
                 |---> evaluating ML models

  Next Step
    - practice hands-on exercises
    - attempt the labs that are port of this course

  other resources
    - get AWS account 
    Machine Learning
      - ACG Introduction to Machine Learining course
    Jupyter Notebooks,
     - ACG Introduction to Jupyter Notebooks
    SageMaker Developer's Guide
      https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://gmoein.github.io/files/Amazon%2520SageMaker.pdf&ved=2ahUKEwjx5KP379eKAxXEMjQIHdwxMsYQFnoECCYQAQ&usg=AOvVaw0n1Yw8Z0tBqzA6bBRAuoNO

       -> Downloaded to: Amazon SageMaker_developer_guide.pdf

------------------------------------------------------
------------------------------------------------------
------------------------------------------------------

